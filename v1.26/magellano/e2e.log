I1117 13:40:49.205178      23 e2e.go:126] Starting e2e run "8bfdd49f-be19-486b-977c-05f3d6ba48c3" on Ginkgo node 1
Nov 17 13:40:49.231: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1700228449 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 17 13:40:49.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 13:40:49.426: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 17 13:40:49.450: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 17 13:40:49.494: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 17 13:40:49.494: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Nov 17 13:40:49.494: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 17 13:40:49.502: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Nov 17 13:40:49.503: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 17 13:40:49.503: INFO: e2e test version: v1.26.11
Nov 17 13:40:49.504: INFO: kube-apiserver version: v1.26.11
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Nov 17 13:40:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 13:40:49.508: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.084 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 17 13:40:49.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 13:40:49.426: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Nov 17 13:40:49.450: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Nov 17 13:40:49.494: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Nov 17 13:40:49.494: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Nov 17 13:40:49.494: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Nov 17 13:40:49.502: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Nov 17 13:40:49.503: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Nov 17 13:40:49.503: INFO: e2e test version: v1.26.11
    Nov 17 13:40:49.504: INFO: kube-apiserver version: v1.26.11
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Nov 17 13:40:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 13:40:49.508: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:40:49.543
Nov 17 13:40:49.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir-wrapper 11/17/23 13:40:49.545
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:40:49.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:40:49.569
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Nov 17 13:40:49.619: INFO: Waiting up to 5m0s for pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb" in namespace "emptydir-wrapper-8026" to be "running and ready"
Nov 17 13:40:49.628: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.49008ms
Nov 17 13:40:49.628: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:40:51.632: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012131301s
Nov 17 13:40:51.632: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:40:53.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013995343s
Nov 17 13:40:53.634: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:40:55.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Running", Reason="", readiness=true. Elapsed: 6.014344936s
Nov 17 13:40:55.634: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Running (Ready = true)
Nov 17 13:40:55.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb" satisfied condition "running and ready"
STEP: Cleaning up the secret 11/17/23 13:40:55.637
STEP: Cleaning up the configmap 11/17/23 13:40:55.644
STEP: Cleaning up the pod 11/17/23 13:40:55.649
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 13:40:55.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8026" for this suite. 11/17/23 13:40:55.679
------------------------------
â€¢ [SLOW TEST] [6.171 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:40:49.543
    Nov 17 13:40:49.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir-wrapper 11/17/23 13:40:49.545
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:40:49.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:40:49.569
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Nov 17 13:40:49.619: INFO: Waiting up to 5m0s for pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb" in namespace "emptydir-wrapper-8026" to be "running and ready"
    Nov 17 13:40:49.628: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.49008ms
    Nov 17 13:40:49.628: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:40:51.632: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012131301s
    Nov 17 13:40:51.632: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:40:53.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013995343s
    Nov 17 13:40:53.634: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:40:55.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb": Phase="Running", Reason="", readiness=true. Elapsed: 6.014344936s
    Nov 17 13:40:55.634: INFO: The phase of Pod pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb is Running (Ready = true)
    Nov 17 13:40:55.634: INFO: Pod "pod-secrets-afe26aa3-8616-4d71-a34b-cde1f1b3eebb" satisfied condition "running and ready"
    STEP: Cleaning up the secret 11/17/23 13:40:55.637
    STEP: Cleaning up the configmap 11/17/23 13:40:55.644
    STEP: Cleaning up the pod 11/17/23 13:40:55.649
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:40:55.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8026" for this suite. 11/17/23 13:40:55.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:40:55.714
Nov 17 13:40:55.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 13:40:55.716
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:40:55.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:40:55.75
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Nov 17 13:40:55.799: INFO: Create a RollingUpdate DaemonSet
Nov 17 13:40:55.807: INFO: Check that daemon pods launch on every node of the cluster
Nov 17 13:40:55.812: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:40:55.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:40:55.816: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:40:56.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:40:56.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:40:56.824: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:40:57.821: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:40:57.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:40:57.825: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:40:58.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:40:58.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:40:58.824: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:40:59.840: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:40:59.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:40:59.863: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:41:00.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:41:00.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 13:41:00.825: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Nov 17 13:41:00.825: INFO: Update the DaemonSet to trigger a rollout
Nov 17 13:41:00.833: INFO: Updating DaemonSet daemon-set
Nov 17 13:41:03.853: INFO: Roll back the DaemonSet before rollout is complete
Nov 17 13:41:03.867: INFO: Updating DaemonSet daemon-set
Nov 17 13:41:03.867: INFO: Make sure DaemonSet rollback is complete
Nov 17 13:41:03.870: INFO: Wrong image for pod: daemon-set-l64k4. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Nov 17 13:41:03.870: INFO: Pod daemon-set-l64k4 is not available
Nov 17 13:41:03.877: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:41:04.886: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:41:05.886: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:41:06.881: INFO: Pod daemon-set-9k777 is not available
Nov 17 13:41:06.884: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 13:41:06.891
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-891, will wait for the garbage collector to delete the pods 11/17/23 13:41:06.891
Nov 17 13:41:06.951: INFO: Deleting DaemonSet.extensions daemon-set took: 5.862ms
Nov 17 13:41:07.052: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.296708ms
Nov 17 13:41:08.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:41:08.856: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 13:41:08.862: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13367"},"items":null}

Nov 17 13:41:08.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13367"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:41:08.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-891" for this suite. 11/17/23 13:41:08.881
------------------------------
â€¢ [SLOW TEST] [13.173 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:40:55.714
    Nov 17 13:40:55.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 13:40:55.716
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:40:55.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:40:55.75
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Nov 17 13:40:55.799: INFO: Create a RollingUpdate DaemonSet
    Nov 17 13:40:55.807: INFO: Check that daemon pods launch on every node of the cluster
    Nov 17 13:40:55.812: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:40:55.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:40:55.816: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:40:56.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:40:56.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:40:56.824: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:40:57.821: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:40:57.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:40:57.825: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:40:58.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:40:58.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:40:58.824: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:40:59.840: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:40:59.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:40:59.863: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:41:00.820: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:41:00.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 13:41:00.825: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Nov 17 13:41:00.825: INFO: Update the DaemonSet to trigger a rollout
    Nov 17 13:41:00.833: INFO: Updating DaemonSet daemon-set
    Nov 17 13:41:03.853: INFO: Roll back the DaemonSet before rollout is complete
    Nov 17 13:41:03.867: INFO: Updating DaemonSet daemon-set
    Nov 17 13:41:03.867: INFO: Make sure DaemonSet rollback is complete
    Nov 17 13:41:03.870: INFO: Wrong image for pod: daemon-set-l64k4. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Nov 17 13:41:03.870: INFO: Pod daemon-set-l64k4 is not available
    Nov 17 13:41:03.877: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:41:04.886: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:41:05.886: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:41:06.881: INFO: Pod daemon-set-9k777 is not available
    Nov 17 13:41:06.884: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 13:41:06.891
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-891, will wait for the garbage collector to delete the pods 11/17/23 13:41:06.891
    Nov 17 13:41:06.951: INFO: Deleting DaemonSet.extensions daemon-set took: 5.862ms
    Nov 17 13:41:07.052: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.296708ms
    Nov 17 13:41:08.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:41:08.856: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 13:41:08.862: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13367"},"items":null}

    Nov 17 13:41:08.865: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13367"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:41:08.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-891" for this suite. 11/17/23 13:41:08.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:41:08.889
Nov 17 13:41:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 13:41:08.89
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:08.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:08.912
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 11/17/23 13:41:08.915
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:08.92
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:08.92
STEP: creating a pod to probe DNS 11/17/23 13:41:08.92
STEP: submitting the pod to kubernetes 11/17/23 13:41:08.92
Nov 17 13:41:08.930: INFO: Waiting up to 15m0s for pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b" in namespace "dns-276" to be "running"
Nov 17 13:41:08.933: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670524ms
Nov 17 13:41:10.939: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008500767s
Nov 17 13:41:12.938: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007946137s
Nov 17 13:41:14.938: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007433858s
Nov 17 13:41:16.937: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Running", Reason="", readiness=true. Elapsed: 8.006690334s
Nov 17 13:41:16.937: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b" satisfied condition "running"
STEP: retrieving the pod 11/17/23 13:41:16.937
STEP: looking for the results for each expected name from probers 11/17/23 13:41:16.941
Nov 17 13:41:16.952: INFO: DNS probes using dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b succeeded

STEP: deleting the pod 11/17/23 13:41:16.952
STEP: changing the externalName to bar.example.com 11/17/23 13:41:16.964
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:16.974
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:16.974
STEP: creating a second pod to probe DNS 11/17/23 13:41:16.974
STEP: submitting the pod to kubernetes 11/17/23 13:41:16.974
Nov 17 13:41:16.981: INFO: Waiting up to 15m0s for pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e" in namespace "dns-276" to be "running"
Nov 17 13:41:16.986: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925823ms
Nov 17 13:41:18.991: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009044994s
Nov 17 13:41:18.991: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e" satisfied condition "running"
STEP: retrieving the pod 11/17/23 13:41:18.991
STEP: looking for the results for each expected name from probers 11/17/23 13:41:18.993
Nov 17 13:41:18.999: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:19.005: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:19.005: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

Nov 17 13:41:24.011: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:24.021: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:24.021: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

Nov 17 13:41:29.017: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:29.021: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:29.022: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

Nov 17 13:41:34.020: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:34.024: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:34.025: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

Nov 17 13:41:39.011: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:39.016: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 17 13:41:39.016: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

Nov 17 13:41:44.017: INFO: DNS probes using dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e succeeded

STEP: deleting the pod 11/17/23 13:41:44.017
STEP: changing the service to type=ClusterIP 11/17/23 13:41:44.041
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:44.096
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
 11/17/23 13:41:44.097
STEP: creating a third pod to probe DNS 11/17/23 13:41:44.097
STEP: submitting the pod to kubernetes 11/17/23 13:41:44.103
Nov 17 13:41:44.150: INFO: Waiting up to 15m0s for pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db" in namespace "dns-276" to be "running"
Nov 17 13:41:44.166: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db": Phase="Pending", Reason="", readiness=false. Elapsed: 15.700656ms
Nov 17 13:41:46.171: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db": Phase="Running", Reason="", readiness=true. Elapsed: 2.020234606s
Nov 17 13:41:46.171: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db" satisfied condition "running"
STEP: retrieving the pod 11/17/23 13:41:46.171
STEP: looking for the results for each expected name from probers 11/17/23 13:41:46.175
Nov 17 13:41:46.184: INFO: DNS probes using dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db succeeded

STEP: deleting the pod 11/17/23 13:41:46.184
STEP: deleting the test externalName service 11/17/23 13:41:46.202
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 13:41:46.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-276" for this suite. 11/17/23 13:41:46.252
------------------------------
â€¢ [SLOW TEST] [37.376 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:41:08.889
    Nov 17 13:41:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 13:41:08.89
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:08.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:08.912
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 11/17/23 13:41:08.915
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:08.92
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:08.92
    STEP: creating a pod to probe DNS 11/17/23 13:41:08.92
    STEP: submitting the pod to kubernetes 11/17/23 13:41:08.92
    Nov 17 13:41:08.930: INFO: Waiting up to 15m0s for pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b" in namespace "dns-276" to be "running"
    Nov 17 13:41:08.933: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670524ms
    Nov 17 13:41:10.939: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008500767s
    Nov 17 13:41:12.938: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007946137s
    Nov 17 13:41:14.938: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007433858s
    Nov 17 13:41:16.937: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b": Phase="Running", Reason="", readiness=true. Elapsed: 8.006690334s
    Nov 17 13:41:16.937: INFO: Pod "dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 13:41:16.937
    STEP: looking for the results for each expected name from probers 11/17/23 13:41:16.941
    Nov 17 13:41:16.952: INFO: DNS probes using dns-test-0612f807-2960-41c9-9a1c-00a15ac08c6b succeeded

    STEP: deleting the pod 11/17/23 13:41:16.952
    STEP: changing the externalName to bar.example.com 11/17/23 13:41:16.964
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:16.974
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:16.974
    STEP: creating a second pod to probe DNS 11/17/23 13:41:16.974
    STEP: submitting the pod to kubernetes 11/17/23 13:41:16.974
    Nov 17 13:41:16.981: INFO: Waiting up to 15m0s for pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e" in namespace "dns-276" to be "running"
    Nov 17 13:41:16.986: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925823ms
    Nov 17 13:41:18.991: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009044994s
    Nov 17 13:41:18.991: INFO: Pod "dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 13:41:18.991
    STEP: looking for the results for each expected name from probers 11/17/23 13:41:18.993
    Nov 17 13:41:18.999: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:19.005: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:19.005: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

    Nov 17 13:41:24.011: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:24.021: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:24.021: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

    Nov 17 13:41:29.017: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:29.021: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:29.022: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

    Nov 17 13:41:34.020: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:34.024: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:34.025: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

    Nov 17 13:41:39.011: INFO: File wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:39.016: INFO: File jessie_udp@dns-test-service-3.dns-276.svc.cluster.local from pod  dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Nov 17 13:41:39.016: INFO: Lookups using dns-276/dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e failed for: [wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local jessie_udp@dns-test-service-3.dns-276.svc.cluster.local]

    Nov 17 13:41:44.017: INFO: DNS probes using dns-test-13e7ef5f-1356-41c8-ab7f-46fcfd23566e succeeded

    STEP: deleting the pod 11/17/23 13:41:44.017
    STEP: changing the service to type=ClusterIP 11/17/23 13:41:44.041
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:44.096
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-276.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-276.svc.cluster.local; sleep 1; done
     11/17/23 13:41:44.097
    STEP: creating a third pod to probe DNS 11/17/23 13:41:44.097
    STEP: submitting the pod to kubernetes 11/17/23 13:41:44.103
    Nov 17 13:41:44.150: INFO: Waiting up to 15m0s for pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db" in namespace "dns-276" to be "running"
    Nov 17 13:41:44.166: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db": Phase="Pending", Reason="", readiness=false. Elapsed: 15.700656ms
    Nov 17 13:41:46.171: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db": Phase="Running", Reason="", readiness=true. Elapsed: 2.020234606s
    Nov 17 13:41:46.171: INFO: Pod "dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 13:41:46.171
    STEP: looking for the results for each expected name from probers 11/17/23 13:41:46.175
    Nov 17 13:41:46.184: INFO: DNS probes using dns-test-7d6663b3-c327-492c-96fd-cb3b22fca4db succeeded

    STEP: deleting the pod 11/17/23 13:41:46.184
    STEP: deleting the test externalName service 11/17/23 13:41:46.202
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:41:46.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-276" for this suite. 11/17/23 13:41:46.252
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:41:46.266
Nov 17 13:41:46.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename tables 11/17/23 13:41:46.268
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:46.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:46.291
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Nov 17 13:41:46.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-1738" for this suite. 11/17/23 13:41:46.307
------------------------------
â€¢ [0.048 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:41:46.266
    Nov 17 13:41:46.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename tables 11/17/23 13:41:46.268
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:46.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:46.291
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:41:46.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-1738" for this suite. 11/17/23 13:41:46.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:41:46.318
Nov 17 13:41:46.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir-wrapper 11/17/23 13:41:46.319
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:46.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:46.344
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 11/17/23 13:41:46.348
STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:41:46.638
Nov 17 13:41:46.675: INFO: Pod name wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c: Found 0 pods out of 5
Nov 17 13:41:51.682: INFO: Pod name wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/17/23 13:41:51.682
Nov 17 13:41:51.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:41:51.686: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764526ms
Nov 17 13:41:53.691: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008589311s
Nov 17 13:41:55.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009456191s
Nov 17 13:41:57.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010296326s
Nov 17 13:41:59.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010178051s
Nov 17 13:42:01.693: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Running", Reason="", readiness=true. Elapsed: 10.010818496s
Nov 17 13:42:01.693: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm" satisfied condition "running"
Nov 17 13:42:01.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:01.697: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd": Phase="Running", Reason="", readiness=true. Elapsed: 4.150853ms
Nov 17 13:42:01.697: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd" satisfied condition "running"
Nov 17 13:42:01.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:01.701: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r": Phase="Running", Reason="", readiness=true. Elapsed: 3.807831ms
Nov 17 13:42:01.701: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r" satisfied condition "running"
Nov 17 13:42:01.701: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:01.705: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll": Phase="Running", Reason="", readiness=true. Elapsed: 3.458369ms
Nov 17 13:42:01.705: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll" satisfied condition "running"
Nov 17 13:42:01.705: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:01.709: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd": Phase="Running", Reason="", readiness=true. Elapsed: 4.073113ms
Nov 17 13:42:01.709: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:01.709
Nov 17 13:42:01.772: INFO: Deleting ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c took: 7.512081ms
Nov 17 13:42:01.873: INFO: Terminating ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c pods took: 101.21378ms
STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:42:05.7
Nov 17 13:42:05.746: INFO: Pod name wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173: Found 0 pods out of 5
Nov 17 13:42:10.754: INFO: Pod name wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/17/23 13:42:10.754
Nov 17 13:42:10.754: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:10.758: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44931ms
Nov 17 13:42:12.766: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011675278s
Nov 17 13:42:14.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010218672s
Nov 17 13:42:16.765: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01064903s
Nov 17 13:42:18.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009701202s
Nov 17 13:42:20.765: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010695652s
Nov 17 13:42:22.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Running", Reason="", readiness=true. Elapsed: 12.009623516s
Nov 17 13:42:22.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb" satisfied condition "running"
Nov 17 13:42:22.764: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:22.767: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm": Phase="Running", Reason="", readiness=true. Elapsed: 3.739657ms
Nov 17 13:42:22.767: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm" satisfied condition "running"
Nov 17 13:42:22.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:22.771: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg": Phase="Running", Reason="", readiness=true. Elapsed: 3.435513ms
Nov 17 13:42:22.771: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg" satisfied condition "running"
Nov 17 13:42:22.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:22.775: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr": Phase="Running", Reason="", readiness=true. Elapsed: 3.650897ms
Nov 17 13:42:22.775: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr" satisfied condition "running"
Nov 17 13:42:22.775: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:22.778: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk": Phase="Running", Reason="", readiness=true. Elapsed: 3.25038ms
Nov 17 13:42:22.778: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:22.778
Nov 17 13:42:22.839: INFO: Deleting ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 took: 6.802836ms
Nov 17 13:42:22.939: INFO: Terminating ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 pods took: 100.33023ms
STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:42:26.044
Nov 17 13:42:26.062: INFO: Pod name wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c: Found 0 pods out of 5
Nov 17 13:42:31.069: INFO: Pod name wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c: Found 5 pods out of 5
STEP: Ensuring each pod is running 11/17/23 13:42:31.069
Nov 17 13:42:31.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:31.072: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953856ms
Nov 17 13:42:33.076: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007177969s
Nov 17 13:42:35.077: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007824826s
Nov 17 13:42:37.077: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008205052s
Nov 17 13:42:39.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009018194s
Nov 17 13:42:41.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Running", Reason="", readiness=true. Elapsed: 10.008757623s
Nov 17 13:42:41.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w" satisfied condition "running"
Nov 17 13:42:41.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:41.082: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq": Phase="Running", Reason="", readiness=true. Elapsed: 3.575673ms
Nov 17 13:42:41.082: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq" satisfied condition "running"
Nov 17 13:42:41.082: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:41.085: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk": Phase="Running", Reason="", readiness=true. Elapsed: 3.238606ms
Nov 17 13:42:41.085: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk" satisfied condition "running"
Nov 17 13:42:41.085: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:41.088: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8": Phase="Running", Reason="", readiness=true. Elapsed: 3.015348ms
Nov 17 13:42:41.088: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8" satisfied condition "running"
Nov 17 13:42:41.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm" in namespace "emptydir-wrapper-1949" to be "running"
Nov 17 13:42:41.091: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm": Phase="Running", Reason="", readiness=true. Elapsed: 3.127335ms
Nov 17 13:42:41.091: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:41.091
Nov 17 13:42:41.156: INFO: Deleting ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c took: 10.418843ms
Nov 17 13:42:41.256: INFO: Terminating ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c pods took: 100.628024ms
STEP: Cleaning up the configMaps 11/17/23 13:42:44.657
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 13:42:44.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1949" for this suite. 11/17/23 13:42:44.97
------------------------------
â€¢ [SLOW TEST] [58.659 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:41:46.318
    Nov 17 13:41:46.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir-wrapper 11/17/23 13:41:46.319
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:41:46.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:41:46.344
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 11/17/23 13:41:46.348
    STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:41:46.638
    Nov 17 13:41:46.675: INFO: Pod name wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c: Found 0 pods out of 5
    Nov 17 13:41:51.682: INFO: Pod name wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/17/23 13:41:51.682
    Nov 17 13:41:51.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:41:51.686: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764526ms
    Nov 17 13:41:53.691: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008589311s
    Nov 17 13:41:55.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009456191s
    Nov 17 13:41:57.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010296326s
    Nov 17 13:41:59.692: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010178051s
    Nov 17 13:42:01.693: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm": Phase="Running", Reason="", readiness=true. Elapsed: 10.010818496s
    Nov 17 13:42:01.693: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-799zm" satisfied condition "running"
    Nov 17 13:42:01.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:01.697: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd": Phase="Running", Reason="", readiness=true. Elapsed: 4.150853ms
    Nov 17 13:42:01.697: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-8kscd" satisfied condition "running"
    Nov 17 13:42:01.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:01.701: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r": Phase="Running", Reason="", readiness=true. Elapsed: 3.807831ms
    Nov 17 13:42:01.701: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-jd75r" satisfied condition "running"
    Nov 17 13:42:01.701: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:01.705: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll": Phase="Running", Reason="", readiness=true. Elapsed: 3.458369ms
    Nov 17 13:42:01.705: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-k62ll" satisfied condition "running"
    Nov 17 13:42:01.705: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:01.709: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd": Phase="Running", Reason="", readiness=true. Elapsed: 4.073113ms
    Nov 17 13:42:01.709: INFO: Pod "wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c-rsnsd" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:01.709
    Nov 17 13:42:01.772: INFO: Deleting ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c took: 7.512081ms
    Nov 17 13:42:01.873: INFO: Terminating ReplicationController wrapped-volume-race-e924f673-d352-4ed6-a412-7a761048041c pods took: 101.21378ms
    STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:42:05.7
    Nov 17 13:42:05.746: INFO: Pod name wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173: Found 0 pods out of 5
    Nov 17 13:42:10.754: INFO: Pod name wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/17/23 13:42:10.754
    Nov 17 13:42:10.754: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:10.758: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.44931ms
    Nov 17 13:42:12.766: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011675278s
    Nov 17 13:42:14.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010218672s
    Nov 17 13:42:16.765: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01064903s
    Nov 17 13:42:18.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009701202s
    Nov 17 13:42:20.765: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.010695652s
    Nov 17 13:42:22.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb": Phase="Running", Reason="", readiness=true. Elapsed: 12.009623516s
    Nov 17 13:42:22.764: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-524nb" satisfied condition "running"
    Nov 17 13:42:22.764: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:22.767: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm": Phase="Running", Reason="", readiness=true. Elapsed: 3.739657ms
    Nov 17 13:42:22.767: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-59vrm" satisfied condition "running"
    Nov 17 13:42:22.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:22.771: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg": Phase="Running", Reason="", readiness=true. Elapsed: 3.435513ms
    Nov 17 13:42:22.771: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-8m2bg" satisfied condition "running"
    Nov 17 13:42:22.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:22.775: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr": Phase="Running", Reason="", readiness=true. Elapsed: 3.650897ms
    Nov 17 13:42:22.775: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-9wjqr" satisfied condition "running"
    Nov 17 13:42:22.775: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:22.778: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk": Phase="Running", Reason="", readiness=true. Elapsed: 3.25038ms
    Nov 17 13:42:22.778: INFO: Pod "wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173-cbhhk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:22.778
    Nov 17 13:42:22.839: INFO: Deleting ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 took: 6.802836ms
    Nov 17 13:42:22.939: INFO: Terminating ReplicationController wrapped-volume-race-f6384937-7d60-4b26-9f4b-a76622dc2173 pods took: 100.33023ms
    STEP: Creating RC which spawns configmap-volume pods 11/17/23 13:42:26.044
    Nov 17 13:42:26.062: INFO: Pod name wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c: Found 0 pods out of 5
    Nov 17 13:42:31.069: INFO: Pod name wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 11/17/23 13:42:31.069
    Nov 17 13:42:31.069: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:31.072: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.953856ms
    Nov 17 13:42:33.076: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007177969s
    Nov 17 13:42:35.077: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007824826s
    Nov 17 13:42:37.077: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008205052s
    Nov 17 13:42:39.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009018194s
    Nov 17 13:42:41.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w": Phase="Running", Reason="", readiness=true. Elapsed: 10.008757623s
    Nov 17 13:42:41.078: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-hq85w" satisfied condition "running"
    Nov 17 13:42:41.078: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:41.082: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq": Phase="Running", Reason="", readiness=true. Elapsed: 3.575673ms
    Nov 17 13:42:41.082: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-jfwmq" satisfied condition "running"
    Nov 17 13:42:41.082: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:41.085: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk": Phase="Running", Reason="", readiness=true. Elapsed: 3.238606ms
    Nov 17 13:42:41.085: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-qwmvk" satisfied condition "running"
    Nov 17 13:42:41.085: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:41.088: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8": Phase="Running", Reason="", readiness=true. Elapsed: 3.015348ms
    Nov 17 13:42:41.088: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xkln8" satisfied condition "running"
    Nov 17 13:42:41.088: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm" in namespace "emptydir-wrapper-1949" to be "running"
    Nov 17 13:42:41.091: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm": Phase="Running", Reason="", readiness=true. Elapsed: 3.127335ms
    Nov 17 13:42:41.091: INFO: Pod "wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c-xszqm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c in namespace emptydir-wrapper-1949, will wait for the garbage collector to delete the pods 11/17/23 13:42:41.091
    Nov 17 13:42:41.156: INFO: Deleting ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c took: 10.418843ms
    Nov 17 13:42:41.256: INFO: Terminating ReplicationController wrapped-volume-race-1f541ec7-b23b-44f5-a788-7ed2a317cb3c pods took: 100.628024ms
    STEP: Cleaning up the configMaps 11/17/23 13:42:44.657
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:42:44.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1949" for this suite. 11/17/23 13:42:44.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:42:44.981
Nov 17 13:42:44.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 13:42:44.982
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:44.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:45.003
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 13:42:45.058
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:42:45.671
STEP: Deploying the webhook pod 11/17/23 13:42:45.679
STEP: Wait for the deployment to be ready 11/17/23 13:42:45.692
Nov 17 13:42:45.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 13:42:47.718
STEP: Verifying the service has paired with the endpoint 11/17/23 13:42:47.734
Nov 17 13:42:48.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Nov 17 13:42:48.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1776-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 13:42:49.257
STEP: Creating a custom resource that should be mutated by the webhook 11/17/23 13:42:49.282
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:42:51.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9873" for this suite. 11/17/23 13:42:52.063
STEP: Destroying namespace "webhook-9873-markers" for this suite. 11/17/23 13:42:52.086
------------------------------
â€¢ [SLOW TEST] [7.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:42:44.981
    Nov 17 13:42:44.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 13:42:44.982
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:44.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:45.003
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 13:42:45.058
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:42:45.671
    STEP: Deploying the webhook pod 11/17/23 13:42:45.679
    STEP: Wait for the deployment to be ready 11/17/23 13:42:45.692
    Nov 17 13:42:45.707: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 13:42:47.718
    STEP: Verifying the service has paired with the endpoint 11/17/23 13:42:47.734
    Nov 17 13:42:48.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Nov 17 13:42:48.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1776-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 13:42:49.257
    STEP: Creating a custom resource that should be mutated by the webhook 11/17/23 13:42:49.282
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:42:51.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9873" for this suite. 11/17/23 13:42:52.063
    STEP: Destroying namespace "webhook-9873-markers" for this suite. 11/17/23 13:42:52.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:42:52.114
Nov 17 13:42:52.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename containers 11/17/23 13:42:52.115
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:52.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:52.161
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Nov 17 13:42:52.174: INFO: Waiting up to 5m0s for pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453" in namespace "containers-1942" to be "running"
Nov 17 13:42:52.178: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648132ms
Nov 17 13:42:54.253: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453": Phase="Running", Reason="", readiness=true. Elapsed: 2.079273431s
Nov 17 13:42:54.253: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 17 13:42:54.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1942" for this suite. 11/17/23 13:42:54.394
------------------------------
â€¢ [2.361 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:42:52.114
    Nov 17 13:42:52.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename containers 11/17/23 13:42:52.115
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:52.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:52.161
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Nov 17 13:42:52.174: INFO: Waiting up to 5m0s for pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453" in namespace "containers-1942" to be "running"
    Nov 17 13:42:52.178: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648132ms
    Nov 17 13:42:54.253: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453": Phase="Running", Reason="", readiness=true. Elapsed: 2.079273431s
    Nov 17 13:42:54.253: INFO: Pod "client-containers-5e859a23-bd90-48ab-a75f-b38a99f60453" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:42:54.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1942" for this suite. 11/17/23 13:42:54.394
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:42:54.475
Nov 17 13:42:54.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename csiinlinevolumes 11/17/23 13:42:54.477
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:54.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:54.768
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 11/17/23 13:42:54.779
STEP: getting 11/17/23 13:42:54.816
STEP: listing in namespace 11/17/23 13:42:54.831
STEP: patching 11/17/23 13:42:54.843
STEP: deleting 11/17/23 13:42:54.859
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 17 13:42:54.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2348" for this suite. 11/17/23 13:42:54.895
------------------------------
â€¢ [0.433 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:42:54.475
    Nov 17 13:42:54.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename csiinlinevolumes 11/17/23 13:42:54.477
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:54.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:54.768
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 11/17/23 13:42:54.779
    STEP: getting 11/17/23 13:42:54.816
    STEP: listing in namespace 11/17/23 13:42:54.831
    STEP: patching 11/17/23 13:42:54.843
    STEP: deleting 11/17/23 13:42:54.859
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:42:54.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2348" for this suite. 11/17/23 13:42:54.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:42:54.909
Nov 17 13:42:54.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 13:42:54.911
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:54.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:54.936
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 13:42:54.959
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:42:55.339
STEP: Deploying the webhook pod 11/17/23 13:42:55.376
STEP: Wait for the deployment to be ready 11/17/23 13:42:55.413
Nov 17 13:42:55.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 17 13:42:57.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/17/23 13:42:59.489
STEP: Verifying the service has paired with the endpoint 11/17/23 13:42:59.514
Nov 17 13:43:00.514: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/17/23 13:43:00.518
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/17/23 13:43:00.548
STEP: Creating a dummy validating-webhook-configuration object 11/17/23 13:43:00.568
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/17/23 13:43:00.581
STEP: Creating a dummy mutating-webhook-configuration object 11/17/23 13:43:00.589
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/17/23 13:43:00.604
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4525" for this suite. 11/17/23 13:43:00.737
STEP: Destroying namespace "webhook-4525-markers" for this suite. 11/17/23 13:43:00.759
------------------------------
â€¢ [SLOW TEST] [5.867 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:42:54.909
    Nov 17 13:42:54.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 13:42:54.911
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:42:54.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:42:54.936
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 13:42:54.959
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:42:55.339
    STEP: Deploying the webhook pod 11/17/23 13:42:55.376
    STEP: Wait for the deployment to be ready 11/17/23 13:42:55.413
    Nov 17 13:42:55.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 17 13:42:57.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 13, 42, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/17/23 13:42:59.489
    STEP: Verifying the service has paired with the endpoint 11/17/23 13:42:59.514
    Nov 17 13:43:00.514: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/17/23 13:43:00.518
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 11/17/23 13:43:00.548
    STEP: Creating a dummy validating-webhook-configuration object 11/17/23 13:43:00.568
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 11/17/23 13:43:00.581
    STEP: Creating a dummy mutating-webhook-configuration object 11/17/23 13:43:00.589
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 11/17/23 13:43:00.604
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4525" for this suite. 11/17/23 13:43:00.737
    STEP: Destroying namespace "webhook-4525-markers" for this suite. 11/17/23 13:43:00.759
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:00.777
Nov 17 13:43:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 13:43:00.778
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:00.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:00.827
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-2377 11/17/23 13:43:00.833
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[] 11/17/23 13:43:00.868
Nov 17 13:43:00.879: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 17 13:43:01.886: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2377 11/17/23 13:43:01.886
Nov 17 13:43:01.895: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2377" to be "running and ready"
Nov 17 13:43:01.918: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.207086ms
Nov 17 13:43:01.918: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:43:03.921: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026521762s
Nov 17 13:43:03.921: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 17 13:43:03.922: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod1:[100]] 11/17/23 13:43:03.924
Nov 17 13:43:03.934: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2377 11/17/23 13:43:03.934
Nov 17 13:43:03.940: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2377" to be "running and ready"
Nov 17 13:43:03.944: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589196ms
Nov 17 13:43:03.944: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:43:05.949: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008903132s
Nov 17 13:43:05.949: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:43:07.949: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009134068s
Nov 17 13:43:07.949: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:43:09.950: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.009706594s
Nov 17 13:43:09.950: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 17 13:43:09.950: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod1:[100] pod2:[101]] 11/17/23 13:43:09.953
Nov 17 13:43:09.965: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 11/17/23 13:43:09.965
Nov 17 13:43:09.965: INFO: Creating new exec pod
Nov 17 13:43:09.971: INFO: Waiting up to 5m0s for pod "execpod624tg" in namespace "services-2377" to be "running"
Nov 17 13:43:09.976: INFO: Pod "execpod624tg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.316925ms
Nov 17 13:43:11.980: INFO: Pod "execpod624tg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009336682s
Nov 17 13:43:11.980: INFO: Pod "execpod624tg" satisfied condition "running"
Nov 17 13:43:12.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Nov 17 13:43:13.314: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 17 13:43:13.314: INFO: stdout: ""
Nov 17 13:43:13.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 10.96.166.13 80'
Nov 17 13:43:13.564: INFO: stderr: "+ nc -v -z -w 2 10.96.166.13 80\nConnection to 10.96.166.13 80 port [tcp/http] succeeded!\n"
Nov 17 13:43:13.564: INFO: stdout: ""
Nov 17 13:43:13.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Nov 17 13:43:13.797: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 17 13:43:13.797: INFO: stdout: ""
Nov 17 13:43:13.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 10.96.166.13 81'
Nov 17 13:43:13.979: INFO: stderr: "+ nc -v -z -w 2 10.96.166.13 81\nConnection to 10.96.166.13 81 port [tcp/*] succeeded!\n"
Nov 17 13:43:13.979: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2377 11/17/23 13:43:13.979
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod2:[101]] 11/17/23 13:43:14.009
Nov 17 13:43:14.087: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2377 11/17/23 13:43:14.087
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[] 11/17/23 13:43:14.111
Nov 17 13:43:14.128: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:14.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2377" for this suite. 11/17/23 13:43:14.177
------------------------------
â€¢ [SLOW TEST] [13.410 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:00.777
    Nov 17 13:43:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 13:43:00.778
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:00.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:00.827
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-2377 11/17/23 13:43:00.833
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[] 11/17/23 13:43:00.868
    Nov 17 13:43:00.879: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Nov 17 13:43:01.886: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2377 11/17/23 13:43:01.886
    Nov 17 13:43:01.895: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2377" to be "running and ready"
    Nov 17 13:43:01.918: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.207086ms
    Nov 17 13:43:01.918: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:43:03.921: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026521762s
    Nov 17 13:43:03.921: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 17 13:43:03.922: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod1:[100]] 11/17/23 13:43:03.924
    Nov 17 13:43:03.934: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2377 11/17/23 13:43:03.934
    Nov 17 13:43:03.940: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2377" to be "running and ready"
    Nov 17 13:43:03.944: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589196ms
    Nov 17 13:43:03.944: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:43:05.949: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008903132s
    Nov 17 13:43:05.949: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:43:07.949: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009134068s
    Nov 17 13:43:07.949: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:43:09.950: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.009706594s
    Nov 17 13:43:09.950: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 17 13:43:09.950: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod1:[100] pod2:[101]] 11/17/23 13:43:09.953
    Nov 17 13:43:09.965: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 11/17/23 13:43:09.965
    Nov 17 13:43:09.965: INFO: Creating new exec pod
    Nov 17 13:43:09.971: INFO: Waiting up to 5m0s for pod "execpod624tg" in namespace "services-2377" to be "running"
    Nov 17 13:43:09.976: INFO: Pod "execpod624tg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.316925ms
    Nov 17 13:43:11.980: INFO: Pod "execpod624tg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009336682s
    Nov 17 13:43:11.980: INFO: Pod "execpod624tg" satisfied condition "running"
    Nov 17 13:43:12.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Nov 17 13:43:13.314: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Nov 17 13:43:13.314: INFO: stdout: ""
    Nov 17 13:43:13.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 10.96.166.13 80'
    Nov 17 13:43:13.564: INFO: stderr: "+ nc -v -z -w 2 10.96.166.13 80\nConnection to 10.96.166.13 80 port [tcp/http] succeeded!\n"
    Nov 17 13:43:13.564: INFO: stdout: ""
    Nov 17 13:43:13.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Nov 17 13:43:13.797: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Nov 17 13:43:13.797: INFO: stdout: ""
    Nov 17 13:43:13.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-2377 exec execpod624tg -- /bin/sh -x -c nc -v -z -w 2 10.96.166.13 81'
    Nov 17 13:43:13.979: INFO: stderr: "+ nc -v -z -w 2 10.96.166.13 81\nConnection to 10.96.166.13 81 port [tcp/*] succeeded!\n"
    Nov 17 13:43:13.979: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2377 11/17/23 13:43:13.979
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[pod2:[101]] 11/17/23 13:43:14.009
    Nov 17 13:43:14.087: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2377 11/17/23 13:43:14.087
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2377 to expose endpoints map[] 11/17/23 13:43:14.111
    Nov 17 13:43:14.128: INFO: successfully validated that service multi-endpoint-test in namespace services-2377 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:14.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2377" for this suite. 11/17/23 13:43:14.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:14.196
Nov 17 13:43:14.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 13:43:14.197
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:14.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:14.232
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 11/17/23 13:43:14.236
STEP: Creating a ResourceQuota 11/17/23 13:43:19.242
STEP: Ensuring resource quota status is calculated 11/17/23 13:43:19.26
STEP: Creating a Service 11/17/23 13:43:21.265
STEP: Creating a NodePort Service 11/17/23 13:43:21.287
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/17/23 13:43:21.318
STEP: Ensuring resource quota status captures service creation 11/17/23 13:43:21.35
STEP: Deleting Services 11/17/23 13:43:23.355
STEP: Ensuring resource quota status released usage 11/17/23 13:43:23.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:25.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4562" for this suite. 11/17/23 13:43:25.422
------------------------------
â€¢ [SLOW TEST] [11.232 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:14.196
    Nov 17 13:43:14.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 13:43:14.197
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:14.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:14.232
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 11/17/23 13:43:14.236
    STEP: Creating a ResourceQuota 11/17/23 13:43:19.242
    STEP: Ensuring resource quota status is calculated 11/17/23 13:43:19.26
    STEP: Creating a Service 11/17/23 13:43:21.265
    STEP: Creating a NodePort Service 11/17/23 13:43:21.287
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 11/17/23 13:43:21.318
    STEP: Ensuring resource quota status captures service creation 11/17/23 13:43:21.35
    STEP: Deleting Services 11/17/23 13:43:23.355
    STEP: Ensuring resource quota status released usage 11/17/23 13:43:23.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:25.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4562" for this suite. 11/17/23 13:43:25.422
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:25.428
Nov 17 13:43:25.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 13:43:25.429
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:25.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:25.459
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 11/17/23 13:43:25.462
Nov 17 13:43:25.472: INFO: Waiting up to 5m0s for pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23" in namespace "var-expansion-8686" to be "Succeeded or Failed"
Nov 17 13:43:25.477: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982977ms
Nov 17 13:43:27.481: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009532595s
Nov 17 13:43:29.480: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008933156s
STEP: Saw pod success 11/17/23 13:43:29.481
Nov 17 13:43:29.481: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23" satisfied condition "Succeeded or Failed"
Nov 17 13:43:29.484: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 container dapi-container: <nil>
STEP: delete the pod 11/17/23 13:43:29.492
Nov 17 13:43:29.504: INFO: Waiting for pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 to disappear
Nov 17 13:43:29.508: INFO: Pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:29.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8686" for this suite. 11/17/23 13:43:29.512
------------------------------
â€¢ [4.090 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:25.428
    Nov 17 13:43:25.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 13:43:25.429
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:25.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:25.459
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 11/17/23 13:43:25.462
    Nov 17 13:43:25.472: INFO: Waiting up to 5m0s for pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23" in namespace "var-expansion-8686" to be "Succeeded or Failed"
    Nov 17 13:43:25.477: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982977ms
    Nov 17 13:43:27.481: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009532595s
    Nov 17 13:43:29.480: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008933156s
    STEP: Saw pod success 11/17/23 13:43:29.481
    Nov 17 13:43:29.481: INFO: Pod "var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23" satisfied condition "Succeeded or Failed"
    Nov 17 13:43:29.484: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 13:43:29.492
    Nov 17 13:43:29.504: INFO: Waiting for pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 to disappear
    Nov 17 13:43:29.508: INFO: Pod var-expansion-56504dcf-7608-402f-ae43-a88df8dddf23 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:29.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8686" for this suite. 11/17/23 13:43:29.512
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:29.518
Nov 17 13:43:29.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 13:43:29.519
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:29.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:29.545
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 11/17/23 13:43:29.548
Nov 17 13:43:29.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 17 13:43:29.645: INFO: stderr: ""
Nov 17 13:43:29.645: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 11/17/23 13:43:29.645
Nov 17 13:43:29.645: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 17 13:43:29.645: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8886" to be "running and ready, or succeeded"
Nov 17 13:43:29.650: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467235ms
Nov 17 13:43:29.650: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-worker-2.c.operations-lab.internal' to be 'Running' but was 'Pending'
Nov 17 13:43:31.654: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009032053s
Nov 17 13:43:31.654: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 17 13:43:31.654: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 11/17/23 13:43:31.654
Nov 17 13:43:31.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator'
Nov 17 13:43:31.750: INFO: stderr: ""
Nov 17 13:43:31.750: INFO: stdout: "I1117 13:43:30.504306       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/kxhx 334\nI1117 13:43:30.704767       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qwg 377\nI1117 13:43:30.905155       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/dpr2 443\nI1117 13:43:31.104583       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/8lcw 578\nI1117 13:43:31.305093       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cwc 484\nI1117 13:43:31.504458       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/zz9 398\nI1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
STEP: limiting log lines 11/17/23 13:43:31.75
Nov 17 13:43:31.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --tail=1'
Nov 17 13:43:31.846: INFO: stderr: ""
Nov 17 13:43:31.846: INFO: stdout: "I1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
Nov 17 13:43:31.846: INFO: got output "I1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
STEP: limiting log bytes 11/17/23 13:43:31.846
Nov 17 13:43:31.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --limit-bytes=1'
Nov 17 13:43:31.953: INFO: stderr: ""
Nov 17 13:43:31.953: INFO: stdout: "I"
Nov 17 13:43:31.953: INFO: got output "I"
STEP: exposing timestamps 11/17/23 13:43:31.953
Nov 17 13:43:31.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 17 13:43:32.052: INFO: stderr: ""
Nov 17 13:43:32.052: INFO: stdout: "2023-11-17T13:43:31.905043178Z I1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\n"
Nov 17 13:43:32.052: INFO: got output "2023-11-17T13:43:31.905043178Z I1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\n"
STEP: restricting to a time range 11/17/23 13:43:32.052
Nov 17 13:43:34.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --since=1s'
Nov 17 13:43:34.649: INFO: stderr: ""
Nov 17 13:43:34.649: INFO: stdout: "I1117 13:43:33.704899       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/8rqs 298\nI1117 13:43:33.904437       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zdn 244\nI1117 13:43:34.105141       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/dpx 558\nI1117 13:43:34.304791       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/wrn 484\nI1117 13:43:34.505355       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/v28 357\n"
Nov 17 13:43:34.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --since=24h'
Nov 17 13:43:34.744: INFO: stderr: ""
Nov 17 13:43:34.744: INFO: stdout: "I1117 13:43:30.504306       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/kxhx 334\nI1117 13:43:30.704767       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qwg 377\nI1117 13:43:30.905155       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/dpr2 443\nI1117 13:43:31.104583       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/8lcw 578\nI1117 13:43:31.305093       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cwc 484\nI1117 13:43:31.504458       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/zz9 398\nI1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\nI1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\nI1117 13:43:32.104626       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/drjg 266\nI1117 13:43:32.304785       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/fll 250\nI1117 13:43:32.505389       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI1117 13:43:32.704871       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/dn6n 359\nI1117 13:43:32.905179       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6ts 207\nI1117 13:43:33.104456       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/m8f4 229\nI1117 13:43:33.304916       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/xv66 253\nI1117 13:43:33.505351       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/v4xs 441\nI1117 13:43:33.704899       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/8rqs 298\nI1117 13:43:33.904437       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zdn 244\nI1117 13:43:34.105141       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/dpx 558\nI1117 13:43:34.304791       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/wrn 484\nI1117 13:43:34.505355       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/v28 357\nI1117 13:43:34.707669       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/grvx 494\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Nov 17 13:43:34.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 delete pod logs-generator'
Nov 17 13:43:36.104: INFO: stderr: ""
Nov 17 13:43:36.104: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:36.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8886" for this suite. 11/17/23 13:43:36.109
------------------------------
â€¢ [SLOW TEST] [6.597 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:29.518
    Nov 17 13:43:29.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 13:43:29.519
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:29.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:29.545
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 11/17/23 13:43:29.548
    Nov 17 13:43:29.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Nov 17 13:43:29.645: INFO: stderr: ""
    Nov 17 13:43:29.645: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 11/17/23 13:43:29.645
    Nov 17 13:43:29.645: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Nov 17 13:43:29.645: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8886" to be "running and ready, or succeeded"
    Nov 17 13:43:29.650: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467235ms
    Nov 17 13:43:29.650: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-worker-2.c.operations-lab.internal' to be 'Running' but was 'Pending'
    Nov 17 13:43:31.654: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009032053s
    Nov 17 13:43:31.654: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Nov 17 13:43:31.654: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 11/17/23 13:43:31.654
    Nov 17 13:43:31.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator'
    Nov 17 13:43:31.750: INFO: stderr: ""
    Nov 17 13:43:31.750: INFO: stdout: "I1117 13:43:30.504306       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/kxhx 334\nI1117 13:43:30.704767       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qwg 377\nI1117 13:43:30.905155       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/dpr2 443\nI1117 13:43:31.104583       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/8lcw 578\nI1117 13:43:31.305093       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cwc 484\nI1117 13:43:31.504458       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/zz9 398\nI1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
    STEP: limiting log lines 11/17/23 13:43:31.75
    Nov 17 13:43:31.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --tail=1'
    Nov 17 13:43:31.846: INFO: stderr: ""
    Nov 17 13:43:31.846: INFO: stdout: "I1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
    Nov 17 13:43:31.846: INFO: got output "I1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\n"
    STEP: limiting log bytes 11/17/23 13:43:31.846
    Nov 17 13:43:31.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --limit-bytes=1'
    Nov 17 13:43:31.953: INFO: stderr: ""
    Nov 17 13:43:31.953: INFO: stdout: "I"
    Nov 17 13:43:31.953: INFO: got output "I"
    STEP: exposing timestamps 11/17/23 13:43:31.953
    Nov 17 13:43:31.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --tail=1 --timestamps'
    Nov 17 13:43:32.052: INFO: stderr: ""
    Nov 17 13:43:32.052: INFO: stdout: "2023-11-17T13:43:31.905043178Z I1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\n"
    Nov 17 13:43:32.052: INFO: got output "2023-11-17T13:43:31.905043178Z I1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\n"
    STEP: restricting to a time range 11/17/23 13:43:32.052
    Nov 17 13:43:34.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --since=1s'
    Nov 17 13:43:34.649: INFO: stderr: ""
    Nov 17 13:43:34.649: INFO: stdout: "I1117 13:43:33.704899       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/8rqs 298\nI1117 13:43:33.904437       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zdn 244\nI1117 13:43:34.105141       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/dpx 558\nI1117 13:43:34.304791       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/wrn 484\nI1117 13:43:34.505355       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/v28 357\n"
    Nov 17 13:43:34.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 logs logs-generator logs-generator --since=24h'
    Nov 17 13:43:34.744: INFO: stderr: ""
    Nov 17 13:43:34.744: INFO: stdout: "I1117 13:43:30.504306       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/kxhx 334\nI1117 13:43:30.704767       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/qwg 377\nI1117 13:43:30.905155       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/dpr2 443\nI1117 13:43:31.104583       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/8lcw 578\nI1117 13:43:31.305093       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/cwc 484\nI1117 13:43:31.504458       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/zz9 398\nI1117 13:43:31.704427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/29v7 507\nI1117 13:43:31.904823       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5zdv 322\nI1117 13:43:32.104626       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/drjg 266\nI1117 13:43:32.304785       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/fll 250\nI1117 13:43:32.505389       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/kz2 467\nI1117 13:43:32.704871       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/dn6n 359\nI1117 13:43:32.905179       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/6ts 207\nI1117 13:43:33.104456       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/m8f4 229\nI1117 13:43:33.304916       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/xv66 253\nI1117 13:43:33.505351       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/v4xs 441\nI1117 13:43:33.704899       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/8rqs 298\nI1117 13:43:33.904437       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/zdn 244\nI1117 13:43:34.105141       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/dpx 558\nI1117 13:43:34.304791       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/wrn 484\nI1117 13:43:34.505355       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/v28 357\nI1117 13:43:34.707669       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/grvx 494\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Nov 17 13:43:34.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8886 delete pod logs-generator'
    Nov 17 13:43:36.104: INFO: stderr: ""
    Nov 17 13:43:36.104: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:36.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8886" for this suite. 11/17/23 13:43:36.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:36.117
Nov 17 13:43:36.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-runtime 11/17/23 13:43:36.118
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:36.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:36.14
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 11/17/23 13:43:36.143
STEP: wait for the container to reach Succeeded 11/17/23 13:43:36.151
STEP: get the container status 11/17/23 13:43:39.167
STEP: the container should be terminated 11/17/23 13:43:39.171
STEP: the termination message should be set 11/17/23 13:43:39.172
Nov 17 13:43:39.172: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 11/17/23 13:43:39.172
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4313" for this suite. 11/17/23 13:43:39.194
------------------------------
â€¢ [3.084 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:36.117
    Nov 17 13:43:36.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-runtime 11/17/23 13:43:36.118
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:36.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:36.14
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 11/17/23 13:43:36.143
    STEP: wait for the container to reach Succeeded 11/17/23 13:43:36.151
    STEP: get the container status 11/17/23 13:43:39.167
    STEP: the container should be terminated 11/17/23 13:43:39.171
    STEP: the termination message should be set 11/17/23 13:43:39.172
    Nov 17 13:43:39.172: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 11/17/23 13:43:39.172
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4313" for this suite. 11/17/23 13:43:39.194
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:39.201
Nov 17 13:43:39.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename watch 11/17/23 13:43:39.202
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:39.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:39.28
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 11/17/23 13:43:39.284
STEP: creating a watch on configmaps with label B 11/17/23 13:43:39.286
STEP: creating a watch on configmaps with label A or B 11/17/23 13:43:39.288
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.289
Nov 17 13:43:39.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15595 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:39.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15595 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.296
Nov 17 13:43:39.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15596 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:39.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15596 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/17/23 13:43:39.304
Nov 17 13:43:39.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15597 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:39.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15597 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.317
Nov 17 13:43:39.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15598 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:39.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15598 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/17/23 13:43:39.326
Nov 17 13:43:39.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15599 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:39.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15599 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/17/23 13:43:49.333
Nov 17 13:43:49.339: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15672 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:43:49.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15672 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:59.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1493" for this suite. 11/17/23 13:43:59.344
------------------------------
â€¢ [SLOW TEST] [20.149 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:39.201
    Nov 17 13:43:39.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename watch 11/17/23 13:43:39.202
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:39.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:39.28
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 11/17/23 13:43:39.284
    STEP: creating a watch on configmaps with label B 11/17/23 13:43:39.286
    STEP: creating a watch on configmaps with label A or B 11/17/23 13:43:39.288
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.289
    Nov 17 13:43:39.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15595 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:39.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15595 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.296
    Nov 17 13:43:39.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15596 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:39.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15596 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 11/17/23 13:43:39.304
    Nov 17 13:43:39.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15597 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:39.317: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15597 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 11/17/23 13:43:39.317
    Nov 17 13:43:39.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15598 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:39.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1493  6bddfeae-dc18-4c69-94ab-f8795a91da22 15598 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 11/17/23 13:43:39.326
    Nov 17 13:43:39.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15599 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:39.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15599 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 11/17/23 13:43:49.333
    Nov 17 13:43:49.339: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15672 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:43:49.340: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1493  bb3fc1bb-9837-4d92-b2ad-844f2bacd11e 15672 0 2023-11-17 13:43:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-11-17 13:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:59.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1493" for this suite. 11/17/23 13:43:59.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:59.353
Nov 17 13:43:59.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 13:43:59.354
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.381
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Nov 17 13:43:59.403: INFO: created pod pod-service-account-defaultsa
Nov 17 13:43:59.404: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 17 13:43:59.421: INFO: created pod pod-service-account-mountsa
Nov 17 13:43:59.421: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 17 13:43:59.430: INFO: created pod pod-service-account-nomountsa
Nov 17 13:43:59.430: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 17 13:43:59.443: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 17 13:43:59.443: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 17 13:43:59.473: INFO: created pod pod-service-account-mountsa-mountspec
Nov 17 13:43:59.473: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 17 13:43:59.491: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 17 13:43:59.491: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 17 13:43:59.507: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 17 13:43:59.507: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 17 13:43:59.525: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 17 13:43:59.525: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 17 13:43:59.535: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 17 13:43:59.535: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-247" for this suite. 11/17/23 13:43:59.547
------------------------------
â€¢ [0.225 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:59.353
    Nov 17 13:43:59.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 13:43:59.354
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.381
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Nov 17 13:43:59.403: INFO: created pod pod-service-account-defaultsa
    Nov 17 13:43:59.404: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Nov 17 13:43:59.421: INFO: created pod pod-service-account-mountsa
    Nov 17 13:43:59.421: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Nov 17 13:43:59.430: INFO: created pod pod-service-account-nomountsa
    Nov 17 13:43:59.430: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Nov 17 13:43:59.443: INFO: created pod pod-service-account-defaultsa-mountspec
    Nov 17 13:43:59.443: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Nov 17 13:43:59.473: INFO: created pod pod-service-account-mountsa-mountspec
    Nov 17 13:43:59.473: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Nov 17 13:43:59.491: INFO: created pod pod-service-account-nomountsa-mountspec
    Nov 17 13:43:59.491: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Nov 17 13:43:59.507: INFO: created pod pod-service-account-defaultsa-nomountspec
    Nov 17 13:43:59.507: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Nov 17 13:43:59.525: INFO: created pod pod-service-account-mountsa-nomountspec
    Nov 17 13:43:59.525: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Nov 17 13:43:59.535: INFO: created pod pod-service-account-nomountsa-nomountspec
    Nov 17 13:43:59.535: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-247" for this suite. 11/17/23 13:43:59.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:59.581
Nov 17 13:43:59.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename endpointslice 11/17/23 13:43:59.582
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.605
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 11/17/23 13:43:59.61
STEP: getting /apis/discovery.k8s.io 11/17/23 13:43:59.614
STEP: getting /apis/discovery.k8s.iov1 11/17/23 13:43:59.616
STEP: creating 11/17/23 13:43:59.618
STEP: getting 11/17/23 13:43:59.668
STEP: listing 11/17/23 13:43:59.671
STEP: watching 11/17/23 13:43:59.674
Nov 17 13:43:59.674: INFO: starting watch
STEP: cluster-wide listing 11/17/23 13:43:59.677
STEP: cluster-wide watching 11/17/23 13:43:59.681
Nov 17 13:43:59.681: INFO: starting watch
STEP: patching 11/17/23 13:43:59.685
STEP: updating 11/17/23 13:43:59.692
Nov 17 13:43:59.702: INFO: waiting for watch events with expected annotations
Nov 17 13:43:59.702: INFO: saw patched and updated annotations
STEP: deleting 11/17/23 13:43:59.702
STEP: deleting a collection 11/17/23 13:43:59.73
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 17 13:43:59.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5860" for this suite. 11/17/23 13:43:59.766
------------------------------
â€¢ [0.193 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:59.581
    Nov 17 13:43:59.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename endpointslice 11/17/23 13:43:59.582
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.605
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 11/17/23 13:43:59.61
    STEP: getting /apis/discovery.k8s.io 11/17/23 13:43:59.614
    STEP: getting /apis/discovery.k8s.iov1 11/17/23 13:43:59.616
    STEP: creating 11/17/23 13:43:59.618
    STEP: getting 11/17/23 13:43:59.668
    STEP: listing 11/17/23 13:43:59.671
    STEP: watching 11/17/23 13:43:59.674
    Nov 17 13:43:59.674: INFO: starting watch
    STEP: cluster-wide listing 11/17/23 13:43:59.677
    STEP: cluster-wide watching 11/17/23 13:43:59.681
    Nov 17 13:43:59.681: INFO: starting watch
    STEP: patching 11/17/23 13:43:59.685
    STEP: updating 11/17/23 13:43:59.692
    Nov 17 13:43:59.702: INFO: waiting for watch events with expected annotations
    Nov 17 13:43:59.702: INFO: saw patched and updated annotations
    STEP: deleting 11/17/23 13:43:59.702
    STEP: deleting a collection 11/17/23 13:43:59.73
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:43:59.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5860" for this suite. 11/17/23 13:43:59.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:43:59.777
Nov 17 13:43:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 13:43:59.779
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.808
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Nov 17 13:43:59.812: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/17/23 13:44:00.833
STEP: Checking rc "condition-test" has the desired failure condition set 11/17/23 13:44:00.84
STEP: Scaling down rc "condition-test" to satisfy pod quota 11/17/23 13:44:01.85
Nov 17 13:44:01.864: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 11/17/23 13:44:01.864
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:01.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3562" for this suite. 11/17/23 13:44:01.882
------------------------------
â€¢ [2.124 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:43:59.777
    Nov 17 13:43:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 13:43:59.779
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:43:59.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:43:59.808
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Nov 17 13:43:59.812: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 11/17/23 13:44:00.833
    STEP: Checking rc "condition-test" has the desired failure condition set 11/17/23 13:44:00.84
    STEP: Scaling down rc "condition-test" to satisfy pod quota 11/17/23 13:44:01.85
    Nov 17 13:44:01.864: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 11/17/23 13:44:01.864
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:01.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3562" for this suite. 11/17/23 13:44:01.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:01.902
Nov 17 13:44:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 13:44:01.904
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:01.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:01.935
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-5c3bde25-5271-4a5c-8d11-14b82a645694 11/17/23 13:44:01.939
STEP: Creating a pod to test consume secrets 11/17/23 13:44:01.946
Nov 17 13:44:01.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8" in namespace "projected-7942" to be "Succeeded or Failed"
Nov 17 13:44:01.983: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.356267ms
Nov 17 13:44:03.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020279116s
Nov 17 13:44:05.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020345517s
STEP: Saw pod success 11/17/23 13:44:05.987
Nov 17 13:44:05.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8" satisfied condition "Succeeded or Failed"
Nov 17 13:44:05.990: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/17/23 13:44:06.028
Nov 17 13:44:06.045: INFO: Waiting for pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 to disappear
Nov 17 13:44:06.049: INFO: Pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7942" for this suite. 11/17/23 13:44:06.055
------------------------------
â€¢ [4.163 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:01.902
    Nov 17 13:44:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 13:44:01.904
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:01.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:01.935
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-5c3bde25-5271-4a5c-8d11-14b82a645694 11/17/23 13:44:01.939
    STEP: Creating a pod to test consume secrets 11/17/23 13:44:01.946
    Nov 17 13:44:01.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8" in namespace "projected-7942" to be "Succeeded or Failed"
    Nov 17 13:44:01.983: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.356267ms
    Nov 17 13:44:03.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020279116s
    Nov 17 13:44:05.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020345517s
    STEP: Saw pod success 11/17/23 13:44:05.987
    Nov 17 13:44:05.987: INFO: Pod "pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:05.990: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 13:44:06.028
    Nov 17 13:44:06.045: INFO: Waiting for pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 to disappear
    Nov 17 13:44:06.049: INFO: Pod pod-projected-secrets-3b1927af-40e9-46e6-ad97-74f564f1f9e8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:06.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7942" for this suite. 11/17/23 13:44:06.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:06.067
Nov 17 13:44:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 13:44:06.068
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:06.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:06.102
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 11/17/23 13:44:06.108
Nov 17 13:44:06.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25" in namespace "projected-9179" to be "Succeeded or Failed"
Nov 17 13:44:06.124: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41445ms
Nov 17 13:44:08.129: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011627993s
Nov 17 13:44:10.130: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352284s
STEP: Saw pod success 11/17/23 13:44:10.13
Nov 17 13:44:10.130: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25" satisfied condition "Succeeded or Failed"
Nov 17 13:44:10.133: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 container client-container: <nil>
STEP: delete the pod 11/17/23 13:44:10.139
Nov 17 13:44:10.150: INFO: Waiting for pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 to disappear
Nov 17 13:44:10.152: INFO: Pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:10.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9179" for this suite. 11/17/23 13:44:10.156
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:06.067
    Nov 17 13:44:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 13:44:06.068
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:06.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:06.102
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 11/17/23 13:44:06.108
    Nov 17 13:44:06.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25" in namespace "projected-9179" to be "Succeeded or Failed"
    Nov 17 13:44:06.124: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41445ms
    Nov 17 13:44:08.129: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011627993s
    Nov 17 13:44:10.130: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352284s
    STEP: Saw pod success 11/17/23 13:44:10.13
    Nov 17 13:44:10.130: INFO: Pod "downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:10.133: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 container client-container: <nil>
    STEP: delete the pod 11/17/23 13:44:10.139
    Nov 17 13:44:10.150: INFO: Waiting for pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 to disappear
    Nov 17 13:44:10.152: INFO: Pod downwardapi-volume-19f12ae4-f244-4056-8203-fa1689d4cb25 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:10.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9179" for this suite. 11/17/23 13:44:10.156
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:10.165
Nov 17 13:44:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:44:10.166
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:10.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:10.186
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 11/17/23 13:44:10.189
STEP: fetching the ConfigMap 11/17/23 13:44:10.195
STEP: patching the ConfigMap 11/17/23 13:44:10.197
STEP: listing all ConfigMaps in all namespaces with a label selector 11/17/23 13:44:10.203
STEP: deleting the ConfigMap by collection with a label selector 11/17/23 13:44:10.208
STEP: listing all ConfigMaps in test namespace 11/17/23 13:44:10.215
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:10.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7496" for this suite. 11/17/23 13:44:10.222
------------------------------
â€¢ [0.064 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:10.165
    Nov 17 13:44:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:44:10.166
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:10.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:10.186
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 11/17/23 13:44:10.189
    STEP: fetching the ConfigMap 11/17/23 13:44:10.195
    STEP: patching the ConfigMap 11/17/23 13:44:10.197
    STEP: listing all ConfigMaps in all namespaces with a label selector 11/17/23 13:44:10.203
    STEP: deleting the ConfigMap by collection with a label selector 11/17/23 13:44:10.208
    STEP: listing all ConfigMaps in test namespace 11/17/23 13:44:10.215
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:10.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7496" for this suite. 11/17/23 13:44:10.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:10.231
Nov 17 13:44:10.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 13:44:10.232
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:10.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:10.253
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 11/17/23 13:44:10.256
Nov 17 13:44:10.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63" in namespace "downward-api-7709" to be "Succeeded or Failed"
Nov 17 13:44:10.270: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230226ms
Nov 17 13:44:12.273: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008755606s
Nov 17 13:44:14.274: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009883565s
STEP: Saw pod success 11/17/23 13:44:14.274
Nov 17 13:44:14.274: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63" satisfied condition "Succeeded or Failed"
Nov 17 13:44:14.278: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 container client-container: <nil>
STEP: delete the pod 11/17/23 13:44:14.284
Nov 17 13:44:14.294: INFO: Waiting for pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 to disappear
Nov 17 13:44:14.297: INFO: Pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7709" for this suite. 11/17/23 13:44:14.3
------------------------------
â€¢ [4.075 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:10.231
    Nov 17 13:44:10.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 13:44:10.232
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:10.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:10.253
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 11/17/23 13:44:10.256
    Nov 17 13:44:10.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63" in namespace "downward-api-7709" to be "Succeeded or Failed"
    Nov 17 13:44:10.270: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230226ms
    Nov 17 13:44:12.273: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008755606s
    Nov 17 13:44:14.274: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009883565s
    STEP: Saw pod success 11/17/23 13:44:14.274
    Nov 17 13:44:14.274: INFO: Pod "downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:14.278: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 container client-container: <nil>
    STEP: delete the pod 11/17/23 13:44:14.284
    Nov 17 13:44:14.294: INFO: Waiting for pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 to disappear
    Nov 17 13:44:14.297: INFO: Pod downwardapi-volume-3b1537ad-3f2f-48f3-895c-c15d0cde9d63 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7709" for this suite. 11/17/23 13:44:14.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:14.31
Nov 17 13:44:14.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:44:14.311
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:14.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:14.382
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-272a6a9e-b3c8-41c2-a4cf-689779d39557 11/17/23 13:44:14.385
STEP: Creating a pod to test consume configMaps 11/17/23 13:44:14.389
Nov 17 13:44:14.397: INFO: Waiting up to 5m0s for pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841" in namespace "configmap-2767" to be "Succeeded or Failed"
Nov 17 13:44:14.406: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Pending", Reason="", readiness=false. Elapsed: 8.363086ms
Nov 17 13:44:16.409: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219317s
Nov 17 13:44:18.411: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013365308s
STEP: Saw pod success 11/17/23 13:44:18.411
Nov 17 13:44:18.411: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841" satisfied condition "Succeeded or Failed"
Nov 17 13:44:18.414: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 container configmap-volume-test: <nil>
STEP: delete the pod 11/17/23 13:44:18.42
Nov 17 13:44:18.434: INFO: Waiting for pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 to disappear
Nov 17 13:44:18.436: INFO: Pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:18.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2767" for this suite. 11/17/23 13:44:18.441
------------------------------
â€¢ [4.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:14.31
    Nov 17 13:44:14.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:44:14.311
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:14.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:14.382
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-272a6a9e-b3c8-41c2-a4cf-689779d39557 11/17/23 13:44:14.385
    STEP: Creating a pod to test consume configMaps 11/17/23 13:44:14.389
    Nov 17 13:44:14.397: INFO: Waiting up to 5m0s for pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841" in namespace "configmap-2767" to be "Succeeded or Failed"
    Nov 17 13:44:14.406: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Pending", Reason="", readiness=false. Elapsed: 8.363086ms
    Nov 17 13:44:16.409: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219317s
    Nov 17 13:44:18.411: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013365308s
    STEP: Saw pod success 11/17/23 13:44:18.411
    Nov 17 13:44:18.411: INFO: Pod "pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:18.414: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 container configmap-volume-test: <nil>
    STEP: delete the pod 11/17/23 13:44:18.42
    Nov 17 13:44:18.434: INFO: Waiting for pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 to disappear
    Nov 17 13:44:18.436: INFO: Pod pod-configmaps-d34c1dc5-6699-48d0-b7f6-a826caaf3841 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:18.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2767" for this suite. 11/17/23 13:44:18.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:18.449
Nov 17 13:44:18.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 13:44:18.45
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:18.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:18.469
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-03434844-f84d-442d-b4c3-6a126ffe5fe9 11/17/23 13:44:18.472
STEP: Creating a pod to test consume secrets 11/17/23 13:44:18.478
Nov 17 13:44:18.486: INFO: Waiting up to 5m0s for pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d" in namespace "secrets-6305" to be "Succeeded or Failed"
Nov 17 13:44:18.491: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872085ms
Nov 17 13:44:20.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591118s
Nov 17 13:44:22.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009367852s
STEP: Saw pod success 11/17/23 13:44:22.496
Nov 17 13:44:22.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d" satisfied condition "Succeeded or Failed"
Nov 17 13:44:22.499: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 13:44:22.506
Nov 17 13:44:22.522: INFO: Waiting for pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d to disappear
Nov 17 13:44:22.526: INFO: Pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:22.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6305" for this suite. 11/17/23 13:44:22.532
------------------------------
â€¢ [4.090 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:18.449
    Nov 17 13:44:18.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 13:44:18.45
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:18.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:18.469
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-03434844-f84d-442d-b4c3-6a126ffe5fe9 11/17/23 13:44:18.472
    STEP: Creating a pod to test consume secrets 11/17/23 13:44:18.478
    Nov 17 13:44:18.486: INFO: Waiting up to 5m0s for pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d" in namespace "secrets-6305" to be "Succeeded or Failed"
    Nov 17 13:44:18.491: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872085ms
    Nov 17 13:44:20.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591118s
    Nov 17 13:44:22.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009367852s
    STEP: Saw pod success 11/17/23 13:44:22.496
    Nov 17 13:44:22.496: INFO: Pod "pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:22.499: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 13:44:22.506
    Nov 17 13:44:22.522: INFO: Waiting for pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d to disappear
    Nov 17 13:44:22.526: INFO: Pod pod-secrets-e5d77b8d-14f2-46e4-a11f-f378d6d40d8d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:22.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6305" for this suite. 11/17/23 13:44:22.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:22.541
Nov 17 13:44:22.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:44:22.542
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:22.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:22.565
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Nov 17 13:44:22.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-214" for this suite. 11/17/23 13:44:23.13
------------------------------
â€¢ [0.602 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:22.541
    Nov 17 13:44:22.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:44:22.542
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:22.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:22.565
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Nov 17 13:44:22.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-214" for this suite. 11/17/23 13:44:23.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:23.145
Nov 17 13:44:23.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 13:44:23.147
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:23.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:23.168
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 11/17/23 13:44:23.172
Nov 17 13:44:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 create -f -'
Nov 17 13:44:27.136: INFO: stderr: ""
Nov 17 13:44:27.136: INFO: stdout: "pod/pause created\n"
Nov 17 13:44:27.136: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 17 13:44:27.136: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3754" to be "running and ready"
Nov 17 13:44:27.142: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027883ms
Nov 17 13:44:27.142: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-worker-3.c.operations-lab.internal' to be 'Running' but was 'Pending'
Nov 17 13:44:29.146: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010027814s
Nov 17 13:44:29.146: INFO: Pod "pause" satisfied condition "running and ready"
Nov 17 13:44:29.146: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 11/17/23 13:44:29.146
Nov 17 13:44:29.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 label pods pause testing-label=testing-label-value'
Nov 17 13:44:29.259: INFO: stderr: ""
Nov 17 13:44:29.259: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 11/17/23 13:44:29.259
Nov 17 13:44:29.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pod pause -L testing-label'
Nov 17 13:44:29.352: INFO: stderr: ""
Nov 17 13:44:29.352: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 11/17/23 13:44:29.352
Nov 17 13:44:29.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 label pods pause testing-label-'
Nov 17 13:44:29.461: INFO: stderr: ""
Nov 17 13:44:29.461: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 11/17/23 13:44:29.461
Nov 17 13:44:29.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pod pause -L testing-label'
Nov 17 13:44:29.554: INFO: stderr: ""
Nov 17 13:44:29.554: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 11/17/23 13:44:29.554
Nov 17 13:44:29.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 delete --grace-period=0 --force -f -'
Nov 17 13:44:29.655: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 13:44:29.655: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 17 13:44:29.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get rc,svc -l name=pause --no-headers'
Nov 17 13:44:29.751: INFO: stderr: "No resources found in kubectl-3754 namespace.\n"
Nov 17 13:44:29.751: INFO: stdout: ""
Nov 17 13:44:29.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 17 13:44:29.835: INFO: stderr: ""
Nov 17 13:44:29.835: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3754" for this suite. 11/17/23 13:44:29.84
------------------------------
â€¢ [SLOW TEST] [6.705 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:23.145
    Nov 17 13:44:23.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 13:44:23.147
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:23.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:23.168
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 11/17/23 13:44:23.172
    Nov 17 13:44:23.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 create -f -'
    Nov 17 13:44:27.136: INFO: stderr: ""
    Nov 17 13:44:27.136: INFO: stdout: "pod/pause created\n"
    Nov 17 13:44:27.136: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Nov 17 13:44:27.136: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3754" to be "running and ready"
    Nov 17 13:44:27.142: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027883ms
    Nov 17 13:44:27.142: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-worker-3.c.operations-lab.internal' to be 'Running' but was 'Pending'
    Nov 17 13:44:29.146: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010027814s
    Nov 17 13:44:29.146: INFO: Pod "pause" satisfied condition "running and ready"
    Nov 17 13:44:29.146: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 11/17/23 13:44:29.146
    Nov 17 13:44:29.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 label pods pause testing-label=testing-label-value'
    Nov 17 13:44:29.259: INFO: stderr: ""
    Nov 17 13:44:29.259: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 11/17/23 13:44:29.259
    Nov 17 13:44:29.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pod pause -L testing-label'
    Nov 17 13:44:29.352: INFO: stderr: ""
    Nov 17 13:44:29.352: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 11/17/23 13:44:29.352
    Nov 17 13:44:29.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 label pods pause testing-label-'
    Nov 17 13:44:29.461: INFO: stderr: ""
    Nov 17 13:44:29.461: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 11/17/23 13:44:29.461
    Nov 17 13:44:29.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pod pause -L testing-label'
    Nov 17 13:44:29.554: INFO: stderr: ""
    Nov 17 13:44:29.554: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 11/17/23 13:44:29.554
    Nov 17 13:44:29.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 delete --grace-period=0 --force -f -'
    Nov 17 13:44:29.655: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 13:44:29.655: INFO: stdout: "pod \"pause\" force deleted\n"
    Nov 17 13:44:29.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get rc,svc -l name=pause --no-headers'
    Nov 17 13:44:29.751: INFO: stderr: "No resources found in kubectl-3754 namespace.\n"
    Nov 17 13:44:29.751: INFO: stdout: ""
    Nov 17 13:44:29.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3754 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 17 13:44:29.835: INFO: stderr: ""
    Nov 17 13:44:29.835: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3754" for this suite. 11/17/23 13:44:29.84
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:29.851
Nov 17 13:44:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:44:29.852
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:29.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:29.878
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-95c275d0-e761-4b16-9b1a-562b1bbd53e6 11/17/23 13:44:29.882
STEP: Creating a pod to test consume configMaps 11/17/23 13:44:29.888
Nov 17 13:44:29.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e" in namespace "configmap-2465" to be "Succeeded or Failed"
Nov 17 13:44:29.906: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04582ms
Nov 17 13:44:31.910: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012400053s
Nov 17 13:44:33.918: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020846117s
STEP: Saw pod success 11/17/23 13:44:33.918
Nov 17 13:44:33.919: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e" satisfied condition "Succeeded or Failed"
Nov 17 13:44:33.925: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e container agnhost-container: <nil>
STEP: delete the pod 11/17/23 13:44:33.941
Nov 17 13:44:33.983: INFO: Waiting for pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e to disappear
Nov 17 13:44:34.000: INFO: Pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:34.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2465" for this suite. 11/17/23 13:44:34.009
------------------------------
â€¢ [4.221 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:29.851
    Nov 17 13:44:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:44:29.852
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:29.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:29.878
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-95c275d0-e761-4b16-9b1a-562b1bbd53e6 11/17/23 13:44:29.882
    STEP: Creating a pod to test consume configMaps 11/17/23 13:44:29.888
    Nov 17 13:44:29.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e" in namespace "configmap-2465" to be "Succeeded or Failed"
    Nov 17 13:44:29.906: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04582ms
    Nov 17 13:44:31.910: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012400053s
    Nov 17 13:44:33.918: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020846117s
    STEP: Saw pod success 11/17/23 13:44:33.918
    Nov 17 13:44:33.919: INFO: Pod "pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:33.925: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 13:44:33.941
    Nov 17 13:44:33.983: INFO: Waiting for pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e to disappear
    Nov 17 13:44:34.000: INFO: Pod pod-configmaps-6ca31c0f-3527-412a-88ed-311db499093e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:34.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2465" for this suite. 11/17/23 13:44:34.009
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:34.072
Nov 17 13:44:34.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 13:44:34.073
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:34.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:34.184
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-58761987-c7bb-449b-9fbb-c9a3e07a0823 11/17/23 13:44:34.205
STEP: Creating a pod to test consume secrets 11/17/23 13:44:34.22
Nov 17 13:44:34.256: INFO: Waiting up to 5m0s for pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da" in namespace "secrets-4517" to be "Succeeded or Failed"
Nov 17 13:44:34.283: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Pending", Reason="", readiness=false. Elapsed: 27.075142ms
Nov 17 13:44:36.287: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031024562s
Nov 17 13:44:38.288: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032077114s
STEP: Saw pod success 11/17/23 13:44:38.288
Nov 17 13:44:38.289: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da" satisfied condition "Succeeded or Failed"
Nov 17 13:44:38.294: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da container secret-env-test: <nil>
STEP: delete the pod 11/17/23 13:44:38.304
Nov 17 13:44:38.346: INFO: Waiting for pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da to disappear
Nov 17 13:44:38.350: INFO: Pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:38.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4517" for this suite. 11/17/23 13:44:38.356
------------------------------
â€¢ [4.291 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:34.072
    Nov 17 13:44:34.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 13:44:34.073
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:34.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:34.184
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-58761987-c7bb-449b-9fbb-c9a3e07a0823 11/17/23 13:44:34.205
    STEP: Creating a pod to test consume secrets 11/17/23 13:44:34.22
    Nov 17 13:44:34.256: INFO: Waiting up to 5m0s for pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da" in namespace "secrets-4517" to be "Succeeded or Failed"
    Nov 17 13:44:34.283: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Pending", Reason="", readiness=false. Elapsed: 27.075142ms
    Nov 17 13:44:36.287: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031024562s
    Nov 17 13:44:38.288: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032077114s
    STEP: Saw pod success 11/17/23 13:44:38.288
    Nov 17 13:44:38.289: INFO: Pod "pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:38.294: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da container secret-env-test: <nil>
    STEP: delete the pod 11/17/23 13:44:38.304
    Nov 17 13:44:38.346: INFO: Waiting for pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da to disappear
    Nov 17 13:44:38.350: INFO: Pod pod-secrets-6a678769-0ced-4c27-9b01-3dd68ce0c6da no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:38.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4517" for this suite. 11/17/23 13:44:38.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:38.364
Nov 17 13:44:38.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 13:44:38.366
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:38.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:38.391
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/17/23 13:44:38.394
Nov 17 13:44:38.407: INFO: Waiting up to 5m0s for pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186" in namespace "emptydir-6406" to be "Succeeded or Failed"
Nov 17 13:44:38.412: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.611406ms
Nov 17 13:44:40.417: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010657636s
Nov 17 13:44:42.420: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013668672s
STEP: Saw pod success 11/17/23 13:44:42.421
Nov 17 13:44:42.421: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186" satisfied condition "Succeeded or Failed"
Nov 17 13:44:42.424: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 container test-container: <nil>
STEP: delete the pod 11/17/23 13:44:42.429
Nov 17 13:44:42.443: INFO: Waiting for pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 to disappear
Nov 17 13:44:42.447: INFO: Pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:42.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6406" for this suite. 11/17/23 13:44:42.452
------------------------------
â€¢ [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:38.364
    Nov 17 13:44:38.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 13:44:38.366
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:38.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:38.391
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/17/23 13:44:38.394
    Nov 17 13:44:38.407: INFO: Waiting up to 5m0s for pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186" in namespace "emptydir-6406" to be "Succeeded or Failed"
    Nov 17 13:44:38.412: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.611406ms
    Nov 17 13:44:40.417: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010657636s
    Nov 17 13:44:42.420: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013668672s
    STEP: Saw pod success 11/17/23 13:44:42.421
    Nov 17 13:44:42.421: INFO: Pod "pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186" satisfied condition "Succeeded or Failed"
    Nov 17 13:44:42.424: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 container test-container: <nil>
    STEP: delete the pod 11/17/23 13:44:42.429
    Nov 17 13:44:42.443: INFO: Waiting for pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 to disappear
    Nov 17 13:44:42.447: INFO: Pod pod-838bd7ab-4bc6-41a6-9e1d-579b03ee0186 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:42.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6406" for this suite. 11/17/23 13:44:42.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:42.459
Nov 17 13:44:42.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 13:44:42.46
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:42.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:42.535
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8951" for this suite. 11/17/23 13:44:42.546
------------------------------
â€¢ [0.093 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:42.459
    Nov 17 13:44:42.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 13:44:42.46
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:42.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:42.535
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8951" for this suite. 11/17/23 13:44:42.546
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:42.552
Nov 17 13:44:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 13:44:42.554
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:42.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:42.575
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 11/17/23 13:44:42.578
STEP: Wait for the Deployment to create new ReplicaSet 11/17/23 13:44:42.584
STEP: delete the deployment 11/17/23 13:44:43.097
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/17/23 13:44:43.103
STEP: Gathering metrics 11/17/23 13:44:43.629
Nov 17 13:44:43.674: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 13:44:43.677: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.01665ms
Nov 17 13:44:43.677: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 13:44:43.677: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 13:44:43.794: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9743" for this suite. 11/17/23 13:44:43.801
------------------------------
â€¢ [1.259 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:42.552
    Nov 17 13:44:42.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 13:44:42.554
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:42.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:42.575
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 11/17/23 13:44:42.578
    STEP: Wait for the Deployment to create new ReplicaSet 11/17/23 13:44:42.584
    STEP: delete the deployment 11/17/23 13:44:43.097
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 11/17/23 13:44:43.103
    STEP: Gathering metrics 11/17/23 13:44:43.629
    Nov 17 13:44:43.674: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 13:44:43.677: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.01665ms
    Nov 17 13:44:43.677: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 13:44:43.677: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 13:44:43.794: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9743" for this suite. 11/17/23 13:44:43.801
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:43.812
Nov 17 13:44:43.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename watch 11/17/23 13:44:43.813
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:43.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:43.846
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 11/17/23 13:44:43.85
STEP: modifying the configmap once 11/17/23 13:44:43.855
STEP: modifying the configmap a second time 11/17/23 13:44:43.864
STEP: deleting the configmap 11/17/23 13:44:43.875
STEP: creating a watch on configmaps from the resource version returned by the first update 11/17/23 13:44:43.883
STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/17/23 13:44:43.885
Nov 17 13:44:43.885: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-773  9029e0c5-e18a-46a6-ba5e-9123174484ab 16454 0 2023-11-17 13:44:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-17 13:44:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 13:44:43.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-773  9029e0c5-e18a-46a6-ba5e-9123174484ab 16455 0 2023-11-17 13:44:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-17 13:44:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 17 13:44:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-773" for this suite. 11/17/23 13:44:43.895
------------------------------
â€¢ [0.116 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:43.812
    Nov 17 13:44:43.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename watch 11/17/23 13:44:43.813
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:43.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:43.846
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 11/17/23 13:44:43.85
    STEP: modifying the configmap once 11/17/23 13:44:43.855
    STEP: modifying the configmap a second time 11/17/23 13:44:43.864
    STEP: deleting the configmap 11/17/23 13:44:43.875
    STEP: creating a watch on configmaps from the resource version returned by the first update 11/17/23 13:44:43.883
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 11/17/23 13:44:43.885
    Nov 17 13:44:43.885: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-773  9029e0c5-e18a-46a6-ba5e-9123174484ab 16454 0 2023-11-17 13:44:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-17 13:44:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 13:44:43.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-773  9029e0c5-e18a-46a6-ba5e-9123174484ab 16455 0 2023-11-17 13:44:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-11-17 13:44:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:44:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-773" for this suite. 11/17/23 13:44:43.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:44:43.932
Nov 17 13:44:43.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename cronjob 11/17/23 13:44:43.933
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:43.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:43.991
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 11/17/23 13:44:43.994
STEP: Ensuring a job is scheduled 11/17/23 13:44:44.004
STEP: Ensuring exactly one is scheduled 11/17/23 13:45:02.012
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/17/23 13:45:02.02
STEP: Ensuring no more jobs are scheduled 11/17/23 13:45:02.031
STEP: Removing cronjob 11/17/23 13:50:02.04
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 17 13:50:02.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-968" for this suite. 11/17/23 13:50:02.055
------------------------------
â€¢ [SLOW TEST] [318.158 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:44:43.932
    Nov 17 13:44:43.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename cronjob 11/17/23 13:44:43.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:44:43.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:44:43.991
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 11/17/23 13:44:43.994
    STEP: Ensuring a job is scheduled 11/17/23 13:44:44.004
    STEP: Ensuring exactly one is scheduled 11/17/23 13:45:02.012
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/17/23 13:45:02.02
    STEP: Ensuring no more jobs are scheduled 11/17/23 13:45:02.031
    STEP: Removing cronjob 11/17/23 13:50:02.04
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:50:02.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-968" for this suite. 11/17/23 13:50:02.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:50:02.094
Nov 17 13:50:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 13:50:02.096
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:50:02.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:50:02.126
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 11/17/23 13:50:02.135
STEP: Patching the Job 11/17/23 13:50:02.143
STEP: Watching for Job to be patched 11/17/23 13:50:02.165
Nov 17 13:50:02.169: INFO: Event ADDED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
Nov 17 13:50:02.169: INFO: Event MODIFIED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 11/17/23 13:50:02.169
STEP: Watching for Job to be updated 11/17/23 13:50:02.206
Nov 17 13:50:02.211: INFO: Event MODIFIED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:02.211: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 11/17/23 13:50:02.211
Nov 17 13:50:02.214: INFO: Job: e2e-zrgdm as labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched]
STEP: Waiting for job to complete 11/17/23 13:50:02.214
STEP: Delete a job collection with a labelselector 11/17/23 13:50:12.218
STEP: Watching for Job to be deleted 11/17/23 13:50:12.226
Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Nov 17 13:50:12.229: INFO: Event DELETED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 11/17/23 13:50:12.229
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 13:50:12.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7469" for this suite. 11/17/23 13:50:12.262
------------------------------
â€¢ [SLOW TEST] [10.191 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:50:02.094
    Nov 17 13:50:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 13:50:02.096
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:50:02.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:50:02.126
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 11/17/23 13:50:02.135
    STEP: Patching the Job 11/17/23 13:50:02.143
    STEP: Watching for Job to be patched 11/17/23 13:50:02.165
    Nov 17 13:50:02.169: INFO: Event ADDED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm] and annotations: map[batch.kubernetes.io/job-tracking:]
    Nov 17 13:50:02.169: INFO: Event MODIFIED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 11/17/23 13:50:02.169
    STEP: Watching for Job to be updated 11/17/23 13:50:02.206
    Nov 17 13:50:02.211: INFO: Event MODIFIED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:02.211: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 11/17/23 13:50:02.211
    Nov 17 13:50:02.214: INFO: Job: e2e-zrgdm as labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched]
    STEP: Waiting for job to complete 11/17/23 13:50:02.214
    STEP: Delete a job collection with a labelselector 11/17/23 13:50:12.218
    STEP: Watching for Job to be deleted 11/17/23 13:50:12.226
    Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:12.229: INFO: Event MODIFIED observed for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Nov 17 13:50:12.229: INFO: Event DELETED found for Job e2e-zrgdm in namespace job-7469 with labels: map[e2e-job-label:e2e-zrgdm e2e-zrgdm:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 11/17/23 13:50:12.229
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:50:12.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7469" for this suite. 11/17/23 13:50:12.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:50:12.285
Nov 17 13:50:12.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 13:50:12.287
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:50:12.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:50:12.33
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 in namespace container-probe-6875 11/17/23 13:50:12.336
Nov 17 13:50:12.347: INFO: Waiting up to 5m0s for pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4" in namespace "container-probe-6875" to be "not pending"
Nov 17 13:50:12.358: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.925475ms
Nov 17 13:50:14.363: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016018653s
Nov 17 13:50:14.364: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4" satisfied condition "not pending"
Nov 17 13:50:14.364: INFO: Started pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 in namespace container-probe-6875
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 13:50:14.364
Nov 17 13:50:14.368: INFO: Initial restart count of pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 is 0
STEP: deleting the pod 11/17/23 13:54:14.986
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 13:54:15.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6875" for this suite. 11/17/23 13:54:15.029
------------------------------
â€¢ [SLOW TEST] [242.818 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:50:12.285
    Nov 17 13:50:12.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 13:50:12.287
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:50:12.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:50:12.33
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 in namespace container-probe-6875 11/17/23 13:50:12.336
    Nov 17 13:50:12.347: INFO: Waiting up to 5m0s for pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4" in namespace "container-probe-6875" to be "not pending"
    Nov 17 13:50:12.358: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.925475ms
    Nov 17 13:50:14.363: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016018653s
    Nov 17 13:50:14.364: INFO: Pod "liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4" satisfied condition "not pending"
    Nov 17 13:50:14.364: INFO: Started pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 in namespace container-probe-6875
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 13:50:14.364
    Nov 17 13:50:14.368: INFO: Initial restart count of pod liveness-a23cf812-7d73-47c7-9eb6-8015123d09e4 is 0
    STEP: deleting the pod 11/17/23 13:54:14.986
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:54:15.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6875" for this suite. 11/17/23 13:54:15.029
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:54:15.104
Nov 17 13:54:15.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 13:54:15.106
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:15.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:15.15
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 11/17/23 13:54:15.154
Nov 17 13:54:15.166: INFO: Waiting up to 5m0s for pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b" in namespace "emptydir-2907" to be "Succeeded or Failed"
Nov 17 13:54:15.175: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.564014ms
Nov 17 13:54:17.179: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013626116s
Nov 17 13:54:19.180: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013844813s
STEP: Saw pod success 11/17/23 13:54:19.18
Nov 17 13:54:19.180: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b" satisfied condition "Succeeded or Failed"
Nov 17 13:54:19.183: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b container test-container: <nil>
STEP: delete the pod 11/17/23 13:54:19.202
Nov 17 13:54:19.214: INFO: Waiting for pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b to disappear
Nov 17 13:54:19.218: INFO: Pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 13:54:19.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2907" for this suite. 11/17/23 13:54:19.222
------------------------------
â€¢ [4.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:54:15.104
    Nov 17 13:54:15.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 13:54:15.106
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:15.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:15.15
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/17/23 13:54:15.154
    Nov 17 13:54:15.166: INFO: Waiting up to 5m0s for pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b" in namespace "emptydir-2907" to be "Succeeded or Failed"
    Nov 17 13:54:15.175: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.564014ms
    Nov 17 13:54:17.179: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013626116s
    Nov 17 13:54:19.180: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013844813s
    STEP: Saw pod success 11/17/23 13:54:19.18
    Nov 17 13:54:19.180: INFO: Pod "pod-c40cd888-a8c7-44be-8a42-68244a884d1b" satisfied condition "Succeeded or Failed"
    Nov 17 13:54:19.183: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b container test-container: <nil>
    STEP: delete the pod 11/17/23 13:54:19.202
    Nov 17 13:54:19.214: INFO: Waiting for pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b to disappear
    Nov 17 13:54:19.218: INFO: Pod pod-c40cd888-a8c7-44be-8a42-68244a884d1b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:54:19.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2907" for this suite. 11/17/23 13:54:19.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:54:19.232
Nov 17 13:54:19.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:54:19.234
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:19.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:19.264
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-ced1b7ce-f9d5-471c-a486-d872cc75c23a 11/17/23 13:54:19.268
STEP: Creating a pod to test consume configMaps 11/17/23 13:54:19.275
Nov 17 13:54:19.285: INFO: Waiting up to 5m0s for pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8" in namespace "configmap-9286" to be "Succeeded or Failed"
Nov 17 13:54:19.290: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387382ms
Nov 17 13:54:21.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009430875s
Nov 17 13:54:23.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009285474s
STEP: Saw pod success 11/17/23 13:54:23.294
Nov 17 13:54:23.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8" satisfied condition "Succeeded or Failed"
Nov 17 13:54:23.297: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 13:54:23.303
Nov 17 13:54:23.313: INFO: Waiting for pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 to disappear
Nov 17 13:54:23.315: INFO: Pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:54:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9286" for this suite. 11/17/23 13:54:23.318
------------------------------
â€¢ [4.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:54:19.232
    Nov 17 13:54:19.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:54:19.234
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:19.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:19.264
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-ced1b7ce-f9d5-471c-a486-d872cc75c23a 11/17/23 13:54:19.268
    STEP: Creating a pod to test consume configMaps 11/17/23 13:54:19.275
    Nov 17 13:54:19.285: INFO: Waiting up to 5m0s for pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8" in namespace "configmap-9286" to be "Succeeded or Failed"
    Nov 17 13:54:19.290: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.387382ms
    Nov 17 13:54:21.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009430875s
    Nov 17 13:54:23.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009285474s
    STEP: Saw pod success 11/17/23 13:54:23.294
    Nov 17 13:54:23.294: INFO: Pod "pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8" satisfied condition "Succeeded or Failed"
    Nov 17 13:54:23.297: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 13:54:23.303
    Nov 17 13:54:23.313: INFO: Waiting for pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 to disappear
    Nov 17 13:54:23.315: INFO: Pod pod-configmaps-eab9887a-0ff2-4e03-bf00-c29a70fb1bf8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:54:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9286" for this suite. 11/17/23 13:54:23.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:54:23.326
Nov 17 13:54:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 13:54:23.327
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:23.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:23.347
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Nov 17 13:54:23.350: INFO: Creating simple deployment test-new-deployment
Nov 17 13:54:23.359: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 11/17/23 13:54:25.374
STEP: updating a scale subresource 11/17/23 13:54:25.378
STEP: verifying the deployment Spec.Replicas was modified 11/17/23 13:54:25.388
STEP: Patch a scale subresource 11/17/23 13:54:25.395
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 13:54:25.428: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6418  0fdddd25-917f-49fa-84bb-2131454c97c8 19913 3 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-17 13:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051299e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 13:54:25 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-17 13:54:25 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 17 13:54:25.444: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-6418  ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 19919 2 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0fdddd25-917f-49fa-84bb-2131454c97c8 0xc005129e27 0xc005129e28}] [] [{kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fdddd25-917f-49fa-84bb-2131454c97c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005129eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 17 13:54:25.450: INFO: Pod "test-new-deployment-7f5969cbc7-4cdbx" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4cdbx test-new-deployment-7f5969cbc7- deployment-6418  6eebc5d4-ef3a-42ae-bede-af6db93b8bb5 19908 0 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 0xc00503f797 0xc00503f798}] [] [{kube-controller-manager Update v1 2023-11-17 13:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba39b0e9-b59b-475a-a643-ca14bd0d2ee6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmf6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmf6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.199,StartTime:2023-11-17 13:54:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 13:54:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://54b7a2838c316878cfdd81f7523a7e5461ef56357eab1e5e167af137845d4c51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 13:54:25.450: INFO: Pod "test-new-deployment-7f5969cbc7-v2cm4" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-v2cm4 test-new-deployment-7f5969cbc7- deployment-6418  ff902302-9e1a-4437-8cba-cbb484491c09 19918 0 2023-11-17 13:54:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 0xc00503f970 0xc00503f971}] [] [{kube-controller-manager Update v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba39b0e9-b59b-475a-a643-ca14bd0d2ee6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lctll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lctll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 13:54:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6418" for this suite. 11/17/23 13:54:25.456
------------------------------
â€¢ [2.145 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:54:23.326
    Nov 17 13:54:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 13:54:23.327
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:23.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:23.347
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Nov 17 13:54:23.350: INFO: Creating simple deployment test-new-deployment
    Nov 17 13:54:23.359: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 11/17/23 13:54:25.374
    STEP: updating a scale subresource 11/17/23 13:54:25.378
    STEP: verifying the deployment Spec.Replicas was modified 11/17/23 13:54:25.388
    STEP: Patch a scale subresource 11/17/23 13:54:25.395
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 13:54:25.428: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6418  0fdddd25-917f-49fa-84bb-2131454c97c8 19913 3 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-11-17 13:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051299e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 13:54:25 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-11-17 13:54:25 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 17 13:54:25.444: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-6418  ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 19919 2 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0fdddd25-917f-49fa-84bb-2131454c97c8 0xc005129e27 0xc005129e28}] [] [{kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fdddd25-917f-49fa-84bb-2131454c97c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005129eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 13:54:25.450: INFO: Pod "test-new-deployment-7f5969cbc7-4cdbx" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4cdbx test-new-deployment-7f5969cbc7- deployment-6418  6eebc5d4-ef3a-42ae-bede-af6db93b8bb5 19908 0 2023-11-17 13:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 0xc00503f797 0xc00503f798}] [] [{kube-controller-manager Update v1 2023-11-17 13:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba39b0e9-b59b-475a-a643-ca14bd0d2ee6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmf6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmf6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.199,StartTime:2023-11-17 13:54:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 13:54:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://54b7a2838c316878cfdd81f7523a7e5461ef56357eab1e5e167af137845d4c51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 13:54:25.450: INFO: Pod "test-new-deployment-7f5969cbc7-v2cm4" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-v2cm4 test-new-deployment-7f5969cbc7- deployment-6418  ff902302-9e1a-4437-8cba-cbb484491c09 19918 0 2023-11-17 13:54:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ba39b0e9-b59b-475a-a643-ca14bd0d2ee6 0xc00503f970 0xc00503f971}] [] [{kube-controller-manager Update v1 2023-11-17 13:54:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba39b0e9-b59b-475a-a643-ca14bd0d2ee6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lctll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lctll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 13:54:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:54:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6418" for this suite. 11/17/23 13:54:25.456
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:54:25.472
Nov 17 13:54:25.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:54:25.474
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:25.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:25.515
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Nov 17 13:54:25.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:54:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8665" for this suite. 11/17/23 13:54:29.312
------------------------------
â€¢ [3.848 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:54:25.472
    Nov 17 13:54:25.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:54:25.474
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:25.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:25.515
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Nov 17 13:54:25.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:54:29.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8665" for this suite. 11/17/23 13:54:29.312
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:54:29.321
Nov 17 13:54:29.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 13:54:29.323
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:29.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:29.347
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8933 11/17/23 13:54:29.351
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 11/17/23 13:54:29.359
STEP: Creating stateful set ss in namespace statefulset-8933 11/17/23 13:54:29.364
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8933 11/17/23 13:54:29.375
Nov 17 13:54:29.381: INFO: Found 0 stateful pods, waiting for 1
Nov 17 13:54:39.385: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/17/23 13:54:39.385
Nov 17 13:54:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 13:54:39.601: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 13:54:39.601: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 13:54:39.601: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 13:54:39.605: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 17 13:54:49.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 13:54:49.610: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 13:54:49.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999547s
Nov 17 13:54:50.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994217331s
Nov 17 13:54:51.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989164302s
Nov 17 13:54:52.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984628693s
Nov 17 13:54:53.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981066703s
Nov 17 13:54:54.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.933676028s
Nov 17 13:54:55.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.929720355s
Nov 17 13:54:56.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.924916212s
Nov 17 13:54:57.705: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.920957892s
Nov 17 13:54:58.709: INFO: Verifying statefulset ss doesn't scale past 1 for another 917.20007ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8933 11/17/23 13:54:59.71
Nov 17 13:54:59.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 13:54:59.913: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 13:54:59.913: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 13:54:59.913: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 13:54:59.916: INFO: Found 1 stateful pods, waiting for 3
Nov 17 13:55:09.925: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 13:55:09.925: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 13:55:09.925: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 11/17/23 13:55:09.925
STEP: Scale down will halt with unhealthy stateful pod 11/17/23 13:55:09.926
Nov 17 13:55:09.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 13:55:10.150: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 13:55:10.150: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 13:55:10.150: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 13:55:10.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 13:55:10.376: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 13:55:10.376: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 13:55:10.376: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 13:55:10.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 13:55:10.578: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 13:55:10.578: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 13:55:10.578: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 13:55:10.578: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 13:55:10.582: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 17 13:55:20.590: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 13:55:20.590: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 13:55:20.591: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 13:55:20.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999547s
Nov 17 13:55:21.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992674605s
Nov 17 13:55:22.623: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987054047s
Nov 17 13:55:23.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982738404s
Nov 17 13:55:24.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977459022s
Nov 17 13:55:25.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97372139s
Nov 17 13:55:26.642: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969033745s
Nov 17 13:55:27.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963156495s
Nov 17 13:55:28.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957361942s
Nov 17 13:55:29.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.428448ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8933 11/17/23 13:55:30.656
Nov 17 13:55:30.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 13:55:30.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 13:55:30.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 13:55:30.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 13:55:30.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 13:55:31.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 13:55:31.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 13:55:31.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 13:55:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 13:55:31.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 13:55:31.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 13:55:31.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 13:55:31.266: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 11/17/23 13:55:41.288
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 13:55:41.288: INFO: Deleting all statefulset in ns statefulset-8933
Nov 17 13:55:41.292: INFO: Scaling statefulset ss to 0
Nov 17 13:55:41.305: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 13:55:41.309: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:41.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8933" for this suite. 11/17/23 13:55:41.364
------------------------------
â€¢ [SLOW TEST] [72.054 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:54:29.321
    Nov 17 13:54:29.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 13:54:29.323
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:54:29.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:54:29.347
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8933 11/17/23 13:54:29.351
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 11/17/23 13:54:29.359
    STEP: Creating stateful set ss in namespace statefulset-8933 11/17/23 13:54:29.364
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8933 11/17/23 13:54:29.375
    Nov 17 13:54:29.381: INFO: Found 0 stateful pods, waiting for 1
    Nov 17 13:54:39.385: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 11/17/23 13:54:39.385
    Nov 17 13:54:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 13:54:39.601: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 13:54:39.601: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 13:54:39.601: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 13:54:39.605: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 17 13:54:49.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 13:54:49.610: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 13:54:49.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999547s
    Nov 17 13:54:50.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994217331s
    Nov 17 13:54:51.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989164302s
    Nov 17 13:54:52.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984628693s
    Nov 17 13:54:53.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981066703s
    Nov 17 13:54:54.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.933676028s
    Nov 17 13:54:55.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.929720355s
    Nov 17 13:54:56.701: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.924916212s
    Nov 17 13:54:57.705: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.920957892s
    Nov 17 13:54:58.709: INFO: Verifying statefulset ss doesn't scale past 1 for another 917.20007ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8933 11/17/23 13:54:59.71
    Nov 17 13:54:59.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 13:54:59.913: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 13:54:59.913: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 13:54:59.913: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 13:54:59.916: INFO: Found 1 stateful pods, waiting for 3
    Nov 17 13:55:09.925: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 13:55:09.925: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 13:55:09.925: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 11/17/23 13:55:09.925
    STEP: Scale down will halt with unhealthy stateful pod 11/17/23 13:55:09.926
    Nov 17 13:55:09.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 13:55:10.150: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 13:55:10.150: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 13:55:10.150: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 13:55:10.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 13:55:10.376: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 13:55:10.376: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 13:55:10.376: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 13:55:10.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 13:55:10.578: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 13:55:10.578: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 13:55:10.578: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 13:55:10.578: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 13:55:10.582: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Nov 17 13:55:20.590: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 13:55:20.590: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 13:55:20.591: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 13:55:20.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999547s
    Nov 17 13:55:21.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992674605s
    Nov 17 13:55:22.623: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987054047s
    Nov 17 13:55:23.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982738404s
    Nov 17 13:55:24.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977459022s
    Nov 17 13:55:25.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97372139s
    Nov 17 13:55:26.642: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969033745s
    Nov 17 13:55:27.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963156495s
    Nov 17 13:55:28.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957361942s
    Nov 17 13:55:29.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.428448ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8933 11/17/23 13:55:30.656
    Nov 17 13:55:30.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 13:55:30.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 13:55:30.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 13:55:30.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 13:55:30.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 13:55:31.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 13:55:31.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 13:55:31.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 13:55:31.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-8933 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 13:55:31.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 13:55:31.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 13:55:31.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 13:55:31.266: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 11/17/23 13:55:41.288
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 13:55:41.288: INFO: Deleting all statefulset in ns statefulset-8933
    Nov 17 13:55:41.292: INFO: Scaling statefulset ss to 0
    Nov 17 13:55:41.305: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 13:55:41.309: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:41.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8933" for this suite. 11/17/23 13:55:41.364
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:41.376
Nov 17 13:55:41.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:55:41.378
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:41.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:41.403
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-be36c815-2a2e-45fb-9ccf-9ac10a8e9a76 11/17/23 13:55:41.412
STEP: Creating the pod 11/17/23 13:55:41.419
Nov 17 13:55:41.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698" in namespace "configmap-3869" to be "running"
Nov 17 13:55:41.438: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683518ms
Nov 17 13:55:43.442: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698": Phase="Running", Reason="", readiness=false. Elapsed: 2.010408593s
Nov 17 13:55:43.442: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698" satisfied condition "running"
STEP: Waiting for pod with text data 11/17/23 13:55:43.442
STEP: Waiting for pod with binary data 11/17/23 13:55:43.45
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:43.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3869" for this suite. 11/17/23 13:55:43.462
------------------------------
â€¢ [2.094 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:41.376
    Nov 17 13:55:41.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:55:41.378
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:41.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:41.403
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-be36c815-2a2e-45fb-9ccf-9ac10a8e9a76 11/17/23 13:55:41.412
    STEP: Creating the pod 11/17/23 13:55:41.419
    Nov 17 13:55:41.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698" in namespace "configmap-3869" to be "running"
    Nov 17 13:55:41.438: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683518ms
    Nov 17 13:55:43.442: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698": Phase="Running", Reason="", readiness=false. Elapsed: 2.010408593s
    Nov 17 13:55:43.442: INFO: Pod "pod-configmaps-22c70236-b33f-4f47-99cd-95e76fc5e698" satisfied condition "running"
    STEP: Waiting for pod with text data 11/17/23 13:55:43.442
    STEP: Waiting for pod with binary data 11/17/23 13:55:43.45
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:43.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3869" for this suite. 11/17/23 13:55:43.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:43.474
Nov 17 13:55:43.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context-test 11/17/23 13:55:43.475
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:43.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:43.497
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Nov 17 13:55:43.511: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640" in namespace "security-context-test-9652" to be "Succeeded or Failed"
Nov 17 13:55:43.516: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296444ms
Nov 17 13:55:45.525: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013896601s
Nov 17 13:55:47.523: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230761s
Nov 17 13:55:47.524: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9652" for this suite. 11/17/23 13:55:47.533
------------------------------
â€¢ [4.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:43.474
    Nov 17 13:55:43.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context-test 11/17/23 13:55:43.475
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:43.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:43.497
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Nov 17 13:55:43.511: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640" in namespace "security-context-test-9652" to be "Succeeded or Failed"
    Nov 17 13:55:43.516: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296444ms
    Nov 17 13:55:45.525: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013896601s
    Nov 17 13:55:47.523: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012230761s
    Nov 17 13:55:47.524: INFO: Pod "busybox-readonly-false-f961032b-655a-4a2a-a8b9-82dd4dc33640" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:47.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9652" for this suite. 11/17/23 13:55:47.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:47.55
Nov 17 13:55:47.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubelet-test 11/17/23 13:55:47.551
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:47.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:47.616
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Nov 17 13:55:47.640: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a" in namespace "kubelet-test-3868" to be "running and ready"
Nov 17 13:55:47.646: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.692898ms
Nov 17 13:55:47.646: INFO: The phase of Pod busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:55:49.659: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.018954229s
Nov 17 13:55:49.659: INFO: The phase of Pod busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a is Running (Ready = true)
Nov 17 13:55:49.659: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:49.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3868" for this suite. 11/17/23 13:55:49.717
------------------------------
â€¢ [2.183 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:47.55
    Nov 17 13:55:47.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubelet-test 11/17/23 13:55:47.551
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:47.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:47.616
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Nov 17 13:55:47.640: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a" in namespace "kubelet-test-3868" to be "running and ready"
    Nov 17 13:55:47.646: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.692898ms
    Nov 17 13:55:47.646: INFO: The phase of Pod busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:55:49.659: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a": Phase="Running", Reason="", readiness=true. Elapsed: 2.018954229s
    Nov 17 13:55:49.659: INFO: The phase of Pod busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a is Running (Ready = true)
    Nov 17 13:55:49.659: INFO: Pod "busybox-readonly-fs16304330-f693-47da-b231-0ca299886c5a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:49.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3868" for this suite. 11/17/23 13:55:49.717
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:49.733
Nov 17 13:55:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:55:49.735
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:49.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:49.8
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-8afc7115-8a70-4f3a-a41a-0c76e52e74b5 11/17/23 13:55:49.813
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:49.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1704" for this suite. 11/17/23 13:55:49.826
------------------------------
â€¢ [0.109 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:49.733
    Nov 17 13:55:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:55:49.735
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:49.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:49.8
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-8afc7115-8a70-4f3a-a41a-0c76e52e74b5 11/17/23 13:55:49.813
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:49.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1704" for this suite. 11/17/23 13:55:49.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:49.847
Nov 17 13:55:49.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:55:49.848
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:49.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:49.897
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-760b0327-d26d-466d-8e33-ce931ed86c06 11/17/23 13:55:49.904
STEP: Creating a pod to test consume configMaps 11/17/23 13:55:49.915
Nov 17 13:55:49.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647" in namespace "configmap-5243" to be "Succeeded or Failed"
Nov 17 13:55:49.958: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Pending", Reason="", readiness=false. Elapsed: 15.850549ms
Nov 17 13:55:51.965: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023091959s
Nov 17 13:55:53.984: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041587206s
STEP: Saw pod success 11/17/23 13:55:53.984
Nov 17 13:55:53.985: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647" satisfied condition "Succeeded or Failed"
Nov 17 13:55:53.994: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 13:55:54.015
Nov 17 13:55:54.078: INFO: Waiting for pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 to disappear
Nov 17 13:55:54.090: INFO: Pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:55:54.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5243" for this suite. 11/17/23 13:55:54.1
------------------------------
â€¢ [4.268 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:49.847
    Nov 17 13:55:49.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:55:49.848
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:49.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:49.897
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-760b0327-d26d-466d-8e33-ce931ed86c06 11/17/23 13:55:49.904
    STEP: Creating a pod to test consume configMaps 11/17/23 13:55:49.915
    Nov 17 13:55:49.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647" in namespace "configmap-5243" to be "Succeeded or Failed"
    Nov 17 13:55:49.958: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Pending", Reason="", readiness=false. Elapsed: 15.850549ms
    Nov 17 13:55:51.965: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023091959s
    Nov 17 13:55:53.984: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041587206s
    STEP: Saw pod success 11/17/23 13:55:53.984
    Nov 17 13:55:53.985: INFO: Pod "pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647" satisfied condition "Succeeded or Failed"
    Nov 17 13:55:53.994: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 13:55:54.015
    Nov 17 13:55:54.078: INFO: Waiting for pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 to disappear
    Nov 17 13:55:54.090: INFO: Pod pod-configmaps-76854e4c-2745-4f45-9a1e-7ecc1c1a7647 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:55:54.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5243" for this suite. 11/17/23 13:55:54.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:55:54.12
Nov 17 13:55:54.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:55:54.121
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:54.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:54.189
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Nov 17 13:55:54.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:07.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6054" for this suite. 11/17/23 13:56:07.402
------------------------------
â€¢ [SLOW TEST] [13.288 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:55:54.12
    Nov 17 13:55:54.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 13:55:54.121
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:55:54.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:55:54.189
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Nov 17 13:55:54.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:07.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6054" for this suite. 11/17/23 13:56:07.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:07.412
Nov 17 13:56:07.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 13:56:07.413
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:07.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:07.447
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 11/17/23 13:56:07.498
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 13:56:07.505
Nov 17 13:56:07.516: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:56:07.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:56:07.522: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:56:08.564: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:56:08.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 17 13:56:08.602: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 13:56:09.571: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 13:56:09.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 13:56:09.696: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 11/17/23 13:56:09.788
Nov 17 13:56:09.876: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 11/17/23 13:56:09.876
Nov 17 13:56:10.096: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 11/17/23 13:56:10.096
Nov 17 13:56:10.120: INFO: Observed &DaemonSet event: ADDED
Nov 17 13:56:10.120: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.157: INFO: Found daemon set daemon-set in namespace daemonsets-4552 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 17 13:56:10.157: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 11/17/23 13:56:10.157
STEP: watching for the daemon set status to be patched 11/17/23 13:56:10.198
Nov 17 13:56:10.225: INFO: Observed &DaemonSet event: ADDED
Nov 17 13:56:10.225: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.226: INFO: Observed daemon set daemon-set in namespace daemonsets-4552 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
Nov 17 13:56:10.226: INFO: Found daemon set daemon-set in namespace daemonsets-4552 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 17 13:56:10.227: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 13:56:10.28
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4552, will wait for the garbage collector to delete the pods 11/17/23 13:56:10.28
Nov 17 13:56:10.527: INFO: Deleting DaemonSet.extensions daemon-set took: 78.160658ms
Nov 17 13:56:10.927: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.189343ms
Nov 17 13:56:11.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 13:56:11.931: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 13:56:11.934: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21024"},"items":null}

Nov 17 13:56:11.936: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21024"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:11.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4552" for this suite. 11/17/23 13:56:11.952
------------------------------
â€¢ [4.547 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:07.412
    Nov 17 13:56:07.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 13:56:07.413
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:07.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:07.447
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 11/17/23 13:56:07.498
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 13:56:07.505
    Nov 17 13:56:07.516: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:56:07.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:56:07.522: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:56:08.564: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:56:08.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 17 13:56:08.602: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 13:56:09.571: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 13:56:09.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 13:56:09.696: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 11/17/23 13:56:09.788
    Nov 17 13:56:09.876: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 11/17/23 13:56:09.876
    Nov 17 13:56:10.096: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 11/17/23 13:56:10.096
    Nov 17 13:56:10.120: INFO: Observed &DaemonSet event: ADDED
    Nov 17 13:56:10.120: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.157: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.157: INFO: Found daemon set daemon-set in namespace daemonsets-4552 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 17 13:56:10.157: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 11/17/23 13:56:10.157
    STEP: watching for the daemon set status to be patched 11/17/23 13:56:10.198
    Nov 17 13:56:10.225: INFO: Observed &DaemonSet event: ADDED
    Nov 17 13:56:10.225: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.226: INFO: Observed daemon set daemon-set in namespace daemonsets-4552 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 17 13:56:10.226: INFO: Observed &DaemonSet event: MODIFIED
    Nov 17 13:56:10.226: INFO: Found daemon set daemon-set in namespace daemonsets-4552 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Nov 17 13:56:10.227: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 13:56:10.28
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4552, will wait for the garbage collector to delete the pods 11/17/23 13:56:10.28
    Nov 17 13:56:10.527: INFO: Deleting DaemonSet.extensions daemon-set took: 78.160658ms
    Nov 17 13:56:10.927: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.189343ms
    Nov 17 13:56:11.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 13:56:11.931: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 13:56:11.934: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21024"},"items":null}

    Nov 17 13:56:11.936: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21024"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:11.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4552" for this suite. 11/17/23 13:56:11.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:11.96
Nov 17 13:56:11.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 13:56:11.961
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:11.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:11.996
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/17/23 13:56:12.022
Nov 17 13:56:12.056: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4588" to be "running and ready"
Nov 17 13:56:12.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.664582ms
Nov 17 13:56:12.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:56:14.071: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015149745s
Nov 17 13:56:14.071: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 17 13:56:14.071: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 11/17/23 13:56:14.075
Nov 17 13:56:14.083: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4588" to be "running and ready"
Nov 17 13:56:14.089: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.941744ms
Nov 17 13:56:14.089: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:56:16.094: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011479655s
Nov 17 13:56:16.094: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Nov 17 13:56:16.094: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/17/23 13:56:16.097
STEP: delete the pod with lifecycle hook 11/17/23 13:56:16.118
Nov 17 13:56:16.125: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 17 13:56:16.129: INFO: Pod pod-with-poststart-http-hook still exists
Nov 17 13:56:18.130: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 17 13:56:18.134: INFO: Pod pod-with-poststart-http-hook still exists
Nov 17 13:56:20.130: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 17 13:56:20.134: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:20.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4588" for this suite. 11/17/23 13:56:20.139
------------------------------
â€¢ [SLOW TEST] [8.190 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:11.96
    Nov 17 13:56:11.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 13:56:11.961
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:11.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:11.996
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/17/23 13:56:12.022
    Nov 17 13:56:12.056: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4588" to be "running and ready"
    Nov 17 13:56:12.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.664582ms
    Nov 17 13:56:12.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:56:14.071: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015149745s
    Nov 17 13:56:14.071: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 17 13:56:14.071: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 11/17/23 13:56:14.075
    Nov 17 13:56:14.083: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4588" to be "running and ready"
    Nov 17 13:56:14.089: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.941744ms
    Nov 17 13:56:14.089: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:56:16.094: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011479655s
    Nov 17 13:56:16.094: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Nov 17 13:56:16.094: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/17/23 13:56:16.097
    STEP: delete the pod with lifecycle hook 11/17/23 13:56:16.118
    Nov 17 13:56:16.125: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 17 13:56:16.129: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 17 13:56:18.130: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 17 13:56:18.134: INFO: Pod pod-with-poststart-http-hook still exists
    Nov 17 13:56:20.130: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Nov 17 13:56:20.134: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:20.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4588" for this suite. 11/17/23 13:56:20.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:20.151
Nov 17 13:56:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 13:56:20.152
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:20.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:20.176
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 11/17/23 13:56:20.18
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_tcp@PTR;sleep 1; done
 11/17/23 13:56:20.211
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_tcp@PTR;sleep 1; done
 11/17/23 13:56:20.211
STEP: creating a pod to probe DNS 11/17/23 13:56:20.211
STEP: submitting the pod to kubernetes 11/17/23 13:56:20.211
Nov 17 13:56:20.232: INFO: Waiting up to 15m0s for pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074" in namespace "dns-6164" to be "running"
Nov 17 13:56:20.238: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5157ms
Nov 17 13:56:22.244: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074": Phase="Running", Reason="", readiness=true. Elapsed: 2.011375234s
Nov 17 13:56:22.244: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074" satisfied condition "running"
STEP: retrieving the pod 11/17/23 13:56:22.244
STEP: looking for the results for each expected name from probers 11/17/23 13:56:22.247
Nov 17 13:56:22.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.269: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.299: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.303: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.308: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.312: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:22.332: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:27.341: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.357: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.364: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.390: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.395: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.400: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.406: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:27.435: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:32.337: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.378: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.382: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.399: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:32.420: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:37.338: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.378: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.383: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:37.410: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:42.340: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.354: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.384: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.390: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.395: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.399: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:42.416: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:47.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.346: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.351: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.357: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.380: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.390: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.394: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
Nov 17 13:56:47.413: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

Nov 17 13:56:52.413: INFO: DNS probes using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 succeeded

STEP: deleting the pod 11/17/23 13:56:52.413
STEP: deleting the test service 11/17/23 13:56:52.446
STEP: deleting the test headless service 11/17/23 13:56:52.54
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:52.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6164" for this suite. 11/17/23 13:56:52.578
------------------------------
â€¢ [SLOW TEST] [32.443 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:20.151
    Nov 17 13:56:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 13:56:20.152
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:20.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:20.176
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 11/17/23 13:56:20.18
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_tcp@PTR;sleep 1; done
     11/17/23 13:56:20.211
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6164.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6164.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6164.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.62.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.62.119_tcp@PTR;sleep 1; done
     11/17/23 13:56:20.211
    STEP: creating a pod to probe DNS 11/17/23 13:56:20.211
    STEP: submitting the pod to kubernetes 11/17/23 13:56:20.211
    Nov 17 13:56:20.232: INFO: Waiting up to 15m0s for pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074" in namespace "dns-6164" to be "running"
    Nov 17 13:56:20.238: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5157ms
    Nov 17 13:56:22.244: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074": Phase="Running", Reason="", readiness=true. Elapsed: 2.011375234s
    Nov 17 13:56:22.244: INFO: Pod "dns-test-37c9f685-3bf3-4201-bcfe-73437b361074" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 13:56:22.244
    STEP: looking for the results for each expected name from probers 11/17/23 13:56:22.247
    Nov 17 13:56:22.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.259: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.269: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.299: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.303: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.308: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.312: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:22.332: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:27.341: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.357: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.364: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.390: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.395: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.400: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.406: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:27.435: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:32.337: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.378: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.382: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.399: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:32.420: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:37.338: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.355: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.378: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.383: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.387: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.392: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:37.410: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:42.340: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.344: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.354: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.384: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.390: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.395: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.399: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:42.416: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:47.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.346: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.351: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.357: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.380: INFO: Unable to read jessie_udp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.390: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.394: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local from pod dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074: the server could not find the requested resource (get pods dns-test-37c9f685-3bf3-4201-bcfe-73437b361074)
    Nov 17 13:56:47.413: INFO: Lookups using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 failed for: [wheezy_udp@dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@dns-test-service.dns-6164.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_udp@dns-test-service.dns-6164.svc.cluster.local jessie_tcp@dns-test-service.dns-6164.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6164.svc.cluster.local]

    Nov 17 13:56:52.413: INFO: DNS probes using dns-6164/dns-test-37c9f685-3bf3-4201-bcfe-73437b361074 succeeded

    STEP: deleting the pod 11/17/23 13:56:52.413
    STEP: deleting the test service 11/17/23 13:56:52.446
    STEP: deleting the test headless service 11/17/23 13:56:52.54
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:52.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6164" for this suite. 11/17/23 13:56:52.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:52.596
Nov 17 13:56:52.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename server-version 11/17/23 13:56:52.598
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:52.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:52.628
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 11/17/23 13:56:52.633
STEP: Confirm major version 11/17/23 13:56:52.635
Nov 17 13:56:52.635: INFO: Major version: 1
STEP: Confirm minor version 11/17/23 13:56:52.635
Nov 17 13:56:52.635: INFO: cleanMinorVersion: 26
Nov 17 13:56:52.635: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:52.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8242" for this suite. 11/17/23 13:56:52.64
------------------------------
â€¢ [0.053 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:52.596
    Nov 17 13:56:52.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename server-version 11/17/23 13:56:52.598
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:52.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:52.628
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 11/17/23 13:56:52.633
    STEP: Confirm major version 11/17/23 13:56:52.635
    Nov 17 13:56:52.635: INFO: Major version: 1
    STEP: Confirm minor version 11/17/23 13:56:52.635
    Nov 17 13:56:52.635: INFO: cleanMinorVersion: 26
    Nov 17 13:56:52.635: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:52.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8242" for this suite. 11/17/23 13:56:52.64
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:52.649
Nov 17 13:56:52.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename ephemeral-containers-test 11/17/23 13:56:52.65
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:52.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:52.678
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 11/17/23 13:56:52.682
Nov 17 13:56:52.695: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-244" to be "running and ready"
Nov 17 13:56:52.700: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575801ms
Nov 17 13:56:52.700: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:56:54.703: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007802258s
Nov 17 13:56:54.703: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Nov 17 13:56:54.703: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 11/17/23 13:56:54.705
Nov 17 13:56:54.720: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-244" to be "container debugger running"
Nov 17 13:56:54.722: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.560242ms
Nov 17 13:56:56.726: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006201655s
Nov 17 13:56:56.726: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 11/17/23 13:56:56.726
Nov 17 13:56:56.726: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-244 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 13:56:56.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 13:56:56.727: INFO: ExecWithOptions: Clientset creation
Nov 17 13:56:56.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-244/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Nov 17 13:56:56.837: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:56:56.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-244" for this suite. 11/17/23 13:56:56.848
------------------------------
â€¢ [4.207 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:52.649
    Nov 17 13:56:52.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename ephemeral-containers-test 11/17/23 13:56:52.65
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:52.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:52.678
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 11/17/23 13:56:52.682
    Nov 17 13:56:52.695: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-244" to be "running and ready"
    Nov 17 13:56:52.700: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575801ms
    Nov 17 13:56:52.700: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:56:54.703: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007802258s
    Nov 17 13:56:54.703: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Nov 17 13:56:54.703: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 11/17/23 13:56:54.705
    Nov 17 13:56:54.720: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-244" to be "container debugger running"
    Nov 17 13:56:54.722: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.560242ms
    Nov 17 13:56:56.726: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006201655s
    Nov 17 13:56:56.726: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 11/17/23 13:56:56.726
    Nov 17 13:56:56.726: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-244 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 13:56:56.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 13:56:56.727: INFO: ExecWithOptions: Clientset creation
    Nov 17 13:56:56.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-244/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Nov 17 13:56:56.837: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:56:56.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-244" for this suite. 11/17/23 13:56:56.848
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:56:56.856
Nov 17 13:56:56.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 13:56:56.857
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:56.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:56.876
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 11/17/23 13:56:56.879
Nov 17 13:56:56.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b" in namespace "projected-2285" to be "Succeeded or Failed"
Nov 17 13:56:56.894: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868779ms
Nov 17 13:56:58.899: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00949497s
Nov 17 13:57:00.900: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010287621s
STEP: Saw pod success 11/17/23 13:57:00.9
Nov 17 13:57:00.900: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b" satisfied condition "Succeeded or Failed"
Nov 17 13:57:00.904: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b container client-container: <nil>
STEP: delete the pod 11/17/23 13:57:00.911
Nov 17 13:57:00.929: INFO: Waiting for pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b to disappear
Nov 17 13:57:00.933: INFO: Pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 13:57:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2285" for this suite. 11/17/23 13:57:00.937
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:56:56.856
    Nov 17 13:56:56.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 13:56:56.857
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:56:56.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:56:56.876
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 11/17/23 13:56:56.879
    Nov 17 13:56:56.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b" in namespace "projected-2285" to be "Succeeded or Failed"
    Nov 17 13:56:56.894: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868779ms
    Nov 17 13:56:58.899: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00949497s
    Nov 17 13:57:00.900: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010287621s
    STEP: Saw pod success 11/17/23 13:57:00.9
    Nov 17 13:57:00.900: INFO: Pod "downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b" satisfied condition "Succeeded or Failed"
    Nov 17 13:57:00.904: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b container client-container: <nil>
    STEP: delete the pod 11/17/23 13:57:00.911
    Nov 17 13:57:00.929: INFO: Waiting for pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b to disappear
    Nov 17 13:57:00.933: INFO: Pod downwardapi-volume-fd2a5754-971a-4faa-b492-e7f5a91b574b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:57:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2285" for this suite. 11/17/23 13:57:00.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:57:00.95
Nov 17 13:57:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 13:57:00.952
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:00.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:00.978
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 13:57:01.003
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:57:01.273
STEP: Deploying the webhook pod 11/17/23 13:57:01.28
STEP: Wait for the deployment to be ready 11/17/23 13:57:01.298
Nov 17 13:57:01.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 13:57:03.328
STEP: Verifying the service has paired with the endpoint 11/17/23 13:57:03.351
Nov 17 13:57:04.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 11/17/23 13:57:04.473
STEP: Creating a configMap that should be mutated 11/17/23 13:57:04.496
STEP: Deleting the collection of validation webhooks 11/17/23 13:57:04.559
STEP: Creating a configMap that should not be mutated 11/17/23 13:57:04.676
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:57:04.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7739" for this suite. 11/17/23 13:57:04.88
STEP: Destroying namespace "webhook-7739-markers" for this suite. 11/17/23 13:57:04.907
------------------------------
â€¢ [3.991 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:57:00.95
    Nov 17 13:57:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 13:57:00.952
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:00.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:00.978
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 13:57:01.003
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 13:57:01.273
    STEP: Deploying the webhook pod 11/17/23 13:57:01.28
    STEP: Wait for the deployment to be ready 11/17/23 13:57:01.298
    Nov 17 13:57:01.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 13:57:03.328
    STEP: Verifying the service has paired with the endpoint 11/17/23 13:57:03.351
    Nov 17 13:57:04.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 11/17/23 13:57:04.473
    STEP: Creating a configMap that should be mutated 11/17/23 13:57:04.496
    STEP: Deleting the collection of validation webhooks 11/17/23 13:57:04.559
    STEP: Creating a configMap that should not be mutated 11/17/23 13:57:04.676
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:57:04.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7739" for this suite. 11/17/23 13:57:04.88
    STEP: Destroying namespace "webhook-7739-markers" for this suite. 11/17/23 13:57:04.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:57:04.946
Nov 17 13:57:04.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 13:57:04.948
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:05.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:05.015
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 11/17/23 13:57:05.019
Nov 17 13:57:05.043: INFO: Waiting up to 5m0s for pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6" in namespace "projected-4826" to be "running and ready"
Nov 17 13:57:05.054: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346983ms
Nov 17 13:57:05.054: INFO: The phase of Pod annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:57:07.058: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014607452s
Nov 17 13:57:07.058: INFO: The phase of Pod annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6 is Running (Ready = true)
Nov 17 13:57:07.058: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6" satisfied condition "running and ready"
Nov 17 13:57:07.582: INFO: Successfully updated pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 13:57:09.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4826" for this suite. 11/17/23 13:57:09.604
------------------------------
â€¢ [4.667 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:57:04.946
    Nov 17 13:57:04.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 13:57:04.948
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:05.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:05.015
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 11/17/23 13:57:05.019
    Nov 17 13:57:05.043: INFO: Waiting up to 5m0s for pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6" in namespace "projected-4826" to be "running and ready"
    Nov 17 13:57:05.054: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346983ms
    Nov 17 13:57:05.054: INFO: The phase of Pod annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:57:07.058: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014607452s
    Nov 17 13:57:07.058: INFO: The phase of Pod annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6 is Running (Ready = true)
    Nov 17 13:57:07.058: INFO: Pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6" satisfied condition "running and ready"
    Nov 17 13:57:07.582: INFO: Successfully updated pod "annotationupdate0143db00-29a7-417d-90bd-5987b7e496d6"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:57:09.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4826" for this suite. 11/17/23 13:57:09.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:57:09.615
Nov 17 13:57:09.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename init-container 11/17/23 13:57:09.616
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:09.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:09.644
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 11/17/23 13:57:09.649
Nov 17 13:57:09.649: INFO: PodSpec: initContainers in spec.initContainers
Nov 17 13:57:51.862: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9d21802b-bfa3-4cc1-b4d9-774aae5ca6df", GenerateName:"", Namespace:"init-container-8319", SelfLink:"", UID:"06901763-188c-45ce-96f7-417058e336fb", ResourceVersion:"21932", Generation:0, CreationTimestamp:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"649262156"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e2d290), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 17, 13, 57, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e2d2c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zw4fk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e87260), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0045f1818), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-worker-2.c.operations-lab.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003b26070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0045f1890)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0045f18b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0045f18b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0045f18bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0011f96a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.4", PodIP:"10.10.0.161", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.161"}}, StartTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b26150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b261c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://71537aecd4743fe02aced3065d41ac2f013b23886c3e3bc5d44634bf3848cc20", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e872e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e872c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0045f193f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 13:57:51.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8319" for this suite. 11/17/23 13:57:51.869
------------------------------
â€¢ [SLOW TEST] [42.262 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:57:09.615
    Nov 17 13:57:09.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename init-container 11/17/23 13:57:09.616
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:09.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:09.644
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 11/17/23 13:57:09.649
    Nov 17 13:57:09.649: INFO: PodSpec: initContainers in spec.initContainers
    Nov 17 13:57:51.862: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9d21802b-bfa3-4cc1-b4d9-774aae5ca6df", GenerateName:"", Namespace:"init-container-8319", SelfLink:"", UID:"06901763-188c-45ce-96f7-417058e336fb", ResourceVersion:"21932", Generation:0, CreationTimestamp:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"649262156"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e2d290), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.November, 17, 13, 57, 51, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e2d2c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zw4fk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e87260), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zw4fk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0045f1818), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-worker-2.c.operations-lab.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003b26070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0045f1890)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0045f18b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0045f18b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0045f18bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0011f96a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.4", PodIP:"10.10.0.161", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.161"}}, StartTime:time.Date(2023, time.November, 17, 13, 57, 9, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b26150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b261c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://71537aecd4743fe02aced3065d41ac2f013b23886c3e3bc5d44634bf3848cc20", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e872e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e872c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0045f193f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:57:51.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8319" for this suite. 11/17/23 13:57:51.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:57:51.878
Nov 17 13:57:51.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 13:57:51.88
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:51.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:51.905
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5088 11/17/23 13:57:51.91
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[] 11/17/23 13:57:51.929
Nov 17 13:57:51.938: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Nov 17 13:57:52.948: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5088 11/17/23 13:57:52.948
Nov 17 13:57:52.960: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5088" to be "running and ready"
Nov 17 13:57:52.972: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.153409ms
Nov 17 13:57:52.972: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:57:54.977: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016447587s
Nov 17 13:57:54.977: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 17 13:57:54.977: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod1:[80]] 11/17/23 13:57:54.98
Nov 17 13:57:54.991: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 11/17/23 13:57:54.991
Nov 17 13:57:54.991: INFO: Creating new exec pod
Nov 17 13:57:54.998: INFO: Waiting up to 5m0s for pod "execpodxtm8s" in namespace "services-5088" to be "running"
Nov 17 13:57:55.003: INFO: Pod "execpodxtm8s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86802ms
Nov 17 13:57:57.012: INFO: Pod "execpodxtm8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.014561097s
Nov 17 13:57:57.012: INFO: Pod "execpodxtm8s" satisfied condition "running"
Nov 17 13:57:58.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 17 13:57:58.219: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 17 13:57:58.219: INFO: stdout: ""
Nov 17 13:57:58.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
Nov 17 13:57:58.432: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
Nov 17 13:57:58.432: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5088 11/17/23 13:57:58.432
Nov 17 13:57:58.442: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5088" to be "running and ready"
Nov 17 13:57:58.448: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606731ms
Nov 17 13:57:58.448: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 13:58:00.453: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011604744s
Nov 17 13:58:00.453: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 17 13:58:00.453: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod1:[80] pod2:[80]] 11/17/23 13:58:00.457
Nov 17 13:58:00.470: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 11/17/23 13:58:00.47
Nov 17 13:58:01.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 17 13:58:01.669: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 17 13:58:01.669: INFO: stdout: ""
Nov 17 13:58:01.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
Nov 17 13:58:01.885: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
Nov 17 13:58:01.885: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5088 11/17/23 13:58:01.885
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod2:[80]] 11/17/23 13:58:01.915
Nov 17 13:58:01.979: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 11/17/23 13:58:01.98
Nov 17 13:58:02.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Nov 17 13:58:03.194: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 17 13:58:03.194: INFO: stdout: ""
Nov 17 13:58:03.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
Nov 17 13:58:03.426: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
Nov 17 13:58:03.426: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5088 11/17/23 13:58:03.426
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[] 11/17/23 13:58:03.462
Nov 17 13:58:04.504: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 13:58:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5088" for this suite. 11/17/23 13:58:04.554
------------------------------
â€¢ [SLOW TEST] [12.686 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:57:51.878
    Nov 17 13:57:51.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 13:57:51.88
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:57:51.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:57:51.905
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5088 11/17/23 13:57:51.91
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[] 11/17/23 13:57:51.929
    Nov 17 13:57:51.938: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Nov 17 13:57:52.948: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5088 11/17/23 13:57:52.948
    Nov 17 13:57:52.960: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5088" to be "running and ready"
    Nov 17 13:57:52.972: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.153409ms
    Nov 17 13:57:52.972: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:57:54.977: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016447587s
    Nov 17 13:57:54.977: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 17 13:57:54.977: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod1:[80]] 11/17/23 13:57:54.98
    Nov 17 13:57:54.991: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 11/17/23 13:57:54.991
    Nov 17 13:57:54.991: INFO: Creating new exec pod
    Nov 17 13:57:54.998: INFO: Waiting up to 5m0s for pod "execpodxtm8s" in namespace "services-5088" to be "running"
    Nov 17 13:57:55.003: INFO: Pod "execpodxtm8s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86802ms
    Nov 17 13:57:57.012: INFO: Pod "execpodxtm8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.014561097s
    Nov 17 13:57:57.012: INFO: Pod "execpodxtm8s" satisfied condition "running"
    Nov 17 13:57:58.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 17 13:57:58.219: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 17 13:57:58.219: INFO: stdout: ""
    Nov 17 13:57:58.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
    Nov 17 13:57:58.432: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
    Nov 17 13:57:58.432: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5088 11/17/23 13:57:58.432
    Nov 17 13:57:58.442: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5088" to be "running and ready"
    Nov 17 13:57:58.448: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.606731ms
    Nov 17 13:57:58.448: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 13:58:00.453: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011604744s
    Nov 17 13:58:00.453: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 17 13:58:00.453: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod1:[80] pod2:[80]] 11/17/23 13:58:00.457
    Nov 17 13:58:00.470: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 11/17/23 13:58:00.47
    Nov 17 13:58:01.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 17 13:58:01.669: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 17 13:58:01.669: INFO: stdout: ""
    Nov 17 13:58:01.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
    Nov 17 13:58:01.885: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
    Nov 17 13:58:01.885: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5088 11/17/23 13:58:01.885
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[pod2:[80]] 11/17/23 13:58:01.915
    Nov 17 13:58:01.979: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 11/17/23 13:58:01.98
    Nov 17 13:58:02.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Nov 17 13:58:03.194: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Nov 17 13:58:03.194: INFO: stdout: ""
    Nov 17 13:58:03.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5088 exec execpodxtm8s -- /bin/sh -x -c nc -v -z -w 2 10.107.48.217 80'
    Nov 17 13:58:03.426: INFO: stderr: "+ nc -v -z -w 2 10.107.48.217 80\nConnection to 10.107.48.217 80 port [tcp/http] succeeded!\n"
    Nov 17 13:58:03.426: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5088 11/17/23 13:58:03.426
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5088 to expose endpoints map[] 11/17/23 13:58:03.462
    Nov 17 13:58:04.504: INFO: successfully validated that service endpoint-test2 in namespace services-5088 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:58:04.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5088" for this suite. 11/17/23 13:58:04.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:58:04.567
Nov 17 13:58:04.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 13:58:04.569
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:04.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:04.598
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 13:58:04.602
Nov 17 13:58:04.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 17 13:58:04.698: INFO: stderr: ""
Nov 17 13:58:04.698: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 11/17/23 13:58:04.698
STEP: verifying the pod e2e-test-httpd-pod was created 11/17/23 13:58:09.75
Nov 17 13:58:09.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 get pod e2e-test-httpd-pod -o json'
Nov 17 13:58:09.841: INFO: stderr: ""
Nov 17 13:58:09.841: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-11-17T13:58:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3001\",\n        \"resourceVersion\": \"22098\",\n        \"uid\": \"298c2e18-8330-4cb4-a7f8-74aacb0511a0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5z2f6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-worker-2.c.operations-lab.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5z2f6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e2b4062f8412da363e3fa7593d4b048874cbf6ea6c220957e989328ef51a69a8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-17T13:58:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.93\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.93\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-17T13:58:04Z\"\n    }\n}\n"
STEP: replace the image in the pod 11/17/23 13:58:09.841
Nov 17 13:58:09.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 replace -f -'
Nov 17 13:58:10.424: INFO: stderr: ""
Nov 17 13:58:10.424: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/17/23 13:58:10.424
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Nov 17 13:58:10.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 delete pods e2e-test-httpd-pod'
Nov 17 13:58:11.963: INFO: stderr: ""
Nov 17 13:58:11.963: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 13:58:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3001" for this suite. 11/17/23 13:58:11.97
------------------------------
â€¢ [SLOW TEST] [7.411 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:58:04.567
    Nov 17 13:58:04.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 13:58:04.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:04.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:04.598
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 13:58:04.602
    Nov 17 13:58:04.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 17 13:58:04.698: INFO: stderr: ""
    Nov 17 13:58:04.698: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 11/17/23 13:58:04.698
    STEP: verifying the pod e2e-test-httpd-pod was created 11/17/23 13:58:09.75
    Nov 17 13:58:09.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 get pod e2e-test-httpd-pod -o json'
    Nov 17 13:58:09.841: INFO: stderr: ""
    Nov 17 13:58:09.841: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-11-17T13:58:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3001\",\n        \"resourceVersion\": \"22098\",\n        \"uid\": \"298c2e18-8330-4cb4-a7f8-74aacb0511a0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5z2f6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-worker-2.c.operations-lab.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5z2f6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-11-17T13:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e2b4062f8412da363e3fa7593d4b048874cbf6ea6c220957e989328ef51a69a8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-11-17T13:58:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.93\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.93\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-11-17T13:58:04Z\"\n    }\n}\n"
    STEP: replace the image in the pod 11/17/23 13:58:09.841
    Nov 17 13:58:09.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 replace -f -'
    Nov 17 13:58:10.424: INFO: stderr: ""
    Nov 17 13:58:10.424: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 11/17/23 13:58:10.424
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Nov 17 13:58:10.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3001 delete pods e2e-test-httpd-pod'
    Nov 17 13:58:11.963: INFO: stderr: ""
    Nov 17 13:58:11.963: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:58:11.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3001" for this suite. 11/17/23 13:58:11.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:58:11.978
Nov 17 13:58:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 13:58:11.979
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:11.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:12.001
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 11/17/23 13:58:12.01
STEP: delete the rc 11/17/23 13:58:17.025
STEP: wait for the rc to be deleted 11/17/23 13:58:17.04
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/17/23 13:58:22.046
STEP: Gathering metrics 11/17/23 13:58:52.06
Nov 17 13:58:52.091: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 13:58:52.094: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.308103ms
Nov 17 13:58:52.094: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 13:58:52.094: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 13:58:52.205: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 17 13:58:52.205: INFO: Deleting pod "simpletest.rc-22mhk" in namespace "gc-9722"
Nov 17 13:58:52.215: INFO: Deleting pod "simpletest.rc-2dj9q" in namespace "gc-9722"
Nov 17 13:58:52.263: INFO: Deleting pod "simpletest.rc-2h5r5" in namespace "gc-9722"
Nov 17 13:58:52.289: INFO: Deleting pod "simpletest.rc-4687v" in namespace "gc-9722"
Nov 17 13:58:52.312: INFO: Deleting pod "simpletest.rc-4h2mx" in namespace "gc-9722"
Nov 17 13:58:52.340: INFO: Deleting pod "simpletest.rc-4h8mz" in namespace "gc-9722"
Nov 17 13:58:52.363: INFO: Deleting pod "simpletest.rc-58jz4" in namespace "gc-9722"
Nov 17 13:58:52.380: INFO: Deleting pod "simpletest.rc-58xvn" in namespace "gc-9722"
Nov 17 13:58:52.405: INFO: Deleting pod "simpletest.rc-5hkll" in namespace "gc-9722"
Nov 17 13:58:52.435: INFO: Deleting pod "simpletest.rc-5wpdx" in namespace "gc-9722"
Nov 17 13:58:52.453: INFO: Deleting pod "simpletest.rc-6frxc" in namespace "gc-9722"
Nov 17 13:58:52.480: INFO: Deleting pod "simpletest.rc-6ncr7" in namespace "gc-9722"
Nov 17 13:58:52.506: INFO: Deleting pod "simpletest.rc-6psfg" in namespace "gc-9722"
Nov 17 13:58:52.543: INFO: Deleting pod "simpletest.rc-72tmj" in namespace "gc-9722"
Nov 17 13:58:52.568: INFO: Deleting pod "simpletest.rc-767r2" in namespace "gc-9722"
Nov 17 13:58:52.590: INFO: Deleting pod "simpletest.rc-7zzzs" in namespace "gc-9722"
Nov 17 13:58:52.620: INFO: Deleting pod "simpletest.rc-88c5j" in namespace "gc-9722"
Nov 17 13:58:52.646: INFO: Deleting pod "simpletest.rc-8z4q4" in namespace "gc-9722"
Nov 17 13:58:52.665: INFO: Deleting pod "simpletest.rc-96b4j" in namespace "gc-9722"
Nov 17 13:58:52.694: INFO: Deleting pod "simpletest.rc-9gldp" in namespace "gc-9722"
Nov 17 13:58:52.726: INFO: Deleting pod "simpletest.rc-9hks5" in namespace "gc-9722"
Nov 17 13:58:52.760: INFO: Deleting pod "simpletest.rc-9l24d" in namespace "gc-9722"
Nov 17 13:58:52.780: INFO: Deleting pod "simpletest.rc-9l9r9" in namespace "gc-9722"
Nov 17 13:58:52.816: INFO: Deleting pod "simpletest.rc-bg75l" in namespace "gc-9722"
Nov 17 13:58:52.869: INFO: Deleting pod "simpletest.rc-brsjp" in namespace "gc-9722"
Nov 17 13:58:52.896: INFO: Deleting pod "simpletest.rc-bxctp" in namespace "gc-9722"
Nov 17 13:58:52.930: INFO: Deleting pod "simpletest.rc-c55zp" in namespace "gc-9722"
Nov 17 13:58:52.952: INFO: Deleting pod "simpletest.rc-c7smd" in namespace "gc-9722"
Nov 17 13:58:52.979: INFO: Deleting pod "simpletest.rc-cd5td" in namespace "gc-9722"
Nov 17 13:58:53.002: INFO: Deleting pod "simpletest.rc-cdcv2" in namespace "gc-9722"
Nov 17 13:58:53.024: INFO: Deleting pod "simpletest.rc-chlq7" in namespace "gc-9722"
Nov 17 13:58:53.053: INFO: Deleting pod "simpletest.rc-clcgn" in namespace "gc-9722"
Nov 17 13:58:53.073: INFO: Deleting pod "simpletest.rc-cs994" in namespace "gc-9722"
Nov 17 13:58:53.101: INFO: Deleting pod "simpletest.rc-dcxcr" in namespace "gc-9722"
Nov 17 13:58:53.123: INFO: Deleting pod "simpletest.rc-dgfmq" in namespace "gc-9722"
Nov 17 13:58:53.139: INFO: Deleting pod "simpletest.rc-dtr68" in namespace "gc-9722"
Nov 17 13:58:53.164: INFO: Deleting pod "simpletest.rc-dz42l" in namespace "gc-9722"
Nov 17 13:58:53.186: INFO: Deleting pod "simpletest.rc-f4ztn" in namespace "gc-9722"
Nov 17 13:58:53.208: INFO: Deleting pod "simpletest.rc-f5v2t" in namespace "gc-9722"
Nov 17 13:58:53.232: INFO: Deleting pod "simpletest.rc-fd6qg" in namespace "gc-9722"
Nov 17 13:58:53.261: INFO: Deleting pod "simpletest.rc-fn82m" in namespace "gc-9722"
Nov 17 13:58:53.282: INFO: Deleting pod "simpletest.rc-g9vwj" in namespace "gc-9722"
Nov 17 13:58:53.297: INFO: Deleting pod "simpletest.rc-gbj4g" in namespace "gc-9722"
Nov 17 13:58:53.311: INFO: Deleting pod "simpletest.rc-ggvzn" in namespace "gc-9722"
Nov 17 13:58:53.346: INFO: Deleting pod "simpletest.rc-gk9tk" in namespace "gc-9722"
Nov 17 13:58:53.362: INFO: Deleting pod "simpletest.rc-h5kdp" in namespace "gc-9722"
Nov 17 13:58:53.384: INFO: Deleting pod "simpletest.rc-h5smc" in namespace "gc-9722"
Nov 17 13:58:53.407: INFO: Deleting pod "simpletest.rc-h6h2c" in namespace "gc-9722"
Nov 17 13:58:53.423: INFO: Deleting pod "simpletest.rc-hs7jv" in namespace "gc-9722"
Nov 17 13:58:53.438: INFO: Deleting pod "simpletest.rc-hs82c" in namespace "gc-9722"
Nov 17 13:58:53.454: INFO: Deleting pod "simpletest.rc-jdq8s" in namespace "gc-9722"
Nov 17 13:58:53.479: INFO: Deleting pod "simpletest.rc-jgqdg" in namespace "gc-9722"
Nov 17 13:58:53.496: INFO: Deleting pod "simpletest.rc-jnd7x" in namespace "gc-9722"
Nov 17 13:58:53.514: INFO: Deleting pod "simpletest.rc-jqcfp" in namespace "gc-9722"
Nov 17 13:58:53.530: INFO: Deleting pod "simpletest.rc-jv7zd" in namespace "gc-9722"
Nov 17 13:58:53.546: INFO: Deleting pod "simpletest.rc-jwzmc" in namespace "gc-9722"
Nov 17 13:58:53.559: INFO: Deleting pod "simpletest.rc-jzng8" in namespace "gc-9722"
Nov 17 13:58:53.574: INFO: Deleting pod "simpletest.rc-kdlz7" in namespace "gc-9722"
Nov 17 13:58:53.591: INFO: Deleting pod "simpletest.rc-khqz7" in namespace "gc-9722"
Nov 17 13:58:53.609: INFO: Deleting pod "simpletest.rc-ksbcz" in namespace "gc-9722"
Nov 17 13:58:53.629: INFO: Deleting pod "simpletest.rc-l9nhm" in namespace "gc-9722"
Nov 17 13:58:53.646: INFO: Deleting pod "simpletest.rc-lbktl" in namespace "gc-9722"
Nov 17 13:58:53.666: INFO: Deleting pod "simpletest.rc-lfx88" in namespace "gc-9722"
Nov 17 13:58:53.685: INFO: Deleting pod "simpletest.rc-lkk2c" in namespace "gc-9722"
Nov 17 13:58:53.704: INFO: Deleting pod "simpletest.rc-lkrnh" in namespace "gc-9722"
Nov 17 13:58:53.724: INFO: Deleting pod "simpletest.rc-ltkbd" in namespace "gc-9722"
Nov 17 13:58:53.743: INFO: Deleting pod "simpletest.rc-mbbj6" in namespace "gc-9722"
Nov 17 13:58:53.762: INFO: Deleting pod "simpletest.rc-mp5g4" in namespace "gc-9722"
Nov 17 13:58:53.784: INFO: Deleting pod "simpletest.rc-mp5mz" in namespace "gc-9722"
Nov 17 13:58:53.812: INFO: Deleting pod "simpletest.rc-mz6vx" in namespace "gc-9722"
Nov 17 13:58:53.838: INFO: Deleting pod "simpletest.rc-nhl75" in namespace "gc-9722"
Nov 17 13:58:53.862: INFO: Deleting pod "simpletest.rc-nlzhk" in namespace "gc-9722"
Nov 17 13:58:53.885: INFO: Deleting pod "simpletest.rc-ntk8f" in namespace "gc-9722"
Nov 17 13:58:53.903: INFO: Deleting pod "simpletest.rc-pcxl2" in namespace "gc-9722"
Nov 17 13:58:53.921: INFO: Deleting pod "simpletest.rc-pp648" in namespace "gc-9722"
Nov 17 13:58:53.945: INFO: Deleting pod "simpletest.rc-pqcb5" in namespace "gc-9722"
Nov 17 13:58:53.964: INFO: Deleting pod "simpletest.rc-prk8t" in namespace "gc-9722"
Nov 17 13:58:53.994: INFO: Deleting pod "simpletest.rc-pt2ws" in namespace "gc-9722"
Nov 17 13:58:54.019: INFO: Deleting pod "simpletest.rc-pv5w9" in namespace "gc-9722"
Nov 17 13:58:54.039: INFO: Deleting pod "simpletest.rc-q2b6v" in namespace "gc-9722"
Nov 17 13:58:54.063: INFO: Deleting pod "simpletest.rc-q5t5j" in namespace "gc-9722"
Nov 17 13:58:54.080: INFO: Deleting pod "simpletest.rc-r59cd" in namespace "gc-9722"
Nov 17 13:58:54.096: INFO: Deleting pod "simpletest.rc-r82kv" in namespace "gc-9722"
Nov 17 13:58:54.140: INFO: Deleting pod "simpletest.rc-rbcqv" in namespace "gc-9722"
Nov 17 13:58:54.199: INFO: Deleting pod "simpletest.rc-rflxm" in namespace "gc-9722"
Nov 17 13:58:54.332: INFO: Deleting pod "simpletest.rc-rrhqn" in namespace "gc-9722"
Nov 17 13:58:54.531: INFO: Deleting pod "simpletest.rc-stlpb" in namespace "gc-9722"
Nov 17 13:58:54.676: INFO: Deleting pod "simpletest.rc-tghr2" in namespace "gc-9722"
Nov 17 13:58:54.983: INFO: Deleting pod "simpletest.rc-tv9tm" in namespace "gc-9722"
Nov 17 13:58:55.044: INFO: Deleting pod "simpletest.rc-v7ngp" in namespace "gc-9722"
Nov 17 13:58:55.183: INFO: Deleting pod "simpletest.rc-vc4jz" in namespace "gc-9722"
Nov 17 13:58:55.264: INFO: Deleting pod "simpletest.rc-vpvpn" in namespace "gc-9722"
Nov 17 13:58:55.350: INFO: Deleting pod "simpletest.rc-vzqp9" in namespace "gc-9722"
Nov 17 13:58:55.374: INFO: Deleting pod "simpletest.rc-wl5kw" in namespace "gc-9722"
Nov 17 13:58:55.387: INFO: Deleting pod "simpletest.rc-wsvr5" in namespace "gc-9722"
Nov 17 13:58:55.401: INFO: Deleting pod "simpletest.rc-x9nzb" in namespace "gc-9722"
Nov 17 13:58:55.421: INFO: Deleting pod "simpletest.rc-xk9nd" in namespace "gc-9722"
Nov 17 13:58:55.434: INFO: Deleting pod "simpletest.rc-xxrl4" in namespace "gc-9722"
Nov 17 13:58:55.446: INFO: Deleting pod "simpletest.rc-xxxqw" in namespace "gc-9722"
Nov 17 13:58:55.463: INFO: Deleting pod "simpletest.rc-z4qmb" in namespace "gc-9722"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 13:58:55.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9722" for this suite. 11/17/23 13:58:55.488
------------------------------
â€¢ [SLOW TEST] [43.519 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:58:11.978
    Nov 17 13:58:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 13:58:11.979
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:11.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:12.001
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 11/17/23 13:58:12.01
    STEP: delete the rc 11/17/23 13:58:17.025
    STEP: wait for the rc to be deleted 11/17/23 13:58:17.04
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 11/17/23 13:58:22.046
    STEP: Gathering metrics 11/17/23 13:58:52.06
    Nov 17 13:58:52.091: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 13:58:52.094: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.308103ms
    Nov 17 13:58:52.094: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 13:58:52.094: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 13:58:52.205: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 17 13:58:52.205: INFO: Deleting pod "simpletest.rc-22mhk" in namespace "gc-9722"
    Nov 17 13:58:52.215: INFO: Deleting pod "simpletest.rc-2dj9q" in namespace "gc-9722"
    Nov 17 13:58:52.263: INFO: Deleting pod "simpletest.rc-2h5r5" in namespace "gc-9722"
    Nov 17 13:58:52.289: INFO: Deleting pod "simpletest.rc-4687v" in namespace "gc-9722"
    Nov 17 13:58:52.312: INFO: Deleting pod "simpletest.rc-4h2mx" in namespace "gc-9722"
    Nov 17 13:58:52.340: INFO: Deleting pod "simpletest.rc-4h8mz" in namespace "gc-9722"
    Nov 17 13:58:52.363: INFO: Deleting pod "simpletest.rc-58jz4" in namespace "gc-9722"
    Nov 17 13:58:52.380: INFO: Deleting pod "simpletest.rc-58xvn" in namespace "gc-9722"
    Nov 17 13:58:52.405: INFO: Deleting pod "simpletest.rc-5hkll" in namespace "gc-9722"
    Nov 17 13:58:52.435: INFO: Deleting pod "simpletest.rc-5wpdx" in namespace "gc-9722"
    Nov 17 13:58:52.453: INFO: Deleting pod "simpletest.rc-6frxc" in namespace "gc-9722"
    Nov 17 13:58:52.480: INFO: Deleting pod "simpletest.rc-6ncr7" in namespace "gc-9722"
    Nov 17 13:58:52.506: INFO: Deleting pod "simpletest.rc-6psfg" in namespace "gc-9722"
    Nov 17 13:58:52.543: INFO: Deleting pod "simpletest.rc-72tmj" in namespace "gc-9722"
    Nov 17 13:58:52.568: INFO: Deleting pod "simpletest.rc-767r2" in namespace "gc-9722"
    Nov 17 13:58:52.590: INFO: Deleting pod "simpletest.rc-7zzzs" in namespace "gc-9722"
    Nov 17 13:58:52.620: INFO: Deleting pod "simpletest.rc-88c5j" in namespace "gc-9722"
    Nov 17 13:58:52.646: INFO: Deleting pod "simpletest.rc-8z4q4" in namespace "gc-9722"
    Nov 17 13:58:52.665: INFO: Deleting pod "simpletest.rc-96b4j" in namespace "gc-9722"
    Nov 17 13:58:52.694: INFO: Deleting pod "simpletest.rc-9gldp" in namespace "gc-9722"
    Nov 17 13:58:52.726: INFO: Deleting pod "simpletest.rc-9hks5" in namespace "gc-9722"
    Nov 17 13:58:52.760: INFO: Deleting pod "simpletest.rc-9l24d" in namespace "gc-9722"
    Nov 17 13:58:52.780: INFO: Deleting pod "simpletest.rc-9l9r9" in namespace "gc-9722"
    Nov 17 13:58:52.816: INFO: Deleting pod "simpletest.rc-bg75l" in namespace "gc-9722"
    Nov 17 13:58:52.869: INFO: Deleting pod "simpletest.rc-brsjp" in namespace "gc-9722"
    Nov 17 13:58:52.896: INFO: Deleting pod "simpletest.rc-bxctp" in namespace "gc-9722"
    Nov 17 13:58:52.930: INFO: Deleting pod "simpletest.rc-c55zp" in namespace "gc-9722"
    Nov 17 13:58:52.952: INFO: Deleting pod "simpletest.rc-c7smd" in namespace "gc-9722"
    Nov 17 13:58:52.979: INFO: Deleting pod "simpletest.rc-cd5td" in namespace "gc-9722"
    Nov 17 13:58:53.002: INFO: Deleting pod "simpletest.rc-cdcv2" in namespace "gc-9722"
    Nov 17 13:58:53.024: INFO: Deleting pod "simpletest.rc-chlq7" in namespace "gc-9722"
    Nov 17 13:58:53.053: INFO: Deleting pod "simpletest.rc-clcgn" in namespace "gc-9722"
    Nov 17 13:58:53.073: INFO: Deleting pod "simpletest.rc-cs994" in namespace "gc-9722"
    Nov 17 13:58:53.101: INFO: Deleting pod "simpletest.rc-dcxcr" in namespace "gc-9722"
    Nov 17 13:58:53.123: INFO: Deleting pod "simpletest.rc-dgfmq" in namespace "gc-9722"
    Nov 17 13:58:53.139: INFO: Deleting pod "simpletest.rc-dtr68" in namespace "gc-9722"
    Nov 17 13:58:53.164: INFO: Deleting pod "simpletest.rc-dz42l" in namespace "gc-9722"
    Nov 17 13:58:53.186: INFO: Deleting pod "simpletest.rc-f4ztn" in namespace "gc-9722"
    Nov 17 13:58:53.208: INFO: Deleting pod "simpletest.rc-f5v2t" in namespace "gc-9722"
    Nov 17 13:58:53.232: INFO: Deleting pod "simpletest.rc-fd6qg" in namespace "gc-9722"
    Nov 17 13:58:53.261: INFO: Deleting pod "simpletest.rc-fn82m" in namespace "gc-9722"
    Nov 17 13:58:53.282: INFO: Deleting pod "simpletest.rc-g9vwj" in namespace "gc-9722"
    Nov 17 13:58:53.297: INFO: Deleting pod "simpletest.rc-gbj4g" in namespace "gc-9722"
    Nov 17 13:58:53.311: INFO: Deleting pod "simpletest.rc-ggvzn" in namespace "gc-9722"
    Nov 17 13:58:53.346: INFO: Deleting pod "simpletest.rc-gk9tk" in namespace "gc-9722"
    Nov 17 13:58:53.362: INFO: Deleting pod "simpletest.rc-h5kdp" in namespace "gc-9722"
    Nov 17 13:58:53.384: INFO: Deleting pod "simpletest.rc-h5smc" in namespace "gc-9722"
    Nov 17 13:58:53.407: INFO: Deleting pod "simpletest.rc-h6h2c" in namespace "gc-9722"
    Nov 17 13:58:53.423: INFO: Deleting pod "simpletest.rc-hs7jv" in namespace "gc-9722"
    Nov 17 13:58:53.438: INFO: Deleting pod "simpletest.rc-hs82c" in namespace "gc-9722"
    Nov 17 13:58:53.454: INFO: Deleting pod "simpletest.rc-jdq8s" in namespace "gc-9722"
    Nov 17 13:58:53.479: INFO: Deleting pod "simpletest.rc-jgqdg" in namespace "gc-9722"
    Nov 17 13:58:53.496: INFO: Deleting pod "simpletest.rc-jnd7x" in namespace "gc-9722"
    Nov 17 13:58:53.514: INFO: Deleting pod "simpletest.rc-jqcfp" in namespace "gc-9722"
    Nov 17 13:58:53.530: INFO: Deleting pod "simpletest.rc-jv7zd" in namespace "gc-9722"
    Nov 17 13:58:53.546: INFO: Deleting pod "simpletest.rc-jwzmc" in namespace "gc-9722"
    Nov 17 13:58:53.559: INFO: Deleting pod "simpletest.rc-jzng8" in namespace "gc-9722"
    Nov 17 13:58:53.574: INFO: Deleting pod "simpletest.rc-kdlz7" in namespace "gc-9722"
    Nov 17 13:58:53.591: INFO: Deleting pod "simpletest.rc-khqz7" in namespace "gc-9722"
    Nov 17 13:58:53.609: INFO: Deleting pod "simpletest.rc-ksbcz" in namespace "gc-9722"
    Nov 17 13:58:53.629: INFO: Deleting pod "simpletest.rc-l9nhm" in namespace "gc-9722"
    Nov 17 13:58:53.646: INFO: Deleting pod "simpletest.rc-lbktl" in namespace "gc-9722"
    Nov 17 13:58:53.666: INFO: Deleting pod "simpletest.rc-lfx88" in namespace "gc-9722"
    Nov 17 13:58:53.685: INFO: Deleting pod "simpletest.rc-lkk2c" in namespace "gc-9722"
    Nov 17 13:58:53.704: INFO: Deleting pod "simpletest.rc-lkrnh" in namespace "gc-9722"
    Nov 17 13:58:53.724: INFO: Deleting pod "simpletest.rc-ltkbd" in namespace "gc-9722"
    Nov 17 13:58:53.743: INFO: Deleting pod "simpletest.rc-mbbj6" in namespace "gc-9722"
    Nov 17 13:58:53.762: INFO: Deleting pod "simpletest.rc-mp5g4" in namespace "gc-9722"
    Nov 17 13:58:53.784: INFO: Deleting pod "simpletest.rc-mp5mz" in namespace "gc-9722"
    Nov 17 13:58:53.812: INFO: Deleting pod "simpletest.rc-mz6vx" in namespace "gc-9722"
    Nov 17 13:58:53.838: INFO: Deleting pod "simpletest.rc-nhl75" in namespace "gc-9722"
    Nov 17 13:58:53.862: INFO: Deleting pod "simpletest.rc-nlzhk" in namespace "gc-9722"
    Nov 17 13:58:53.885: INFO: Deleting pod "simpletest.rc-ntk8f" in namespace "gc-9722"
    Nov 17 13:58:53.903: INFO: Deleting pod "simpletest.rc-pcxl2" in namespace "gc-9722"
    Nov 17 13:58:53.921: INFO: Deleting pod "simpletest.rc-pp648" in namespace "gc-9722"
    Nov 17 13:58:53.945: INFO: Deleting pod "simpletest.rc-pqcb5" in namespace "gc-9722"
    Nov 17 13:58:53.964: INFO: Deleting pod "simpletest.rc-prk8t" in namespace "gc-9722"
    Nov 17 13:58:53.994: INFO: Deleting pod "simpletest.rc-pt2ws" in namespace "gc-9722"
    Nov 17 13:58:54.019: INFO: Deleting pod "simpletest.rc-pv5w9" in namespace "gc-9722"
    Nov 17 13:58:54.039: INFO: Deleting pod "simpletest.rc-q2b6v" in namespace "gc-9722"
    Nov 17 13:58:54.063: INFO: Deleting pod "simpletest.rc-q5t5j" in namespace "gc-9722"
    Nov 17 13:58:54.080: INFO: Deleting pod "simpletest.rc-r59cd" in namespace "gc-9722"
    Nov 17 13:58:54.096: INFO: Deleting pod "simpletest.rc-r82kv" in namespace "gc-9722"
    Nov 17 13:58:54.140: INFO: Deleting pod "simpletest.rc-rbcqv" in namespace "gc-9722"
    Nov 17 13:58:54.199: INFO: Deleting pod "simpletest.rc-rflxm" in namespace "gc-9722"
    Nov 17 13:58:54.332: INFO: Deleting pod "simpletest.rc-rrhqn" in namespace "gc-9722"
    Nov 17 13:58:54.531: INFO: Deleting pod "simpletest.rc-stlpb" in namespace "gc-9722"
    Nov 17 13:58:54.676: INFO: Deleting pod "simpletest.rc-tghr2" in namespace "gc-9722"
    Nov 17 13:58:54.983: INFO: Deleting pod "simpletest.rc-tv9tm" in namespace "gc-9722"
    Nov 17 13:58:55.044: INFO: Deleting pod "simpletest.rc-v7ngp" in namespace "gc-9722"
    Nov 17 13:58:55.183: INFO: Deleting pod "simpletest.rc-vc4jz" in namespace "gc-9722"
    Nov 17 13:58:55.264: INFO: Deleting pod "simpletest.rc-vpvpn" in namespace "gc-9722"
    Nov 17 13:58:55.350: INFO: Deleting pod "simpletest.rc-vzqp9" in namespace "gc-9722"
    Nov 17 13:58:55.374: INFO: Deleting pod "simpletest.rc-wl5kw" in namespace "gc-9722"
    Nov 17 13:58:55.387: INFO: Deleting pod "simpletest.rc-wsvr5" in namespace "gc-9722"
    Nov 17 13:58:55.401: INFO: Deleting pod "simpletest.rc-x9nzb" in namespace "gc-9722"
    Nov 17 13:58:55.421: INFO: Deleting pod "simpletest.rc-xk9nd" in namespace "gc-9722"
    Nov 17 13:58:55.434: INFO: Deleting pod "simpletest.rc-xxrl4" in namespace "gc-9722"
    Nov 17 13:58:55.446: INFO: Deleting pod "simpletest.rc-xxxqw" in namespace "gc-9722"
    Nov 17 13:58:55.463: INFO: Deleting pod "simpletest.rc-z4qmb" in namespace "gc-9722"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:58:55.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9722" for this suite. 11/17/23 13:58:55.488
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:58:55.499
Nov 17 13:58:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 13:58:55.5
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:55.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:55.529
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 11/17/23 13:58:55.533
Nov 17 13:58:55.546: INFO: Waiting up to 5m0s for pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404" in namespace "downward-api-9389" to be "Succeeded or Failed"
Nov 17 13:58:55.552: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822512ms
Nov 17 13:58:57.557: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010897215s
Nov 17 13:58:59.557: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011212845s
Nov 17 13:59:01.556: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010157887s
Nov 17 13:59:03.559: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013249873s
STEP: Saw pod success 11/17/23 13:59:03.56
Nov 17 13:59:03.560: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404" satisfied condition "Succeeded or Failed"
Nov 17 13:59:03.565: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 container dapi-container: <nil>
STEP: delete the pod 11/17/23 13:59:03.588
Nov 17 13:59:03.605: INFO: Waiting for pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 to disappear
Nov 17 13:59:03.610: INFO: Pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 17 13:59:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9389" for this suite. 11/17/23 13:59:03.617
------------------------------
â€¢ [SLOW TEST] [8.128 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:58:55.499
    Nov 17 13:58:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 13:58:55.5
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:58:55.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:58:55.529
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 11/17/23 13:58:55.533
    Nov 17 13:58:55.546: INFO: Waiting up to 5m0s for pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404" in namespace "downward-api-9389" to be "Succeeded or Failed"
    Nov 17 13:58:55.552: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822512ms
    Nov 17 13:58:57.557: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010897215s
    Nov 17 13:58:59.557: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011212845s
    Nov 17 13:59:01.556: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010157887s
    Nov 17 13:59:03.559: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013249873s
    STEP: Saw pod success 11/17/23 13:59:03.56
    Nov 17 13:59:03.560: INFO: Pod "downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404" satisfied condition "Succeeded or Failed"
    Nov 17 13:59:03.565: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 13:59:03.588
    Nov 17 13:59:03.605: INFO: Waiting for pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 to disappear
    Nov 17 13:59:03.610: INFO: Pod downward-api-6f96ce5e-e089-44e8-adfb-001a1f772404 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:59:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9389" for this suite. 11/17/23 13:59:03.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:59:03.637
Nov 17 13:59:03.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 13:59:03.638
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:03.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:03.666
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-8417 11/17/23 13:59:03.671
STEP: creating service affinity-clusterip-transition in namespace services-8417 11/17/23 13:59:03.671
STEP: creating replication controller affinity-clusterip-transition in namespace services-8417 11/17/23 13:59:03.69
I1117 13:59:03.706704      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8417, replica count: 3
I1117 13:59:06.758124      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1117 13:59:09.758460      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 13:59:09.764: INFO: Creating new exec pod
Nov 17 13:59:09.771: INFO: Waiting up to 5m0s for pod "execpod-affinity9kkfk" in namespace "services-8417" to be "running"
Nov 17 13:59:09.774: INFO: Pod "execpod-affinity9kkfk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486762ms
Nov 17 13:59:11.779: INFO: Pod "execpod-affinity9kkfk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007900745s
Nov 17 13:59:11.779: INFO: Pod "execpod-affinity9kkfk" satisfied condition "running"
Nov 17 13:59:12.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Nov 17 13:59:12.969: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 17 13:59:12.969: INFO: stdout: ""
Nov 17 13:59:12.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c nc -v -z -w 2 10.106.4.13 80'
Nov 17 13:59:13.163: INFO: stderr: "+ nc -v -z -w 2 10.106.4.13 80\nConnection to 10.106.4.13 80 port [tcp/http] succeeded!\n"
Nov 17 13:59:13.163: INFO: stdout: ""
Nov 17 13:59:13.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.4.13:80/ ; done'
Nov 17 13:59:13.490: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n"
Nov 17 13:59:13.490: INFO: stdout: "\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj"
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
Nov 17 13:59:13.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.4.13:80/ ; done'
Nov 17 13:59:13.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n"
Nov 17 13:59:13.802: INFO: stdout: "\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k"
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
Nov 17 13:59:13.802: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8417, will wait for the garbage collector to delete the pods 11/17/23 13:59:13.819
Nov 17 13:59:13.882: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.107656ms
Nov 17 13:59:13.983: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.403204ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 13:59:16.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8417" for this suite. 11/17/23 13:59:16.22
------------------------------
â€¢ [SLOW TEST] [12.599 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:59:03.637
    Nov 17 13:59:03.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 13:59:03.638
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:03.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:03.666
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-8417 11/17/23 13:59:03.671
    STEP: creating service affinity-clusterip-transition in namespace services-8417 11/17/23 13:59:03.671
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8417 11/17/23 13:59:03.69
    I1117 13:59:03.706704      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8417, replica count: 3
    I1117 13:59:06.758124      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1117 13:59:09.758460      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 13:59:09.764: INFO: Creating new exec pod
    Nov 17 13:59:09.771: INFO: Waiting up to 5m0s for pod "execpod-affinity9kkfk" in namespace "services-8417" to be "running"
    Nov 17 13:59:09.774: INFO: Pod "execpod-affinity9kkfk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486762ms
    Nov 17 13:59:11.779: INFO: Pod "execpod-affinity9kkfk": Phase="Running", Reason="", readiness=true. Elapsed: 2.007900745s
    Nov 17 13:59:11.779: INFO: Pod "execpod-affinity9kkfk" satisfied condition "running"
    Nov 17 13:59:12.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Nov 17 13:59:12.969: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Nov 17 13:59:12.969: INFO: stdout: ""
    Nov 17 13:59:12.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c nc -v -z -w 2 10.106.4.13 80'
    Nov 17 13:59:13.163: INFO: stderr: "+ nc -v -z -w 2 10.106.4.13 80\nConnection to 10.106.4.13 80 port [tcp/http] succeeded!\n"
    Nov 17 13:59:13.163: INFO: stdout: ""
    Nov 17 13:59:13.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.4.13:80/ ; done'
    Nov 17 13:59:13.490: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n"
    Nov 17 13:59:13.490: INFO: stdout: "\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-ddzdj\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-p5rv8\naffinity-clusterip-transition-ddzdj"
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-p5rv8
    Nov 17 13:59:13.490: INFO: Received response from host: affinity-clusterip-transition-ddzdj
    Nov 17 13:59:13.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-8417 exec execpod-affinity9kkfk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.4.13:80/ ; done'
    Nov 17 13:59:13.802: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.4.13:80/\n"
    Nov 17 13:59:13.802: INFO: stdout: "\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k\naffinity-clusterip-transition-mw96k"
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Received response from host: affinity-clusterip-transition-mw96k
    Nov 17 13:59:13.802: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8417, will wait for the garbage collector to delete the pods 11/17/23 13:59:13.819
    Nov 17 13:59:13.882: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.107656ms
    Nov 17 13:59:13.983: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.403204ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:59:16.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8417" for this suite. 11/17/23 13:59:16.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:59:16.242
Nov 17 13:59:16.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename subpath 11/17/23 13:59:16.244
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:16.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:16.275
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/17/23 13:59:16.278
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-8klf 11/17/23 13:59:16.291
STEP: Creating a pod to test atomic-volume-subpath 11/17/23 13:59:16.292
Nov 17 13:59:16.304: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8klf" in namespace "subpath-9035" to be "Succeeded or Failed"
Nov 17 13:59:16.310: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.145557ms
Nov 17 13:59:18.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009782208s
Nov 17 13:59:20.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011097361s
Nov 17 13:59:22.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 6.010727604s
Nov 17 13:59:24.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 8.010649579s
Nov 17 13:59:26.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 10.010412434s
Nov 17 13:59:28.322: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 12.018392924s
Nov 17 13:59:30.317: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 14.013548119s
Nov 17 13:59:32.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 16.012292805s
Nov 17 13:59:34.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 18.012545112s
Nov 17 13:59:36.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 20.01244987s
Nov 17 13:59:38.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=false. Elapsed: 22.011433156s
Nov 17 13:59:40.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01058073s
STEP: Saw pod success 11/17/23 13:59:40.315
Nov 17 13:59:40.315: INFO: Pod "pod-subpath-test-configmap-8klf" satisfied condition "Succeeded or Failed"
Nov 17 13:59:40.320: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-configmap-8klf container test-container-subpath-configmap-8klf: <nil>
STEP: delete the pod 11/17/23 13:59:40.333
Nov 17 13:59:40.346: INFO: Waiting for pod pod-subpath-test-configmap-8klf to disappear
Nov 17 13:59:40.349: INFO: Pod pod-subpath-test-configmap-8klf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8klf 11/17/23 13:59:40.349
Nov 17 13:59:40.350: INFO: Deleting pod "pod-subpath-test-configmap-8klf" in namespace "subpath-9035"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 17 13:59:40.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9035" for this suite. 11/17/23 13:59:40.363
------------------------------
â€¢ [SLOW TEST] [24.127 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:59:16.242
    Nov 17 13:59:16.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename subpath 11/17/23 13:59:16.244
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:16.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:16.275
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/17/23 13:59:16.278
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-8klf 11/17/23 13:59:16.291
    STEP: Creating a pod to test atomic-volume-subpath 11/17/23 13:59:16.292
    Nov 17 13:59:16.304: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8klf" in namespace "subpath-9035" to be "Succeeded or Failed"
    Nov 17 13:59:16.310: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.145557ms
    Nov 17 13:59:18.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009782208s
    Nov 17 13:59:20.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011097361s
    Nov 17 13:59:22.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 6.010727604s
    Nov 17 13:59:24.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 8.010649579s
    Nov 17 13:59:26.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 10.010412434s
    Nov 17 13:59:28.322: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 12.018392924s
    Nov 17 13:59:30.317: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 14.013548119s
    Nov 17 13:59:32.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 16.012292805s
    Nov 17 13:59:34.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 18.012545112s
    Nov 17 13:59:36.316: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=true. Elapsed: 20.01244987s
    Nov 17 13:59:38.315: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Running", Reason="", readiness=false. Elapsed: 22.011433156s
    Nov 17 13:59:40.314: INFO: Pod "pod-subpath-test-configmap-8klf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.01058073s
    STEP: Saw pod success 11/17/23 13:59:40.315
    Nov 17 13:59:40.315: INFO: Pod "pod-subpath-test-configmap-8klf" satisfied condition "Succeeded or Failed"
    Nov 17 13:59:40.320: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-configmap-8klf container test-container-subpath-configmap-8klf: <nil>
    STEP: delete the pod 11/17/23 13:59:40.333
    Nov 17 13:59:40.346: INFO: Waiting for pod pod-subpath-test-configmap-8klf to disappear
    Nov 17 13:59:40.349: INFO: Pod pod-subpath-test-configmap-8klf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8klf 11/17/23 13:59:40.349
    Nov 17 13:59:40.350: INFO: Deleting pod "pod-subpath-test-configmap-8klf" in namespace "subpath-9035"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:59:40.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9035" for this suite. 11/17/23 13:59:40.363
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:59:40.37
Nov 17 13:59:40.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 13:59:40.372
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:40.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:40.397
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-3105b6a8-b31d-49b3-bc68-297d4e624093 11/17/23 13:59:40.401
STEP: Creating a pod to test consume configMaps 11/17/23 13:59:40.406
Nov 17 13:59:40.414: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819" in namespace "configmap-2602" to be "Succeeded or Failed"
Nov 17 13:59:40.425: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Pending", Reason="", readiness=false. Elapsed: 10.256309ms
Nov 17 13:59:42.431: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016337379s
Nov 17 13:59:44.429: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014838517s
STEP: Saw pod success 11/17/23 13:59:44.429
Nov 17 13:59:44.430: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819" satisfied condition "Succeeded or Failed"
Nov 17 13:59:44.433: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 13:59:44.441
Nov 17 13:59:44.456: INFO: Waiting for pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 to disappear
Nov 17 13:59:44.459: INFO: Pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 13:59:44.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2602" for this suite. 11/17/23 13:59:44.464
------------------------------
â€¢ [4.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:59:40.37
    Nov 17 13:59:40.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 13:59:40.372
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:40.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:40.397
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-3105b6a8-b31d-49b3-bc68-297d4e624093 11/17/23 13:59:40.401
    STEP: Creating a pod to test consume configMaps 11/17/23 13:59:40.406
    Nov 17 13:59:40.414: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819" in namespace "configmap-2602" to be "Succeeded or Failed"
    Nov 17 13:59:40.425: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Pending", Reason="", readiness=false. Elapsed: 10.256309ms
    Nov 17 13:59:42.431: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016337379s
    Nov 17 13:59:44.429: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014838517s
    STEP: Saw pod success 11/17/23 13:59:44.429
    Nov 17 13:59:44.430: INFO: Pod "pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819" satisfied condition "Succeeded or Failed"
    Nov 17 13:59:44.433: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 13:59:44.441
    Nov 17 13:59:44.456: INFO: Waiting for pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 to disappear
    Nov 17 13:59:44.459: INFO: Pod pod-configmaps-bf0f0c29-4f53-482d-bbe4-ae3c1c4e7819 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 13:59:44.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2602" for this suite. 11/17/23 13:59:44.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 13:59:44.476
Nov 17 13:59:44.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename taint-single-pod 11/17/23 13:59:44.479
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:44.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:44.501
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Nov 17 13:59:44.505: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:00:44.561: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Nov 17 14:00:44.565: INFO: Starting informer...
STEP: Starting pod... 11/17/23 14:00:44.565
Nov 17 14:00:44.786: INFO: Pod is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
STEP: Trying to apply a taint on the Node 11/17/23 14:00:44.786
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:00:44.805
STEP: Waiting short time to make sure Pod is queued for deletion 11/17/23 14:00:44.815
Nov 17 14:00:44.815: INFO: Pod wasn't evicted. Proceeding
Nov 17 14:00:44.815: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:00:44.854
STEP: Waiting some time to make sure that toleration time passed. 11/17/23 14:00:44.884
Nov 17 14:01:59.885: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:01:59.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3857" for this suite. 11/17/23 14:01:59.89
------------------------------
â€¢ [SLOW TEST] [135.419 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 13:59:44.476
    Nov 17 13:59:44.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename taint-single-pod 11/17/23 13:59:44.479
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 13:59:44.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 13:59:44.501
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Nov 17 13:59:44.505: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:00:44.561: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Nov 17 14:00:44.565: INFO: Starting informer...
    STEP: Starting pod... 11/17/23 14:00:44.565
    Nov 17 14:00:44.786: INFO: Pod is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 11/17/23 14:00:44.786
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:00:44.805
    STEP: Waiting short time to make sure Pod is queued for deletion 11/17/23 14:00:44.815
    Nov 17 14:00:44.815: INFO: Pod wasn't evicted. Proceeding
    Nov 17 14:00:44.815: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:00:44.854
    STEP: Waiting some time to make sure that toleration time passed. 11/17/23 14:00:44.884
    Nov 17 14:01:59.885: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:01:59.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3857" for this suite. 11/17/23 14:01:59.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:01:59.898
Nov 17 14:01:59.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:01:59.899
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:01:59.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:01:59.92
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-36888e31-f4c6-4eee-ad61-c12401ec4a3c 11/17/23 14:01:59.924
STEP: Creating a pod to test consume secrets 11/17/23 14:01:59.929
Nov 17 14:01:59.938: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66" in namespace "projected-2280" to be "Succeeded or Failed"
Nov 17 14:01:59.946: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.339277ms
Nov 17 14:02:01.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012880816s
Nov 17 14:02:03.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013466185s
STEP: Saw pod success 11/17/23 14:02:03.951
Nov 17 14:02:03.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66" satisfied condition "Succeeded or Failed"
Nov 17 14:02:03.954: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:02:03.967
Nov 17 14:02:03.981: INFO: Waiting for pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 to disappear
Nov 17 14:02:03.984: INFO: Pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2280" for this suite. 11/17/23 14:02:03.988
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:01:59.898
    Nov 17 14:01:59.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:01:59.899
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:01:59.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:01:59.92
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-36888e31-f4c6-4eee-ad61-c12401ec4a3c 11/17/23 14:01:59.924
    STEP: Creating a pod to test consume secrets 11/17/23 14:01:59.929
    Nov 17 14:01:59.938: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66" in namespace "projected-2280" to be "Succeeded or Failed"
    Nov 17 14:01:59.946: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.339277ms
    Nov 17 14:02:01.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012880816s
    Nov 17 14:02:03.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013466185s
    STEP: Saw pod success 11/17/23 14:02:03.951
    Nov 17 14:02:03.951: INFO: Pod "pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66" satisfied condition "Succeeded or Failed"
    Nov 17 14:02:03.954: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:02:03.967
    Nov 17 14:02:03.981: INFO: Waiting for pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 to disappear
    Nov 17 14:02:03.984: INFO: Pod pod-projected-secrets-ad3390c4-78a4-4b3f-89f7-9fc741107f66 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2280" for this suite. 11/17/23 14:02:03.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:03.997
Nov 17 14:02:03.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 14:02:03.999
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:04.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:04.021
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 11/17/23 14:02:04.024
STEP: Verify that the required pods have come up 11/17/23 14:02:04.03
Nov 17 14:02:04.033: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 17 14:02:09.038: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 11/17/23 14:02:09.038
Nov 17 14:02:09.040: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 11/17/23 14:02:09.041
STEP: DeleteCollection of the ReplicaSets 11/17/23 14:02:09.049
STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/17/23 14:02:09.058
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3112" for this suite. 11/17/23 14:02:09.092
------------------------------
â€¢ [SLOW TEST] [5.106 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:03.997
    Nov 17 14:02:03.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 14:02:03.999
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:04.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:04.021
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 11/17/23 14:02:04.024
    STEP: Verify that the required pods have come up 11/17/23 14:02:04.03
    Nov 17 14:02:04.033: INFO: Pod name sample-pod: Found 0 pods out of 3
    Nov 17 14:02:09.038: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 11/17/23 14:02:09.038
    Nov 17 14:02:09.040: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 11/17/23 14:02:09.041
    STEP: DeleteCollection of the ReplicaSets 11/17/23 14:02:09.049
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 11/17/23 14:02:09.058
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3112" for this suite. 11/17/23 14:02:09.092
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:09.104
Nov 17 14:02:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:02:09.106
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:09.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:09.15
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:02:09.164
Nov 17 14:02:09.178: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2228" to be "running and ready"
Nov 17 14:02:09.186: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.789037ms
Nov 17 14:02:09.187: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:02:11.194: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016208388s
Nov 17 14:02:11.194: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 17 14:02:11.194: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 11/17/23 14:02:11.198
Nov 17 14:02:11.206: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2228" to be "running and ready"
Nov 17 14:02:11.211: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.864286ms
Nov 17 14:02:11.211: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:02:13.215: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759371s
Nov 17 14:02:13.215: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Nov 17 14:02:13.215: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/17/23 14:02:13.218
Nov 17 14:02:13.224: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 17 14:02:13.227: INFO: Pod pod-with-prestop-http-hook still exists
Nov 17 14:02:15.228: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 17 14:02:15.233: INFO: Pod pod-with-prestop-http-hook still exists
Nov 17 14:02:17.229: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 17 14:02:17.234: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 11/17/23 14:02:17.234
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2228" for this suite. 11/17/23 14:02:17.248
------------------------------
â€¢ [SLOW TEST] [8.149 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:09.104
    Nov 17 14:02:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:02:09.106
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:09.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:09.15
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:02:09.164
    Nov 17 14:02:09.178: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2228" to be "running and ready"
    Nov 17 14:02:09.186: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.789037ms
    Nov 17 14:02:09.187: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:02:11.194: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016208388s
    Nov 17 14:02:11.194: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 17 14:02:11.194: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 11/17/23 14:02:11.198
    Nov 17 14:02:11.206: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2228" to be "running and ready"
    Nov 17 14:02:11.211: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.864286ms
    Nov 17 14:02:11.211: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:02:13.215: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759371s
    Nov 17 14:02:13.215: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Nov 17 14:02:13.215: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/17/23 14:02:13.218
    Nov 17 14:02:13.224: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 17 14:02:13.227: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 17 14:02:15.228: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 17 14:02:15.233: INFO: Pod pod-with-prestop-http-hook still exists
    Nov 17 14:02:17.229: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Nov 17 14:02:17.234: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 11/17/23 14:02:17.234
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2228" for this suite. 11/17/23 14:02:17.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:17.257
Nov 17 14:02:17.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename proxy 11/17/23 14:02:17.258
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:17.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:17.286
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Nov 17 14:02:17.290: INFO: Creating pod...
Nov 17 14:02:17.300: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3670" to be "running"
Nov 17 14:02:17.305: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556149ms
Nov 17 14:02:19.310: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009901189s
Nov 17 14:02:19.310: INFO: Pod "agnhost" satisfied condition "running"
Nov 17 14:02:19.310: INFO: Creating service...
Nov 17 14:02:19.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=DELETE
Nov 17 14:02:19.338: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 17 14:02:19.338: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=OPTIONS
Nov 17 14:02:19.343: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 17 14:02:19.343: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=PATCH
Nov 17 14:02:19.348: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 17 14:02:19.348: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=POST
Nov 17 14:02:19.352: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 17 14:02:19.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=PUT
Nov 17 14:02:19.357: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 17 14:02:19.358: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=DELETE
Nov 17 14:02:19.364: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 17 14:02:19.364: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=OPTIONS
Nov 17 14:02:19.370: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 17 14:02:19.370: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=PATCH
Nov 17 14:02:19.377: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 17 14:02:19.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=POST
Nov 17 14:02:19.385: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 17 14:02:19.385: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=PUT
Nov 17 14:02:19.392: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 17 14:02:19.392: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=GET
Nov 17 14:02:19.395: INFO: http.Client request:GET StatusCode:301
Nov 17 14:02:19.396: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=GET
Nov 17 14:02:19.402: INFO: http.Client request:GET StatusCode:301
Nov 17 14:02:19.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=HEAD
Nov 17 14:02:19.405: INFO: http.Client request:HEAD StatusCode:301
Nov 17 14:02:19.406: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=HEAD
Nov 17 14:02:19.410: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3670" for this suite. 11/17/23 14:02:19.42
------------------------------
â€¢ [2.173 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:17.257
    Nov 17 14:02:17.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename proxy 11/17/23 14:02:17.258
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:17.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:17.286
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Nov 17 14:02:17.290: INFO: Creating pod...
    Nov 17 14:02:17.300: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3670" to be "running"
    Nov 17 14:02:17.305: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.556149ms
    Nov 17 14:02:19.310: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009901189s
    Nov 17 14:02:19.310: INFO: Pod "agnhost" satisfied condition "running"
    Nov 17 14:02:19.310: INFO: Creating service...
    Nov 17 14:02:19.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=DELETE
    Nov 17 14:02:19.338: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 17 14:02:19.338: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=OPTIONS
    Nov 17 14:02:19.343: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 17 14:02:19.343: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=PATCH
    Nov 17 14:02:19.348: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 17 14:02:19.348: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=POST
    Nov 17 14:02:19.352: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 17 14:02:19.352: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=PUT
    Nov 17 14:02:19.357: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 17 14:02:19.358: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=DELETE
    Nov 17 14:02:19.364: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 17 14:02:19.364: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Nov 17 14:02:19.370: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 17 14:02:19.370: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=PATCH
    Nov 17 14:02:19.377: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 17 14:02:19.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=POST
    Nov 17 14:02:19.385: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 17 14:02:19.385: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=PUT
    Nov 17 14:02:19.392: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 17 14:02:19.392: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=GET
    Nov 17 14:02:19.395: INFO: http.Client request:GET StatusCode:301
    Nov 17 14:02:19.396: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=GET
    Nov 17 14:02:19.402: INFO: http.Client request:GET StatusCode:301
    Nov 17 14:02:19.402: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/pods/agnhost/proxy?method=HEAD
    Nov 17 14:02:19.405: INFO: http.Client request:HEAD StatusCode:301
    Nov 17 14:02:19.406: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3670/services/e2e-proxy-test-service/proxy?method=HEAD
    Nov 17 14:02:19.410: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3670" for this suite. 11/17/23 14:02:19.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:19.435
Nov 17 14:02:19.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 14:02:19.437
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:19.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:19.483
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Nov 17 14:02:19.515: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 17 14:02:24.567: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 14:02:24.567
Nov 17 14:02:24.567: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 17 14:02:26.572: INFO: Creating deployment "test-rollover-deployment"
Nov 17 14:02:26.585: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 17 14:02:28.592: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 17 14:02:28.598: INFO: Ensure that both replica sets have 1 created replica
Nov 17 14:02:28.604: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 17 14:02:28.615: INFO: Updating deployment test-rollover-deployment
Nov 17 14:02:28.615: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 17 14:02:30.623: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 17 14:02:30.628: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 17 14:02:30.633: INFO: all replica sets need to contain the pod-template-hash label
Nov 17 14:02:30.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:02:32.641: INFO: all replica sets need to contain the pod-template-hash label
Nov 17 14:02:32.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:02:34.642: INFO: all replica sets need to contain the pod-template-hash label
Nov 17 14:02:34.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:02:36.640: INFO: all replica sets need to contain the pod-template-hash label
Nov 17 14:02:36.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:02:38.642: INFO: all replica sets need to contain the pod-template-hash label
Nov 17 14:02:38.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:02:40.642: INFO: 
Nov 17 14:02:40.642: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 14:02:40.651: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6696  ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 26238 2 2023-11-17 14:02:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001213558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 14:02:26 +0000 UTC,LastTransitionTime:2023-11-17 14:02:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-17 14:02:40 +0000 UTC,LastTransitionTime:2023-11-17 14:02:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 17 14:02:40.654: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6696  a56774fe-2bfb-4a4c-9a46-f3f4f950dc33 26227 2 2023-11-17 14:02:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d9187 0xc0034d9188}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:02:40.654: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 17 14:02:40.655: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6696  093c6ea7-f9e7-4a36-bc78-9cecba502ae7 26236 2 2023-11-17 14:02:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d9057 0xc0034d9058}] [] [{e2e.test Update apps/v1 2023-11-17 14:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0034d9118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:02:40.655: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6696  5a3fedc4-0028-4f7c-aa49-96032a09e4cb 26149 2 2023-11-17 14:02:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d92a7 0xc0034d92a8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:02:40.659: INFO: Pod "test-rollover-deployment-6c6df9974f-mn2w2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mn2w2 test-rollover-deployment-6c6df9974f- deployment-6696  1628553c-e474-41e2-b86d-7895d09bd30e 26170 0 2023-11-17 14:02:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a56774fe-2bfb-4a4c-9a46-f3f4f950dc33 0xc00308e277 0xc00308e278}] [] [{kube-controller-manager Update v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a56774fe-2bfb-4a4c-9a46-f3f4f950dc33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:02:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xgst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xgst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.66,StartTime:2023-11-17 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:02:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f0d228e4906fe74f0c2c54d0f106d27a8f21490ef90fead5dd02fd7bc1fdedcf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:40.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6696" for this suite. 11/17/23 14:02:40.664
------------------------------
â€¢ [SLOW TEST] [21.235 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:19.435
    Nov 17 14:02:19.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 14:02:19.437
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:19.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:19.483
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Nov 17 14:02:19.515: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Nov 17 14:02:24.567: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 14:02:24.567
    Nov 17 14:02:24.567: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Nov 17 14:02:26.572: INFO: Creating deployment "test-rollover-deployment"
    Nov 17 14:02:26.585: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Nov 17 14:02:28.592: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Nov 17 14:02:28.598: INFO: Ensure that both replica sets have 1 created replica
    Nov 17 14:02:28.604: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Nov 17 14:02:28.615: INFO: Updating deployment test-rollover-deployment
    Nov 17 14:02:28.615: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Nov 17 14:02:30.623: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Nov 17 14:02:30.628: INFO: Make sure deployment "test-rollover-deployment" is complete
    Nov 17 14:02:30.633: INFO: all replica sets need to contain the pod-template-hash label
    Nov 17 14:02:30.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:02:32.641: INFO: all replica sets need to contain the pod-template-hash label
    Nov 17 14:02:32.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:02:34.642: INFO: all replica sets need to contain the pod-template-hash label
    Nov 17 14:02:34.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:02:36.640: INFO: all replica sets need to contain the pod-template-hash label
    Nov 17 14:02:36.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:02:38.642: INFO: all replica sets need to contain the pod-template-hash label
    Nov 17 14:02:38.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:02:40.642: INFO: 
    Nov 17 14:02:40.642: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 14:02:40.651: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6696  ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 26238 2 2023-11-17 14:02:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001213558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 14:02:26 +0000 UTC,LastTransitionTime:2023-11-17 14:02:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-11-17 14:02:40 +0000 UTC,LastTransitionTime:2023-11-17 14:02:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 17 14:02:40.654: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6696  a56774fe-2bfb-4a4c-9a46-f3f4f950dc33 26227 2 2023-11-17 14:02:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d9187 0xc0034d9188}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:02:40.654: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Nov 17 14:02:40.655: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6696  093c6ea7-f9e7-4a36-bc78-9cecba502ae7 26236 2 2023-11-17 14:02:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d9057 0xc0034d9058}] [] [{e2e.test Update apps/v1 2023-11-17 14:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0034d9118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:02:40.655: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6696  5a3fedc4-0028-4f7c-aa49-96032a09e4cb 26149 2 2023-11-17 14:02:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1 0xc0034d92a7 0xc0034d92a8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca2bb277-63b4-45aa-a9c2-5b4e6ad746c1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:02:40.659: INFO: Pod "test-rollover-deployment-6c6df9974f-mn2w2" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-mn2w2 test-rollover-deployment-6c6df9974f- deployment-6696  1628553c-e474-41e2-b86d-7895d09bd30e 26170 0 2023-11-17 14:02:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a56774fe-2bfb-4a4c-9a46-f3f4f950dc33 0xc00308e277 0xc00308e278}] [] [{kube-controller-manager Update v1 2023-11-17 14:02:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a56774fe-2bfb-4a4c-9a46-f3f4f950dc33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:02:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xgst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xgst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:02:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.66,StartTime:2023-11-17 14:02:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:02:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f0d228e4906fe74f0c2c54d0f106d27a8f21490ef90fead5dd02fd7bc1fdedcf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:40.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6696" for this suite. 11/17/23 14:02:40.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:40.676
Nov 17 14:02:40.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 14:02:40.678
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:40.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:40.701
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 11/17/23 14:02:40.706
Nov 17 14:02:40.716: INFO: Waiting up to 5m0s for pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f" in namespace "var-expansion-9566" to be "Succeeded or Failed"
Nov 17 14:02:40.726: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.278736ms
Nov 17 14:02:42.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014526747s
Nov 17 14:02:44.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014963359s
STEP: Saw pod success 11/17/23 14:02:44.731
Nov 17 14:02:44.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f" satisfied condition "Succeeded or Failed"
Nov 17 14:02:44.734: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:02:44.739
Nov 17 14:02:44.749: INFO: Waiting for pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f to disappear
Nov 17 14:02:44.753: INFO: Pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9566" for this suite. 11/17/23 14:02:44.758
------------------------------
â€¢ [4.087 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:40.676
    Nov 17 14:02:40.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 14:02:40.678
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:40.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:40.701
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 11/17/23 14:02:40.706
    Nov 17 14:02:40.716: INFO: Waiting up to 5m0s for pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f" in namespace "var-expansion-9566" to be "Succeeded or Failed"
    Nov 17 14:02:40.726: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.278736ms
    Nov 17 14:02:42.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014526747s
    Nov 17 14:02:44.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014963359s
    STEP: Saw pod success 11/17/23 14:02:44.731
    Nov 17 14:02:44.731: INFO: Pod "var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f" satisfied condition "Succeeded or Failed"
    Nov 17 14:02:44.734: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:02:44.739
    Nov 17 14:02:44.749: INFO: Waiting for pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f to disappear
    Nov 17 14:02:44.753: INFO: Pod var-expansion-836bfc0f-d245-4d8c-8e0f-60f2f69bc80f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9566" for this suite. 11/17/23 14:02:44.758
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:44.764
Nov 17 14:02:44.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:02:44.765
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:44.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:44.783
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:02:44.806
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:02:45.11
STEP: Deploying the webhook pod 11/17/23 14:02:45.115
STEP: Wait for the deployment to be ready 11/17/23 14:02:45.13
Nov 17 14:02:45.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 17 14:02:47.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/17/23 14:02:49.158
STEP: Verifying the service has paired with the endpoint 11/17/23 14:02:49.18
Nov 17 14:02:50.181: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Nov 17 14:02:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5394-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 14:02:50.702
STEP: Creating a custom resource that should be mutated by the webhook 11/17/23 14:02:50.724
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:02:53.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6015" for this suite. 11/17/23 14:02:53.489
STEP: Destroying namespace "webhook-6015-markers" for this suite. 11/17/23 14:02:53.52
------------------------------
â€¢ [SLOW TEST] [8.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:44.764
    Nov 17 14:02:44.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:02:44.765
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:44.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:44.783
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:02:44.806
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:02:45.11
    STEP: Deploying the webhook pod 11/17/23 14:02:45.115
    STEP: Wait for the deployment to be ready 11/17/23 14:02:45.13
    Nov 17 14:02:45.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 17 14:02:47.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/17/23 14:02:49.158
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:02:49.18
    Nov 17 14:02:50.181: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Nov 17 14:02:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5394-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 14:02:50.702
    STEP: Creating a custom resource that should be mutated by the webhook 11/17/23 14:02:50.724
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:02:53.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6015" for this suite. 11/17/23 14:02:53.489
    STEP: Destroying namespace "webhook-6015-markers" for this suite. 11/17/23 14:02:53.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:02:53.573
Nov 17 14:02:53.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:02:53.574
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:53.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:53.655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:02:53.702
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:02:54.077
STEP: Deploying the webhook pod 11/17/23 14:02:54.087
STEP: Wait for the deployment to be ready 11/17/23 14:02:54.116
Nov 17 14:02:54.164: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:02:56.173
STEP: Verifying the service has paired with the endpoint 11/17/23 14:02:56.187
Nov 17 14:02:57.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Nov 17 14:02:57.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/17/23 14:02:57.706
STEP: Creating a custom resource that should be denied by the webhook 11/17/23 14:02:57.727
STEP: Creating a custom resource whose deletion would be denied by the webhook 11/17/23 14:02:59.801
STEP: Updating the custom resource with disallowed data should be denied 11/17/23 14:02:59.862
STEP: Deleting the custom resource should be denied 11/17/23 14:02:59.898
STEP: Remove the offending key and value from the custom resource data 11/17/23 14:03:00.012
STEP: Deleting the updated custom resource should be successful 11/17/23 14:03:00.09
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:03:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-897" for this suite. 11/17/23 14:03:00.826
STEP: Destroying namespace "webhook-897-markers" for this suite. 11/17/23 14:03:00.841
------------------------------
â€¢ [SLOW TEST] [7.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:02:53.573
    Nov 17 14:02:53.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:02:53.574
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:02:53.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:02:53.655
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:02:53.702
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:02:54.077
    STEP: Deploying the webhook pod 11/17/23 14:02:54.087
    STEP: Wait for the deployment to be ready 11/17/23 14:02:54.116
    Nov 17 14:02:54.164: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:02:56.173
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:02:56.187
    Nov 17 14:02:57.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Nov 17 14:02:57.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 11/17/23 14:02:57.706
    STEP: Creating a custom resource that should be denied by the webhook 11/17/23 14:02:57.727
    STEP: Creating a custom resource whose deletion would be denied by the webhook 11/17/23 14:02:59.801
    STEP: Updating the custom resource with disallowed data should be denied 11/17/23 14:02:59.862
    STEP: Deleting the custom resource should be denied 11/17/23 14:02:59.898
    STEP: Remove the offending key and value from the custom resource data 11/17/23 14:03:00.012
    STEP: Deleting the updated custom resource should be successful 11/17/23 14:03:00.09
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:03:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-897" for this suite. 11/17/23 14:03:00.826
    STEP: Destroying namespace "webhook-897-markers" for this suite. 11/17/23 14:03:00.841
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:03:00.869
Nov 17 14:03:00.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:03:00.87
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:03:00.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:03:00.915
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Nov 17 14:03:00.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/17/23 14:03:06.836
Nov 17 14:03:06.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
Nov 17 14:03:08.786: INFO: stderr: ""
Nov 17 14:03:08.786: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 17 14:03:08.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 delete e2e-test-crd-publish-openapi-147-crds test-foo'
Nov 17 14:03:08.894: INFO: stderr: ""
Nov 17 14:03:08.894: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 17 14:03:08.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
Nov 17 14:03:09.462: INFO: stderr: ""
Nov 17 14:03:09.462: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 17 14:03:09.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 delete e2e-test-crd-publish-openapi-147-crds test-foo'
Nov 17 14:03:09.558: INFO: stderr: ""
Nov 17 14:03:09.558: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/17/23 14:03:09.558
Nov 17 14:03:09.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
Nov 17 14:03:10.124: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/17/23 14:03:10.124
Nov 17 14:03:10.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
Nov 17 14:03:10.683: INFO: rc: 1
Nov 17 14:03:10.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
Nov 17 14:03:11.245: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/17/23 14:03:11.245
Nov 17 14:03:11.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
Nov 17 14:03:11.781: INFO: rc: 1
Nov 17 14:03:11.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
Nov 17 14:03:12.328: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 11/17/23 14:03:12.328
Nov 17 14:03:12.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds'
Nov 17 14:03:12.853: INFO: stderr: ""
Nov 17 14:03:12.853: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 11/17/23 14:03:12.853
Nov 17 14:03:12.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.metadata'
Nov 17 14:03:13.401: INFO: stderr: ""
Nov 17 14:03:13.401: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 17 14:03:13.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec'
Nov 17 14:03:13.983: INFO: stderr: ""
Nov 17 14:03:13.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 17 14:03:13.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec.bars'
Nov 17 14:03:14.521: INFO: stderr: ""
Nov 17 14:03:14.522: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/17/23 14:03:14.522
Nov 17 14:03:14.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec.bars2'
Nov 17 14:03:15.061: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:03:18.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5842" for this suite. 11/17/23 14:03:18.922
------------------------------
â€¢ [SLOW TEST] [18.059 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:03:00.869
    Nov 17 14:03:00.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:03:00.87
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:03:00.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:03:00.915
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Nov 17 14:03:00.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 11/17/23 14:03:06.836
    Nov 17 14:03:06.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
    Nov 17 14:03:08.786: INFO: stderr: ""
    Nov 17 14:03:08.786: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 17 14:03:08.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 delete e2e-test-crd-publish-openapi-147-crds test-foo'
    Nov 17 14:03:08.894: INFO: stderr: ""
    Nov 17 14:03:08.894: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Nov 17 14:03:08.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
    Nov 17 14:03:09.462: INFO: stderr: ""
    Nov 17 14:03:09.462: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Nov 17 14:03:09.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 delete e2e-test-crd-publish-openapi-147-crds test-foo'
    Nov 17 14:03:09.558: INFO: stderr: ""
    Nov 17 14:03:09.558: INFO: stdout: "e2e-test-crd-publish-openapi-147-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 11/17/23 14:03:09.558
    Nov 17 14:03:09.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
    Nov 17 14:03:10.124: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 11/17/23 14:03:10.124
    Nov 17 14:03:10.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
    Nov 17 14:03:10.683: INFO: rc: 1
    Nov 17 14:03:10.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
    Nov 17 14:03:11.245: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 11/17/23 14:03:11.245
    Nov 17 14:03:11.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 create -f -'
    Nov 17 14:03:11.781: INFO: rc: 1
    Nov 17 14:03:11.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 --namespace=crd-publish-openapi-5842 apply -f -'
    Nov 17 14:03:12.328: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 11/17/23 14:03:12.328
    Nov 17 14:03:12.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds'
    Nov 17 14:03:12.853: INFO: stderr: ""
    Nov 17 14:03:12.853: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 11/17/23 14:03:12.853
    Nov 17 14:03:12.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.metadata'
    Nov 17 14:03:13.401: INFO: stderr: ""
    Nov 17 14:03:13.401: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Nov 17 14:03:13.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec'
    Nov 17 14:03:13.983: INFO: stderr: ""
    Nov 17 14:03:13.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Nov 17 14:03:13.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec.bars'
    Nov 17 14:03:14.521: INFO: stderr: ""
    Nov 17 14:03:14.522: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-147-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 11/17/23 14:03:14.522
    Nov 17 14:03:14.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-5842 explain e2e-test-crd-publish-openapi-147-crds.spec.bars2'
    Nov 17 14:03:15.061: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:03:18.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5842" for this suite. 11/17/23 14:03:18.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:03:18.931
Nov 17 14:03:18.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:03:18.933
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:03:18.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:03:18.952
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 17 14:03:18.969: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:04:19.020: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:04:19.023
Nov 17 14:04:19.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption-path 11/17/23 14:04:19.024
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:19.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:19.048
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Nov 17 14:04:19.063: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Nov 17 14:04:19.065: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Nov 17 14:04:19.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:04:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1823" for this suite. 11/17/23 14:04:19.143
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-283" for this suite. 11/17/23 14:04:19.152
------------------------------
â€¢ [SLOW TEST] [60.230 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:03:18.931
    Nov 17 14:03:18.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:03:18.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:03:18.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:03:18.952
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 17 14:03:18.969: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:04:19.020: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:04:19.023
    Nov 17 14:04:19.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption-path 11/17/23 14:04:19.024
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:19.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:19.048
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Nov 17 14:04:19.063: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Nov 17 14:04:19.065: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:04:19.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:04:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1823" for this suite. 11/17/23 14:04:19.143
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-283" for this suite. 11/17/23 14:04:19.152
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:04:19.162
Nov 17 14:04:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename proxy 11/17/23 14:04:19.164
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:19.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:19.18
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Nov 17 14:04:19.184: INFO: Creating pod...
Nov 17 14:04:19.194: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3718" to be "running"
Nov 17 14:04:19.197: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.016586ms
Nov 17 14:04:21.202: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008285223s
Nov 17 14:04:21.202: INFO: Pod "agnhost" satisfied condition "running"
Nov 17 14:04:21.202: INFO: Creating service...
Nov 17 14:04:21.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/DELETE
Nov 17 14:04:21.227: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 17 14:04:21.227: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/GET
Nov 17 14:04:21.233: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 17 14:04:21.233: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/HEAD
Nov 17 14:04:21.237: INFO: http.Client request:HEAD | StatusCode:200
Nov 17 14:04:21.237: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 17 14:04:21.242: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 17 14:04:21.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/PATCH
Nov 17 14:04:21.247: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 17 14:04:21.247: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/POST
Nov 17 14:04:21.252: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 17 14:04:21.252: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/PUT
Nov 17 14:04:21.258: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 17 14:04:21.258: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/DELETE
Nov 17 14:04:21.265: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 17 14:04:21.266: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/GET
Nov 17 14:04:21.274: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 17 14:04:21.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/HEAD
Nov 17 14:04:21.281: INFO: http.Client request:HEAD | StatusCode:200
Nov 17 14:04:21.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/OPTIONS
Nov 17 14:04:21.286: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 17 14:04:21.286: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/PATCH
Nov 17 14:04:21.294: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 17 14:04:21.294: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/POST
Nov 17 14:04:21.300: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 17 14:04:21.300: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/PUT
Nov 17 14:04:21.306: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 17 14:04:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3718" for this suite. 11/17/23 14:04:21.313
------------------------------
â€¢ [2.157 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:04:19.162
    Nov 17 14:04:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename proxy 11/17/23 14:04:19.164
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:19.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:19.18
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Nov 17 14:04:19.184: INFO: Creating pod...
    Nov 17 14:04:19.194: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3718" to be "running"
    Nov 17 14:04:19.197: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.016586ms
    Nov 17 14:04:21.202: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008285223s
    Nov 17 14:04:21.202: INFO: Pod "agnhost" satisfied condition "running"
    Nov 17 14:04:21.202: INFO: Creating service...
    Nov 17 14:04:21.217: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/DELETE
    Nov 17 14:04:21.227: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 17 14:04:21.227: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/GET
    Nov 17 14:04:21.233: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 17 14:04:21.233: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/HEAD
    Nov 17 14:04:21.237: INFO: http.Client request:HEAD | StatusCode:200
    Nov 17 14:04:21.237: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/OPTIONS
    Nov 17 14:04:21.242: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 17 14:04:21.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/PATCH
    Nov 17 14:04:21.247: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 17 14:04:21.247: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/POST
    Nov 17 14:04:21.252: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 17 14:04:21.252: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/pods/agnhost/proxy/some/path/with/PUT
    Nov 17 14:04:21.258: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Nov 17 14:04:21.258: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/DELETE
    Nov 17 14:04:21.265: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Nov 17 14:04:21.266: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/GET
    Nov 17 14:04:21.274: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Nov 17 14:04:21.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/HEAD
    Nov 17 14:04:21.281: INFO: http.Client request:HEAD | StatusCode:200
    Nov 17 14:04:21.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/OPTIONS
    Nov 17 14:04:21.286: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Nov 17 14:04:21.286: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/PATCH
    Nov 17 14:04:21.294: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Nov 17 14:04:21.294: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/POST
    Nov 17 14:04:21.300: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Nov 17 14:04:21.300: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3718/services/test-service/proxy/some/path/with/PUT
    Nov 17 14:04:21.306: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:04:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3718" for this suite. 11/17/23 14:04:21.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:04:21.324
Nov 17 14:04:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:04:21.326
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:21.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:21.352
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 11/17/23 14:04:21.356
Nov 17 14:04:21.368: INFO: Waiting up to 5m0s for pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9" in namespace "downward-api-9195" to be "running and ready"
Nov 17 14:04:21.372: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845093ms
Nov 17 14:04:21.372: INFO: The phase of Pod labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:04:23.377: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008655598s
Nov 17 14:04:23.377: INFO: The phase of Pod labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9 is Running (Ready = true)
Nov 17 14:04:23.377: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9" satisfied condition "running and ready"
Nov 17 14:04:23.916: INFO: Successfully updated pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:04:27.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9195" for this suite. 11/17/23 14:04:27.952
------------------------------
â€¢ [SLOW TEST] [6.638 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:04:21.324
    Nov 17 14:04:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:04:21.326
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:21.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:21.352
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 11/17/23 14:04:21.356
    Nov 17 14:04:21.368: INFO: Waiting up to 5m0s for pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9" in namespace "downward-api-9195" to be "running and ready"
    Nov 17 14:04:21.372: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.845093ms
    Nov 17 14:04:21.372: INFO: The phase of Pod labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:04:23.377: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008655598s
    Nov 17 14:04:23.377: INFO: The phase of Pod labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9 is Running (Ready = true)
    Nov 17 14:04:23.377: INFO: Pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9" satisfied condition "running and ready"
    Nov 17 14:04:23.916: INFO: Successfully updated pod "labelsupdate3e240821-0ccb-4dd9-9a28-08297caecce9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:04:27.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9195" for this suite. 11/17/23 14:04:27.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:04:27.964
Nov 17 14:04:27.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename cronjob 11/17/23 14:04:27.965
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:27.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:27.991
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 11/17/23 14:04:27.995
STEP: Ensuring a job is scheduled 11/17/23 14:04:28.003
STEP: Ensuring exactly one is scheduled 11/17/23 14:05:02.008
STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/17/23 14:05:02.011
STEP: Ensuring the job is replaced with a new one 11/17/23 14:05:02.014
STEP: Removing cronjob 11/17/23 14:06:02.021
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:02.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7568" for this suite. 11/17/23 14:06:02.035
------------------------------
â€¢ [SLOW TEST] [94.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:04:27.964
    Nov 17 14:04:27.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename cronjob 11/17/23 14:04:27.965
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:04:27.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:04:27.991
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 11/17/23 14:04:27.995
    STEP: Ensuring a job is scheduled 11/17/23 14:04:28.003
    STEP: Ensuring exactly one is scheduled 11/17/23 14:05:02.008
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 11/17/23 14:05:02.011
    STEP: Ensuring the job is replaced with a new one 11/17/23 14:05:02.014
    STEP: Removing cronjob 11/17/23 14:06:02.021
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:02.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7568" for this suite. 11/17/23 14:06:02.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:02.05
Nov 17 14:06:02.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename endpointslicemirroring 11/17/23 14:06:02.051
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:02.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:02.093
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 11/17/23 14:06:02.125
Nov 17 14:06:02.152: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 11/17/23 14:06:04.156
Nov 17 14:06:04.165: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 11/17/23 14:06:06.168
Nov 17 14:06:06.177: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:08.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6926" for this suite. 11/17/23 14:06:08.186
------------------------------
â€¢ [SLOW TEST] [6.141 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:02.05
    Nov 17 14:06:02.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename endpointslicemirroring 11/17/23 14:06:02.051
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:02.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:02.093
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 11/17/23 14:06:02.125
    Nov 17 14:06:02.152: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 11/17/23 14:06:04.156
    Nov 17 14:06:04.165: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 11/17/23 14:06:06.168
    Nov 17 14:06:06.177: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:08.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6926" for this suite. 11/17/23 14:06:08.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:08.195
Nov 17 14:06:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:06:08.196
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:08.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:08.214
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Nov 17 14:06:08.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 14:06:12.974
Nov 17 14:06:12.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 create -f -'
Nov 17 14:06:17.380: INFO: stderr: ""
Nov 17 14:06:17.380: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 17 14:06:17.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 delete e2e-test-crd-publish-openapi-7318-crds test-cr'
Nov 17 14:06:17.482: INFO: stderr: ""
Nov 17 14:06:17.482: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 17 14:06:17.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 apply -f -'
Nov 17 14:06:18.074: INFO: stderr: ""
Nov 17 14:06:18.074: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 17 14:06:18.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 delete e2e-test-crd-publish-openapi-7318-crds test-cr'
Nov 17 14:06:18.173: INFO: stderr: ""
Nov 17 14:06:18.173: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/17/23 14:06:18.173
Nov 17 14:06:18.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 explain e2e-test-crd-publish-openapi-7318-crds'
Nov 17 14:06:18.716: INFO: stderr: ""
Nov 17 14:06:18.716: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7318-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:23.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6844" for this suite. 11/17/23 14:06:23.049
------------------------------
â€¢ [SLOW TEST] [14.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:08.195
    Nov 17 14:06:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:06:08.196
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:08.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:08.214
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Nov 17 14:06:08.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 14:06:12.974
    Nov 17 14:06:12.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 create -f -'
    Nov 17 14:06:17.380: INFO: stderr: ""
    Nov 17 14:06:17.380: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 17 14:06:17.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 delete e2e-test-crd-publish-openapi-7318-crds test-cr'
    Nov 17 14:06:17.482: INFO: stderr: ""
    Nov 17 14:06:17.482: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Nov 17 14:06:17.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 apply -f -'
    Nov 17 14:06:18.074: INFO: stderr: ""
    Nov 17 14:06:18.074: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Nov 17 14:06:18.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 --namespace=crd-publish-openapi-6844 delete e2e-test-crd-publish-openapi-7318-crds test-cr'
    Nov 17 14:06:18.173: INFO: stderr: ""
    Nov 17 14:06:18.173: INFO: stdout: "e2e-test-crd-publish-openapi-7318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/17/23 14:06:18.173
    Nov 17 14:06:18.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-6844 explain e2e-test-crd-publish-openapi-7318-crds'
    Nov 17 14:06:18.716: INFO: stderr: ""
    Nov 17 14:06:18.716: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7318-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:23.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6844" for this suite. 11/17/23 14:06:23.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:23.056
Nov 17 14:06:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption 11/17/23 14:06:23.057
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:23.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:23.077
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 11/17/23 14:06:23.079
STEP: Waiting for the pdb to be processed 11/17/23 14:06:23.085
STEP: updating the pdb 11/17/23 14:06:25.093
STEP: Waiting for the pdb to be processed 11/17/23 14:06:25.104
STEP: patching the pdb 11/17/23 14:06:27.118
STEP: Waiting for the pdb to be processed 11/17/23 14:06:27.127
STEP: Waiting for the pdb to be deleted 11/17/23 14:06:29.14
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4384" for this suite. 11/17/23 14:06:29.146
------------------------------
â€¢ [SLOW TEST] [6.098 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:23.056
    Nov 17 14:06:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption 11/17/23 14:06:23.057
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:23.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:23.077
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 11/17/23 14:06:23.079
    STEP: Waiting for the pdb to be processed 11/17/23 14:06:23.085
    STEP: updating the pdb 11/17/23 14:06:25.093
    STEP: Waiting for the pdb to be processed 11/17/23 14:06:25.104
    STEP: patching the pdb 11/17/23 14:06:27.118
    STEP: Waiting for the pdb to be processed 11/17/23 14:06:27.127
    STEP: Waiting for the pdb to be deleted 11/17/23 14:06:29.14
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4384" for this suite. 11/17/23 14:06:29.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:29.157
Nov 17 14:06:29.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:06:29.159
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:29.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:29.179
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Nov 17 14:06:29.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: creating the pod 11/17/23 14:06:29.183
STEP: submitting the pod to kubernetes 11/17/23 14:06:29.183
Nov 17 14:06:29.190: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08" in namespace "pods-3702" to be "running and ready"
Nov 17 14:06:29.194: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044566ms
Nov 17 14:06:29.194: INFO: The phase of Pod pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:06:31.198: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08": Phase="Running", Reason="", readiness=true. Elapsed: 2.008081723s
Nov 17 14:06:31.199: INFO: The phase of Pod pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08 is Running (Ready = true)
Nov 17 14:06:31.199: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:31.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3702" for this suite. 11/17/23 14:06:31.322
------------------------------
â€¢ [2.173 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:29.157
    Nov 17 14:06:29.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:06:29.159
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:29.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:29.179
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Nov 17 14:06:29.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: creating the pod 11/17/23 14:06:29.183
    STEP: submitting the pod to kubernetes 11/17/23 14:06:29.183
    Nov 17 14:06:29.190: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08" in namespace "pods-3702" to be "running and ready"
    Nov 17 14:06:29.194: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044566ms
    Nov 17 14:06:29.194: INFO: The phase of Pod pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:06:31.198: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08": Phase="Running", Reason="", readiness=true. Elapsed: 2.008081723s
    Nov 17 14:06:31.199: INFO: The phase of Pod pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08 is Running (Ready = true)
    Nov 17 14:06:31.199: INFO: Pod "pod-exec-websocket-b8d08cb8-beb3-4158-8218-4982d42cef08" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:31.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3702" for this suite. 11/17/23 14:06:31.322
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:31.33
Nov 17 14:06:31.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:06:31.332
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:31.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:31.35
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-2071fb9b-1afb-4606-a22e-fe9db16aff7b 11/17/23 14:06:31.354
STEP: Creating a pod to test consume configMaps 11/17/23 14:06:31.358
Nov 17 14:06:31.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e" in namespace "projected-7849" to be "Succeeded or Failed"
Nov 17 14:06:31.370: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907832ms
Nov 17 14:06:33.378: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011102875s
Nov 17 14:06:35.410: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042715131s
STEP: Saw pod success 11/17/23 14:06:35.41
Nov 17 14:06:35.410: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e" satisfied condition "Succeeded or Failed"
Nov 17 14:06:35.457: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:06:35.529
Nov 17 14:06:35.572: INFO: Waiting for pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e to disappear
Nov 17 14:06:35.584: INFO: Pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:35.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7849" for this suite. 11/17/23 14:06:35.611
------------------------------
â€¢ [4.313 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:31.33
    Nov 17 14:06:31.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:06:31.332
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:31.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:31.35
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-2071fb9b-1afb-4606-a22e-fe9db16aff7b 11/17/23 14:06:31.354
    STEP: Creating a pod to test consume configMaps 11/17/23 14:06:31.358
    Nov 17 14:06:31.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e" in namespace "projected-7849" to be "Succeeded or Failed"
    Nov 17 14:06:31.370: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907832ms
    Nov 17 14:06:33.378: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011102875s
    Nov 17 14:06:35.410: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042715131s
    STEP: Saw pod success 11/17/23 14:06:35.41
    Nov 17 14:06:35.410: INFO: Pod "pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e" satisfied condition "Succeeded or Failed"
    Nov 17 14:06:35.457: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:06:35.529
    Nov 17 14:06:35.572: INFO: Waiting for pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e to disappear
    Nov 17 14:06:35.584: INFO: Pod pod-projected-configmaps-a98cddb5-f95f-41e2-9768-58397b91ee0e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:35.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7849" for this suite. 11/17/23 14:06:35.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:35.645
Nov 17 14:06:35.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename watch 11/17/23 14:06:35.646
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:35.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:35.706
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 11/17/23 14:06:35.71
STEP: starting a background goroutine to produce watch events 11/17/23 14:06:35.714
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/17/23 14:06:35.714
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:38.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5363" for this suite. 11/17/23 14:06:38.503
------------------------------
â€¢ [2.911 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:35.645
    Nov 17 14:06:35.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename watch 11/17/23 14:06:35.646
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:35.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:35.706
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 11/17/23 14:06:35.71
    STEP: starting a background goroutine to produce watch events 11/17/23 14:06:35.714
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 11/17/23 14:06:35.714
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:38.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5363" for this suite. 11/17/23 14:06:38.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:38.559
Nov 17 14:06:38.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:06:38.56
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:38.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:38.59
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 11/17/23 14:06:38.594
Nov 17 14:06:38.607: INFO: Waiting up to 5m0s for pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5" in namespace "downward-api-52" to be "Succeeded or Failed"
Nov 17 14:06:38.616: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.339819ms
Nov 17 14:06:40.620: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013664885s
Nov 17 14:06:42.621: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014388263s
STEP: Saw pod success 11/17/23 14:06:42.621
Nov 17 14:06:42.622: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5" satisfied condition "Succeeded or Failed"
Nov 17 14:06:42.625: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:06:42.631
Nov 17 14:06:42.648: INFO: Waiting for pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 to disappear
Nov 17 14:06:42.655: INFO: Pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:42.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-52" for this suite. 11/17/23 14:06:42.659
------------------------------
â€¢ [4.110 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:38.559
    Nov 17 14:06:38.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:06:38.56
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:38.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:38.59
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 11/17/23 14:06:38.594
    Nov 17 14:06:38.607: INFO: Waiting up to 5m0s for pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5" in namespace "downward-api-52" to be "Succeeded or Failed"
    Nov 17 14:06:38.616: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.339819ms
    Nov 17 14:06:40.620: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013664885s
    Nov 17 14:06:42.621: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014388263s
    STEP: Saw pod success 11/17/23 14:06:42.621
    Nov 17 14:06:42.622: INFO: Pod "downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5" satisfied condition "Succeeded or Failed"
    Nov 17 14:06:42.625: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:06:42.631
    Nov 17 14:06:42.648: INFO: Waiting for pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 to disappear
    Nov 17 14:06:42.655: INFO: Pod downward-api-fff4d141-0dea-4f37-9b41-bbe4250b37c5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:42.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-52" for this suite. 11/17/23 14:06:42.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:42.676
Nov 17 14:06:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename hostport 11/17/23 14:06:42.677
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:42.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:42.709
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/17/23 14:06:42.716
Nov 17 14:06:42.727: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4181" to be "running and ready"
Nov 17 14:06:42.732: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905686ms
Nov 17 14:06:42.732: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:06:44.737: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009814162s
Nov 17 14:06:44.737: INFO: The phase of Pod pod1 is Running (Ready = true)
Nov 17 14:06:44.737: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.5 on the node which pod1 resides and expect scheduled 11/17/23 14:06:44.737
Nov 17 14:06:44.751: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4181" to be "running and ready"
Nov 17 14:06:44.755: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389885ms
Nov 17 14:06:44.755: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:06:46.760: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008655496s
Nov 17 14:06:46.760: INFO: The phase of Pod pod2 is Running (Ready = true)
Nov 17 14:06:46.760: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.5 but use UDP protocol on the node which pod2 resides 11/17/23 14:06:46.76
Nov 17 14:06:46.769: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4181" to be "running and ready"
Nov 17 14:06:46.775: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.219243ms
Nov 17 14:06:46.775: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:06:48.780: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010949262s
Nov 17 14:06:48.780: INFO: The phase of Pod pod3 is Running (Ready = false)
Nov 17 14:06:50.779: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009718794s
Nov 17 14:06:50.779: INFO: The phase of Pod pod3 is Running (Ready = true)
Nov 17 14:06:50.779: INFO: Pod "pod3" satisfied condition "running and ready"
Nov 17 14:06:50.785: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4181" to be "running and ready"
Nov 17 14:06:50.789: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.735087ms
Nov 17 14:06:50.789: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:06:52.793: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007488783s
Nov 17 14:06:52.793: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Nov 17 14:06:52.793: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/17/23 14:06:52.796
Nov 17 14:06:52.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:06:52.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:06:52.797: INFO: ExecWithOptions: Clientset creation
Nov 17 14:06:52.797: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.5, port: 54323 11/17/23 14:06:52.897
Nov 17 14:06:52.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.5:54323/hostname] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:06:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:06:52.898: INFO: ExecWithOptions: Clientset creation
Nov 17 14:06:52.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.5, port: 54323 UDP 11/17/23 14:06:53.012
Nov 17 14:06:53.012: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.5 54323] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:06:53.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:06:53.014: INFO: ExecWithOptions: Clientset creation
Nov 17 14:06:53.014: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:58.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-4181" for this suite. 11/17/23 14:06:58.131
------------------------------
â€¢ [SLOW TEST] [15.464 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:42.676
    Nov 17 14:06:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename hostport 11/17/23 14:06:42.677
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:42.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:42.709
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 11/17/23 14:06:42.716
    Nov 17 14:06:42.727: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4181" to be "running and ready"
    Nov 17 14:06:42.732: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.905686ms
    Nov 17 14:06:42.732: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:06:44.737: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009814162s
    Nov 17 14:06:44.737: INFO: The phase of Pod pod1 is Running (Ready = true)
    Nov 17 14:06:44.737: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.0.5 on the node which pod1 resides and expect scheduled 11/17/23 14:06:44.737
    Nov 17 14:06:44.751: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4181" to be "running and ready"
    Nov 17 14:06:44.755: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389885ms
    Nov 17 14:06:44.755: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:06:46.760: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008655496s
    Nov 17 14:06:46.760: INFO: The phase of Pod pod2 is Running (Ready = true)
    Nov 17 14:06:46.760: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.0.5 but use UDP protocol on the node which pod2 resides 11/17/23 14:06:46.76
    Nov 17 14:06:46.769: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4181" to be "running and ready"
    Nov 17 14:06:46.775: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.219243ms
    Nov 17 14:06:46.775: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:06:48.780: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010949262s
    Nov 17 14:06:48.780: INFO: The phase of Pod pod3 is Running (Ready = false)
    Nov 17 14:06:50.779: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.009718794s
    Nov 17 14:06:50.779: INFO: The phase of Pod pod3 is Running (Ready = true)
    Nov 17 14:06:50.779: INFO: Pod "pod3" satisfied condition "running and ready"
    Nov 17 14:06:50.785: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4181" to be "running and ready"
    Nov 17 14:06:50.789: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.735087ms
    Nov 17 14:06:50.789: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:06:52.793: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007488783s
    Nov 17 14:06:52.793: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Nov 17 14:06:52.793: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 11/17/23 14:06:52.796
    Nov 17 14:06:52.796: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:06:52.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:06:52.797: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:06:52.797: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.5, port: 54323 11/17/23 14:06:52.897
    Nov 17 14:06:52.897: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.5:54323/hostname] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:06:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:06:52.898: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:06:52.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.5, port: 54323 UDP 11/17/23 14:06:53.012
    Nov 17 14:06:53.012: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.0.5 54323] Namespace:hostport-4181 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:06:53.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:06:53.014: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:06:53.014: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4181/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:58.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-4181" for this suite. 11/17/23 14:06:58.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:58.141
Nov 17 14:06:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename events 11/17/23 14:06:58.142
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:58.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:58.165
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 11/17/23 14:06:58.168
STEP: get a list of Events with a label in the current namespace 11/17/23 14:06:58.185
STEP: delete a list of events 11/17/23 14:06:58.189
Nov 17 14:06:58.189: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/17/23 14:06:58.205
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:06:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-605" for this suite. 11/17/23 14:06:58.212
------------------------------
â€¢ [0.078 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:58.141
    Nov 17 14:06:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename events 11/17/23 14:06:58.142
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:58.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:58.165
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 11/17/23 14:06:58.168
    STEP: get a list of Events with a label in the current namespace 11/17/23 14:06:58.185
    STEP: delete a list of events 11/17/23 14:06:58.189
    Nov 17 14:06:58.189: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/17/23 14:06:58.205
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:06:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-605" for this suite. 11/17/23 14:06:58.212
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:06:58.22
Nov 17 14:06:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:06:58.222
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:58.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:58.24
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:06:58.243
Nov 17 14:06:58.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83" in namespace "projected-9175" to be "Succeeded or Failed"
Nov 17 14:06:58.256: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09673ms
Nov 17 14:07:00.260: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00682268s
Nov 17 14:07:02.261: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007733077s
STEP: Saw pod success 11/17/23 14:07:02.261
Nov 17 14:07:02.261: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83" satisfied condition "Succeeded or Failed"
Nov 17 14:07:02.265: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 container client-container: <nil>
STEP: delete the pod 11/17/23 14:07:02.273
Nov 17 14:07:02.288: INFO: Waiting for pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 to disappear
Nov 17 14:07:02.291: INFO: Pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:02.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9175" for this suite. 11/17/23 14:07:02.297
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:06:58.22
    Nov 17 14:06:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:06:58.222
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:06:58.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:06:58.24
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:06:58.243
    Nov 17 14:06:58.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83" in namespace "projected-9175" to be "Succeeded or Failed"
    Nov 17 14:06:58.256: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09673ms
    Nov 17 14:07:00.260: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00682268s
    Nov 17 14:07:02.261: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007733077s
    STEP: Saw pod success 11/17/23 14:07:02.261
    Nov 17 14:07:02.261: INFO: Pod "downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83" satisfied condition "Succeeded or Failed"
    Nov 17 14:07:02.265: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:07:02.273
    Nov 17 14:07:02.288: INFO: Waiting for pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 to disappear
    Nov 17 14:07:02.291: INFO: Pod downwardapi-volume-72a4d7cb-880a-406f-b9cc-a762e450bb83 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:02.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9175" for this suite. 11/17/23 14:07:02.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:02.305
Nov 17 14:07:02.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename discovery 11/17/23 14:07:02.307
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:02.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:02.336
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 11/17/23 14:07:02.34
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Nov 17 14:07:02.727: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 17 14:07:02.727: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 17 14:07:02.728: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 17 14:07:02.728: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 17 14:07:02.728: INFO: Checking APIGroup: apps
Nov 17 14:07:02.728: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 17 14:07:02.728: INFO: Versions found [{apps/v1 v1}]
Nov 17 14:07:02.728: INFO: apps/v1 matches apps/v1
Nov 17 14:07:02.728: INFO: Checking APIGroup: events.k8s.io
Nov 17 14:07:02.729: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 17 14:07:02.729: INFO: Versions found [{events.k8s.io/v1 v1}]
Nov 17 14:07:02.729: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 17 14:07:02.729: INFO: Checking APIGroup: authentication.k8s.io
Nov 17 14:07:02.730: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 17 14:07:02.730: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 17 14:07:02.730: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 17 14:07:02.730: INFO: Checking APIGroup: authorization.k8s.io
Nov 17 14:07:02.731: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 17 14:07:02.731: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 17 14:07:02.731: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 17 14:07:02.731: INFO: Checking APIGroup: autoscaling
Nov 17 14:07:02.733: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Nov 17 14:07:02.733: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Nov 17 14:07:02.733: INFO: autoscaling/v2 matches autoscaling/v2
Nov 17 14:07:02.733: INFO: Checking APIGroup: batch
Nov 17 14:07:02.734: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 17 14:07:02.734: INFO: Versions found [{batch/v1 v1}]
Nov 17 14:07:02.735: INFO: batch/v1 matches batch/v1
Nov 17 14:07:02.735: INFO: Checking APIGroup: certificates.k8s.io
Nov 17 14:07:02.736: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 17 14:07:02.736: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 17 14:07:02.736: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 17 14:07:02.736: INFO: Checking APIGroup: networking.k8s.io
Nov 17 14:07:02.736: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 17 14:07:02.737: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 17 14:07:02.737: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 17 14:07:02.737: INFO: Checking APIGroup: policy
Nov 17 14:07:02.737: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 17 14:07:02.737: INFO: Versions found [{policy/v1 v1}]
Nov 17 14:07:02.737: INFO: policy/v1 matches policy/v1
Nov 17 14:07:02.737: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 17 14:07:02.739: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 17 14:07:02.739: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 17 14:07:02.739: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 17 14:07:02.739: INFO: Checking APIGroup: storage.k8s.io
Nov 17 14:07:02.740: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 17 14:07:02.740: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 17 14:07:02.740: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 17 14:07:02.740: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 17 14:07:02.741: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 17 14:07:02.741: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 17 14:07:02.741: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 17 14:07:02.741: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 17 14:07:02.742: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 17 14:07:02.742: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 17 14:07:02.742: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 17 14:07:02.742: INFO: Checking APIGroup: scheduling.k8s.io
Nov 17 14:07:02.742: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 17 14:07:02.742: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 17 14:07:02.742: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 17 14:07:02.742: INFO: Checking APIGroup: coordination.k8s.io
Nov 17 14:07:02.743: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 17 14:07:02.743: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 17 14:07:02.743: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 17 14:07:02.743: INFO: Checking APIGroup: node.k8s.io
Nov 17 14:07:02.744: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 17 14:07:02.744: INFO: Versions found [{node.k8s.io/v1 v1}]
Nov 17 14:07:02.744: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 17 14:07:02.744: INFO: Checking APIGroup: discovery.k8s.io
Nov 17 14:07:02.745: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 17 14:07:02.745: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Nov 17 14:07:02.745: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 17 14:07:02.745: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 17 14:07:02.746: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Nov 17 14:07:02.746: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Nov 17 14:07:02.746: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Nov 17 14:07:02.746: INFO: Checking APIGroup: acme.cert-manager.io
Nov 17 14:07:02.747: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Nov 17 14:07:02.747: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Nov 17 14:07:02.747: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Nov 17 14:07:02.748: INFO: Checking APIGroup: cert-manager.io
Nov 17 14:07:02.749: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Nov 17 14:07:02.749: INFO: Versions found [{cert-manager.io/v1 v1}]
Nov 17 14:07:02.749: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Nov 17 14:07:02.749: INFO: Checking APIGroup: kyverno.io
Nov 17 14:07:02.750: INFO: PreferredVersion.GroupVersion: kyverno.io/v1
Nov 17 14:07:02.750: INFO: Versions found [{kyverno.io/v1 v1} {kyverno.io/v2beta1 v2beta1} {kyverno.io/v1beta1 v1beta1} {kyverno.io/v2alpha1 v2alpha1} {kyverno.io/v1alpha2 v1alpha2}]
Nov 17 14:07:02.750: INFO: kyverno.io/v1 matches kyverno.io/v1
Nov 17 14:07:02.750: INFO: Checking APIGroup: monitoring.coreos.com
Nov 17 14:07:02.751: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Nov 17 14:07:02.751: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Nov 17 14:07:02.751: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Nov 17 14:07:02.751: INFO: Checking APIGroup: velero.io
Nov 17 14:07:02.752: INFO: PreferredVersion.GroupVersion: velero.io/v1
Nov 17 14:07:02.752: INFO: Versions found [{velero.io/v1 v1}]
Nov 17 14:07:02.752: INFO: velero.io/v1 matches velero.io/v1
Nov 17 14:07:02.752: INFO: Checking APIGroup: kube-green.com
Nov 17 14:07:02.753: INFO: PreferredVersion.GroupVersion: kube-green.com/v1alpha1
Nov 17 14:07:02.753: INFO: Versions found [{kube-green.com/v1alpha1 v1alpha1}]
Nov 17 14:07:02.753: INFO: kube-green.com/v1alpha1 matches kube-green.com/v1alpha1
Nov 17 14:07:02.753: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
Nov 17 14:07:02.754: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
Nov 17 14:07:02.754: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
Nov 17 14:07:02.754: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
Nov 17 14:07:02.754: INFO: Checking APIGroup: logging.banzaicloud.io
Nov 17 14:07:02.755: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
Nov 17 14:07:02.755: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
Nov 17 14:07:02.756: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
Nov 17 14:07:02.756: INFO: Checking APIGroup: traefik.containo.us
Nov 17 14:07:02.757: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
Nov 17 14:07:02.757: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
Nov 17 14:07:02.757: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
Nov 17 14:07:02.757: INFO: Checking APIGroup: traefik.io
Nov 17 14:07:02.758: INFO: PreferredVersion.GroupVersion: traefik.io/v1alpha1
Nov 17 14:07:02.758: INFO: Versions found [{traefik.io/v1alpha1 v1alpha1}]
Nov 17 14:07:02.758: INFO: traefik.io/v1alpha1 matches traefik.io/v1alpha1
Nov 17 14:07:02.758: INFO: Checking APIGroup: wgpolicyk8s.io
Nov 17 14:07:02.758: INFO: PreferredVersion.GroupVersion: wgpolicyk8s.io/v1alpha2
Nov 17 14:07:02.758: INFO: Versions found [{wgpolicyk8s.io/v1alpha2 v1alpha2}]
Nov 17 14:07:02.758: INFO: wgpolicyk8s.io/v1alpha2 matches wgpolicyk8s.io/v1alpha2
Nov 17 14:07:02.759: INFO: Checking APIGroup: rbacmanager.reactiveops.io
Nov 17 14:07:02.759: INFO: PreferredVersion.GroupVersion: rbacmanager.reactiveops.io/v1beta1
Nov 17 14:07:02.759: INFO: Versions found [{rbacmanager.reactiveops.io/v1beta1 v1beta1}]
Nov 17 14:07:02.759: INFO: rbacmanager.reactiveops.io/v1beta1 matches rbacmanager.reactiveops.io/v1beta1
Nov 17 14:07:02.759: INFO: Checking APIGroup: cilium.io
Nov 17 14:07:02.760: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Nov 17 14:07:02.760: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Nov 17 14:07:02.760: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:02.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8060" for this suite. 11/17/23 14:07:02.765
------------------------------
â€¢ [0.464 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:02.305
    Nov 17 14:07:02.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename discovery 11/17/23 14:07:02.307
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:02.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:02.336
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 11/17/23 14:07:02.34
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Nov 17 14:07:02.727: INFO: Checking APIGroup: apiregistration.k8s.io
    Nov 17 14:07:02.727: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Nov 17 14:07:02.728: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Nov 17 14:07:02.728: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Nov 17 14:07:02.728: INFO: Checking APIGroup: apps
    Nov 17 14:07:02.728: INFO: PreferredVersion.GroupVersion: apps/v1
    Nov 17 14:07:02.728: INFO: Versions found [{apps/v1 v1}]
    Nov 17 14:07:02.728: INFO: apps/v1 matches apps/v1
    Nov 17 14:07:02.728: INFO: Checking APIGroup: events.k8s.io
    Nov 17 14:07:02.729: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Nov 17 14:07:02.729: INFO: Versions found [{events.k8s.io/v1 v1}]
    Nov 17 14:07:02.729: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Nov 17 14:07:02.729: INFO: Checking APIGroup: authentication.k8s.io
    Nov 17 14:07:02.730: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Nov 17 14:07:02.730: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Nov 17 14:07:02.730: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Nov 17 14:07:02.730: INFO: Checking APIGroup: authorization.k8s.io
    Nov 17 14:07:02.731: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Nov 17 14:07:02.731: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Nov 17 14:07:02.731: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Nov 17 14:07:02.731: INFO: Checking APIGroup: autoscaling
    Nov 17 14:07:02.733: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Nov 17 14:07:02.733: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Nov 17 14:07:02.733: INFO: autoscaling/v2 matches autoscaling/v2
    Nov 17 14:07:02.733: INFO: Checking APIGroup: batch
    Nov 17 14:07:02.734: INFO: PreferredVersion.GroupVersion: batch/v1
    Nov 17 14:07:02.734: INFO: Versions found [{batch/v1 v1}]
    Nov 17 14:07:02.735: INFO: batch/v1 matches batch/v1
    Nov 17 14:07:02.735: INFO: Checking APIGroup: certificates.k8s.io
    Nov 17 14:07:02.736: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Nov 17 14:07:02.736: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Nov 17 14:07:02.736: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Nov 17 14:07:02.736: INFO: Checking APIGroup: networking.k8s.io
    Nov 17 14:07:02.736: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Nov 17 14:07:02.737: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Nov 17 14:07:02.737: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Nov 17 14:07:02.737: INFO: Checking APIGroup: policy
    Nov 17 14:07:02.737: INFO: PreferredVersion.GroupVersion: policy/v1
    Nov 17 14:07:02.737: INFO: Versions found [{policy/v1 v1}]
    Nov 17 14:07:02.737: INFO: policy/v1 matches policy/v1
    Nov 17 14:07:02.737: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Nov 17 14:07:02.739: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Nov 17 14:07:02.739: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Nov 17 14:07:02.739: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Nov 17 14:07:02.739: INFO: Checking APIGroup: storage.k8s.io
    Nov 17 14:07:02.740: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Nov 17 14:07:02.740: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Nov 17 14:07:02.740: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Nov 17 14:07:02.740: INFO: Checking APIGroup: admissionregistration.k8s.io
    Nov 17 14:07:02.741: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Nov 17 14:07:02.741: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Nov 17 14:07:02.741: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Nov 17 14:07:02.741: INFO: Checking APIGroup: apiextensions.k8s.io
    Nov 17 14:07:02.742: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Nov 17 14:07:02.742: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Nov 17 14:07:02.742: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Nov 17 14:07:02.742: INFO: Checking APIGroup: scheduling.k8s.io
    Nov 17 14:07:02.742: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Nov 17 14:07:02.742: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Nov 17 14:07:02.742: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Nov 17 14:07:02.742: INFO: Checking APIGroup: coordination.k8s.io
    Nov 17 14:07:02.743: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Nov 17 14:07:02.743: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Nov 17 14:07:02.743: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Nov 17 14:07:02.743: INFO: Checking APIGroup: node.k8s.io
    Nov 17 14:07:02.744: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Nov 17 14:07:02.744: INFO: Versions found [{node.k8s.io/v1 v1}]
    Nov 17 14:07:02.744: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Nov 17 14:07:02.744: INFO: Checking APIGroup: discovery.k8s.io
    Nov 17 14:07:02.745: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Nov 17 14:07:02.745: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Nov 17 14:07:02.745: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Nov 17 14:07:02.745: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Nov 17 14:07:02.746: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Nov 17 14:07:02.746: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Nov 17 14:07:02.746: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Nov 17 14:07:02.746: INFO: Checking APIGroup: acme.cert-manager.io
    Nov 17 14:07:02.747: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Nov 17 14:07:02.747: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Nov 17 14:07:02.747: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Nov 17 14:07:02.748: INFO: Checking APIGroup: cert-manager.io
    Nov 17 14:07:02.749: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Nov 17 14:07:02.749: INFO: Versions found [{cert-manager.io/v1 v1}]
    Nov 17 14:07:02.749: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Nov 17 14:07:02.749: INFO: Checking APIGroup: kyverno.io
    Nov 17 14:07:02.750: INFO: PreferredVersion.GroupVersion: kyverno.io/v1
    Nov 17 14:07:02.750: INFO: Versions found [{kyverno.io/v1 v1} {kyverno.io/v2beta1 v2beta1} {kyverno.io/v1beta1 v1beta1} {kyverno.io/v2alpha1 v2alpha1} {kyverno.io/v1alpha2 v1alpha2}]
    Nov 17 14:07:02.750: INFO: kyverno.io/v1 matches kyverno.io/v1
    Nov 17 14:07:02.750: INFO: Checking APIGroup: monitoring.coreos.com
    Nov 17 14:07:02.751: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Nov 17 14:07:02.751: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.751: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Nov 17 14:07:02.751: INFO: Checking APIGroup: velero.io
    Nov 17 14:07:02.752: INFO: PreferredVersion.GroupVersion: velero.io/v1
    Nov 17 14:07:02.752: INFO: Versions found [{velero.io/v1 v1}]
    Nov 17 14:07:02.752: INFO: velero.io/v1 matches velero.io/v1
    Nov 17 14:07:02.752: INFO: Checking APIGroup: kube-green.com
    Nov 17 14:07:02.753: INFO: PreferredVersion.GroupVersion: kube-green.com/v1alpha1
    Nov 17 14:07:02.753: INFO: Versions found [{kube-green.com/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.753: INFO: kube-green.com/v1alpha1 matches kube-green.com/v1alpha1
    Nov 17 14:07:02.753: INFO: Checking APIGroup: logging-extensions.banzaicloud.io
    Nov 17 14:07:02.754: INFO: PreferredVersion.GroupVersion: logging-extensions.banzaicloud.io/v1alpha1
    Nov 17 14:07:02.754: INFO: Versions found [{logging-extensions.banzaicloud.io/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.754: INFO: logging-extensions.banzaicloud.io/v1alpha1 matches logging-extensions.banzaicloud.io/v1alpha1
    Nov 17 14:07:02.754: INFO: Checking APIGroup: logging.banzaicloud.io
    Nov 17 14:07:02.755: INFO: PreferredVersion.GroupVersion: logging.banzaicloud.io/v1beta1
    Nov 17 14:07:02.755: INFO: Versions found [{logging.banzaicloud.io/v1beta1 v1beta1} {logging.banzaicloud.io/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.756: INFO: logging.banzaicloud.io/v1beta1 matches logging.banzaicloud.io/v1beta1
    Nov 17 14:07:02.756: INFO: Checking APIGroup: traefik.containo.us
    Nov 17 14:07:02.757: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
    Nov 17 14:07:02.757: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.757: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
    Nov 17 14:07:02.757: INFO: Checking APIGroup: traefik.io
    Nov 17 14:07:02.758: INFO: PreferredVersion.GroupVersion: traefik.io/v1alpha1
    Nov 17 14:07:02.758: INFO: Versions found [{traefik.io/v1alpha1 v1alpha1}]
    Nov 17 14:07:02.758: INFO: traefik.io/v1alpha1 matches traefik.io/v1alpha1
    Nov 17 14:07:02.758: INFO: Checking APIGroup: wgpolicyk8s.io
    Nov 17 14:07:02.758: INFO: PreferredVersion.GroupVersion: wgpolicyk8s.io/v1alpha2
    Nov 17 14:07:02.758: INFO: Versions found [{wgpolicyk8s.io/v1alpha2 v1alpha2}]
    Nov 17 14:07:02.758: INFO: wgpolicyk8s.io/v1alpha2 matches wgpolicyk8s.io/v1alpha2
    Nov 17 14:07:02.759: INFO: Checking APIGroup: rbacmanager.reactiveops.io
    Nov 17 14:07:02.759: INFO: PreferredVersion.GroupVersion: rbacmanager.reactiveops.io/v1beta1
    Nov 17 14:07:02.759: INFO: Versions found [{rbacmanager.reactiveops.io/v1beta1 v1beta1}]
    Nov 17 14:07:02.759: INFO: rbacmanager.reactiveops.io/v1beta1 matches rbacmanager.reactiveops.io/v1beta1
    Nov 17 14:07:02.759: INFO: Checking APIGroup: cilium.io
    Nov 17 14:07:02.760: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Nov 17 14:07:02.760: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Nov 17 14:07:02.760: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:02.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8060" for this suite. 11/17/23 14:07:02.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:02.771
Nov 17 14:07:02.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:07:02.772
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:02.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:02.841
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-699557ee-9605-4571-abeb-2dd4d94d0bdd 11/17/23 14:07:02.845
STEP: Creating a pod to test consume secrets 11/17/23 14:07:02.85
Nov 17 14:07:02.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa" in namespace "projected-4978" to be "Succeeded or Failed"
Nov 17 14:07:02.865: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.855632ms
Nov 17 14:07:04.876: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015904288s
Nov 17 14:07:06.872: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011696849s
STEP: Saw pod success 11/17/23 14:07:06.872
Nov 17 14:07:06.872: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa" satisfied condition "Succeeded or Failed"
Nov 17 14:07:06.877: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa container projected-secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:07:06.886
Nov 17 14:07:06.907: INFO: Waiting for pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa to disappear
Nov 17 14:07:06.910: INFO: Pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:06.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4978" for this suite. 11/17/23 14:07:06.916
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:02.771
    Nov 17 14:07:02.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:07:02.772
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:02.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:02.841
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-699557ee-9605-4571-abeb-2dd4d94d0bdd 11/17/23 14:07:02.845
    STEP: Creating a pod to test consume secrets 11/17/23 14:07:02.85
    Nov 17 14:07:02.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa" in namespace "projected-4978" to be "Succeeded or Failed"
    Nov 17 14:07:02.865: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.855632ms
    Nov 17 14:07:04.876: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015904288s
    Nov 17 14:07:06.872: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011696849s
    STEP: Saw pod success 11/17/23 14:07:06.872
    Nov 17 14:07:06.872: INFO: Pod "pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa" satisfied condition "Succeeded or Failed"
    Nov 17 14:07:06.877: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:07:06.886
    Nov 17 14:07:06.907: INFO: Waiting for pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa to disappear
    Nov 17 14:07:06.910: INFO: Pod pod-projected-secrets-9b094480-2e9c-4132-807e-02ac39a31ffa no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:06.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4978" for this suite. 11/17/23 14:07:06.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:06.931
Nov 17 14:07:06.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:07:06.932
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:06.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:06.957
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7344 11/17/23 14:07:06.96
STEP: changing the ExternalName service to type=ClusterIP 11/17/23 14:07:06.97
STEP: creating replication controller externalname-service in namespace services-7344 11/17/23 14:07:07.001
I1117 14:07:07.012363      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7344, replica count: 2
I1117 14:07:10.063545      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 14:07:10.063: INFO: Creating new exec pod
Nov 17 14:07:10.070: INFO: Waiting up to 5m0s for pod "execpodlrwhv" in namespace "services-7344" to be "running"
Nov 17 14:07:10.076: INFO: Pod "execpodlrwhv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.405861ms
Nov 17 14:07:12.081: INFO: Pod "execpodlrwhv": Phase="Running", Reason="", readiness=true. Elapsed: 2.010812619s
Nov 17 14:07:12.081: INFO: Pod "execpodlrwhv" satisfied condition "running"
Nov 17 14:07:13.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7344 exec execpodlrwhv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 17 14:07:13.296: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 17 14:07:13.296: INFO: stdout: ""
Nov 17 14:07:13.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7344 exec execpodlrwhv -- /bin/sh -x -c nc -v -z -w 2 10.108.209.238 80'
Nov 17 14:07:13.505: INFO: stderr: "+ nc -v -z -w 2 10.108.209.238 80\nConnection to 10.108.209.238 80 port [tcp/http] succeeded!\n"
Nov 17 14:07:13.505: INFO: stdout: ""
Nov 17 14:07:13.505: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7344" for this suite. 11/17/23 14:07:13.56
------------------------------
â€¢ [SLOW TEST] [6.640 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:06.931
    Nov 17 14:07:06.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:07:06.932
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:06.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:06.957
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7344 11/17/23 14:07:06.96
    STEP: changing the ExternalName service to type=ClusterIP 11/17/23 14:07:06.97
    STEP: creating replication controller externalname-service in namespace services-7344 11/17/23 14:07:07.001
    I1117 14:07:07.012363      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7344, replica count: 2
    I1117 14:07:10.063545      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 14:07:10.063: INFO: Creating new exec pod
    Nov 17 14:07:10.070: INFO: Waiting up to 5m0s for pod "execpodlrwhv" in namespace "services-7344" to be "running"
    Nov 17 14:07:10.076: INFO: Pod "execpodlrwhv": Phase="Pending", Reason="", readiness=false. Elapsed: 5.405861ms
    Nov 17 14:07:12.081: INFO: Pod "execpodlrwhv": Phase="Running", Reason="", readiness=true. Elapsed: 2.010812619s
    Nov 17 14:07:12.081: INFO: Pod "execpodlrwhv" satisfied condition "running"
    Nov 17 14:07:13.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7344 exec execpodlrwhv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 17 14:07:13.296: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 17 14:07:13.296: INFO: stdout: ""
    Nov 17 14:07:13.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7344 exec execpodlrwhv -- /bin/sh -x -c nc -v -z -w 2 10.108.209.238 80'
    Nov 17 14:07:13.505: INFO: stderr: "+ nc -v -z -w 2 10.108.209.238 80\nConnection to 10.108.209.238 80 port [tcp/http] succeeded!\n"
    Nov 17 14:07:13.505: INFO: stdout: ""
    Nov 17 14:07:13.505: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7344" for this suite. 11/17/23 14:07:13.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:13.574
Nov 17 14:07:13.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:07:13.576
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:13.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:13.6
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:07:13.604
Nov 17 14:07:13.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9" in namespace "downward-api-6621" to be "Succeeded or Failed"
Nov 17 14:07:13.619: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442367ms
Nov 17 14:07:15.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008164611s
Nov 17 14:07:17.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008043875s
STEP: Saw pod success 11/17/23 14:07:17.624
Nov 17 14:07:17.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9" satisfied condition "Succeeded or Failed"
Nov 17 14:07:17.627: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 container client-container: <nil>
STEP: delete the pod 11/17/23 14:07:17.634
Nov 17 14:07:17.651: INFO: Waiting for pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 to disappear
Nov 17 14:07:17.654: INFO: Pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6621" for this suite. 11/17/23 14:07:17.661
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:13.574
    Nov 17 14:07:13.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:07:13.576
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:13.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:13.6
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:07:13.604
    Nov 17 14:07:13.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9" in namespace "downward-api-6621" to be "Succeeded or Failed"
    Nov 17 14:07:13.619: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442367ms
    Nov 17 14:07:15.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008164611s
    Nov 17 14:07:17.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008043875s
    STEP: Saw pod success 11/17/23 14:07:17.624
    Nov 17 14:07:17.624: INFO: Pod "downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9" satisfied condition "Succeeded or Failed"
    Nov 17 14:07:17.627: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:07:17.634
    Nov 17 14:07:17.651: INFO: Waiting for pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 to disappear
    Nov 17 14:07:17.654: INFO: Pod downwardapi-volume-3c92f11e-292d-40a6-98b0-a35d817763c9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6621" for this suite. 11/17/23 14:07:17.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:17.669
Nov 17 14:07:17.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:07:17.671
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:17.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:17.695
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-0ce1ec30-aa36-48f5-b23e-619c34bc48da 11/17/23 14:07:17.698
STEP: Creating a pod to test consume secrets 11/17/23 14:07:17.704
Nov 17 14:07:17.716: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed" in namespace "projected-7391" to be "Succeeded or Failed"
Nov 17 14:07:17.726: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 9.976558ms
Nov 17 14:07:19.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015771395s
Nov 17 14:07:21.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015529738s
STEP: Saw pod success 11/17/23 14:07:21.732
Nov 17 14:07:21.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed" satisfied condition "Succeeded or Failed"
Nov 17 14:07:21.736: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:07:21.744
Nov 17 14:07:21.763: INFO: Waiting for pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed to disappear
Nov 17 14:07:21.774: INFO: Pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:21.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7391" for this suite. 11/17/23 14:07:21.783
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:17.669
    Nov 17 14:07:17.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:07:17.671
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:17.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:17.695
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-0ce1ec30-aa36-48f5-b23e-619c34bc48da 11/17/23 14:07:17.698
    STEP: Creating a pod to test consume secrets 11/17/23 14:07:17.704
    Nov 17 14:07:17.716: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed" in namespace "projected-7391" to be "Succeeded or Failed"
    Nov 17 14:07:17.726: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 9.976558ms
    Nov 17 14:07:19.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015771395s
    Nov 17 14:07:21.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015529738s
    STEP: Saw pod success 11/17/23 14:07:21.732
    Nov 17 14:07:21.732: INFO: Pod "pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed" satisfied condition "Succeeded or Failed"
    Nov 17 14:07:21.736: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:07:21.744
    Nov 17 14:07:21.763: INFO: Waiting for pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed to disappear
    Nov 17 14:07:21.774: INFO: Pod pod-projected-secrets-26af265a-e711-4206-904f-85f1c609e4ed no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:21.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7391" for this suite. 11/17/23 14:07:21.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:21.796
Nov 17 14:07:21.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:07:21.797
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:21.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:21.821
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-412 11/17/23 14:07:21.825
STEP: creating a selector 11/17/23 14:07:21.825
STEP: Creating the service pods in kubernetes 11/17/23 14:07:21.825
Nov 17 14:07:21.825: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 17 14:07:21.883: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-412" to be "running and ready"
Nov 17 14:07:21.890: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.791793ms
Nov 17 14:07:21.890: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:07:23.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011587009s
Nov 17 14:07:23.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:07:25.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012565058s
Nov 17 14:07:25.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:07:27.897: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01437693s
Nov 17 14:07:27.897: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:07:29.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011150974s
Nov 17 14:07:29.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:07:31.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01122299s
Nov 17 14:07:31.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:07:33.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011417838s
Nov 17 14:07:33.895: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 17 14:07:33.895: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 17 14:07:33.899: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-412" to be "running and ready"
Nov 17 14:07:33.904: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.561888ms
Nov 17 14:07:33.904: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 17 14:07:33.904: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 17 14:07:33.907: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-412" to be "running and ready"
Nov 17 14:07:33.911: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.967226ms
Nov 17 14:07:33.912: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 17 14:07:33.912: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/17/23 14:07:33.916
Nov 17 14:07:33.933: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-412" to be "running"
Nov 17 14:07:33.941: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994092ms
Nov 17 14:07:35.945: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012753227s
Nov 17 14:07:35.945: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 17 14:07:35.948: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-412" to be "running"
Nov 17 14:07:35.951: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.641806ms
Nov 17 14:07:35.951: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 17 14:07:35.953: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 17 14:07:35.953: INFO: Going to poll 10.10.2.195 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:07:35.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.2.195 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:07:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:07:35.957: INFO: ExecWithOptions: Clientset creation
Nov 17 14:07:35.957: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.2.195+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:07:37.069: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 17 14:07:37.070: INFO: Going to poll 10.10.0.245 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:07:37.073: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:07:37.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:07:37.074: INFO: ExecWithOptions: Clientset creation
Nov 17 14:07:37.074: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.245+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:07:38.186: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 17 14:07:38.186: INFO: Going to poll 10.10.1.242 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:07:38.190: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.242 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:07:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:07:38.191: INFO: ExecWithOptions: Clientset creation
Nov 17 14:07:38.191: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.242+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:07:39.302: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:39.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-412" for this suite. 11/17/23 14:07:39.307
------------------------------
â€¢ [SLOW TEST] [17.518 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:21.796
    Nov 17 14:07:21.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:07:21.797
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:21.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:21.821
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-412 11/17/23 14:07:21.825
    STEP: creating a selector 11/17/23 14:07:21.825
    STEP: Creating the service pods in kubernetes 11/17/23 14:07:21.825
    Nov 17 14:07:21.825: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 17 14:07:21.883: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-412" to be "running and ready"
    Nov 17 14:07:21.890: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.791793ms
    Nov 17 14:07:21.890: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:07:23.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011587009s
    Nov 17 14:07:23.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:07:25.895: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012565058s
    Nov 17 14:07:25.895: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:07:27.897: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01437693s
    Nov 17 14:07:27.897: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:07:29.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011150974s
    Nov 17 14:07:29.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:07:31.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01122299s
    Nov 17 14:07:31.894: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:07:33.894: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011417838s
    Nov 17 14:07:33.895: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 17 14:07:33.895: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 17 14:07:33.899: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-412" to be "running and ready"
    Nov 17 14:07:33.904: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.561888ms
    Nov 17 14:07:33.904: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 17 14:07:33.904: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 17 14:07:33.907: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-412" to be "running and ready"
    Nov 17 14:07:33.911: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.967226ms
    Nov 17 14:07:33.912: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 17 14:07:33.912: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/17/23 14:07:33.916
    Nov 17 14:07:33.933: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-412" to be "running"
    Nov 17 14:07:33.941: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.994092ms
    Nov 17 14:07:35.945: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012753227s
    Nov 17 14:07:35.945: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 17 14:07:35.948: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-412" to be "running"
    Nov 17 14:07:35.951: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.641806ms
    Nov 17 14:07:35.951: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 17 14:07:35.953: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 17 14:07:35.953: INFO: Going to poll 10.10.2.195 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:07:35.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.2.195 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:07:35.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:07:35.957: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:07:35.957: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.2.195+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:07:37.069: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 17 14:07:37.070: INFO: Going to poll 10.10.0.245 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:07:37.073: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.245 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:07:37.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:07:37.074: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:07:37.074: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.245+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:07:38.186: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 17 14:07:38.186: INFO: Going to poll 10.10.1.242 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:07:38.190: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.242 8081 | grep -v '^\s*$'] Namespace:pod-network-test-412 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:07:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:07:38.191: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:07:38.191: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-412/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.242+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:07:39.302: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:39.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-412" for this suite. 11/17/23 14:07:39.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:39.315
Nov 17 14:07:39.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename csistoragecapacity 11/17/23 14:07:39.317
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:39.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:39.34
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 11/17/23 14:07:39.342
STEP: getting /apis/storage.k8s.io 11/17/23 14:07:39.345
STEP: getting /apis/storage.k8s.io/v1 11/17/23 14:07:39.346
STEP: creating 11/17/23 14:07:39.347
STEP: watching 11/17/23 14:07:39.363
Nov 17 14:07:39.363: INFO: starting watch
STEP: getting 11/17/23 14:07:39.37
STEP: listing in namespace 11/17/23 14:07:39.372
STEP: listing across namespaces 11/17/23 14:07:39.375
STEP: patching 11/17/23 14:07:39.378
STEP: updating 11/17/23 14:07:39.385
Nov 17 14:07:39.391: INFO: waiting for watch events with expected annotations in namespace
Nov 17 14:07:39.391: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 11/17/23 14:07:39.391
STEP: deleting a collection 11/17/23 14:07:39.401
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:39.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-3650" for this suite. 11/17/23 14:07:39.418
------------------------------
â€¢ [0.110 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:39.315
    Nov 17 14:07:39.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename csistoragecapacity 11/17/23 14:07:39.317
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:39.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:39.34
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 11/17/23 14:07:39.342
    STEP: getting /apis/storage.k8s.io 11/17/23 14:07:39.345
    STEP: getting /apis/storage.k8s.io/v1 11/17/23 14:07:39.346
    STEP: creating 11/17/23 14:07:39.347
    STEP: watching 11/17/23 14:07:39.363
    Nov 17 14:07:39.363: INFO: starting watch
    STEP: getting 11/17/23 14:07:39.37
    STEP: listing in namespace 11/17/23 14:07:39.372
    STEP: listing across namespaces 11/17/23 14:07:39.375
    STEP: patching 11/17/23 14:07:39.378
    STEP: updating 11/17/23 14:07:39.385
    Nov 17 14:07:39.391: INFO: waiting for watch events with expected annotations in namespace
    Nov 17 14:07:39.391: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 11/17/23 14:07:39.391
    STEP: deleting a collection 11/17/23 14:07:39.401
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:39.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-3650" for this suite. 11/17/23 14:07:39.418
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:39.426
Nov 17 14:07:39.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:07:39.427
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:39.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:39.447
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 11/17/23 14:07:39.451
Nov 17 14:07:39.458: INFO: Waiting up to 5m0s for pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1" in namespace "emptydir-3530" to be "Succeeded or Failed"
Nov 17 14:07:39.462: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86301ms
Nov 17 14:07:41.467: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008463384s
Nov 17 14:07:43.466: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007764146s
STEP: Saw pod success 11/17/23 14:07:43.466
Nov 17 14:07:43.466: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1" satisfied condition "Succeeded or Failed"
Nov 17 14:07:43.469: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 container test-container: <nil>
STEP: delete the pod 11/17/23 14:07:43.474
Nov 17 14:07:43.486: INFO: Waiting for pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 to disappear
Nov 17 14:07:43.490: INFO: Pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:43.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3530" for this suite. 11/17/23 14:07:43.494
------------------------------
â€¢ [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:39.426
    Nov 17 14:07:39.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:07:39.427
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:39.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:39.447
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/17/23 14:07:39.451
    Nov 17 14:07:39.458: INFO: Waiting up to 5m0s for pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1" in namespace "emptydir-3530" to be "Succeeded or Failed"
    Nov 17 14:07:39.462: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86301ms
    Nov 17 14:07:41.467: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008463384s
    Nov 17 14:07:43.466: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007764146s
    STEP: Saw pod success 11/17/23 14:07:43.466
    Nov 17 14:07:43.466: INFO: Pod "pod-07f141c6-e24e-4303-b0c4-bd45be1abba1" satisfied condition "Succeeded or Failed"
    Nov 17 14:07:43.469: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:07:43.474
    Nov 17 14:07:43.486: INFO: Waiting for pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 to disappear
    Nov 17 14:07:43.490: INFO: Pod pod-07f141c6-e24e-4303-b0c4-bd45be1abba1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:43.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3530" for this suite. 11/17/23 14:07:43.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:43.502
Nov 17 14:07:43.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 14:07:43.504
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:43.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:43.519
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd 11/17/23 14:07:43.523
Nov 17 14:07:43.535: INFO: Pod name my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Found 0 pods out of 1
Nov 17 14:07:48.540: INFO: Pod name my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Found 1 pods out of 1
Nov 17 14:07:48.540: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd" are running
Nov 17 14:07:48.540: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" in namespace "replication-controller-7421" to be "running"
Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f": Phase="Running", Reason="", readiness=true. Elapsed: 3.073447ms
Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" satisfied condition "running"
Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:43 +0000 UTC Reason: Message:}])
Nov 17 14:07:48.544: INFO: Trying to dial the pod
Nov 17 14:07:53.556: INFO: Controller my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Got expected result from replica 1 [my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f]: "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:53.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7421" for this suite. 11/17/23 14:07:53.561
------------------------------
â€¢ [SLOW TEST] [10.066 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:43.502
    Nov 17 14:07:43.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 14:07:43.504
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:43.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:43.519
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd 11/17/23 14:07:43.523
    Nov 17 14:07:43.535: INFO: Pod name my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Found 0 pods out of 1
    Nov 17 14:07:48.540: INFO: Pod name my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Found 1 pods out of 1
    Nov 17 14:07:48.540: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd" are running
    Nov 17 14:07:48.540: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" in namespace "replication-controller-7421" to be "running"
    Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f": Phase="Running", Reason="", readiness=true. Elapsed: 3.073447ms
    Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" satisfied condition "running"
    Nov 17 14:07:48.543: INFO: Pod "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:07:43 +0000 UTC Reason: Message:}])
    Nov 17 14:07:48.544: INFO: Trying to dial the pod
    Nov 17 14:07:53.556: INFO: Controller my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd: Got expected result from replica 1 [my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f]: "my-hostname-basic-6a3df222-7a6e-4228-a4ad-94b0bbf0e9dd-zk98f", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:53.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7421" for this suite. 11/17/23 14:07:53.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:53.571
Nov 17 14:07:53.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename limitrange 11/17/23 14:07:53.572
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:53.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:53.599
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-c57vw" in namespace "limitrange-5335" 11/17/23 14:07:53.601
STEP: Creating another limitRange in another namespace 11/17/23 14:07:53.609
Nov 17 14:07:53.622: INFO: Namespace "e2e-limitrange-c57vw-3786" created
Nov 17 14:07:53.623: INFO: Creating LimitRange "e2e-limitrange-c57vw" in namespace "e2e-limitrange-c57vw-3786"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c57vw" 11/17/23 14:07:53.63
Nov 17 14:07:53.633: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-c57vw" in "limitrange-5335" namespace 11/17/23 14:07:53.633
Nov 17 14:07:53.643: INFO: LimitRange "e2e-limitrange-c57vw" has been patched
STEP: Delete LimitRange "e2e-limitrange-c57vw" by Collection with labelSelector: "e2e-limitrange-c57vw=patched" 11/17/23 14:07:53.643
STEP: Confirm that the limitRange "e2e-limitrange-c57vw" has been deleted 11/17/23 14:07:53.65
Nov 17 14:07:53.650: INFO: Requesting list of LimitRange to confirm quantity
Nov 17 14:07:53.652: INFO: Found 0 LimitRange with label "e2e-limitrange-c57vw=patched"
Nov 17 14:07:53.652: INFO: LimitRange "e2e-limitrange-c57vw" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c57vw" 11/17/23 14:07:53.652
Nov 17 14:07:53.656: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:53.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5335" for this suite. 11/17/23 14:07:53.66
STEP: Destroying namespace "e2e-limitrange-c57vw-3786" for this suite. 11/17/23 14:07:53.667
------------------------------
â€¢ [0.106 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:53.571
    Nov 17 14:07:53.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename limitrange 11/17/23 14:07:53.572
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:53.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:53.599
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-c57vw" in namespace "limitrange-5335" 11/17/23 14:07:53.601
    STEP: Creating another limitRange in another namespace 11/17/23 14:07:53.609
    Nov 17 14:07:53.622: INFO: Namespace "e2e-limitrange-c57vw-3786" created
    Nov 17 14:07:53.623: INFO: Creating LimitRange "e2e-limitrange-c57vw" in namespace "e2e-limitrange-c57vw-3786"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c57vw" 11/17/23 14:07:53.63
    Nov 17 14:07:53.633: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-c57vw" in "limitrange-5335" namespace 11/17/23 14:07:53.633
    Nov 17 14:07:53.643: INFO: LimitRange "e2e-limitrange-c57vw" has been patched
    STEP: Delete LimitRange "e2e-limitrange-c57vw" by Collection with labelSelector: "e2e-limitrange-c57vw=patched" 11/17/23 14:07:53.643
    STEP: Confirm that the limitRange "e2e-limitrange-c57vw" has been deleted 11/17/23 14:07:53.65
    Nov 17 14:07:53.650: INFO: Requesting list of LimitRange to confirm quantity
    Nov 17 14:07:53.652: INFO: Found 0 LimitRange with label "e2e-limitrange-c57vw=patched"
    Nov 17 14:07:53.652: INFO: LimitRange "e2e-limitrange-c57vw" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c57vw" 11/17/23 14:07:53.652
    Nov 17 14:07:53.656: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:53.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5335" for this suite. 11/17/23 14:07:53.66
    STEP: Destroying namespace "e2e-limitrange-c57vw-3786" for this suite. 11/17/23 14:07:53.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:53.683
Nov 17 14:07:53.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-runtime 11/17/23 14:07:53.685
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:53.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:53.711
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 11/17/23 14:07:53.714
STEP: wait for the container to reach Succeeded 11/17/23 14:07:53.726
STEP: get the container status 11/17/23 14:07:57.792
STEP: the container should be terminated 11/17/23 14:07:57.804
STEP: the termination message should be set 11/17/23 14:07:57.805
Nov 17 14:07:57.805: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/17/23 14:07:57.805
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 17 14:07:57.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8101" for this suite. 11/17/23 14:07:57.929
------------------------------
â€¢ [4.260 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:53.683
    Nov 17 14:07:53.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-runtime 11/17/23 14:07:53.685
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:53.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:53.711
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 11/17/23 14:07:53.714
    STEP: wait for the container to reach Succeeded 11/17/23 14:07:53.726
    STEP: get the container status 11/17/23 14:07:57.792
    STEP: the container should be terminated 11/17/23 14:07:57.804
    STEP: the termination message should be set 11/17/23 14:07:57.805
    Nov 17 14:07:57.805: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/17/23 14:07:57.805
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:07:57.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8101" for this suite. 11/17/23 14:07:57.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:07:57.944
Nov 17 14:07:57.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:07:57.946
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:57.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:57.977
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 11/17/23 14:07:57.981
Nov 17 14:07:57.991: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e" in namespace "emptydir-3642" to be "running"
Nov 17 14:07:57.998: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140221ms
Nov 17 14:08:00.002: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e": Phase="Running", Reason="", readiness=false. Elapsed: 2.011485085s
Nov 17 14:08:00.002: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e" satisfied condition "running"
STEP: Reading file content from the nginx-container 11/17/23 14:08:00.002
Nov 17 14:08:00.003: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3642 PodName:pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:08:00.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:08:00.004: INFO: ExecWithOptions: Clientset creation
Nov 17 14:08:00.004: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3642/pods/pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Nov 17 14:08:00.139: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:00.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3642" for this suite. 11/17/23 14:08:00.146
------------------------------
â€¢ [2.210 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:07:57.944
    Nov 17 14:07:57.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:07:57.946
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:07:57.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:07:57.977
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 11/17/23 14:07:57.981
    Nov 17 14:07:57.991: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e" in namespace "emptydir-3642" to be "running"
    Nov 17 14:07:57.998: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.140221ms
    Nov 17 14:08:00.002: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e": Phase="Running", Reason="", readiness=false. Elapsed: 2.011485085s
    Nov 17 14:08:00.002: INFO: Pod "pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 11/17/23 14:08:00.002
    Nov 17 14:08:00.003: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3642 PodName:pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:08:00.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:08:00.004: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:08:00.004: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3642/pods/pod-sharedvolume-b8487218-d460-4b25-9fc5-f26214d87d9e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Nov 17 14:08:00.139: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:00.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3642" for this suite. 11/17/23 14:08:00.146
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:00.156
Nov 17 14:08:00.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:08:00.158
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:00.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:00.191
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Nov 17 14:08:00.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8328 version'
Nov 17 14:08:00.288: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Nov 17 14:08:00.288: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.11\", GitCommit:\"3cd242c51317aed8858119529ccab22079f523b1\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T17:00:54Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.11\", GitCommit:\"3cd242c51317aed8858119529ccab22079f523b1\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:50:12Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:00.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8328" for this suite. 11/17/23 14:08:00.294
------------------------------
â€¢ [0.150 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:00.156
    Nov 17 14:08:00.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:08:00.158
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:00.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:00.191
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Nov 17 14:08:00.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8328 version'
    Nov 17 14:08:00.288: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Nov 17 14:08:00.288: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.11\", GitCommit:\"3cd242c51317aed8858119529ccab22079f523b1\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T17:00:54Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.11\", GitCommit:\"3cd242c51317aed8858119529ccab22079f523b1\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:50:12Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:00.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8328" for this suite. 11/17/23 14:08:00.294
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:00.306
Nov 17 14:08:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:08:00.308
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:00.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:00.341
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 11/17/23 14:08:00.345
Nov 17 14:08:00.355: INFO: Waiting up to 5m0s for pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a" in namespace "downward-api-9342" to be "running and ready"
Nov 17 14:08:00.363: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.657219ms
Nov 17 14:08:00.363: INFO: The phase of Pod annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:08:02.368: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013214113s
Nov 17 14:08:02.368: INFO: The phase of Pod annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a is Running (Ready = true)
Nov 17 14:08:02.368: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a" satisfied condition "running and ready"
Nov 17 14:08:02.894: INFO: Successfully updated pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:04.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9342" for this suite. 11/17/23 14:08:04.918
------------------------------
â€¢ [4.624 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:00.306
    Nov 17 14:08:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:08:00.308
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:00.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:00.341
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 11/17/23 14:08:00.345
    Nov 17 14:08:00.355: INFO: Waiting up to 5m0s for pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a" in namespace "downward-api-9342" to be "running and ready"
    Nov 17 14:08:00.363: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.657219ms
    Nov 17 14:08:00.363: INFO: The phase of Pod annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:08:02.368: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013214113s
    Nov 17 14:08:02.368: INFO: The phase of Pod annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a is Running (Ready = true)
    Nov 17 14:08:02.368: INFO: Pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a" satisfied condition "running and ready"
    Nov 17 14:08:02.894: INFO: Successfully updated pod "annotationupdate4c1dd8ca-0bf7-4f0c-95e2-add83bf9614a"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:04.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9342" for this suite. 11/17/23 14:08:04.918
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:04.932
Nov 17 14:08:04.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 14:08:04.933
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:04.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:04.961
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 11/17/23 14:08:04.97
STEP: create the rc2 11/17/23 14:08:04.981
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/17/23 14:08:09.994
STEP: delete the rc simpletest-rc-to-be-deleted 11/17/23 14:08:10.723
STEP: wait for the rc to be deleted 11/17/23 14:08:10.738
Nov 17 14:08:15.771: INFO: 87 pods remaining
Nov 17 14:08:15.771: INFO: 69 pods has nil DeletionTimestamp
Nov 17 14:08:15.771: INFO: 
Nov 17 14:08:20.759: INFO: 74 pods remaining
Nov 17 14:08:20.760: INFO: 50 pods has nil DeletionTimestamp
Nov 17 14:08:20.760: INFO: 
STEP: Gathering metrics 11/17/23 14:08:25.753
Nov 17 14:08:25.793: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 14:08:25.798: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.453145ms
Nov 17 14:08:25.798: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 14:08:25.798: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 14:08:25.914: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 17 14:08:25.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bzp8" in namespace "gc-5313"
Nov 17 14:08:25.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j24d" in namespace "gc-5313"
Nov 17 14:08:25.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xf69" in namespace "gc-5313"
Nov 17 14:08:25.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-44tpx" in namespace "gc-5313"
Nov 17 14:08:25.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-45rvm" in namespace "gc-5313"
Nov 17 14:08:26.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bbrz" in namespace "gc-5313"
Nov 17 14:08:26.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fmls" in namespace "gc-5313"
Nov 17 14:08:26.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gskq" in namespace "gc-5313"
Nov 17 14:08:26.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s5nc" in namespace "gc-5313"
Nov 17 14:08:26.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-64dn7" in namespace "gc-5313"
Nov 17 14:08:26.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-64mg5" in namespace "gc-5313"
Nov 17 14:08:26.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-65222" in namespace "gc-5313"
Nov 17 14:08:26.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mnmn" in namespace "gc-5313"
Nov 17 14:08:26.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n8dp" in namespace "gc-5313"
Nov 17 14:08:26.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pbxm" in namespace "gc-5313"
Nov 17 14:08:26.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rnd4" in namespace "gc-5313"
Nov 17 14:08:26.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jmdk" in namespace "gc-5313"
Nov 17 14:08:26.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w8cz" in namespace "gc-5313"
Nov 17 14:08:26.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-92b95" in namespace "gc-5313"
Nov 17 14:08:26.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-92fn7" in namespace "gc-5313"
Nov 17 14:08:26.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gxz2" in namespace "gc-5313"
Nov 17 14:08:26.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jxzc" in namespace "gc-5313"
Nov 17 14:08:26.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kxk7" in namespace "gc-5313"
Nov 17 14:08:26.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-9thfp" in namespace "gc-5313"
Nov 17 14:08:26.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wn2t" in namespace "gc-5313"
Nov 17 14:08:26.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbc2v" in namespace "gc-5313"
Nov 17 14:08:26.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcrm4" in namespace "gc-5313"
Nov 17 14:08:26.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-brl5j" in namespace "gc-5313"
Nov 17 14:08:26.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs2j" in namespace "gc-5313"
Nov 17 14:08:26.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnsmp" in namespace "gc-5313"
Nov 17 14:08:26.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctqgt" in namespace "gc-5313"
Nov 17 14:08:26.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4rf" in namespace "gc-5313"
Nov 17 14:08:26.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx8jl" in namespace "gc-5313"
Nov 17 14:08:26.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7dvg" in namespace "gc-5313"
Nov 17 14:08:26.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-db47x" in namespace "gc-5313"
Nov 17 14:08:26.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfvpw" in namespace "gc-5313"
Nov 17 14:08:26.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkjmw" in namespace "gc-5313"
Nov 17 14:08:26.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-drmlv" in namespace "gc-5313"
Nov 17 14:08:26.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds6bw" in namespace "gc-5313"
Nov 17 14:08:26.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj4sj" in namespace "gc-5313"
Nov 17 14:08:26.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpcpf" in namespace "gc-5313"
Nov 17 14:08:26.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6x4t" in namespace "gc-5313"
Nov 17 14:08:26.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-gchlx" in namespace "gc-5313"
Nov 17 14:08:26.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdd2n" in namespace "gc-5313"
Nov 17 14:08:26.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-gls8w" in namespace "gc-5313"
Nov 17 14:08:26.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnz57" in namespace "gc-5313"
Nov 17 14:08:26.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt85m" in namespace "gc-5313"
Nov 17 14:08:27.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvswt" in namespace "gc-5313"
Nov 17 14:08:27.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxtzq" in namespace "gc-5313"
Nov 17 14:08:27.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-h59sz" in namespace "gc-5313"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5313" for this suite. 11/17/23 14:08:27.102
------------------------------
â€¢ [SLOW TEST] [22.186 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:04.932
    Nov 17 14:08:04.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 14:08:04.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:04.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:04.961
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 11/17/23 14:08:04.97
    STEP: create the rc2 11/17/23 14:08:04.981
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 11/17/23 14:08:09.994
    STEP: delete the rc simpletest-rc-to-be-deleted 11/17/23 14:08:10.723
    STEP: wait for the rc to be deleted 11/17/23 14:08:10.738
    Nov 17 14:08:15.771: INFO: 87 pods remaining
    Nov 17 14:08:15.771: INFO: 69 pods has nil DeletionTimestamp
    Nov 17 14:08:15.771: INFO: 
    Nov 17 14:08:20.759: INFO: 74 pods remaining
    Nov 17 14:08:20.760: INFO: 50 pods has nil DeletionTimestamp
    Nov 17 14:08:20.760: INFO: 
    STEP: Gathering metrics 11/17/23 14:08:25.753
    Nov 17 14:08:25.793: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 14:08:25.798: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.453145ms
    Nov 17 14:08:25.798: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 14:08:25.798: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 14:08:25.914: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Nov 17 14:08:25.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bzp8" in namespace "gc-5313"
    Nov 17 14:08:25.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j24d" in namespace "gc-5313"
    Nov 17 14:08:25.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xf69" in namespace "gc-5313"
    Nov 17 14:08:25.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-44tpx" in namespace "gc-5313"
    Nov 17 14:08:25.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-45rvm" in namespace "gc-5313"
    Nov 17 14:08:26.024: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bbrz" in namespace "gc-5313"
    Nov 17 14:08:26.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fmls" in namespace "gc-5313"
    Nov 17 14:08:26.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gskq" in namespace "gc-5313"
    Nov 17 14:08:26.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s5nc" in namespace "gc-5313"
    Nov 17 14:08:26.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-64dn7" in namespace "gc-5313"
    Nov 17 14:08:26.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-64mg5" in namespace "gc-5313"
    Nov 17 14:08:26.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-65222" in namespace "gc-5313"
    Nov 17 14:08:26.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mnmn" in namespace "gc-5313"
    Nov 17 14:08:26.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n8dp" in namespace "gc-5313"
    Nov 17 14:08:26.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pbxm" in namespace "gc-5313"
    Nov 17 14:08:26.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rnd4" in namespace "gc-5313"
    Nov 17 14:08:26.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jmdk" in namespace "gc-5313"
    Nov 17 14:08:26.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w8cz" in namespace "gc-5313"
    Nov 17 14:08:26.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-92b95" in namespace "gc-5313"
    Nov 17 14:08:26.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-92fn7" in namespace "gc-5313"
    Nov 17 14:08:26.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gxz2" in namespace "gc-5313"
    Nov 17 14:08:26.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jxzc" in namespace "gc-5313"
    Nov 17 14:08:26.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kxk7" in namespace "gc-5313"
    Nov 17 14:08:26.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-9thfp" in namespace "gc-5313"
    Nov 17 14:08:26.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wn2t" in namespace "gc-5313"
    Nov 17 14:08:26.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbc2v" in namespace "gc-5313"
    Nov 17 14:08:26.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcrm4" in namespace "gc-5313"
    Nov 17 14:08:26.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-brl5j" in namespace "gc-5313"
    Nov 17 14:08:26.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs2j" in namespace "gc-5313"
    Nov 17 14:08:26.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnsmp" in namespace "gc-5313"
    Nov 17 14:08:26.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctqgt" in namespace "gc-5313"
    Nov 17 14:08:26.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4rf" in namespace "gc-5313"
    Nov 17 14:08:26.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx8jl" in namespace "gc-5313"
    Nov 17 14:08:26.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7dvg" in namespace "gc-5313"
    Nov 17 14:08:26.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-db47x" in namespace "gc-5313"
    Nov 17 14:08:26.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfvpw" in namespace "gc-5313"
    Nov 17 14:08:26.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkjmw" in namespace "gc-5313"
    Nov 17 14:08:26.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-drmlv" in namespace "gc-5313"
    Nov 17 14:08:26.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds6bw" in namespace "gc-5313"
    Nov 17 14:08:26.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj4sj" in namespace "gc-5313"
    Nov 17 14:08:26.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpcpf" in namespace "gc-5313"
    Nov 17 14:08:26.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6x4t" in namespace "gc-5313"
    Nov 17 14:08:26.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-gchlx" in namespace "gc-5313"
    Nov 17 14:08:26.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdd2n" in namespace "gc-5313"
    Nov 17 14:08:26.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-gls8w" in namespace "gc-5313"
    Nov 17 14:08:26.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnz57" in namespace "gc-5313"
    Nov 17 14:08:26.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt85m" in namespace "gc-5313"
    Nov 17 14:08:27.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvswt" in namespace "gc-5313"
    Nov 17 14:08:27.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxtzq" in namespace "gc-5313"
    Nov 17 14:08:27.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-h59sz" in namespace "gc-5313"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5313" for this suite. 11/17/23 14:08:27.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:27.12
Nov 17 14:08:27.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 14:08:27.121
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:27.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:27.16
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 11/17/23 14:08:27.165
Nov 17 14:08:27.174: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-561" to be "running and ready"
Nov 17 14:08:27.179: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363324ms
Nov 17 14:08:27.179: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:08:29.182: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793454s
Nov 17 14:08:29.183: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:08:31.184: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010844134s
Nov 17 14:08:31.185: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:08:33.184: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 6.010500075s
Nov 17 14:08:33.184: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Nov 17 14:08:33.184: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 11/17/23 14:08:33.188
STEP: Then the orphan pod is adopted 11/17/23 14:08:33.196
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:34.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-561" for this suite. 11/17/23 14:08:34.211
------------------------------
â€¢ [SLOW TEST] [7.104 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:27.12
    Nov 17 14:08:27.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 14:08:27.121
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:27.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:27.16
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 11/17/23 14:08:27.165
    Nov 17 14:08:27.174: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-561" to be "running and ready"
    Nov 17 14:08:27.179: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363324ms
    Nov 17 14:08:27.179: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:08:29.182: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008793454s
    Nov 17 14:08:29.183: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:08:31.184: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010844134s
    Nov 17 14:08:31.185: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:08:33.184: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 6.010500075s
    Nov 17 14:08:33.184: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Nov 17 14:08:33.184: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 11/17/23 14:08:33.188
    STEP: Then the orphan pod is adopted 11/17/23 14:08:33.196
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:34.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-561" for this suite. 11/17/23 14:08:34.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:34.228
Nov 17 14:08:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:08:34.229
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:34.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:34.259
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:08:34.282
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:08:34.714
STEP: Deploying the webhook pod 11/17/23 14:08:34.723
STEP: Wait for the deployment to be ready 11/17/23 14:08:34.747
Nov 17 14:08:34.776: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:08:36.785
STEP: Verifying the service has paired with the endpoint 11/17/23 14:08:36.806
Nov 17 14:08:37.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/17/23 14:08:37.811
STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:37.811
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/17/23 14:08:37.832
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/17/23 14:08:38.845
STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:38.845
STEP: Having no error when timeout is longer than webhook latency 11/17/23 14:08:39.885
STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:39.885
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/17/23 14:08:44.925
STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:44.925
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:49.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4928" for this suite. 11/17/23 14:08:50.069
STEP: Destroying namespace "webhook-4928-markers" for this suite. 11/17/23 14:08:50.074
------------------------------
â€¢ [SLOW TEST] [15.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:34.228
    Nov 17 14:08:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:08:34.229
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:34.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:34.259
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:08:34.282
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:08:34.714
    STEP: Deploying the webhook pod 11/17/23 14:08:34.723
    STEP: Wait for the deployment to be ready 11/17/23 14:08:34.747
    Nov 17 14:08:34.776: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:08:36.785
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:08:36.806
    Nov 17 14:08:37.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 11/17/23 14:08:37.811
    STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:37.811
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 11/17/23 14:08:37.832
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 11/17/23 14:08:38.845
    STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:38.845
    STEP: Having no error when timeout is longer than webhook latency 11/17/23 14:08:39.885
    STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:39.885
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 11/17/23 14:08:44.925
    STEP: Registering slow webhook via the AdmissionRegistration API 11/17/23 14:08:44.925
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:49.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4928" for this suite. 11/17/23 14:08:50.069
    STEP: Destroying namespace "webhook-4928-markers" for this suite. 11/17/23 14:08:50.074
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:50.095
Nov 17 14:08:50.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:08:50.096
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:50.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:50.123
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 11/17/23 14:08:50.133
STEP: watching for the Service to be added 11/17/23 14:08:50.153
Nov 17 14:08:50.155: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 17 14:08:50.156: INFO: Service test-service-2q24c created
STEP: Getting /status 11/17/23 14:08:50.156
Nov 17 14:08:50.160: INFO: Service test-service-2q24c has LoadBalancer: {[]}
STEP: patching the ServiceStatus 11/17/23 14:08:50.16
STEP: watching for the Service to be patched 11/17/23 14:08:50.169
Nov 17 14:08:50.181: INFO: observed Service test-service-2q24c in namespace services-9891 with annotations: map[] & LoadBalancer: {[]}
Nov 17 14:08:50.182: INFO: Found Service test-service-2q24c in namespace services-9891 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 17 14:08:50.182: INFO: Service test-service-2q24c has service status patched
STEP: updating the ServiceStatus 11/17/23 14:08:50.182
Nov 17 14:08:50.194: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 11/17/23 14:08:50.195
Nov 17 14:08:50.197: INFO: Observed Service test-service-2q24c in namespace services-9891 with annotations: map[] & Conditions: {[]}
Nov 17 14:08:50.197: INFO: Observed event: &Service{ObjectMeta:{test-service-2q24c  services-9891  3a3b5b9a-389f-4935-a115-22be87558747 31401 0 2023-11-17 14:08:50 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-17 14:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-17 14:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.97.185.46,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.97.185.46],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 17 14:08:50.197: INFO: Found Service test-service-2q24c in namespace services-9891 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 17 14:08:50.197: INFO: Service test-service-2q24c has service status updated
STEP: patching the service 11/17/23 14:08:50.197
STEP: watching for the Service to be patched 11/17/23 14:08:50.216
Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
Nov 17 14:08:50.220: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service:patched test-service-static:true]
Nov 17 14:08:50.220: INFO: Service test-service-2q24c patched
STEP: deleting the service 11/17/23 14:08:50.22
STEP: watching for the Service to be deleted 11/17/23 14:08:50.246
Nov 17 14:08:50.250: INFO: Observed event: ADDED
Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
Nov 17 14:08:50.251: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 17 14:08:50.251: INFO: Service test-service-2q24c deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9891" for this suite. 11/17/23 14:08:50.259
------------------------------
â€¢ [0.181 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:50.095
    Nov 17 14:08:50.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:08:50.096
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:50.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:50.123
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 11/17/23 14:08:50.133
    STEP: watching for the Service to be added 11/17/23 14:08:50.153
    Nov 17 14:08:50.155: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Nov 17 14:08:50.156: INFO: Service test-service-2q24c created
    STEP: Getting /status 11/17/23 14:08:50.156
    Nov 17 14:08:50.160: INFO: Service test-service-2q24c has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 11/17/23 14:08:50.16
    STEP: watching for the Service to be patched 11/17/23 14:08:50.169
    Nov 17 14:08:50.181: INFO: observed Service test-service-2q24c in namespace services-9891 with annotations: map[] & LoadBalancer: {[]}
    Nov 17 14:08:50.182: INFO: Found Service test-service-2q24c in namespace services-9891 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Nov 17 14:08:50.182: INFO: Service test-service-2q24c has service status patched
    STEP: updating the ServiceStatus 11/17/23 14:08:50.182
    Nov 17 14:08:50.194: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 11/17/23 14:08:50.195
    Nov 17 14:08:50.197: INFO: Observed Service test-service-2q24c in namespace services-9891 with annotations: map[] & Conditions: {[]}
    Nov 17 14:08:50.197: INFO: Observed event: &Service{ObjectMeta:{test-service-2q24c  services-9891  3a3b5b9a-389f-4935-a115-22be87558747 31401 0 2023-11-17 14:08:50 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-11-17 14:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-11-17 14:08:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.97.185.46,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.97.185.46],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Nov 17 14:08:50.197: INFO: Found Service test-service-2q24c in namespace services-9891 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 17 14:08:50.197: INFO: Service test-service-2q24c has service status updated
    STEP: patching the service 11/17/23 14:08:50.197
    STEP: watching for the Service to be patched 11/17/23 14:08:50.216
    Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
    Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
    Nov 17 14:08:50.220: INFO: observed Service test-service-2q24c in namespace services-9891 with labels: map[test-service-static:true]
    Nov 17 14:08:50.220: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service:patched test-service-static:true]
    Nov 17 14:08:50.220: INFO: Service test-service-2q24c patched
    STEP: deleting the service 11/17/23 14:08:50.22
    STEP: watching for the Service to be deleted 11/17/23 14:08:50.246
    Nov 17 14:08:50.250: INFO: Observed event: ADDED
    Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
    Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
    Nov 17 14:08:50.251: INFO: Observed event: MODIFIED
    Nov 17 14:08:50.251: INFO: Found Service test-service-2q24c in namespace services-9891 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Nov 17 14:08:50.251: INFO: Service test-service-2q24c deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9891" for this suite. 11/17/23 14:08:50.259
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:50.277
Nov 17 14:08:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename containers 11/17/23 14:08:50.279
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:50.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:50.311
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 11/17/23 14:08:50.32
Nov 17 14:08:50.337: INFO: Waiting up to 5m0s for pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d" in namespace "containers-1897" to be "Succeeded or Failed"
Nov 17 14:08:50.341: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.307513ms
Nov 17 14:08:52.345: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007574052s
Nov 17 14:08:54.346: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008867562s
STEP: Saw pod success 11/17/23 14:08:54.346
Nov 17 14:08:54.346: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d" satisfied condition "Succeeded or Failed"
Nov 17 14:08:54.349: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:08:54.356
Nov 17 14:08:54.370: INFO: Waiting for pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d to disappear
Nov 17 14:08:54.373: INFO: Pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1897" for this suite. 11/17/23 14:08:54.377
------------------------------
â€¢ [4.106 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:50.277
    Nov 17 14:08:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename containers 11/17/23 14:08:50.279
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:50.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:50.311
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 11/17/23 14:08:50.32
    Nov 17 14:08:50.337: INFO: Waiting up to 5m0s for pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d" in namespace "containers-1897" to be "Succeeded or Failed"
    Nov 17 14:08:50.341: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.307513ms
    Nov 17 14:08:52.345: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007574052s
    Nov 17 14:08:54.346: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008867562s
    STEP: Saw pod success 11/17/23 14:08:54.346
    Nov 17 14:08:54.346: INFO: Pod "client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d" satisfied condition "Succeeded or Failed"
    Nov 17 14:08:54.349: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:08:54.356
    Nov 17 14:08:54.370: INFO: Waiting for pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d to disappear
    Nov 17 14:08:54.373: INFO: Pod client-containers-341164eb-1d48-4c9d-bd51-1aeefaf4c66d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:54.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1897" for this suite. 11/17/23 14:08:54.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:54.385
Nov 17 14:08:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:08:54.386
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:54.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:54.407
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b139f786-8fae-4b35-9893-afd76f8fa96c 11/17/23 14:08:54.411
STEP: Creating secret with name secret-projected-all-test-volume-8464da1c-ba8e-4928-bbd0-9525dc6fd63d 11/17/23 14:08:54.415
STEP: Creating a pod to test Check all projections for projected volume plugin 11/17/23 14:08:54.421
Nov 17 14:08:54.433: INFO: Waiting up to 5m0s for pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca" in namespace "projected-1884" to be "Succeeded or Failed"
Nov 17 14:08:54.438: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95916ms
Nov 17 14:08:56.444: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010762856s
Nov 17 14:08:58.442: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007451s
STEP: Saw pod success 11/17/23 14:08:58.442
Nov 17 14:08:58.442: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca" satisfied condition "Succeeded or Failed"
Nov 17 14:08:58.446: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca container projected-all-volume-test: <nil>
STEP: delete the pod 11/17/23 14:08:58.452
Nov 17 14:08:58.467: INFO: Waiting for pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca to disappear
Nov 17 14:08:58.471: INFO: Pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1884" for this suite. 11/17/23 14:08:58.475
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:54.385
    Nov 17 14:08:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:08:54.386
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:54.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:54.407
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b139f786-8fae-4b35-9893-afd76f8fa96c 11/17/23 14:08:54.411
    STEP: Creating secret with name secret-projected-all-test-volume-8464da1c-ba8e-4928-bbd0-9525dc6fd63d 11/17/23 14:08:54.415
    STEP: Creating a pod to test Check all projections for projected volume plugin 11/17/23 14:08:54.421
    Nov 17 14:08:54.433: INFO: Waiting up to 5m0s for pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca" in namespace "projected-1884" to be "Succeeded or Failed"
    Nov 17 14:08:54.438: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95916ms
    Nov 17 14:08:56.444: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010762856s
    Nov 17 14:08:58.442: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007451s
    STEP: Saw pod success 11/17/23 14:08:58.442
    Nov 17 14:08:58.442: INFO: Pod "projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca" satisfied condition "Succeeded or Failed"
    Nov 17 14:08:58.446: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca container projected-all-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:08:58.452
    Nov 17 14:08:58.467: INFO: Waiting for pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca to disappear
    Nov 17 14:08:58.471: INFO: Pod projected-volume-af32785f-054e-40db-adfa-90fb2d9121ca no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1884" for this suite. 11/17/23 14:08:58.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:58.481
Nov 17 14:08:58.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename conformance-tests 11/17/23 14:08:58.482
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:58.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:58.507
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 11/17/23 14:08:58.511
Nov 17 14:08:58.511: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Nov 17 14:08:58.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8965" for this suite. 11/17/23 14:08:58.522
------------------------------
â€¢ [0.049 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:58.481
    Nov 17 14:08:58.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename conformance-tests 11/17/23 14:08:58.482
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:58.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:58.507
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 11/17/23 14:08:58.511
    Nov 17 14:08:58.511: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:08:58.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8965" for this suite. 11/17/23 14:08:58.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:08:58.532
Nov 17 14:08:58.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-webhook 11/17/23 14:08:58.533
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:58.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:58.558
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/17/23 14:08:58.561
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/17/23 14:08:59.051
STEP: Deploying the custom resource conversion webhook pod 11/17/23 14:08:59.056
STEP: Wait for the deployment to be ready 11/17/23 14:08:59.071
Nov 17 14:08:59.079: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:09:01.091
STEP: Verifying the service has paired with the endpoint 11/17/23 14:09:01.116
Nov 17 14:09:02.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Nov 17 14:09:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Creating a v1 custom resource 11/17/23 14:09:05.055
STEP: Create a v2 custom resource 11/17/23 14:09:05.106
STEP: List CRs in v1 11/17/23 14:09:05.238
STEP: List CRs in v2 11/17/23 14:09:05.252
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:05.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-675" for this suite. 11/17/23 14:09:05.917
------------------------------
â€¢ [SLOW TEST] [7.399 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:08:58.532
    Nov 17 14:08:58.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-webhook 11/17/23 14:08:58.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:08:58.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:08:58.558
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/17/23 14:08:58.561
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/17/23 14:08:59.051
    STEP: Deploying the custom resource conversion webhook pod 11/17/23 14:08:59.056
    STEP: Wait for the deployment to be ready 11/17/23 14:08:59.071
    Nov 17 14:08:59.079: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:09:01.091
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:09:01.116
    Nov 17 14:09:02.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Nov 17 14:09:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Creating a v1 custom resource 11/17/23 14:09:05.055
    STEP: Create a v2 custom resource 11/17/23 14:09:05.106
    STEP: List CRs in v1 11/17/23 14:09:05.238
    STEP: List CRs in v2 11/17/23 14:09:05.252
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:05.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-675" for this suite. 11/17/23 14:09:05.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:05.932
Nov 17 14:09:05.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 14:09:05.933
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:05.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:05.98
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 11/17/23 14:09:05.988
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:06.008
STEP: Creating a service in the namespace 11/17/23 14:09:06.012
STEP: Deleting the namespace 11/17/23 14:09:06.05
STEP: Waiting for the namespace to be removed. 11/17/23 14:09:06.132
STEP: Recreating the namespace 11/17/23 14:09:12.136
STEP: Verifying there is no service in the namespace 11/17/23 14:09:12.153
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:12.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4428" for this suite. 11/17/23 14:09:12.16
STEP: Destroying namespace "nsdeletetest-7667" for this suite. 11/17/23 14:09:12.167
Nov 17 14:09:12.170: INFO: Namespace nsdeletetest-7667 was already deleted
STEP: Destroying namespace "nsdeletetest-2576" for this suite. 11/17/23 14:09:12.17
------------------------------
â€¢ [SLOW TEST] [6.246 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:05.932
    Nov 17 14:09:05.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 14:09:05.933
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:05.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:05.98
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 11/17/23 14:09:05.988
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:06.008
    STEP: Creating a service in the namespace 11/17/23 14:09:06.012
    STEP: Deleting the namespace 11/17/23 14:09:06.05
    STEP: Waiting for the namespace to be removed. 11/17/23 14:09:06.132
    STEP: Recreating the namespace 11/17/23 14:09:12.136
    STEP: Verifying there is no service in the namespace 11/17/23 14:09:12.153
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:12.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4428" for this suite. 11/17/23 14:09:12.16
    STEP: Destroying namespace "nsdeletetest-7667" for this suite. 11/17/23 14:09:12.167
    Nov 17 14:09:12.170: INFO: Namespace nsdeletetest-7667 was already deleted
    STEP: Destroying namespace "nsdeletetest-2576" for this suite. 11/17/23 14:09:12.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:12.181
Nov 17 14:09:12.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 14:09:12.182
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:12.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:12.201
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-137dfb3c-ca14-433b-9cc1-9d50a08b7301 11/17/23 14:09:12.209
STEP: Creating configMap with name cm-test-opt-upd-4ff529c5-09d9-4062-9d90-f308fdb928aa 11/17/23 14:09:12.214
STEP: Creating the pod 11/17/23 14:09:12.22
Nov 17 14:09:12.231: INFO: Waiting up to 5m0s for pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32" in namespace "configmap-4381" to be "running and ready"
Nov 17 14:09:12.234: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205269ms
Nov 17 14:09:12.234: INFO: The phase of Pod pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:09:14.239: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32": Phase="Running", Reason="", readiness=true. Elapsed: 2.007710585s
Nov 17 14:09:14.239: INFO: The phase of Pod pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32 is Running (Ready = true)
Nov 17 14:09:14.239: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-137dfb3c-ca14-433b-9cc1-9d50a08b7301 11/17/23 14:09:14.259
STEP: Updating configmap cm-test-opt-upd-4ff529c5-09d9-4062-9d90-f308fdb928aa 11/17/23 14:09:14.265
STEP: Creating configMap with name cm-test-opt-create-edfc84c9-6d64-40be-96d2-776ed5899a0b 11/17/23 14:09:14.27
STEP: waiting to observe update in volume 11/17/23 14:09:14.276
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:16.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4381" for this suite. 11/17/23 14:09:16.315
------------------------------
â€¢ [4.142 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:12.181
    Nov 17 14:09:12.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 14:09:12.182
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:12.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:12.201
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-137dfb3c-ca14-433b-9cc1-9d50a08b7301 11/17/23 14:09:12.209
    STEP: Creating configMap with name cm-test-opt-upd-4ff529c5-09d9-4062-9d90-f308fdb928aa 11/17/23 14:09:12.214
    STEP: Creating the pod 11/17/23 14:09:12.22
    Nov 17 14:09:12.231: INFO: Waiting up to 5m0s for pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32" in namespace "configmap-4381" to be "running and ready"
    Nov 17 14:09:12.234: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205269ms
    Nov 17 14:09:12.234: INFO: The phase of Pod pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:09:14.239: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32": Phase="Running", Reason="", readiness=true. Elapsed: 2.007710585s
    Nov 17 14:09:14.239: INFO: The phase of Pod pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32 is Running (Ready = true)
    Nov 17 14:09:14.239: INFO: Pod "pod-configmaps-421e2d91-e063-4421-bdf9-1f90abb88c32" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-137dfb3c-ca14-433b-9cc1-9d50a08b7301 11/17/23 14:09:14.259
    STEP: Updating configmap cm-test-opt-upd-4ff529c5-09d9-4062-9d90-f308fdb928aa 11/17/23 14:09:14.265
    STEP: Creating configMap with name cm-test-opt-create-edfc84c9-6d64-40be-96d2-776ed5899a0b 11/17/23 14:09:14.27
    STEP: waiting to observe update in volume 11/17/23 14:09:14.276
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:16.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4381" for this suite. 11/17/23 14:09:16.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:16.324
Nov 17 14:09:16.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:09:16.325
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:16.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:16.352
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Nov 17 14:09:16.367: INFO: Waiting up to 5m0s for pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d" in namespace "pods-8963" to be "running and ready"
Nov 17 14:09:16.374: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.028166ms
Nov 17 14:09:16.374: INFO: The phase of Pod server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:09:18.379: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012094161s
Nov 17 14:09:18.379: INFO: The phase of Pod server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d is Running (Ready = true)
Nov 17 14:09:18.379: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d" satisfied condition "running and ready"
Nov 17 14:09:18.439: INFO: Waiting up to 5m0s for pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa" in namespace "pods-8963" to be "Succeeded or Failed"
Nov 17 14:09:18.445: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903067ms
Nov 17 14:09:20.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010480206s
Nov 17 14:09:22.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00994791s
STEP: Saw pod success 11/17/23 14:09:22.449
Nov 17 14:09:22.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa" satisfied condition "Succeeded or Failed"
Nov 17 14:09:22.452: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa container env3cont: <nil>
STEP: delete the pod 11/17/23 14:09:22.458
Nov 17 14:09:22.470: INFO: Waiting for pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa to disappear
Nov 17 14:09:22.473: INFO: Pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:22.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8963" for this suite. 11/17/23 14:09:22.477
------------------------------
â€¢ [SLOW TEST] [6.160 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:16.324
    Nov 17 14:09:16.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:09:16.325
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:16.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:16.352
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Nov 17 14:09:16.367: INFO: Waiting up to 5m0s for pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d" in namespace "pods-8963" to be "running and ready"
    Nov 17 14:09:16.374: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.028166ms
    Nov 17 14:09:16.374: INFO: The phase of Pod server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:09:18.379: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012094161s
    Nov 17 14:09:18.379: INFO: The phase of Pod server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d is Running (Ready = true)
    Nov 17 14:09:18.379: INFO: Pod "server-envvars-770f73a9-96b6-4d5c-b25d-c05684abe68d" satisfied condition "running and ready"
    Nov 17 14:09:18.439: INFO: Waiting up to 5m0s for pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa" in namespace "pods-8963" to be "Succeeded or Failed"
    Nov 17 14:09:18.445: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903067ms
    Nov 17 14:09:20.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010480206s
    Nov 17 14:09:22.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00994791s
    STEP: Saw pod success 11/17/23 14:09:22.449
    Nov 17 14:09:22.449: INFO: Pod "client-envvars-81240db8-aa20-40db-b064-888c499a75fa" satisfied condition "Succeeded or Failed"
    Nov 17 14:09:22.452: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa container env3cont: <nil>
    STEP: delete the pod 11/17/23 14:09:22.458
    Nov 17 14:09:22.470: INFO: Waiting for pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa to disappear
    Nov 17 14:09:22.473: INFO: Pod client-envvars-81240db8-aa20-40db-b064-888c499a75fa no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:22.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8963" for this suite. 11/17/23 14:09:22.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:22.487
Nov 17 14:09:22.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:09:22.488
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:22.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:22.512
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Nov 17 14:09:22.526: INFO: Waiting up to 5m0s for pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d" in namespace "container-probe-6838" to be "running and ready"
Nov 17 14:09:22.531: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610286ms
Nov 17 14:09:22.531: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:09:24.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009966209s
Nov 17 14:09:24.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:26.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 4.010207845s
Nov 17 14:09:26.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:28.538: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 6.012532399s
Nov 17 14:09:28.538: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:30.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 8.009954486s
Nov 17 14:09:30.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:32.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 10.010370763s
Nov 17 14:09:32.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:34.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 12.009575339s
Nov 17 14:09:34.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:36.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 14.009501417s
Nov 17 14:09:36.535: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:38.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 16.009155725s
Nov 17 14:09:38.535: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:40.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 18.009855791s
Nov 17 14:09:40.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:42.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 20.010136519s
Nov 17 14:09:42.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
Nov 17 14:09:44.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=true. Elapsed: 22.00949461s
Nov 17 14:09:44.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = true)
Nov 17 14:09:44.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d" satisfied condition "running and ready"
Nov 17 14:09:44.539: INFO: Container started at 2023-11-17 14:09:23 +0000 UTC, pod became ready at 2023-11-17 14:09:42 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:44.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6838" for this suite. 11/17/23 14:09:44.546
------------------------------
â€¢ [SLOW TEST] [22.067 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:22.487
    Nov 17 14:09:22.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:09:22.488
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:22.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:22.512
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Nov 17 14:09:22.526: INFO: Waiting up to 5m0s for pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d" in namespace "container-probe-6838" to be "running and ready"
    Nov 17 14:09:22.531: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610286ms
    Nov 17 14:09:22.531: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:09:24.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 2.009966209s
    Nov 17 14:09:24.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:26.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 4.010207845s
    Nov 17 14:09:26.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:28.538: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 6.012532399s
    Nov 17 14:09:28.538: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:30.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 8.009954486s
    Nov 17 14:09:30.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:32.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 10.010370763s
    Nov 17 14:09:32.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:34.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 12.009575339s
    Nov 17 14:09:34.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:36.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 14.009501417s
    Nov 17 14:09:36.535: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:38.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 16.009155725s
    Nov 17 14:09:38.535: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:40.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 18.009855791s
    Nov 17 14:09:40.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:42.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=false. Elapsed: 20.010136519s
    Nov 17 14:09:42.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = false)
    Nov 17 14:09:44.535: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d": Phase="Running", Reason="", readiness=true. Elapsed: 22.00949461s
    Nov 17 14:09:44.536: INFO: The phase of Pod test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d is Running (Ready = true)
    Nov 17 14:09:44.536: INFO: Pod "test-webserver-d14944d8-ac7f-48a4-bb7f-78d08200b36d" satisfied condition "running and ready"
    Nov 17 14:09:44.539: INFO: Container started at 2023-11-17 14:09:23 +0000 UTC, pod became ready at 2023-11-17 14:09:42 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:44.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6838" for this suite. 11/17/23 14:09:44.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:44.558
Nov 17 14:09:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 14:09:44.559
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:44.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:44.587
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 11/17/23 14:09:44.591
STEP: Ensuring job reaches completions 11/17/23 14:09:44.601
STEP: Ensuring pods with index for job exist 11/17/23 14:09:54.605
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:54.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8387" for this suite. 11/17/23 14:09:54.613
------------------------------
â€¢ [SLOW TEST] [10.062 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:44.558
    Nov 17 14:09:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 14:09:44.559
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:44.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:44.587
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 11/17/23 14:09:44.591
    STEP: Ensuring job reaches completions 11/17/23 14:09:44.601
    STEP: Ensuring pods with index for job exist 11/17/23 14:09:54.605
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:54.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8387" for this suite. 11/17/23 14:09:54.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:54.621
Nov 17 14:09:54.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context 11/17/23 14:09:54.622
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:54.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:54.642
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/17/23 14:09:54.645
Nov 17 14:09:54.654: INFO: Waiting up to 5m0s for pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad" in namespace "security-context-2331" to be "Succeeded or Failed"
Nov 17 14:09:54.658: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757576ms
Nov 17 14:09:56.663: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008754868s
Nov 17 14:09:58.664: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009986478s
STEP: Saw pod success 11/17/23 14:09:58.664
Nov 17 14:09:58.665: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad" satisfied condition "Succeeded or Failed"
Nov 17 14:09:58.669: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad container test-container: <nil>
STEP: delete the pod 11/17/23 14:09:58.675
Nov 17 14:09:58.688: INFO: Waiting for pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad to disappear
Nov 17 14:09:58.691: INFO: Pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 14:09:58.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2331" for this suite. 11/17/23 14:09:58.697
------------------------------
â€¢ [4.083 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:54.621
    Nov 17 14:09:54.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context 11/17/23 14:09:54.622
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:54.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:54.642
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/17/23 14:09:54.645
    Nov 17 14:09:54.654: INFO: Waiting up to 5m0s for pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad" in namespace "security-context-2331" to be "Succeeded or Failed"
    Nov 17 14:09:54.658: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757576ms
    Nov 17 14:09:56.663: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008754868s
    Nov 17 14:09:58.664: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009986478s
    STEP: Saw pod success 11/17/23 14:09:58.664
    Nov 17 14:09:58.665: INFO: Pod "security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad" satisfied condition "Succeeded or Failed"
    Nov 17 14:09:58.669: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad container test-container: <nil>
    STEP: delete the pod 11/17/23 14:09:58.675
    Nov 17 14:09:58.688: INFO: Waiting for pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad to disappear
    Nov 17 14:09:58.691: INFO: Pod security-context-5985c619-0ccf-4daf-88c1-2957d8cf77ad no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:09:58.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2331" for this suite. 11/17/23 14:09:58.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:09:58.707
Nov 17 14:09:58.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:09:58.708
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:58.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:58.738
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:09:58.762
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:09:59.119
STEP: Deploying the webhook pod 11/17/23 14:09:59.128
STEP: Wait for the deployment to be ready 11/17/23 14:09:59.144
Nov 17 14:09:59.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:10:01.165
STEP: Verifying the service has paired with the endpoint 11/17/23 14:10:01.18
Nov 17 14:10:02.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/17/23 14:10:02.184
STEP: create a namespace for the webhook 11/17/23 14:10:02.206
STEP: create a configmap should be unconditionally rejected by the webhook 11/17/23 14:10:02.213
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:10:02.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5352" for this suite. 11/17/23 14:10:02.343
STEP: Destroying namespace "webhook-5352-markers" for this suite. 11/17/23 14:10:02.357
------------------------------
â€¢ [3.666 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:09:58.707
    Nov 17 14:09:58.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:09:58.708
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:09:58.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:09:58.738
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:09:58.762
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:09:59.119
    STEP: Deploying the webhook pod 11/17/23 14:09:59.128
    STEP: Wait for the deployment to be ready 11/17/23 14:09:59.144
    Nov 17 14:09:59.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:10:01.165
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:10:01.18
    Nov 17 14:10:02.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 11/17/23 14:10:02.184
    STEP: create a namespace for the webhook 11/17/23 14:10:02.206
    STEP: create a configmap should be unconditionally rejected by the webhook 11/17/23 14:10:02.213
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:10:02.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5352" for this suite. 11/17/23 14:10:02.343
    STEP: Destroying namespace "webhook-5352-markers" for this suite. 11/17/23 14:10:02.357
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:10:02.377
Nov 17 14:10:02.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename proxy 11/17/23 14:10:02.378
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:02.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:02.42
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 11/17/23 14:10:02.456
STEP: creating replication controller proxy-service-l7l52 in namespace proxy-5812 11/17/23 14:10:02.456
I1117 14:10:02.470775      23 runners.go:193] Created replication controller with name: proxy-service-l7l52, namespace: proxy-5812, replica count: 1
I1117 14:10:03.522456      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1117 14:10:04.523548      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1117 14:10:05.523973      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 14:10:05.638: INFO: setup took 3.214077051s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/17/23 14:10:05.638
Nov 17 14:10:05.732: INFO: (0) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 93.029892ms)
Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 108.58692ms)
Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 109.186096ms)
Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 108.862171ms)
Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 110.067968ms)
Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 110.777333ms)
Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 110.736593ms)
Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 111.039772ms)
Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 110.739926ms)
Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 110.991662ms)
Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 111.338657ms)
Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 124.73988ms)
Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 125.355753ms)
Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 125.152402ms)
Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 125.131787ms)
Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 125.50354ms)
Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 72.801476ms)
Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 72.782652ms)
Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 72.862471ms)
Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 73.169902ms)
Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 73.408126ms)
Nov 17 14:10:05.838: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 73.361141ms)
Nov 17 14:10:05.838: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 73.324834ms)
Nov 17 14:10:05.852: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 87.762949ms)
Nov 17 14:10:05.852: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 88.129955ms)
Nov 17 14:10:05.854: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 89.489047ms)
Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 101.142622ms)
Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 101.202177ms)
Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 101.012866ms)
Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 101.481245ms)
Nov 17 14:10:05.883: INFO: (1) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 118.699663ms)
Nov 17 14:10:05.883: INFO: (1) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 118.55312ms)
Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 60.305737ms)
Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.419782ms)
Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.445662ms)
Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 60.629767ms)
Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 61.460987ms)
Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 61.573149ms)
Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 61.886337ms)
Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 61.401899ms)
Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 61.673535ms)
Nov 17 14:10:05.947: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 62.793633ms)
Nov 17 14:10:06.006: INFO: (2) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 121.8202ms)
Nov 17 14:10:06.007: INFO: (2) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 123.736596ms)
Nov 17 14:10:06.007: INFO: (2) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 123.466872ms)
Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 124.276595ms)
Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 124.591215ms)
Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 124.370926ms)
Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 89.348894ms)
Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 89.728943ms)
Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 89.333016ms)
Nov 17 14:10:06.100: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 91.316938ms)
Nov 17 14:10:06.107: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 98.668842ms)
Nov 17 14:10:06.107: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 98.871677ms)
Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 98.819007ms)
Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 99.392719ms)
Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 99.019796ms)
Nov 17 14:10:06.127: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 118.287757ms)
Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 119.358223ms)
Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 119.456953ms)
Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 119.665886ms)
Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 119.71564ms)
Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 119.653372ms)
Nov 17 14:10:06.129: INFO: (3) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 120.16661ms)
Nov 17 14:10:06.188: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 59.493386ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 60.830599ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 60.669444ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.603404ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 60.725644ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 60.960355ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 61.183286ms)
Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 61.625269ms)
Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 61.812758ms)
Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 61.687729ms)
Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 61.838163ms)
Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 90.847447ms)
Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 91.15042ms)
Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 91.381287ms)
Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 91.484726ms)
Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 91.424052ms)
Nov 17 14:10:06.293: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 71.683981ms)
Nov 17 14:10:06.293: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 72.182072ms)
Nov 17 14:10:06.340: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 119.042268ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 122.817343ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 123.315236ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 123.236828ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 123.191997ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 123.048565ms)
Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 123.047789ms)
Nov 17 14:10:06.345: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 123.966108ms)
Nov 17 14:10:06.377: INFO: (5) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 155.700602ms)
Nov 17 14:10:06.386: INFO: (5) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 165.390204ms)
Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 169.266509ms)
Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 168.666281ms)
Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 168.596167ms)
Nov 17 14:10:06.392: INFO: (5) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 170.494393ms)
Nov 17 14:10:06.509: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 117.151878ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 117.247135ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 117.741359ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 117.81503ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 117.80326ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 118.201385ms)
Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 118.075107ms)
Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 118.212296ms)
Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 118.717819ms)
Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 119.469579ms)
Nov 17 14:10:06.520: INFO: (6) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 127.483595ms)
Nov 17 14:10:06.520: INFO: (6) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 127.968072ms)
Nov 17 14:10:06.539: INFO: (6) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 146.400885ms)
Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 148.204002ms)
Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 147.956159ms)
Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 147.899447ms)
Nov 17 14:10:06.632: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 91.348121ms)
Nov 17 14:10:06.632: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 91.399335ms)
Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 91.859526ms)
Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 91.796683ms)
Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 92.000684ms)
Nov 17 14:10:06.637: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 96.729011ms)
Nov 17 14:10:06.637: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 96.307028ms)
Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 97.608318ms)
Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 97.961038ms)
Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 97.732853ms)
Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 119.883317ms)
Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 119.969367ms)
Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 120.042967ms)
Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 120.205669ms)
Nov 17 14:10:06.722: INFO: (7) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 180.826054ms)
Nov 17 14:10:06.724: INFO: (7) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 183.114875ms)
Nov 17 14:10:06.847: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 122.749799ms)
Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 123.533867ms)
Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 123.393154ms)
Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 123.690078ms)
Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 125.620202ms)
Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 126.325341ms)
Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 126.202307ms)
Nov 17 14:10:06.851: INFO: (8) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 126.510132ms)
Nov 17 14:10:06.851: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 127.437584ms)
Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 139.641966ms)
Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 139.466552ms)
Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 140.187082ms)
Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 141.234785ms)
Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 141.941878ms)
Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 141.627376ms)
Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 141.587804ms)
Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 81.266062ms)
Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 81.193358ms)
Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 81.139933ms)
Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 81.244521ms)
Nov 17 14:10:06.949: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 82.078985ms)
Nov 17 14:10:06.949: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 82.981772ms)
Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 82.683883ms)
Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 83.194047ms)
Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 83.188221ms)
Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 83.673272ms)
Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 83.253787ms)
Nov 17 14:10:06.956: INFO: (9) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 89.467637ms)
Nov 17 14:10:06.956: INFO: (9) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 89.613363ms)
Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 90.200277ms)
Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 90.440671ms)
Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 90.427961ms)
Nov 17 14:10:07.007: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 49.606524ms)
Nov 17 14:10:07.030: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 72.726064ms)
Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 92.686428ms)
Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 92.662595ms)
Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 92.678288ms)
Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 93.098573ms)
Nov 17 14:10:07.051: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 93.036479ms)
Nov 17 14:10:07.051: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 92.978075ms)
Nov 17 14:10:07.068: INFO: (10) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 110.511599ms)
Nov 17 14:10:07.068: INFO: (10) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 110.537735ms)
Nov 17 14:10:07.069: INFO: (10) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 111.539267ms)
Nov 17 14:10:07.070: INFO: (10) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 112.156073ms)
Nov 17 14:10:07.070: INFO: (10) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 112.25471ms)
Nov 17 14:10:07.086: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 128.663039ms)
Nov 17 14:10:07.086: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 129.216005ms)
Nov 17 14:10:07.148: INFO: (10) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 190.225541ms)
Nov 17 14:10:07.303: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 154.682006ms)
Nov 17 14:10:07.312: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 163.389407ms)
Nov 17 14:10:07.312: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 163.86056ms)
Nov 17 14:10:07.315: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 166.250201ms)
Nov 17 14:10:07.315: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 167.269595ms)
Nov 17 14:10:07.316: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 167.836652ms)
Nov 17 14:10:07.317: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 168.72297ms)
Nov 17 14:10:07.323: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 174.921507ms)
Nov 17 14:10:07.323: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 174.828626ms)
Nov 17 14:10:07.324: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 175.22628ms)
Nov 17 14:10:07.350: INFO: (11) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 201.808989ms)
Nov 17 14:10:07.350: INFO: (11) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 201.787809ms)
Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 202.241523ms)
Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 202.522347ms)
Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 202.549686ms)
Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 202.301936ms)
Nov 17 14:10:07.419: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 68.113617ms)
Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 68.23625ms)
Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 67.91904ms)
Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 68.216443ms)
Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 68.357413ms)
Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 68.24232ms)
Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 84.437104ms)
Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 84.556905ms)
Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 84.538554ms)
Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 84.404526ms)
Nov 17 14:10:07.447: INFO: (12) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 95.75992ms)
Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 96.79727ms)
Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 96.501953ms)
Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 96.879068ms)
Nov 17 14:10:07.455: INFO: (12) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 103.292622ms)
Nov 17 14:10:07.456: INFO: (12) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 104.625685ms)
Nov 17 14:10:07.542: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 85.130276ms)
Nov 17 14:10:07.542: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 85.914601ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 85.945645ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 86.065995ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 86.203173ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 85.978977ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 86.067951ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 86.156946ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 86.683588ms)
Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 86.344284ms)
Nov 17 14:10:07.561: INFO: (13) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 103.949264ms)
Nov 17 14:10:07.561: INFO: (13) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 104.09471ms)
Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 105.46343ms)
Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 105.73181ms)
Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 105.453467ms)
Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 105.791075ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 33.286355ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 32.929007ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 33.06911ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 33.334858ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 33.430145ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 33.322308ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 33.633364ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 33.485908ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 33.311701ms)
Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 33.577424ms)
Nov 17 14:10:07.599: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 36.212853ms)
Nov 17 14:10:07.599: INFO: (14) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 36.448705ms)
Nov 17 14:10:07.600: INFO: (14) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 37.801859ms)
Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 39.411396ms)
Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 39.496488ms)
Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 39.560055ms)
Nov 17 14:10:07.621: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 18.234181ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 18.783547ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 19.129818ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 19.029415ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 19.199259ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 19.365631ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 19.253103ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 19.495852ms)
Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 19.290806ms)
Nov 17 14:10:07.623: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 19.633996ms)
Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 32.879121ms)
Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 33.241571ms)
Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 33.305232ms)
Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 33.181684ms)
Nov 17 14:10:07.637: INFO: (15) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 33.628602ms)
Nov 17 14:10:07.638: INFO: (15) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 34.602623ms)
Nov 17 14:10:07.650: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 12.034873ms)
Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 27.128912ms)
Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 27.066799ms)
Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 27.170957ms)
Nov 17 14:10:07.667: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 28.517205ms)
Nov 17 14:10:07.667: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 28.872573ms)
Nov 17 14:10:07.668: INFO: (16) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 30.570954ms)
Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 30.405924ms)
Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 30.054776ms)
Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 30.805574ms)
Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 30.506373ms)
Nov 17 14:10:07.670: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 32.356909ms)
Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 38.045321ms)
Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 37.839022ms)
Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 38.197914ms)
Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 37.857257ms)
Nov 17 14:10:07.697: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 19.745774ms)
Nov 17 14:10:07.699: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 22.453933ms)
Nov 17 14:10:07.699: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 22.434868ms)
Nov 17 14:10:07.702: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 25.023772ms)
Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 26.246812ms)
Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 26.206915ms)
Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 26.409613ms)
Nov 17 14:10:07.704: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 26.367457ms)
Nov 17 14:10:07.715: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 37.948884ms)
Nov 17 14:10:07.716: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 38.868334ms)
Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 43.818527ms)
Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 43.593531ms)
Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 43.468812ms)
Nov 17 14:10:07.722: INFO: (17) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 44.762872ms)
Nov 17 14:10:07.723: INFO: (17) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 46.133638ms)
Nov 17 14:10:07.736: INFO: (17) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 58.996525ms)
Nov 17 14:10:07.752: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 15.205686ms)
Nov 17 14:10:07.752: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 15.190498ms)
Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 16.68172ms)
Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 16.956243ms)
Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 17.234771ms)
Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 17.796524ms)
Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 18.131297ms)
Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 17.555147ms)
Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 17.6571ms)
Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 17.648574ms)
Nov 17 14:10:07.761: INFO: (18) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 24.0335ms)
Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 24.781243ms)
Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 24.545759ms)
Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 24.764813ms)
Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 24.364987ms)
Nov 17 14:10:07.774: INFO: (18) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 36.419694ms)
Nov 17 14:10:07.793: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 19.422457ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 28.925744ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 28.525332ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 28.316499ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 28.621563ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 28.575331ms)
Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 28.762717ms)
Nov 17 14:10:07.804: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 29.999032ms)
Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 31.567378ms)
Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 31.70553ms)
Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 31.746165ms)
Nov 17 14:10:07.807: INFO: (19) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 32.964626ms)
Nov 17 14:10:07.807: INFO: (19) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 33.06836ms)
Nov 17 14:10:07.808: INFO: (19) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 33.46696ms)
Nov 17 14:10:07.808: INFO: (19) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 33.640753ms)
Nov 17 14:10:07.809: INFO: (19) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 34.831726ms)
STEP: deleting ReplicationController proxy-service-l7l52 in namespace proxy-5812, will wait for the garbage collector to delete the pods 11/17/23 14:10:07.809
Nov 17 14:10:07.894: INFO: Deleting ReplicationController proxy-service-l7l52 took: 26.416572ms
Nov 17 14:10:07.995: INFO: Terminating ReplicationController proxy-service-l7l52 pods took: 100.578658ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Nov 17 14:10:08.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5812" for this suite. 11/17/23 14:10:09.003
------------------------------
â€¢ [SLOW TEST] [6.637 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:10:02.377
    Nov 17 14:10:02.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename proxy 11/17/23 14:10:02.378
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:02.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:02.42
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 11/17/23 14:10:02.456
    STEP: creating replication controller proxy-service-l7l52 in namespace proxy-5812 11/17/23 14:10:02.456
    I1117 14:10:02.470775      23 runners.go:193] Created replication controller with name: proxy-service-l7l52, namespace: proxy-5812, replica count: 1
    I1117 14:10:03.522456      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1117 14:10:04.523548      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I1117 14:10:05.523973      23 runners.go:193] proxy-service-l7l52 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 14:10:05.638: INFO: setup took 3.214077051s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 11/17/23 14:10:05.638
    Nov 17 14:10:05.732: INFO: (0) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 93.029892ms)
    Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 108.58692ms)
    Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 109.186096ms)
    Nov 17 14:10:05.747: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 108.862171ms)
    Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 110.067968ms)
    Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 110.777333ms)
    Nov 17 14:10:05.749: INFO: (0) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 110.736593ms)
    Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 111.039772ms)
    Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 110.739926ms)
    Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 110.991662ms)
    Nov 17 14:10:05.750: INFO: (0) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 111.338657ms)
    Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 124.73988ms)
    Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 125.355753ms)
    Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 125.152402ms)
    Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 125.131787ms)
    Nov 17 14:10:05.764: INFO: (0) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 125.50354ms)
    Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 72.801476ms)
    Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 72.782652ms)
    Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 72.862471ms)
    Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 73.169902ms)
    Nov 17 14:10:05.837: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 73.408126ms)
    Nov 17 14:10:05.838: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 73.361141ms)
    Nov 17 14:10:05.838: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 73.324834ms)
    Nov 17 14:10:05.852: INFO: (1) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 87.762949ms)
    Nov 17 14:10:05.852: INFO: (1) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 88.129955ms)
    Nov 17 14:10:05.854: INFO: (1) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 89.489047ms)
    Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 101.142622ms)
    Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 101.202177ms)
    Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 101.012866ms)
    Nov 17 14:10:05.865: INFO: (1) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 101.481245ms)
    Nov 17 14:10:05.883: INFO: (1) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 118.699663ms)
    Nov 17 14:10:05.883: INFO: (1) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 118.55312ms)
    Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 60.305737ms)
    Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.419782ms)
    Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.445662ms)
    Nov 17 14:10:05.944: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 60.629767ms)
    Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 61.460987ms)
    Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 61.573149ms)
    Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 61.886337ms)
    Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 61.401899ms)
    Nov 17 14:10:05.945: INFO: (2) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 61.673535ms)
    Nov 17 14:10:05.947: INFO: (2) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 62.793633ms)
    Nov 17 14:10:06.006: INFO: (2) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 121.8202ms)
    Nov 17 14:10:06.007: INFO: (2) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 123.736596ms)
    Nov 17 14:10:06.007: INFO: (2) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 123.466872ms)
    Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 124.276595ms)
    Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 124.591215ms)
    Nov 17 14:10:06.008: INFO: (2) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 124.370926ms)
    Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 89.348894ms)
    Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 89.728943ms)
    Nov 17 14:10:06.098: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 89.333016ms)
    Nov 17 14:10:06.100: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 91.316938ms)
    Nov 17 14:10:06.107: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 98.668842ms)
    Nov 17 14:10:06.107: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 98.871677ms)
    Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 98.819007ms)
    Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 99.392719ms)
    Nov 17 14:10:06.108: INFO: (3) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 99.019796ms)
    Nov 17 14:10:06.127: INFO: (3) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 118.287757ms)
    Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 119.358223ms)
    Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 119.456953ms)
    Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 119.665886ms)
    Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 119.71564ms)
    Nov 17 14:10:06.128: INFO: (3) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 119.653372ms)
    Nov 17 14:10:06.129: INFO: (3) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 120.16661ms)
    Nov 17 14:10:06.188: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 59.493386ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 60.830599ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 60.669444ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 60.603404ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 60.725644ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 60.960355ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 61.183286ms)
    Nov 17 14:10:06.190: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 61.625269ms)
    Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 61.812758ms)
    Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 61.687729ms)
    Nov 17 14:10:06.191: INFO: (4) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 61.838163ms)
    Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 90.847447ms)
    Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 91.15042ms)
    Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 91.381287ms)
    Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 91.484726ms)
    Nov 17 14:10:06.220: INFO: (4) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 91.424052ms)
    Nov 17 14:10:06.293: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 71.683981ms)
    Nov 17 14:10:06.293: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 72.182072ms)
    Nov 17 14:10:06.340: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 119.042268ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 122.817343ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 123.315236ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 123.236828ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 123.191997ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 123.048565ms)
    Nov 17 14:10:06.344: INFO: (5) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 123.047789ms)
    Nov 17 14:10:06.345: INFO: (5) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 123.966108ms)
    Nov 17 14:10:06.377: INFO: (5) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 155.700602ms)
    Nov 17 14:10:06.386: INFO: (5) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 165.390204ms)
    Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 169.266509ms)
    Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 168.666281ms)
    Nov 17 14:10:06.390: INFO: (5) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 168.596167ms)
    Nov 17 14:10:06.392: INFO: (5) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 170.494393ms)
    Nov 17 14:10:06.509: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 117.151878ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 117.247135ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 117.741359ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 117.81503ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 117.80326ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 118.201385ms)
    Nov 17 14:10:06.510: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 118.075107ms)
    Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 118.212296ms)
    Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 118.717819ms)
    Nov 17 14:10:06.511: INFO: (6) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 119.469579ms)
    Nov 17 14:10:06.520: INFO: (6) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 127.483595ms)
    Nov 17 14:10:06.520: INFO: (6) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 127.968072ms)
    Nov 17 14:10:06.539: INFO: (6) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 146.400885ms)
    Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 148.204002ms)
    Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 147.956159ms)
    Nov 17 14:10:06.540: INFO: (6) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 147.899447ms)
    Nov 17 14:10:06.632: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 91.348121ms)
    Nov 17 14:10:06.632: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 91.399335ms)
    Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 91.859526ms)
    Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 91.796683ms)
    Nov 17 14:10:06.633: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 92.000684ms)
    Nov 17 14:10:06.637: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 96.729011ms)
    Nov 17 14:10:06.637: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 96.307028ms)
    Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 97.608318ms)
    Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 97.961038ms)
    Nov 17 14:10:06.638: INFO: (7) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 97.732853ms)
    Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 119.883317ms)
    Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 119.969367ms)
    Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 120.042967ms)
    Nov 17 14:10:06.661: INFO: (7) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 120.205669ms)
    Nov 17 14:10:06.722: INFO: (7) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 180.826054ms)
    Nov 17 14:10:06.724: INFO: (7) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 183.114875ms)
    Nov 17 14:10:06.847: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 122.749799ms)
    Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 123.533867ms)
    Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 123.393154ms)
    Nov 17 14:10:06.848: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 123.690078ms)
    Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 125.620202ms)
    Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 126.325341ms)
    Nov 17 14:10:06.850: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 126.202307ms)
    Nov 17 14:10:06.851: INFO: (8) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 126.510132ms)
    Nov 17 14:10:06.851: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 127.437584ms)
    Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 139.641966ms)
    Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 139.466552ms)
    Nov 17 14:10:06.864: INFO: (8) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 140.187082ms)
    Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 141.234785ms)
    Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 141.941878ms)
    Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 141.627376ms)
    Nov 17 14:10:06.866: INFO: (8) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 141.587804ms)
    Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 81.266062ms)
    Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 81.193358ms)
    Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 81.139933ms)
    Nov 17 14:10:06.948: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 81.244521ms)
    Nov 17 14:10:06.949: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 82.078985ms)
    Nov 17 14:10:06.949: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 82.981772ms)
    Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 82.683883ms)
    Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 83.194047ms)
    Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 83.188221ms)
    Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 83.673272ms)
    Nov 17 14:10:06.950: INFO: (9) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 83.253787ms)
    Nov 17 14:10:06.956: INFO: (9) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 89.467637ms)
    Nov 17 14:10:06.956: INFO: (9) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 89.613363ms)
    Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 90.200277ms)
    Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 90.440671ms)
    Nov 17 14:10:06.957: INFO: (9) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 90.427961ms)
    Nov 17 14:10:07.007: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 49.606524ms)
    Nov 17 14:10:07.030: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 72.726064ms)
    Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 92.686428ms)
    Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 92.662595ms)
    Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 92.678288ms)
    Nov 17 14:10:07.050: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 93.098573ms)
    Nov 17 14:10:07.051: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 93.036479ms)
    Nov 17 14:10:07.051: INFO: (10) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 92.978075ms)
    Nov 17 14:10:07.068: INFO: (10) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 110.511599ms)
    Nov 17 14:10:07.068: INFO: (10) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 110.537735ms)
    Nov 17 14:10:07.069: INFO: (10) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 111.539267ms)
    Nov 17 14:10:07.070: INFO: (10) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 112.156073ms)
    Nov 17 14:10:07.070: INFO: (10) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 112.25471ms)
    Nov 17 14:10:07.086: INFO: (10) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 128.663039ms)
    Nov 17 14:10:07.086: INFO: (10) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 129.216005ms)
    Nov 17 14:10:07.148: INFO: (10) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 190.225541ms)
    Nov 17 14:10:07.303: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 154.682006ms)
    Nov 17 14:10:07.312: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 163.389407ms)
    Nov 17 14:10:07.312: INFO: (11) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 163.86056ms)
    Nov 17 14:10:07.315: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 166.250201ms)
    Nov 17 14:10:07.315: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 167.269595ms)
    Nov 17 14:10:07.316: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 167.836652ms)
    Nov 17 14:10:07.317: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 168.72297ms)
    Nov 17 14:10:07.323: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 174.921507ms)
    Nov 17 14:10:07.323: INFO: (11) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 174.828626ms)
    Nov 17 14:10:07.324: INFO: (11) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 175.22628ms)
    Nov 17 14:10:07.350: INFO: (11) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 201.808989ms)
    Nov 17 14:10:07.350: INFO: (11) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 201.787809ms)
    Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 202.241523ms)
    Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 202.522347ms)
    Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 202.549686ms)
    Nov 17 14:10:07.351: INFO: (11) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 202.301936ms)
    Nov 17 14:10:07.419: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 68.113617ms)
    Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 68.23625ms)
    Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 67.91904ms)
    Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 68.216443ms)
    Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 68.357413ms)
    Nov 17 14:10:07.420: INFO: (12) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 68.24232ms)
    Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 84.437104ms)
    Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 84.556905ms)
    Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 84.538554ms)
    Nov 17 14:10:07.436: INFO: (12) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 84.404526ms)
    Nov 17 14:10:07.447: INFO: (12) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 95.75992ms)
    Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 96.79727ms)
    Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 96.501953ms)
    Nov 17 14:10:07.448: INFO: (12) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 96.879068ms)
    Nov 17 14:10:07.455: INFO: (12) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 103.292622ms)
    Nov 17 14:10:07.456: INFO: (12) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 104.625685ms)
    Nov 17 14:10:07.542: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 85.130276ms)
    Nov 17 14:10:07.542: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 85.914601ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 85.945645ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 86.065995ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 86.203173ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 85.978977ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 86.067951ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 86.156946ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 86.683588ms)
    Nov 17 14:10:07.543: INFO: (13) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 86.344284ms)
    Nov 17 14:10:07.561: INFO: (13) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 103.949264ms)
    Nov 17 14:10:07.561: INFO: (13) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 104.09471ms)
    Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 105.46343ms)
    Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 105.73181ms)
    Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 105.453467ms)
    Nov 17 14:10:07.562: INFO: (13) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 105.791075ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 33.286355ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 32.929007ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 33.06911ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 33.334858ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 33.430145ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 33.322308ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 33.633364ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 33.485908ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 33.311701ms)
    Nov 17 14:10:07.596: INFO: (14) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 33.577424ms)
    Nov 17 14:10:07.599: INFO: (14) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 36.212853ms)
    Nov 17 14:10:07.599: INFO: (14) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 36.448705ms)
    Nov 17 14:10:07.600: INFO: (14) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 37.801859ms)
    Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 39.411396ms)
    Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 39.496488ms)
    Nov 17 14:10:07.602: INFO: (14) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 39.560055ms)
    Nov 17 14:10:07.621: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 18.234181ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 18.783547ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 19.129818ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 19.029415ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 19.199259ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 19.365631ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 19.253103ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 19.495852ms)
    Nov 17 14:10:07.622: INFO: (15) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 19.290806ms)
    Nov 17 14:10:07.623: INFO: (15) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 19.633996ms)
    Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 32.879121ms)
    Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 33.241571ms)
    Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 33.305232ms)
    Nov 17 14:10:07.636: INFO: (15) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 33.181684ms)
    Nov 17 14:10:07.637: INFO: (15) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 33.628602ms)
    Nov 17 14:10:07.638: INFO: (15) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 34.602623ms)
    Nov 17 14:10:07.650: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 12.034873ms)
    Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 27.128912ms)
    Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 27.066799ms)
    Nov 17 14:10:07.665: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 27.170957ms)
    Nov 17 14:10:07.667: INFO: (16) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 28.517205ms)
    Nov 17 14:10:07.667: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 28.872573ms)
    Nov 17 14:10:07.668: INFO: (16) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 30.570954ms)
    Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 30.405924ms)
    Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 30.054776ms)
    Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 30.805574ms)
    Nov 17 14:10:07.669: INFO: (16) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 30.506373ms)
    Nov 17 14:10:07.670: INFO: (16) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 32.356909ms)
    Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 38.045321ms)
    Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 37.839022ms)
    Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 38.197914ms)
    Nov 17 14:10:07.676: INFO: (16) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 37.857257ms)
    Nov 17 14:10:07.697: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 19.745774ms)
    Nov 17 14:10:07.699: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 22.453933ms)
    Nov 17 14:10:07.699: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 22.434868ms)
    Nov 17 14:10:07.702: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 25.023772ms)
    Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 26.246812ms)
    Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 26.206915ms)
    Nov 17 14:10:07.703: INFO: (17) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 26.409613ms)
    Nov 17 14:10:07.704: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 26.367457ms)
    Nov 17 14:10:07.715: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 37.948884ms)
    Nov 17 14:10:07.716: INFO: (17) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 38.868334ms)
    Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 43.818527ms)
    Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 43.593531ms)
    Nov 17 14:10:07.721: INFO: (17) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 43.468812ms)
    Nov 17 14:10:07.722: INFO: (17) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 44.762872ms)
    Nov 17 14:10:07.723: INFO: (17) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 46.133638ms)
    Nov 17 14:10:07.736: INFO: (17) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 58.996525ms)
    Nov 17 14:10:07.752: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 15.205686ms)
    Nov 17 14:10:07.752: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 15.190498ms)
    Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 16.68172ms)
    Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 16.956243ms)
    Nov 17 14:10:07.754: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 17.234771ms)
    Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 17.796524ms)
    Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 18.131297ms)
    Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 17.555147ms)
    Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 17.6571ms)
    Nov 17 14:10:07.755: INFO: (18) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 17.648574ms)
    Nov 17 14:10:07.761: INFO: (18) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 24.0335ms)
    Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 24.781243ms)
    Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 24.545759ms)
    Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 24.764813ms)
    Nov 17 14:10:07.762: INFO: (18) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 24.364987ms)
    Nov 17 14:10:07.774: INFO: (18) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 36.419694ms)
    Nov 17 14:10:07.793: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:462/proxy/: tls qux (200; 19.422457ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">... (200; 28.925744ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:460/proxy/: tls baz (200; 28.525332ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/https:proxy-service-l7l52-mzjgk:443/proxy/tlsrewritem... (200; 28.316499ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname1/proxy/: foo (200; 28.621563ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 28.575331ms)
    Nov 17 14:10:07.803: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 28.762717ms)
    Nov 17 14:10:07.804: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:1080/proxy/rewriteme">test<... (200; 29.999032ms)
    Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/: <a href="/api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk/proxy/rewriteme">test</a> (200; 31.567378ms)
    Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/http:proxy-service-l7l52-mzjgk:162/proxy/: bar (200; 31.70553ms)
    Nov 17 14:10:07.806: INFO: (19) /api/v1/namespaces/proxy-5812/pods/proxy-service-l7l52-mzjgk:160/proxy/: foo (200; 31.746165ms)
    Nov 17 14:10:07.807: INFO: (19) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname2/proxy/: bar (200; 32.964626ms)
    Nov 17 14:10:07.807: INFO: (19) /api/v1/namespaces/proxy-5812/services/proxy-service-l7l52:portname1/proxy/: foo (200; 33.06836ms)
    Nov 17 14:10:07.808: INFO: (19) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname1/proxy/: tls baz (200; 33.46696ms)
    Nov 17 14:10:07.808: INFO: (19) /api/v1/namespaces/proxy-5812/services/http:proxy-service-l7l52:portname2/proxy/: bar (200; 33.640753ms)
    Nov 17 14:10:07.809: INFO: (19) /api/v1/namespaces/proxy-5812/services/https:proxy-service-l7l52:tlsportname2/proxy/: tls qux (200; 34.831726ms)
    STEP: deleting ReplicationController proxy-service-l7l52 in namespace proxy-5812, will wait for the garbage collector to delete the pods 11/17/23 14:10:07.809
    Nov 17 14:10:07.894: INFO: Deleting ReplicationController proxy-service-l7l52 took: 26.416572ms
    Nov 17 14:10:07.995: INFO: Terminating ReplicationController proxy-service-l7l52 pods took: 100.578658ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:10:08.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5812" for this suite. 11/17/23 14:10:09.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:10:09.022
Nov 17 14:10:09.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:10:09.024
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:09.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:09.054
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 11/17/23 14:10:09.058
STEP: listing secrets in all namespaces to ensure that there are more than zero 11/17/23 14:10:09.064
STEP: patching the secret 11/17/23 14:10:09.069
STEP: deleting the secret using a LabelSelector 11/17/23 14:10:09.084
STEP: listing secrets in all namespaces, searching for label name and value in patch 11/17/23 14:10:09.097
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:10:09.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5757" for this suite. 11/17/23 14:10:09.111
------------------------------
â€¢ [0.102 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:10:09.022
    Nov 17 14:10:09.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:10:09.024
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:09.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:09.054
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 11/17/23 14:10:09.058
    STEP: listing secrets in all namespaces to ensure that there are more than zero 11/17/23 14:10:09.064
    STEP: patching the secret 11/17/23 14:10:09.069
    STEP: deleting the secret using a LabelSelector 11/17/23 14:10:09.084
    STEP: listing secrets in all namespaces, searching for label name and value in patch 11/17/23 14:10:09.097
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:10:09.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5757" for this suite. 11/17/23 14:10:09.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:10:09.129
Nov 17 14:10:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubelet-test 11/17/23 14:10:09.131
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:09.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:09.168
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 11/17/23 14:10:09.18
Nov 17 14:10:09.181: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff" in namespace "kubelet-test-816" to be "completed"
Nov 17 14:10:09.183: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458525ms
Nov 17 14:10:11.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006945626s
Nov 17 14:10:13.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006999282s
Nov 17 14:10:13.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:10:13.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-816" for this suite. 11/17/23 14:10:13.197
------------------------------
â€¢ [4.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:10:09.129
    Nov 17 14:10:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubelet-test 11/17/23 14:10:09.131
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:09.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:09.168
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 11/17/23 14:10:09.18
    Nov 17 14:10:09.181: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff" in namespace "kubelet-test-816" to be "completed"
    Nov 17 14:10:09.183: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458525ms
    Nov 17 14:10:11.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006945626s
    Nov 17 14:10:13.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006999282s
    Nov 17 14:10:13.188: INFO: Pod "agnhost-host-aliasesd472a7c5-8ff3-4e7c-bc83-8693e0a67aff" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:10:13.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-816" for this suite. 11/17/23 14:10:13.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:10:13.208
Nov 17 14:10:13.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:10:13.209
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:13.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:13.229
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce in namespace container-probe-8753 11/17/23 14:10:13.232
Nov 17 14:10:13.242: INFO: Waiting up to 5m0s for pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce" in namespace "container-probe-8753" to be "not pending"
Nov 17 14:10:13.246: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891035ms
Nov 17 14:10:15.250: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.007576773s
Nov 17 14:10:15.250: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce" satisfied condition "not pending"
Nov 17 14:10:15.250: INFO: Started pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce in namespace container-probe-8753
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:10:15.25
Nov 17 14:10:15.253: INFO: Initial restart count of pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce is 0
STEP: deleting the pod 11/17/23 14:14:15.825
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:15.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8753" for this suite. 11/17/23 14:14:15.874
------------------------------
â€¢ [SLOW TEST] [242.675 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:10:13.208
    Nov 17 14:10:13.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:10:13.209
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:10:13.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:10:13.229
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce in namespace container-probe-8753 11/17/23 14:10:13.232
    Nov 17 14:10:13.242: INFO: Waiting up to 5m0s for pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce" in namespace "container-probe-8753" to be "not pending"
    Nov 17 14:10:13.246: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891035ms
    Nov 17 14:10:15.250: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.007576773s
    Nov 17 14:10:15.250: INFO: Pod "test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce" satisfied condition "not pending"
    Nov 17 14:10:15.250: INFO: Started pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce in namespace container-probe-8753
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:10:15.25
    Nov 17 14:10:15.253: INFO: Initial restart count of pod test-webserver-e0b054fb-f276-402f-ba65-392b48f591ce is 0
    STEP: deleting the pod 11/17/23 14:14:15.825
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:15.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8753" for this suite. 11/17/23 14:14:15.874
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:15.883
Nov 17 14:14:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:14:15.885
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:15.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:15.915
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 11/17/23 14:14:15.919
STEP: watching for the ServiceAccount to be added 11/17/23 14:14:15.928
STEP: patching the ServiceAccount 11/17/23 14:14:15.93
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/17/23 14:14:15.936
STEP: deleting the ServiceAccount 11/17/23 14:14:15.94
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:15.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2036" for this suite. 11/17/23 14:14:15.961
------------------------------
â€¢ [0.085 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:15.883
    Nov 17 14:14:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:14:15.885
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:15.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:15.915
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 11/17/23 14:14:15.919
    STEP: watching for the ServiceAccount to be added 11/17/23 14:14:15.928
    STEP: patching the ServiceAccount 11/17/23 14:14:15.93
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 11/17/23 14:14:15.936
    STEP: deleting the ServiceAccount 11/17/23 14:14:15.94
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:15.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2036" for this suite. 11/17/23 14:14:15.961
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:15.971
Nov 17 14:14:15.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:14:15.973
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:15.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:15.995
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 in namespace container-probe-800 11/17/23 14:14:16
Nov 17 14:14:16.010: INFO: Waiting up to 5m0s for pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419" in namespace "container-probe-800" to be "not pending"
Nov 17 14:14:16.015: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021454ms
Nov 17 14:14:18.018: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419": Phase="Running", Reason="", readiness=true. Elapsed: 2.008771728s
Nov 17 14:14:18.018: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419" satisfied condition "not pending"
Nov 17 14:14:18.019: INFO: Started pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 in namespace container-probe-800
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:14:18.019
Nov 17 14:14:18.021: INFO: Initial restart count of pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 is 0
Nov 17 14:14:38.111: INFO: Restart count of pod container-probe-800/liveness-37135a75-5876-40fc-a02d-3e6282df0419 is now 1 (20.089748395s elapsed)
STEP: deleting the pod 11/17/23 14:14:38.111
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:38.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-800" for this suite. 11/17/23 14:14:38.138
------------------------------
â€¢ [SLOW TEST] [22.204 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:15.971
    Nov 17 14:14:15.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:14:15.973
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:15.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:15.995
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 in namespace container-probe-800 11/17/23 14:14:16
    Nov 17 14:14:16.010: INFO: Waiting up to 5m0s for pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419" in namespace "container-probe-800" to be "not pending"
    Nov 17 14:14:16.015: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021454ms
    Nov 17 14:14:18.018: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419": Phase="Running", Reason="", readiness=true. Elapsed: 2.008771728s
    Nov 17 14:14:18.018: INFO: Pod "liveness-37135a75-5876-40fc-a02d-3e6282df0419" satisfied condition "not pending"
    Nov 17 14:14:18.019: INFO: Started pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 in namespace container-probe-800
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:14:18.019
    Nov 17 14:14:18.021: INFO: Initial restart count of pod liveness-37135a75-5876-40fc-a02d-3e6282df0419 is 0
    Nov 17 14:14:38.111: INFO: Restart count of pod container-probe-800/liveness-37135a75-5876-40fc-a02d-3e6282df0419 is now 1 (20.089748395s elapsed)
    STEP: deleting the pod 11/17/23 14:14:38.111
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:38.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-800" for this suite. 11/17/23 14:14:38.138
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:38.175
Nov 17 14:14:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:14:38.177
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:38.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:38.199
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-4648/secret-test-93347555-9c71-4903-9560-00ba0bca2b40 11/17/23 14:14:38.202
STEP: Creating a pod to test consume secrets 11/17/23 14:14:38.209
Nov 17 14:14:38.218: INFO: Waiting up to 5m0s for pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa" in namespace "secrets-4648" to be "Succeeded or Failed"
Nov 17 14:14:38.223: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073517ms
Nov 17 14:14:40.228: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010168982s
Nov 17 14:14:42.228: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009805077s
STEP: Saw pod success 11/17/23 14:14:42.228
Nov 17 14:14:42.229: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa" satisfied condition "Succeeded or Failed"
Nov 17 14:14:42.232: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa container env-test: <nil>
STEP: delete the pod 11/17/23 14:14:42.253
Nov 17 14:14:42.266: INFO: Waiting for pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa to disappear
Nov 17 14:14:42.269: INFO: Pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:42.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4648" for this suite. 11/17/23 14:14:42.274
------------------------------
â€¢ [4.104 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:38.175
    Nov 17 14:14:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:14:38.177
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:38.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:38.199
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-4648/secret-test-93347555-9c71-4903-9560-00ba0bca2b40 11/17/23 14:14:38.202
    STEP: Creating a pod to test consume secrets 11/17/23 14:14:38.209
    Nov 17 14:14:38.218: INFO: Waiting up to 5m0s for pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa" in namespace "secrets-4648" to be "Succeeded or Failed"
    Nov 17 14:14:38.223: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073517ms
    Nov 17 14:14:40.228: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010168982s
    Nov 17 14:14:42.228: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009805077s
    STEP: Saw pod success 11/17/23 14:14:42.228
    Nov 17 14:14:42.229: INFO: Pod "pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa" satisfied condition "Succeeded or Failed"
    Nov 17 14:14:42.232: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa container env-test: <nil>
    STEP: delete the pod 11/17/23 14:14:42.253
    Nov 17 14:14:42.266: INFO: Waiting for pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa to disappear
    Nov 17 14:14:42.269: INFO: Pod pod-configmaps-016b0c36-20c2-4ad7-b5ad-5b24d3d77eaa no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:42.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4648" for this suite. 11/17/23 14:14:42.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:42.282
Nov 17 14:14:42.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context 11/17/23 14:14:42.284
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:42.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:42.306
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/17/23 14:14:42.31
Nov 17 14:14:42.318: INFO: Waiting up to 5m0s for pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955" in namespace "security-context-2417" to be "Succeeded or Failed"
Nov 17 14:14:42.321: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003938ms
Nov 17 14:14:44.325: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007085742s
Nov 17 14:14:46.326: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008054376s
STEP: Saw pod success 11/17/23 14:14:46.326
Nov 17 14:14:46.326: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955" satisfied condition "Succeeded or Failed"
Nov 17 14:14:46.331: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 container test-container: <nil>
STEP: delete the pod 11/17/23 14:14:46.338
Nov 17 14:14:46.354: INFO: Waiting for pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 to disappear
Nov 17 14:14:46.357: INFO: Pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2417" for this suite. 11/17/23 14:14:46.361
------------------------------
â€¢ [4.085 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:42.282
    Nov 17 14:14:42.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context 11/17/23 14:14:42.284
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:42.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:42.306
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 11/17/23 14:14:42.31
    Nov 17 14:14:42.318: INFO: Waiting up to 5m0s for pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955" in namespace "security-context-2417" to be "Succeeded or Failed"
    Nov 17 14:14:42.321: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003938ms
    Nov 17 14:14:44.325: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007085742s
    Nov 17 14:14:46.326: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008054376s
    STEP: Saw pod success 11/17/23 14:14:46.326
    Nov 17 14:14:46.326: INFO: Pod "security-context-3037cb68-28a3-48b7-92fe-8092751fc955" satisfied condition "Succeeded or Failed"
    Nov 17 14:14:46.331: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:14:46.338
    Nov 17 14:14:46.354: INFO: Waiting for pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 to disappear
    Nov 17 14:14:46.357: INFO: Pod security-context-3037cb68-28a3-48b7-92fe-8092751fc955 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2417" for this suite. 11/17/23 14:14:46.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:46.368
Nov 17 14:14:46.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename certificates 11/17/23 14:14:46.37
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:46.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:46.393
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 11/17/23 14:14:47.157
STEP: getting /apis/certificates.k8s.io 11/17/23 14:14:47.161
STEP: getting /apis/certificates.k8s.io/v1 11/17/23 14:14:47.162
STEP: creating 11/17/23 14:14:47.163
STEP: getting 11/17/23 14:14:47.182
STEP: listing 11/17/23 14:14:47.185
STEP: watching 11/17/23 14:14:47.187
Nov 17 14:14:47.187: INFO: starting watch
STEP: patching 11/17/23 14:14:47.188
STEP: updating 11/17/23 14:14:47.197
Nov 17 14:14:47.203: INFO: waiting for watch events with expected annotations
Nov 17 14:14:47.203: INFO: saw patched and updated annotations
STEP: getting /approval 11/17/23 14:14:47.203
STEP: patching /approval 11/17/23 14:14:47.206
STEP: updating /approval 11/17/23 14:14:47.212
STEP: getting /status 11/17/23 14:14:47.219
STEP: patching /status 11/17/23 14:14:47.222
STEP: updating /status 11/17/23 14:14:47.231
STEP: deleting 11/17/23 14:14:47.239
STEP: deleting a collection 11/17/23 14:14:47.249
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-1972" for this suite. 11/17/23 14:14:47.266
------------------------------
â€¢ [0.904 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:46.368
    Nov 17 14:14:46.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename certificates 11/17/23 14:14:46.37
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:46.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:46.393
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 11/17/23 14:14:47.157
    STEP: getting /apis/certificates.k8s.io 11/17/23 14:14:47.161
    STEP: getting /apis/certificates.k8s.io/v1 11/17/23 14:14:47.162
    STEP: creating 11/17/23 14:14:47.163
    STEP: getting 11/17/23 14:14:47.182
    STEP: listing 11/17/23 14:14:47.185
    STEP: watching 11/17/23 14:14:47.187
    Nov 17 14:14:47.187: INFO: starting watch
    STEP: patching 11/17/23 14:14:47.188
    STEP: updating 11/17/23 14:14:47.197
    Nov 17 14:14:47.203: INFO: waiting for watch events with expected annotations
    Nov 17 14:14:47.203: INFO: saw patched and updated annotations
    STEP: getting /approval 11/17/23 14:14:47.203
    STEP: patching /approval 11/17/23 14:14:47.206
    STEP: updating /approval 11/17/23 14:14:47.212
    STEP: getting /status 11/17/23 14:14:47.219
    STEP: patching /status 11/17/23 14:14:47.222
    STEP: updating /status 11/17/23 14:14:47.231
    STEP: deleting 11/17/23 14:14:47.239
    STEP: deleting a collection 11/17/23 14:14:47.249
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-1972" for this suite. 11/17/23 14:14:47.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:47.274
Nov 17 14:14:47.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:14:47.275
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:47.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:47.298
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/17/23 14:14:47.308
Nov 17 14:14:47.318: INFO: Waiting up to 5m0s for pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55" in namespace "emptydir-5377" to be "Succeeded or Failed"
Nov 17 14:14:47.322: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522739ms
Nov 17 14:14:49.327: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008954511s
Nov 17 14:14:51.328: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010176373s
STEP: Saw pod success 11/17/23 14:14:51.328
Nov 17 14:14:51.328: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55" satisfied condition "Succeeded or Failed"
Nov 17 14:14:51.331: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 container test-container: <nil>
STEP: delete the pod 11/17/23 14:14:51.337
Nov 17 14:14:51.348: INFO: Waiting for pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 to disappear
Nov 17 14:14:51.352: INFO: Pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:51.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5377" for this suite. 11/17/23 14:14:51.357
------------------------------
â€¢ [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:47.274
    Nov 17 14:14:47.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:14:47.275
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:47.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:47.298
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/17/23 14:14:47.308
    Nov 17 14:14:47.318: INFO: Waiting up to 5m0s for pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55" in namespace "emptydir-5377" to be "Succeeded or Failed"
    Nov 17 14:14:47.322: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522739ms
    Nov 17 14:14:49.327: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008954511s
    Nov 17 14:14:51.328: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010176373s
    STEP: Saw pod success 11/17/23 14:14:51.328
    Nov 17 14:14:51.328: INFO: Pod "pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55" satisfied condition "Succeeded or Failed"
    Nov 17 14:14:51.331: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:14:51.337
    Nov 17 14:14:51.348: INFO: Waiting for pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 to disappear
    Nov 17 14:14:51.352: INFO: Pod pod-8a3d9ed1-2dd0-4aa2-98f0-ddb580cfbb55 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:51.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5377" for this suite. 11/17/23 14:14:51.357
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:51.364
Nov 17 14:14:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption 11/17/23 14:14:51.366
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:51.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:51.402
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:51.406
Nov 17 14:14:51.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption-2 11/17/23 14:14:51.407
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:51.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:51.436
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 11/17/23 14:14:51.45
STEP: Waiting for the pdb to be processed 11/17/23 14:14:53.462
STEP: Waiting for the pdb to be processed 11/17/23 14:14:55.474
STEP: listing a collection of PDBs across all namespaces 11/17/23 14:14:57.483
STEP: listing a collection of PDBs in namespace disruption-6524 11/17/23 14:14:57.487
STEP: deleting a collection of PDBs 11/17/23 14:14:57.49
STEP: Waiting for the PDB collection to be deleted 11/17/23 14:14:57.501
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:57.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:14:57.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-9393" for this suite. 11/17/23 14:14:57.514
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6524" for this suite. 11/17/23 14:14:57.521
------------------------------
â€¢ [SLOW TEST] [6.167 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:51.364
    Nov 17 14:14:51.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption 11/17/23 14:14:51.366
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:51.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:51.402
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:51.406
    Nov 17 14:14:51.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption-2 11/17/23 14:14:51.407
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:51.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:51.436
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 11/17/23 14:14:51.45
    STEP: Waiting for the pdb to be processed 11/17/23 14:14:53.462
    STEP: Waiting for the pdb to be processed 11/17/23 14:14:55.474
    STEP: listing a collection of PDBs across all namespaces 11/17/23 14:14:57.483
    STEP: listing a collection of PDBs in namespace disruption-6524 11/17/23 14:14:57.487
    STEP: deleting a collection of PDBs 11/17/23 14:14:57.49
    STEP: Waiting for the PDB collection to be deleted 11/17/23 14:14:57.501
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:57.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:14:57.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-9393" for this suite. 11/17/23 14:14:57.514
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6524" for this suite. 11/17/23 14:14:57.521
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:14:57.532
Nov 17 14:14:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:14:57.533
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:57.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:57.557
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-7d9d5e39-f93b-4976-94c7-6a4aaeb8c5ba 11/17/23 14:14:57.561
STEP: Creating a pod to test consume configMaps 11/17/23 14:14:57.567
Nov 17 14:14:57.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e" in namespace "projected-4119" to be "Succeeded or Failed"
Nov 17 14:14:57.586: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.726148ms
Nov 17 14:14:59.591: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013100125s
Nov 17 14:15:01.592: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014944433s
STEP: Saw pod success 11/17/23 14:15:01.593
Nov 17 14:15:01.593: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e" satisfied condition "Succeeded or Failed"
Nov 17 14:15:01.597: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e container projected-configmap-volume-test: <nil>
STEP: delete the pod 11/17/23 14:15:01.604
Nov 17 14:15:01.617: INFO: Waiting for pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e to disappear
Nov 17 14:15:01.621: INFO: Pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:01.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4119" for this suite. 11/17/23 14:15:01.627
------------------------------
â€¢ [4.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:14:57.532
    Nov 17 14:14:57.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:14:57.533
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:14:57.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:14:57.557
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-7d9d5e39-f93b-4976-94c7-6a4aaeb8c5ba 11/17/23 14:14:57.561
    STEP: Creating a pod to test consume configMaps 11/17/23 14:14:57.567
    Nov 17 14:14:57.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e" in namespace "projected-4119" to be "Succeeded or Failed"
    Nov 17 14:14:57.586: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.726148ms
    Nov 17 14:14:59.591: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013100125s
    Nov 17 14:15:01.592: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014944433s
    STEP: Saw pod success 11/17/23 14:15:01.593
    Nov 17 14:15:01.593: INFO: Pod "pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e" satisfied condition "Succeeded or Failed"
    Nov 17 14:15:01.597: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:15:01.604
    Nov 17 14:15:01.617: INFO: Waiting for pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e to disappear
    Nov 17 14:15:01.621: INFO: Pod pod-projected-configmaps-d5ec6332-fb3f-4d34-af3e-c43f32a9f81e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:01.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4119" for this suite. 11/17/23 14:15:01.627
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:01.637
Nov 17 14:15:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:15:01.638
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:01.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:01.667
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Nov 17 14:15:01.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: creating the pod 11/17/23 14:15:01.671
STEP: submitting the pod to kubernetes 11/17/23 14:15:01.672
Nov 17 14:15:01.684: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c" in namespace "pods-7623" to be "running and ready"
Nov 17 14:15:01.689: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.768977ms
Nov 17 14:15:01.689: INFO: The phase of Pod pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:15:03.695: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011161547s
Nov 17 14:15:03.695: INFO: The phase of Pod pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c is Running (Ready = true)
Nov 17 14:15:03.695: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:03.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7623" for this suite. 11/17/23 14:15:03.718
------------------------------
â€¢ [2.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:01.637
    Nov 17 14:15:01.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:15:01.638
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:01.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:01.667
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Nov 17 14:15:01.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: creating the pod 11/17/23 14:15:01.671
    STEP: submitting the pod to kubernetes 11/17/23 14:15:01.672
    Nov 17 14:15:01.684: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c" in namespace "pods-7623" to be "running and ready"
    Nov 17 14:15:01.689: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.768977ms
    Nov 17 14:15:01.689: INFO: The phase of Pod pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:15:03.695: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011161547s
    Nov 17 14:15:03.695: INFO: The phase of Pod pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c is Running (Ready = true)
    Nov 17 14:15:03.695: INFO: Pod "pod-logs-websocket-41fcbb89-e508-483e-876b-be99f739398c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:03.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7623" for this suite. 11/17/23 14:15:03.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:03.728
Nov 17 14:15:03.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:15:03.729
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:03.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:03.765
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/17/23 14:15:03.77
Nov 17 14:15:03.786: INFO: Waiting up to 5m0s for pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8" in namespace "emptydir-8124" to be "Succeeded or Failed"
Nov 17 14:15:03.792: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28115ms
Nov 17 14:15:05.849: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06242789s
Nov 17 14:15:07.798: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011357842s
STEP: Saw pod success 11/17/23 14:15:07.798
Nov 17 14:15:07.798: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8" satisfied condition "Succeeded or Failed"
Nov 17 14:15:07.806: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 container test-container: <nil>
STEP: delete the pod 11/17/23 14:15:07.813
Nov 17 14:15:07.826: INFO: Waiting for pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 to disappear
Nov 17 14:15:07.829: INFO: Pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8124" for this suite. 11/17/23 14:15:07.833
------------------------------
â€¢ [4.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:03.728
    Nov 17 14:15:03.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:15:03.729
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:03.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:03.765
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/17/23 14:15:03.77
    Nov 17 14:15:03.786: INFO: Waiting up to 5m0s for pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8" in namespace "emptydir-8124" to be "Succeeded or Failed"
    Nov 17 14:15:03.792: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28115ms
    Nov 17 14:15:05.849: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06242789s
    Nov 17 14:15:07.798: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011357842s
    STEP: Saw pod success 11/17/23 14:15:07.798
    Nov 17 14:15:07.798: INFO: Pod "pod-114b22ed-5dc2-4229-a38d-b490b5975dc8" satisfied condition "Succeeded or Failed"
    Nov 17 14:15:07.806: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:15:07.813
    Nov 17 14:15:07.826: INFO: Waiting for pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 to disappear
    Nov 17 14:15:07.829: INFO: Pod pod-114b22ed-5dc2-4229-a38d-b490b5975dc8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8124" for this suite. 11/17/23 14:15:07.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:07.841
Nov 17 14:15:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:15:07.843
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:07.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:07.864
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:15:07.886
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:15:08.394
STEP: Deploying the webhook pod 11/17/23 14:15:08.405
STEP: Wait for the deployment to be ready 11/17/23 14:15:08.425
Nov 17 14:15:08.452: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:15:10.464
STEP: Verifying the service has paired with the endpoint 11/17/23 14:15:10.48
Nov 17 14:15:11.480: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 11/17/23 14:15:11.484
STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/17/23 14:15:11.526
STEP: Creating a configMap that should not be mutated 11/17/23 14:15:11.535
STEP: Patching a mutating webhook configuration's rules to include the create operation 11/17/23 14:15:11.548
STEP: Creating a configMap that should be mutated 11/17/23 14:15:11.557
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:11.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9413" for this suite. 11/17/23 14:15:11.667
STEP: Destroying namespace "webhook-9413-markers" for this suite. 11/17/23 14:15:11.679
------------------------------
â€¢ [3.853 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:07.841
    Nov 17 14:15:07.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:15:07.843
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:07.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:07.864
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:15:07.886
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:15:08.394
    STEP: Deploying the webhook pod 11/17/23 14:15:08.405
    STEP: Wait for the deployment to be ready 11/17/23 14:15:08.425
    Nov 17 14:15:08.452: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:15:10.464
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:15:10.48
    Nov 17 14:15:11.480: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 11/17/23 14:15:11.484
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 11/17/23 14:15:11.526
    STEP: Creating a configMap that should not be mutated 11/17/23 14:15:11.535
    STEP: Patching a mutating webhook configuration's rules to include the create operation 11/17/23 14:15:11.548
    STEP: Creating a configMap that should be mutated 11/17/23 14:15:11.557
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:11.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9413" for this suite. 11/17/23 14:15:11.667
    STEP: Destroying namespace "webhook-9413-markers" for this suite. 11/17/23 14:15:11.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:11.699
Nov 17 14:15:11.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:15:11.7
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:11.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:11.731
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:15:11.761
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:15:11.983
STEP: Deploying the webhook pod 11/17/23 14:15:11.988
STEP: Wait for the deployment to be ready 11/17/23 14:15:12.003
Nov 17 14:15:12.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:15:14.029
STEP: Verifying the service has paired with the endpoint 11/17/23 14:15:14.058
Nov 17 14:15:15.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 11/17/23 14:15:15.065
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/17/23 14:15:15.067
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/17/23 14:15:15.067
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/17/23 14:15:15.067
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/17/23 14:15:15.069
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/17/23 14:15:15.069
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/17/23 14:15:15.072
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:15.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2673" for this suite. 11/17/23 14:15:15.153
STEP: Destroying namespace "webhook-2673-markers" for this suite. 11/17/23 14:15:15.175
------------------------------
â€¢ [3.493 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:11.699
    Nov 17 14:15:11.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:15:11.7
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:11.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:11.731
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:15:11.761
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:15:11.983
    STEP: Deploying the webhook pod 11/17/23 14:15:11.988
    STEP: Wait for the deployment to be ready 11/17/23 14:15:12.003
    Nov 17 14:15:12.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:15:14.029
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:15:14.058
    Nov 17 14:15:15.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 11/17/23 14:15:15.065
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 11/17/23 14:15:15.067
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 11/17/23 14:15:15.067
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 11/17/23 14:15:15.067
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 11/17/23 14:15:15.069
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 11/17/23 14:15:15.069
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 11/17/23 14:15:15.072
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:15.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2673" for this suite. 11/17/23 14:15:15.153
    STEP: Destroying namespace "webhook-2673-markers" for this suite. 11/17/23 14:15:15.175
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:15.195
Nov 17 14:15:15.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:15:15.196
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:15.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:15.229
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:15:15.232
Nov 17 14:15:15.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80" in namespace "projected-1210" to be "Succeeded or Failed"
Nov 17 14:15:15.249: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962414ms
Nov 17 14:15:17.253: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009175456s
Nov 17 14:15:19.254: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010543853s
STEP: Saw pod success 11/17/23 14:15:19.254
Nov 17 14:15:19.255: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80" satisfied condition "Succeeded or Failed"
Nov 17 14:15:19.257: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 container client-container: <nil>
STEP: delete the pod 11/17/23 14:15:19.263
Nov 17 14:15:19.282: INFO: Waiting for pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 to disappear
Nov 17 14:15:19.286: INFO: Pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1210" for this suite. 11/17/23 14:15:19.291
------------------------------
â€¢ [4.102 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:15.195
    Nov 17 14:15:15.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:15:15.196
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:15.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:15.229
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:15:15.232
    Nov 17 14:15:15.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80" in namespace "projected-1210" to be "Succeeded or Failed"
    Nov 17 14:15:15.249: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962414ms
    Nov 17 14:15:17.253: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009175456s
    Nov 17 14:15:19.254: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010543853s
    STEP: Saw pod success 11/17/23 14:15:19.254
    Nov 17 14:15:19.255: INFO: Pod "downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80" satisfied condition "Succeeded or Failed"
    Nov 17 14:15:19.257: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:15:19.263
    Nov 17 14:15:19.282: INFO: Waiting for pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 to disappear
    Nov 17 14:15:19.286: INFO: Pod downwardapi-volume-d0ad3cb5-a537-4b39-a07c-136c15296c80 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1210" for this suite. 11/17/23 14:15:19.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:19.328
Nov 17 14:15:19.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:15:19.329
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:19.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:19.357
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-84facaa1-0e05-42ff-8bca-c417c58be30d 11/17/23 14:15:19.362
STEP: Creating a pod to test consume secrets 11/17/23 14:15:19.367
Nov 17 14:15:19.380: INFO: Waiting up to 5m0s for pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7" in namespace "secrets-4057" to be "Succeeded or Failed"
Nov 17 14:15:19.388: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.83731ms
Nov 17 14:15:21.392: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012533986s
Nov 17 14:15:23.395: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015485724s
STEP: Saw pod success 11/17/23 14:15:23.395
Nov 17 14:15:23.395: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7" satisfied condition "Succeeded or Failed"
Nov 17 14:15:23.401: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:15:23.413
Nov 17 14:15:23.448: INFO: Waiting for pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 to disappear
Nov 17 14:15:23.454: INFO: Pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:23.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4057" for this suite. 11/17/23 14:15:23.46
------------------------------
â€¢ [4.151 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:19.328
    Nov 17 14:15:19.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:15:19.329
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:19.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:19.357
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-84facaa1-0e05-42ff-8bca-c417c58be30d 11/17/23 14:15:19.362
    STEP: Creating a pod to test consume secrets 11/17/23 14:15:19.367
    Nov 17 14:15:19.380: INFO: Waiting up to 5m0s for pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7" in namespace "secrets-4057" to be "Succeeded or Failed"
    Nov 17 14:15:19.388: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.83731ms
    Nov 17 14:15:21.392: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012533986s
    Nov 17 14:15:23.395: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015485724s
    STEP: Saw pod success 11/17/23 14:15:23.395
    Nov 17 14:15:23.395: INFO: Pod "pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7" satisfied condition "Succeeded or Failed"
    Nov 17 14:15:23.401: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:15:23.413
    Nov 17 14:15:23.448: INFO: Waiting for pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 to disappear
    Nov 17 14:15:23.454: INFO: Pod pod-secrets-b61c4b15-aab7-4649-bd7f-479c19f631e7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:23.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4057" for this suite. 11/17/23 14:15:23.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:23.48
Nov 17 14:15:23.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:15:23.481
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:23.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:23.555
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-2a8328a1-d053-43d8-807c-10972d82c679 11/17/23 14:15:23.558
STEP: Creating a pod to test consume configMaps 11/17/23 14:15:23.565
Nov 17 14:15:23.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf" in namespace "projected-6282" to be "Succeeded or Failed"
Nov 17 14:15:23.595: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743541ms
Nov 17 14:15:25.600: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021259808s
Nov 17 14:15:27.601: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02251083s
STEP: Saw pod success 11/17/23 14:15:27.601
Nov 17 14:15:27.601: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf" satisfied condition "Succeeded or Failed"
Nov 17 14:15:27.604: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:15:27.612
Nov 17 14:15:27.629: INFO: Waiting for pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf to disappear
Nov 17 14:15:27.636: INFO: Pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:15:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6282" for this suite. 11/17/23 14:15:27.642
------------------------------
â€¢ [4.169 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:23.48
    Nov 17 14:15:23.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:15:23.481
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:23.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:23.555
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-2a8328a1-d053-43d8-807c-10972d82c679 11/17/23 14:15:23.558
    STEP: Creating a pod to test consume configMaps 11/17/23 14:15:23.565
    Nov 17 14:15:23.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf" in namespace "projected-6282" to be "Succeeded or Failed"
    Nov 17 14:15:23.595: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743541ms
    Nov 17 14:15:25.600: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021259808s
    Nov 17 14:15:27.601: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02251083s
    STEP: Saw pod success 11/17/23 14:15:27.601
    Nov 17 14:15:27.601: INFO: Pod "pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf" satisfied condition "Succeeded or Failed"
    Nov 17 14:15:27.604: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:15:27.612
    Nov 17 14:15:27.629: INFO: Waiting for pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf to disappear
    Nov 17 14:15:27.636: INFO: Pod pod-projected-configmaps-fe141ab5-407e-4086-ad56-76af61e096cf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:15:27.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6282" for this suite. 11/17/23 14:15:27.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:15:27.653
Nov 17 14:15:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:15:27.654
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:27.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:27.681
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 17 14:15:27.699: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:16:27.764: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:16:27.77
Nov 17 14:16:27.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption-path 11/17/23 14:16:27.771
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:27.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:27.795
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 11/17/23 14:16:27.799
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:16:27.799
Nov 17 14:16:27.810: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6761" to be "running"
Nov 17 14:16:27.814: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143979ms
Nov 17 14:16:29.818: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007123449s
Nov 17 14:16:29.818: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:16:29.82
Nov 17 14:16:29.832: INFO: found a healthy node: k8s-worker-2.c.operations-lab.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Nov 17 14:16:35.943: INFO: pods created so far: [1 1 1]
Nov 17 14:16:35.943: INFO: length of pods created so far: 3
Nov 17 14:16:37.958: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Nov 17 14:16:44.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:16:44.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6761" for this suite. 11/17/23 14:16:45.06
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1695" for this suite. 11/17/23 14:16:45.065
------------------------------
â€¢ [SLOW TEST] [77.419 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:15:27.653
    Nov 17 14:15:27.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:15:27.654
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:15:27.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:15:27.681
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 17 14:15:27.699: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:16:27.764: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:16:27.77
    Nov 17 14:16:27.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption-path 11/17/23 14:16:27.771
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:27.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:27.795
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 11/17/23 14:16:27.799
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:16:27.799
    Nov 17 14:16:27.810: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6761" to be "running"
    Nov 17 14:16:27.814: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143979ms
    Nov 17 14:16:29.818: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007123449s
    Nov 17 14:16:29.818: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:16:29.82
    Nov 17 14:16:29.832: INFO: found a healthy node: k8s-worker-2.c.operations-lab.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Nov 17 14:16:35.943: INFO: pods created so far: [1 1 1]
    Nov 17 14:16:35.943: INFO: length of pods created so far: 3
    Nov 17 14:16:37.958: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:16:44.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:16:44.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6761" for this suite. 11/17/23 14:16:45.06
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1695" for this suite. 11/17/23 14:16:45.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:16:45.075
Nov 17 14:16:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename ingress 11/17/23 14:16:45.076
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:45.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:45.097
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 11/17/23 14:16:45.1
STEP: getting /apis/networking.k8s.io 11/17/23 14:16:45.103
STEP: getting /apis/networking.k8s.iov1 11/17/23 14:16:45.104
STEP: creating 11/17/23 14:16:45.105
STEP: getting 11/17/23 14:16:45.13
STEP: listing 11/17/23 14:16:45.133
STEP: watching 11/17/23 14:16:45.136
Nov 17 14:16:45.136: INFO: starting watch
STEP: cluster-wide listing 11/17/23 14:16:45.138
STEP: cluster-wide watching 11/17/23 14:16:45.141
Nov 17 14:16:45.141: INFO: starting watch
STEP: patching 11/17/23 14:16:45.142
STEP: updating 11/17/23 14:16:45.149
Nov 17 14:16:45.157: INFO: waiting for watch events with expected annotations
Nov 17 14:16:45.157: INFO: saw patched and updated annotations
STEP: patching /status 11/17/23 14:16:45.157
STEP: updating /status 11/17/23 14:16:45.164
STEP: get /status 11/17/23 14:16:45.175
STEP: deleting 11/17/23 14:16:45.178
STEP: deleting a collection 11/17/23 14:16:45.188
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:16:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-3636" for this suite. 11/17/23 14:16:45.206
------------------------------
â€¢ [0.139 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:16:45.075
    Nov 17 14:16:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename ingress 11/17/23 14:16:45.076
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:45.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:45.097
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 11/17/23 14:16:45.1
    STEP: getting /apis/networking.k8s.io 11/17/23 14:16:45.103
    STEP: getting /apis/networking.k8s.iov1 11/17/23 14:16:45.104
    STEP: creating 11/17/23 14:16:45.105
    STEP: getting 11/17/23 14:16:45.13
    STEP: listing 11/17/23 14:16:45.133
    STEP: watching 11/17/23 14:16:45.136
    Nov 17 14:16:45.136: INFO: starting watch
    STEP: cluster-wide listing 11/17/23 14:16:45.138
    STEP: cluster-wide watching 11/17/23 14:16:45.141
    Nov 17 14:16:45.141: INFO: starting watch
    STEP: patching 11/17/23 14:16:45.142
    STEP: updating 11/17/23 14:16:45.149
    Nov 17 14:16:45.157: INFO: waiting for watch events with expected annotations
    Nov 17 14:16:45.157: INFO: saw patched and updated annotations
    STEP: patching /status 11/17/23 14:16:45.157
    STEP: updating /status 11/17/23 14:16:45.164
    STEP: get /status 11/17/23 14:16:45.175
    STEP: deleting 11/17/23 14:16:45.178
    STEP: deleting a collection 11/17/23 14:16:45.188
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:16:45.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-3636" for this suite. 11/17/23 14:16:45.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:16:45.215
Nov 17 14:16:45.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 14:16:45.216
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:45.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:45.237
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Nov 17 14:16:45.241: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 17 14:16:45.251: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 17 14:16:50.256: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 14:16:50.257
Nov 17 14:16:50.257: INFO: Creating deployment "test-rolling-update-deployment"
Nov 17 14:16:50.266: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 17 14:16:50.279: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 17 14:16:52.289: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 17 14:16:52.293: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 14:16:52.307: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1911  fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 35385 1 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eeba68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 14:16:50 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-17 14:16:51 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 17 14:16:52.310: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1911  0436ff19-3fca-4d17-a8d3-33b1927fd2d1 35374 1 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 0xc00b49c587 0xc00b49c588}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b49c638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:16:52.310: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 17 14:16:52.311: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1911  75445c6d-f527-4af1-9f1d-4a6368f9d889 35383 2 2023-11-17 14:16:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 0xc00b49c457 0xc00b49c458}] [] [{e2e.test Update apps/v1 2023-11-17 14:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b49c518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:16:52.322: INFO: Pod "test-rolling-update-deployment-7549d9f46d-5pdqq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-5pdqq test-rolling-update-deployment-7549d9f46d- deployment-1911  5343ba74-da99-43ba-adbf-a1bf304c35ab 35373 0 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 0436ff19-3fca-4d17-a8d3-33b1927fd2d1 0xc00b49ca97 0xc00b49ca98}] [] [{kube-controller-manager Update v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0436ff19-3fca-4d17-a8d3-33b1927fd2d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf8ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf8ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.235,StartTime:2023-11-17 14:16:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:16:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://02e3bd34197e0e694e1cc0cf8315b74e8da6bbc55f5467d0e045cd57cf3fc0ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 14:16:52.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1911" for this suite. 11/17/23 14:16:52.335
------------------------------
â€¢ [SLOW TEST] [7.128 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:16:45.215
    Nov 17 14:16:45.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 14:16:45.216
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:45.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:45.237
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Nov 17 14:16:45.241: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Nov 17 14:16:45.251: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 17 14:16:50.256: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 14:16:50.257
    Nov 17 14:16:50.257: INFO: Creating deployment "test-rolling-update-deployment"
    Nov 17 14:16:50.266: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Nov 17 14:16:50.279: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Nov 17 14:16:52.289: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Nov 17 14:16:52.293: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 14:16:52.307: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1911  fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 35385 1 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eeba68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-11-17 14:16:50 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-11-17 14:16:51 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 17 14:16:52.310: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1911  0436ff19-3fca-4d17-a8d3-33b1927fd2d1 35374 1 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 0xc00b49c587 0xc00b49c588}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b49c638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:16:52.310: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Nov 17 14:16:52.311: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1911  75445c6d-f527-4af1-9f1d-4a6368f9d889 35383 2 2023-11-17 14:16:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2 0xc00b49c457 0xc00b49c458}] [] [{e2e.test Update apps/v1 2023-11-17 14:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4ada9c-9dc2-4fde-bb0e-bf6d2d94c0d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00b49c518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:16:52.322: INFO: Pod "test-rolling-update-deployment-7549d9f46d-5pdqq" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-5pdqq test-rolling-update-deployment-7549d9f46d- deployment-1911  5343ba74-da99-43ba-adbf-a1bf304c35ab 35373 0 2023-11-17 14:16:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 0436ff19-3fca-4d17-a8d3-33b1927fd2d1 0xc00b49ca97 0xc00b49ca98}] [] [{kube-controller-manager Update v1 2023-11-17 14:16:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0436ff19-3fca-4d17-a8d3-33b1927fd2d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf8ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf8ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:16:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.235,StartTime:2023-11-17 14:16:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:16:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://02e3bd34197e0e694e1cc0cf8315b74e8da6bbc55f5467d0e045cd57cf3fc0ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:16:52.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1911" for this suite. 11/17/23 14:16:52.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:16:52.343
Nov 17 14:16:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 14:16:52.345
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:52.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:52.368
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 11/17/23 14:16:52.372
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5785;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5785;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +notcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_tcp@PTR;sleep 1; done
 11/17/23 14:16:52.399
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5785;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5785;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +notcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_tcp@PTR;sleep 1; done
 11/17/23 14:16:52.4
STEP: creating a pod to probe DNS 11/17/23 14:16:52.4
STEP: submitting the pod to kubernetes 11/17/23 14:16:52.4
Nov 17 14:16:52.423: INFO: Waiting up to 15m0s for pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1" in namespace "dns-5785" to be "running"
Nov 17 14:16:52.436: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.619628ms
Nov 17 14:16:54.440: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016773256s
Nov 17 14:16:54.440: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1" satisfied condition "running"
STEP: retrieving the pod 11/17/23 14:16:54.44
STEP: looking for the results for each expected name from probers 11/17/23 14:16:54.443
Nov 17 14:16:54.448: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.453: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.458: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.464: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.480: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.504: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.509: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.513: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.516: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.521: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.528: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:54.556: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc jessie_udp@_http._tcp.dns-test-service.dns-5785.svc jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc]

Nov 17 14:16:59.564: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.569: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.575: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.626: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.633: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.638: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.645: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.651: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.655: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:16:59.686: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:04.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.566: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.574: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.577: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.612: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.619: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.623: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.627: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.632: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.636: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:04.665: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:09.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.566: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.572: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.582: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.624: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.629: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.633: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.643: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:09.678: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:14.563: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.572: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.579: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.593: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.639: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.644: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.650: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.656: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.662: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.668: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:14.714: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:19.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.567: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.571: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.577: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.589: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.641: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.647: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.654: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.661: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.665: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:19.699: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:24.608: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.614: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.618: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.630: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
Nov 17 14:17:24.660: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

Nov 17 14:17:29.672: INFO: DNS probes using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 succeeded

STEP: deleting the pod 11/17/23 14:17:29.673
STEP: deleting the test service 11/17/23 14:17:29.696
STEP: deleting the test headless service 11/17/23 14:17:29.791
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 14:17:29.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5785" for this suite. 11/17/23 14:17:29.834
------------------------------
â€¢ [SLOW TEST] [37.503 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:16:52.343
    Nov 17 14:16:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 14:16:52.345
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:16:52.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:16:52.368
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 11/17/23 14:16:52.372
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5785;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5785;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +notcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_tcp@PTR;sleep 1; done
     11/17/23 14:16:52.399
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5785;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5785;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5785.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5785.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5785.svc;check="$$(dig +notcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.147.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.147.226_tcp@PTR;sleep 1; done
     11/17/23 14:16:52.4
    STEP: creating a pod to probe DNS 11/17/23 14:16:52.4
    STEP: submitting the pod to kubernetes 11/17/23 14:16:52.4
    Nov 17 14:16:52.423: INFO: Waiting up to 15m0s for pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1" in namespace "dns-5785" to be "running"
    Nov 17 14:16:52.436: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.619628ms
    Nov 17 14:16:54.440: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016773256s
    Nov 17 14:16:54.440: INFO: Pod "dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 14:16:54.44
    STEP: looking for the results for each expected name from probers 11/17/23 14:16:54.443
    Nov 17 14:16:54.448: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.453: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.458: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.464: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.480: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.504: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.509: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.513: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.516: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.521: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.528: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:54.556: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc wheezy_udp@_http._tcp.dns-test-service.dns-5785.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc jessie_udp@_http._tcp.dns-test-service.dns-5785.svc jessie_tcp@_http._tcp.dns-test-service.dns-5785.svc]

    Nov 17 14:16:59.564: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.569: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.575: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.626: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.633: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.638: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.645: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.651: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.655: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:16:59.686: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:04.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.566: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.574: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.577: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.612: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.619: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.623: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.627: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.632: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.636: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:04.665: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:09.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.566: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.572: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.576: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.582: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.588: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.624: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.629: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.633: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.643: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:09.678: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:14.563: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.572: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.579: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.593: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.639: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.644: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.650: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.656: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.662: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.668: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:14.714: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:19.561: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.567: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.571: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.577: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.589: INFO: Unable to read wheezy_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.641: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.647: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.654: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.661: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.665: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:19.699: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5785 wheezy_tcp@dns-test-service.dns-5785 wheezy_udp@dns-test-service.dns-5785.svc wheezy_tcp@dns-test-service.dns-5785.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:24.608: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.614: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.618: INFO: Unable to read jessie_udp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785 from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.630: INFO: Unable to read jessie_udp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-5785.svc from pod dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1: the server could not find the requested resource (get pods dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1)
    Nov 17 14:17:24.660: INFO: Lookups using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5785 jessie_tcp@dns-test-service.dns-5785 jessie_udp@dns-test-service.dns-5785.svc jessie_tcp@dns-test-service.dns-5785.svc]

    Nov 17 14:17:29.672: INFO: DNS probes using dns-5785/dns-test-1ff73bf3-2d2f-4851-b958-33e66cfdebd1 succeeded

    STEP: deleting the pod 11/17/23 14:17:29.673
    STEP: deleting the test service 11/17/23 14:17:29.696
    STEP: deleting the test headless service 11/17/23 14:17:29.791
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:17:29.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5785" for this suite. 11/17/23 14:17:29.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:17:29.85
Nov 17 14:17:29.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 14:17:29.851
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:17:29.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:17:29.89
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 11/17/23 14:17:29.897
STEP: delete the rc 11/17/23 14:17:34.91
STEP: wait for all pods to be garbage collected 11/17/23 14:17:34.916
STEP: Gathering metrics 11/17/23 14:17:39.928
Nov 17 14:17:39.959: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 14:17:39.963: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.192136ms
Nov 17 14:17:39.963: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 14:17:39.963: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 14:17:40.064: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 14:17:40.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2798" for this suite. 11/17/23 14:17:40.07
------------------------------
â€¢ [SLOW TEST] [10.229 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:17:29.85
    Nov 17 14:17:29.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 14:17:29.851
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:17:29.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:17:29.89
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 11/17/23 14:17:29.897
    STEP: delete the rc 11/17/23 14:17:34.91
    STEP: wait for all pods to be garbage collected 11/17/23 14:17:34.916
    STEP: Gathering metrics 11/17/23 14:17:39.928
    Nov 17 14:17:39.959: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 14:17:39.963: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.192136ms
    Nov 17 14:17:39.963: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 14:17:39.963: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 14:17:40.064: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:17:40.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2798" for this suite. 11/17/23 14:17:40.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:17:40.079
Nov 17 14:17:40.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename aggregator 11/17/23 14:17:40.081
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:17:40.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:17:40.105
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Nov 17 14:17:40.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 11/17/23 14:17:40.109
Nov 17 14:17:40.699: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 17 14:17:42.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:44.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:46.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:48.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:50.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:52.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:54.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:56.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:17:58.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:18:00.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:18:02.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 14:18:04.999: INFO: Waited 226.84599ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 11/17/23 14:18:05.153
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/17/23 14:18:05.183
STEP: List APIServices 11/17/23 14:18:05.244
Nov 17 14:18:05.290: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:06.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2256" for this suite. 11/17/23 14:18:06.135
------------------------------
â€¢ [SLOW TEST] [26.114 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:17:40.079
    Nov 17 14:17:40.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename aggregator 11/17/23 14:17:40.081
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:17:40.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:17:40.105
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Nov 17 14:17:40.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 11/17/23 14:17:40.109
    Nov 17 14:17:40.699: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Nov 17 14:17:42.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:44.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:46.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:48.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:50.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:52.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:54.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:56.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:17:58.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:18:00.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:18:02.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 17, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6bf7684f98\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 14:18:04.999: INFO: Waited 226.84599ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 11/17/23 14:18:05.153
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 11/17/23 14:18:05.183
    STEP: List APIServices 11/17/23 14:18:05.244
    Nov 17 14:18:05.290: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:06.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2256" for this suite. 11/17/23 14:18:06.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:06.194
Nov 17 14:18:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:18:06.195
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:06.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:06.214
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-144 11/17/23 14:18:06.218
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/17/23 14:18:06.241
STEP: creating service externalsvc in namespace services-144 11/17/23 14:18:06.241
STEP: creating replication controller externalsvc in namespace services-144 11/17/23 14:18:06.258
I1117 14:18:06.271713      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-144, replica count: 2
I1117 14:18:09.323932      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 11/17/23 14:18:09.327
Nov 17 14:18:09.345: INFO: Creating new exec pod
Nov 17 14:18:09.353: INFO: Waiting up to 5m0s for pod "execpodz8d56" in namespace "services-144" to be "running"
Nov 17 14:18:09.361: INFO: Pod "execpodz8d56": Phase="Pending", Reason="", readiness=false. Elapsed: 7.755564ms
Nov 17 14:18:11.366: INFO: Pod "execpodz8d56": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128171s
Nov 17 14:18:11.366: INFO: Pod "execpodz8d56" satisfied condition "running"
Nov 17 14:18:11.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-144 exec execpodz8d56 -- /bin/sh -x -c nslookup clusterip-service.services-144.svc.cluster.local'
Nov 17 14:18:11.606: INFO: stderr: "+ nslookup clusterip-service.services-144.svc.cluster.local\n"
Nov 17 14:18:11.607: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-144.svc.cluster.local\tcanonical name = externalsvc.services-144.svc.cluster.local.\nName:\texternalsvc.services-144.svc.cluster.local\nAddress: 10.99.126.191\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-144, will wait for the garbage collector to delete the pods 11/17/23 14:18:11.607
Nov 17 14:18:11.670: INFO: Deleting ReplicationController externalsvc took: 8.520305ms
Nov 17 14:18:11.771: INFO: Terminating ReplicationController externalsvc pods took: 100.841092ms
Nov 17 14:18:13.903: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:13.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-144" for this suite. 11/17/23 14:18:13.923
------------------------------
â€¢ [SLOW TEST] [7.738 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:06.194
    Nov 17 14:18:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:18:06.195
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:06.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:06.214
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-144 11/17/23 14:18:06.218
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/17/23 14:18:06.241
    STEP: creating service externalsvc in namespace services-144 11/17/23 14:18:06.241
    STEP: creating replication controller externalsvc in namespace services-144 11/17/23 14:18:06.258
    I1117 14:18:06.271713      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-144, replica count: 2
    I1117 14:18:09.323932      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 11/17/23 14:18:09.327
    Nov 17 14:18:09.345: INFO: Creating new exec pod
    Nov 17 14:18:09.353: INFO: Waiting up to 5m0s for pod "execpodz8d56" in namespace "services-144" to be "running"
    Nov 17 14:18:09.361: INFO: Pod "execpodz8d56": Phase="Pending", Reason="", readiness=false. Elapsed: 7.755564ms
    Nov 17 14:18:11.366: INFO: Pod "execpodz8d56": Phase="Running", Reason="", readiness=true. Elapsed: 2.0128171s
    Nov 17 14:18:11.366: INFO: Pod "execpodz8d56" satisfied condition "running"
    Nov 17 14:18:11.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-144 exec execpodz8d56 -- /bin/sh -x -c nslookup clusterip-service.services-144.svc.cluster.local'
    Nov 17 14:18:11.606: INFO: stderr: "+ nslookup clusterip-service.services-144.svc.cluster.local\n"
    Nov 17 14:18:11.607: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-144.svc.cluster.local\tcanonical name = externalsvc.services-144.svc.cluster.local.\nName:\texternalsvc.services-144.svc.cluster.local\nAddress: 10.99.126.191\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-144, will wait for the garbage collector to delete the pods 11/17/23 14:18:11.607
    Nov 17 14:18:11.670: INFO: Deleting ReplicationController externalsvc took: 8.520305ms
    Nov 17 14:18:11.771: INFO: Terminating ReplicationController externalsvc pods took: 100.841092ms
    Nov 17 14:18:13.903: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:13.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-144" for this suite. 11/17/23 14:18:13.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:13.934
Nov 17 14:18:13.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:18:13.936
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:13.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:13.961
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7409 11/17/23 14:18:13.965
STEP: creating replication controller nodeport-test in namespace services-7409 11/17/23 14:18:13.984
I1117 14:18:13.994969      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7409, replica count: 2
I1117 14:18:17.045606      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 14:18:17.045: INFO: Creating new exec pod
Nov 17 14:18:17.051: INFO: Waiting up to 5m0s for pod "execpod8k9bk" in namespace "services-7409" to be "running"
Nov 17 14:18:17.060: INFO: Pod "execpod8k9bk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384167ms
Nov 17 14:18:19.064: INFO: Pod "execpod8k9bk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012210347s
Nov 17 14:18:19.064: INFO: Pod "execpod8k9bk" satisfied condition "running"
Nov 17 14:18:20.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Nov 17 14:18:20.303: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 17 14:18:20.303: INFO: stdout: ""
Nov 17 14:18:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 10.97.135.44 80'
Nov 17 14:18:20.552: INFO: stderr: "+ nc -v -z -w 2 10.97.135.44 80\nConnection to 10.97.135.44 80 port [tcp/http] succeeded!\n"
Nov 17 14:18:20.552: INFO: stdout: ""
Nov 17 14:18:20.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 30911'
Nov 17 14:18:20.763: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 30911\nConnection to 172.16.0.5 30911 port [tcp/*] succeeded!\n"
Nov 17 14:18:20.763: INFO: stdout: ""
Nov 17 14:18:20.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 30911'
Nov 17 14:18:20.965: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 30911\nConnection to 172.16.0.3 30911 port [tcp/*] succeeded!\n"
Nov 17 14:18:20.965: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7409" for this suite. 11/17/23 14:18:20.971
------------------------------
â€¢ [SLOW TEST] [7.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:13.934
    Nov 17 14:18:13.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:18:13.936
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:13.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:13.961
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7409 11/17/23 14:18:13.965
    STEP: creating replication controller nodeport-test in namespace services-7409 11/17/23 14:18:13.984
    I1117 14:18:13.994969      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7409, replica count: 2
    I1117 14:18:17.045606      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 14:18:17.045: INFO: Creating new exec pod
    Nov 17 14:18:17.051: INFO: Waiting up to 5m0s for pod "execpod8k9bk" in namespace "services-7409" to be "running"
    Nov 17 14:18:17.060: INFO: Pod "execpod8k9bk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.384167ms
    Nov 17 14:18:19.064: INFO: Pod "execpod8k9bk": Phase="Running", Reason="", readiness=true. Elapsed: 2.012210347s
    Nov 17 14:18:19.064: INFO: Pod "execpod8k9bk" satisfied condition "running"
    Nov 17 14:18:20.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Nov 17 14:18:20.303: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Nov 17 14:18:20.303: INFO: stdout: ""
    Nov 17 14:18:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 10.97.135.44 80'
    Nov 17 14:18:20.552: INFO: stderr: "+ nc -v -z -w 2 10.97.135.44 80\nConnection to 10.97.135.44 80 port [tcp/http] succeeded!\n"
    Nov 17 14:18:20.552: INFO: stdout: ""
    Nov 17 14:18:20.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 30911'
    Nov 17 14:18:20.763: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 30911\nConnection to 172.16.0.5 30911 port [tcp/*] succeeded!\n"
    Nov 17 14:18:20.763: INFO: stdout: ""
    Nov 17 14:18:20.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-7409 exec execpod8k9bk -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 30911'
    Nov 17 14:18:20.965: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 30911\nConnection to 172.16.0.3 30911 port [tcp/*] succeeded!\n"
    Nov 17 14:18:20.965: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:20.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7409" for this suite. 11/17/23 14:18:20.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:20.981
Nov 17 14:18:20.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-pred 11/17/23 14:18:20.982
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:21.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:21.012
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 17 14:18:21.015: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 17 14:18:21.025: INFO: Waiting for terminating namespaces to be deleted...
Nov 17 14:18:21.032: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Nov 17 14:18:21.050: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container cert-manager ready: true, restart count 0
Nov 17 14:18:21.050: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container cainjector ready: true, restart count 0
Nov 17 14:18:21.050: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container webhook ready: true, restart count 0
Nov 17 14:18:21.050: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container minio ready: true, restart count 0
Nov 17 14:18:21.050: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container velero ready: true, restart count 0
Nov 17 14:18:21.050: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container traefik ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kube-green ready: true, restart count 0
Nov 17 14:18:21.050: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:18:21.050: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container coredns ready: true, restart count 0
Nov 17 14:18:21.050: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container coredns ready: true, restart count 0
Nov 17 14:18:21.050: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container hubble-relay ready: true, restart count 0
Nov 17 14:18:21.050: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container backend ready: true, restart count 0
Nov 17 14:18:21.050: INFO: 	Container frontend ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kyverno ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kyverno-background ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kyverno-reports ready: true, restart count 0
Nov 17 14:18:21.050: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container local-path-provisioner ready: true, restart count 0
Nov 17 14:18:21.050: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:18:21.050: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container logging-operator ready: true, restart count 0
Nov 17 14:18:21.050: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 17 14:18:21.050: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:18:21.050: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 17 14:18:21.050: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container rbac-manager ready: true, restart count 0
Nov 17 14:18:21.050: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:18:21.050: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 14:18:21.050: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Nov 17 14:18:21.062: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:18:21.062: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 14:18:21.062: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:18:21.062: INFO: fluentbit-fluentbit-swvsm from logging-system started at 2023-11-17 14:00:46 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:18:21.062: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:00:49 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container alertmanager ready: true, restart count 0
Nov 17 14:18:21.062: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:18:21.062: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:18:21.062: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:00:46 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:18:21.062: INFO: 	Container prometheus ready: true, restart count 0
Nov 17 14:18:21.062: INFO: execpod8k9bk from services-7409 started at 2023-11-17 14:18:17 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container agnhost-container ready: true, restart count 0
Nov 17 14:18:21.062: INFO: nodeport-test-bws8s from services-7409 started at 2023-11-17 14:18:14 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container nodeport-test ready: true, restart count 0
Nov 17 14:18:21.062: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 17 14:18:21.062: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container e2e ready: true, restart count 0
Nov 17 14:18:21.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:18:21.062: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:18:21.062: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 14:18:21.062: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
Nov 17 14:18:21.076: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:18:21.077: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 14:18:21.077: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:18:21.077: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:18:21.077: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:18:21.077: INFO: 	Container fluentd ready: true, restart count 3
Nov 17 14:18:21.077: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:18:21.077: INFO: nodeport-test-7x69v from services-7409 started at 2023-11-17 14:18:14 +0000 UTC (1 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container nodeport-test ready: true, restart count 0
Nov 17 14:18:21.077: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:18:21.077: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:18:21.077: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:18:21.077
Nov 17 14:18:21.088: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2853" to be "running"
Nov 17 14:18:21.093: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971266ms
Nov 17 14:18:23.096: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008805779s
Nov 17 14:18:23.096: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:18:23.099
STEP: Trying to apply a random label on the found node. 11/17/23 14:18:23.11
STEP: verifying the node has the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d 42 11/17/23 14:18:23.133
STEP: Trying to relaunch the pod, now with labels. 11/17/23 14:18:23.14
Nov 17 14:18:23.147: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2853" to be "not pending"
Nov 17 14:18:23.154: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699182ms
Nov 17 14:18:25.160: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01235015s
Nov 17 14:18:25.160: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d off the node k8s-worker-2.c.operations-lab.internal 11/17/23 14:18:25.163
STEP: verifying the node doesn't have the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d 11/17/23 14:18:25.179
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:25.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2853" for this suite. 11/17/23 14:18:25.191
------------------------------
â€¢ [4.219 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:20.981
    Nov 17 14:18:20.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-pred 11/17/23 14:18:20.982
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:21.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:21.012
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 17 14:18:21.015: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 17 14:18:21.025: INFO: Waiting for terminating namespaces to be deleted...
    Nov 17 14:18:21.032: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Nov 17 14:18:21.050: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container cert-manager ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container cainjector ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container webhook ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container minio ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container velero ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container traefik ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kube-green ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container hubble-relay ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container backend ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: 	Container frontend ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kyverno ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kyverno-background ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kyverno-reports ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container logging-operator ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container rbac-manager ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.050: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 14:18:21.050: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Nov 17 14:18:21.062: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: fluentbit-fluentbit-swvsm from logging-system started at 2023-11-17 14:00:46 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:00:49 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container alertmanager ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:00:46 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: 	Container prometheus ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: execpod8k9bk from services-7409 started at 2023-11-17 14:18:17 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container agnhost-container ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: nodeport-test-bws8s from services-7409 started at 2023-11-17 14:18:14 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container nodeport-test ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container e2e ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 14:18:21.062: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
    Nov 17 14:18:21.076: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: 	Container fluentd ready: true, restart count 3
    Nov 17 14:18:21.077: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: nodeport-test-7x69v from services-7409 started at 2023-11-17 14:18:14 +0000 UTC (1 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container nodeport-test ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:18:21.077: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:18:21.077: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:18:21.077
    Nov 17 14:18:21.088: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2853" to be "running"
    Nov 17 14:18:21.093: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971266ms
    Nov 17 14:18:23.096: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008805779s
    Nov 17 14:18:23.096: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:18:23.099
    STEP: Trying to apply a random label on the found node. 11/17/23 14:18:23.11
    STEP: verifying the node has the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d 42 11/17/23 14:18:23.133
    STEP: Trying to relaunch the pod, now with labels. 11/17/23 14:18:23.14
    Nov 17 14:18:23.147: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2853" to be "not pending"
    Nov 17 14:18:23.154: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699182ms
    Nov 17 14:18:25.160: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01235015s
    Nov 17 14:18:25.160: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d off the node k8s-worker-2.c.operations-lab.internal 11/17/23 14:18:25.163
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-45e108d3-3439-408f-9a09-b44fca04eb3d 11/17/23 14:18:25.179
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:25.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2853" for this suite. 11/17/23 14:18:25.191
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:25.207
Nov 17 14:18:25.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename podtemplate 11/17/23 14:18:25.209
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:25.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:25.233
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:25.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4026" for this suite. 11/17/23 14:18:25.282
------------------------------
â€¢ [0.085 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:25.207
    Nov 17 14:18:25.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename podtemplate 11/17/23 14:18:25.209
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:25.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:25.233
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:25.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4026" for this suite. 11/17/23 14:18:25.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:25.297
Nov 17 14:18:25.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:18:25.298
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:25.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:25.325
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3441 11/17/23 14:18:25.329
STEP: creating a selector 11/17/23 14:18:25.329
STEP: Creating the service pods in kubernetes 11/17/23 14:18:25.329
Nov 17 14:18:25.329: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 17 14:18:25.380: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3441" to be "running and ready"
Nov 17 14:18:25.400: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.15793ms
Nov 17 14:18:25.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:18:27.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023929951s
Nov 17 14:18:27.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:29.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025296445s
Nov 17 14:18:29.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:31.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023710367s
Nov 17 14:18:31.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:33.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024315694s
Nov 17 14:18:33.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:35.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025362896s
Nov 17 14:18:35.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:37.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024227777s
Nov 17 14:18:37.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:39.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022807871s
Nov 17 14:18:39.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:41.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02289755s
Nov 17 14:18:41.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:43.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025043862s
Nov 17 14:18:43.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:45.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023433076s
Nov 17 14:18:45.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:18:47.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024747585s
Nov 17 14:18:47.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 17 14:18:47.406: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 17 14:18:47.409: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3441" to be "running and ready"
Nov 17 14:18:47.413: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.53565ms
Nov 17 14:18:47.413: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 17 14:18:47.413: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 17 14:18:47.416: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3441" to be "running and ready"
Nov 17 14:18:47.419: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.845355ms
Nov 17 14:18:47.419: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 17 14:18:47.419: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/17/23 14:18:47.422
Nov 17 14:18:47.430: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3441" to be "running"
Nov 17 14:18:47.435: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860598ms
Nov 17 14:18:49.439: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009253681s
Nov 17 14:18:49.439: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 17 14:18:49.442: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 17 14:18:49.442: INFO: Breadth first check of 10.10.2.161 on host 172.16.0.5...
Nov 17 14:18:49.445: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.2.161&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:18:49.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:18:49.446: INFO: ExecWithOptions: Clientset creation
Nov 17 14:18:49.446: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.2.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:18:49.560: INFO: Waiting for responses: map[]
Nov 17 14:18:49.560: INFO: reached 10.10.2.161 after 0/1 tries
Nov 17 14:18:49.560: INFO: Breadth first check of 10.10.0.88 on host 172.16.0.4...
Nov 17 14:18:49.564: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.0.88&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:18:49.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:18:49.564: INFO: ExecWithOptions: Clientset creation
Nov 17 14:18:49.565: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.88%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:18:49.667: INFO: Waiting for responses: map[]
Nov 17 14:18:49.667: INFO: reached 10.10.0.88 after 0/1 tries
Nov 17 14:18:49.667: INFO: Breadth first check of 10.10.1.191 on host 172.16.0.3...
Nov 17 14:18:49.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.1.191&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:18:49.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:18:49.671: INFO: ExecWithOptions: Clientset creation
Nov 17 14:18:49.671: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.191%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:18:49.792: INFO: Waiting for responses: map[]
Nov 17 14:18:49.792: INFO: reached 10.10.1.191 after 0/1 tries
Nov 17 14:18:49.792: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3441" for this suite. 11/17/23 14:18:49.796
------------------------------
â€¢ [SLOW TEST] [24.509 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:25.297
    Nov 17 14:18:25.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:18:25.298
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:25.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:25.325
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3441 11/17/23 14:18:25.329
    STEP: creating a selector 11/17/23 14:18:25.329
    STEP: Creating the service pods in kubernetes 11/17/23 14:18:25.329
    Nov 17 14:18:25.329: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 17 14:18:25.380: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3441" to be "running and ready"
    Nov 17 14:18:25.400: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.15793ms
    Nov 17 14:18:25.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:18:27.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023929951s
    Nov 17 14:18:27.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:29.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025296445s
    Nov 17 14:18:29.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:31.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023710367s
    Nov 17 14:18:31.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:33.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.024315694s
    Nov 17 14:18:33.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:35.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025362896s
    Nov 17 14:18:35.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:37.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024227777s
    Nov 17 14:18:37.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:39.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022807871s
    Nov 17 14:18:39.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:41.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.02289755s
    Nov 17 14:18:41.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:43.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025043862s
    Nov 17 14:18:43.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:45.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023433076s
    Nov 17 14:18:45.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:18:47.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024747585s
    Nov 17 14:18:47.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 17 14:18:47.406: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 17 14:18:47.409: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3441" to be "running and ready"
    Nov 17 14:18:47.413: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.53565ms
    Nov 17 14:18:47.413: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 17 14:18:47.413: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 17 14:18:47.416: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3441" to be "running and ready"
    Nov 17 14:18:47.419: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.845355ms
    Nov 17 14:18:47.419: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 17 14:18:47.419: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/17/23 14:18:47.422
    Nov 17 14:18:47.430: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3441" to be "running"
    Nov 17 14:18:47.435: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860598ms
    Nov 17 14:18:49.439: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009253681s
    Nov 17 14:18:49.439: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 17 14:18:49.442: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 17 14:18:49.442: INFO: Breadth first check of 10.10.2.161 on host 172.16.0.5...
    Nov 17 14:18:49.445: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.2.161&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:18:49.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:18:49.446: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:18:49.446: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.2.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:18:49.560: INFO: Waiting for responses: map[]
    Nov 17 14:18:49.560: INFO: reached 10.10.2.161 after 0/1 tries
    Nov 17 14:18:49.560: INFO: Breadth first check of 10.10.0.88 on host 172.16.0.4...
    Nov 17 14:18:49.564: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.0.88&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:18:49.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:18:49.564: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:18:49.565: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.88%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:18:49.667: INFO: Waiting for responses: map[]
    Nov 17 14:18:49.667: INFO: reached 10.10.0.88 after 0/1 tries
    Nov 17 14:18:49.667: INFO: Breadth first check of 10.10.1.191 on host 172.16.0.3...
    Nov 17 14:18:49.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.132:9080/dial?request=hostname&protocol=udp&host=10.10.1.191&port=8081&tries=1'] Namespace:pod-network-test-3441 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:18:49.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:18:49.671: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:18:49.671: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3441/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.191%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:18:49.792: INFO: Waiting for responses: map[]
    Nov 17 14:18:49.792: INFO: reached 10.10.1.191 after 0/1 tries
    Nov 17 14:18:49.792: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3441" for this suite. 11/17/23 14:18:49.796
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:49.807
Nov 17 14:18:49.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:18:49.808
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:49.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:49.831
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:18:49.847
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:18:50.264
STEP: Deploying the webhook pod 11/17/23 14:18:50.271
STEP: Wait for the deployment to be ready 11/17/23 14:18:50.286
Nov 17 14:18:50.294: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:18:52.303
STEP: Verifying the service has paired with the endpoint 11/17/23 14:18:52.324
Nov 17 14:18:53.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 11/17/23 14:18:53.328
STEP: Creating a custom resource definition that should be denied by the webhook 11/17/23 14:18:53.348
Nov 17 14:18:53.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:53.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1832" for this suite. 11/17/23 14:18:53.504
STEP: Destroying namespace "webhook-1832-markers" for this suite. 11/17/23 14:18:53.519
------------------------------
â€¢ [3.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:49.807
    Nov 17 14:18:49.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:18:49.808
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:49.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:49.831
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:18:49.847
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:18:50.264
    STEP: Deploying the webhook pod 11/17/23 14:18:50.271
    STEP: Wait for the deployment to be ready 11/17/23 14:18:50.286
    Nov 17 14:18:50.294: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:18:52.303
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:18:52.324
    Nov 17 14:18:53.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 11/17/23 14:18:53.328
    STEP: Creating a custom resource definition that should be denied by the webhook 11/17/23 14:18:53.348
    Nov 17 14:18:53.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:53.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1832" for this suite. 11/17/23 14:18:53.504
    STEP: Destroying namespace "webhook-1832-markers" for this suite. 11/17/23 14:18:53.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:53.535
Nov 17 14:18:53.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:18:53.536
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:53.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:53.587
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Nov 17 14:18:53.618: INFO: Waiting up to 5m0s for pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189" in namespace "svcaccounts-1172" to be "running"
Nov 17 14:18:53.626: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189": Phase="Pending", Reason="", readiness=false. Elapsed: 8.433835ms
Nov 17 14:18:55.635: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189": Phase="Running", Reason="", readiness=true. Elapsed: 2.017185399s
Nov 17 14:18:55.635: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189" satisfied condition "running"
STEP: reading a file in the container 11/17/23 14:18:55.635
Nov 17 14:18:55.636: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 11/17/23 14:18:55.858
Nov 17 14:18:55.858: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 11/17/23 14:18:56.06
Nov 17 14:18:56.060: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Nov 17 14:18:56.276: INFO: Got root ca configmap in namespace "svcaccounts-1172"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1172" for this suite. 11/17/23 14:18:56.285
------------------------------
â€¢ [2.757 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:53.535
    Nov 17 14:18:53.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:18:53.536
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:53.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:53.587
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Nov 17 14:18:53.618: INFO: Waiting up to 5m0s for pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189" in namespace "svcaccounts-1172" to be "running"
    Nov 17 14:18:53.626: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189": Phase="Pending", Reason="", readiness=false. Elapsed: 8.433835ms
    Nov 17 14:18:55.635: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189": Phase="Running", Reason="", readiness=true. Elapsed: 2.017185399s
    Nov 17 14:18:55.635: INFO: Pod "pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189" satisfied condition "running"
    STEP: reading a file in the container 11/17/23 14:18:55.635
    Nov 17 14:18:55.636: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 11/17/23 14:18:55.858
    Nov 17 14:18:55.858: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 11/17/23 14:18:56.06
    Nov 17 14:18:56.060: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1172 pod-service-account-804d1d9a-e697-4359-9ace-3c99386f5189 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Nov 17 14:18:56.276: INFO: Got root ca configmap in namespace "svcaccounts-1172"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1172" for this suite. 11/17/23 14:18:56.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:56.293
Nov 17 14:18:56.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 14:18:56.294
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:56.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:56.32
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 11/17/23 14:18:56.345
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:18:56.354
Nov 17 14:18:56.359: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:18:56.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 14:18:56.363: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:18:57.368: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:18:57.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 14:18:57.372: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:18:58.369: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:18:58.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 14:18:58.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 11/17/23 14:18:58.375
STEP: DeleteCollection of the DaemonSets 11/17/23 14:18:58.379
STEP: Verify that ReplicaSets have been deleted 11/17/23 14:18:58.388
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Nov 17 14:18:58.409: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36807"},"items":null}

Nov 17 14:18:58.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36807"},"items":[{"metadata":{"name":"daemon-set-gk4cm","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"4ecad0dd-e750-4032-9a4c-55e8b8f15417","resourceVersion":"36807","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4zmmw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4zmmw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-3.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-3.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.3","podIP":"10.10.1.113","podIPs":[{"ip":"10.10.1.113"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a9ee49e14550484e7b661cc6d824678189e9454b1f6a65d8cacbd6586bbc424c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gnkqx","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"f1582d83-24ac-4c21-ac4e-e056eb020c6c","resourceVersion":"36804","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d6fzd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d6fzd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-1.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-1.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.5","podIP":"10.10.2.107","podIPs":[{"ip":"10.10.2.107"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e23439de88077895887052701c7dec63b619185e269d84a48cbd8bf500f44e8d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lmxs6","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"ebee61bc-77f8-4994-b0ad-6587c8652e2c","resourceVersion":"36806","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hp4bf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hp4bf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-2.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-2.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.4","podIP":"10.10.0.149","podIPs":[{"ip":"10.10.0.149"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1b3a7aea8cc51f719c3fef0a93636cfb7973d4e6621a9396ee8e50f0b79f3dee","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:18:58.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2954" for this suite. 11/17/23 14:18:58.437
------------------------------
â€¢ [2.151 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:56.293
    Nov 17 14:18:56.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 14:18:56.294
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:56.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:56.32
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 11/17/23 14:18:56.345
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:18:56.354
    Nov 17 14:18:56.359: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:18:56.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 14:18:56.363: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:18:57.368: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:18:57.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 14:18:57.372: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:18:58.369: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:18:58.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 14:18:58.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 11/17/23 14:18:58.375
    STEP: DeleteCollection of the DaemonSets 11/17/23 14:18:58.379
    STEP: Verify that ReplicaSets have been deleted 11/17/23 14:18:58.388
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Nov 17 14:18:58.409: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36807"},"items":null}

    Nov 17 14:18:58.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36807"},"items":[{"metadata":{"name":"daemon-set-gk4cm","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"4ecad0dd-e750-4032-9a4c-55e8b8f15417","resourceVersion":"36807","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4zmmw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4zmmw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-3.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-3.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.3","podIP":"10.10.1.113","podIPs":[{"ip":"10.10.1.113"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a9ee49e14550484e7b661cc6d824678189e9454b1f6a65d8cacbd6586bbc424c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gnkqx","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"f1582d83-24ac-4c21-ac4e-e056eb020c6c","resourceVersion":"36804","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d6fzd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d6fzd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-1.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-1.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.5","podIP":"10.10.2.107","podIPs":[{"ip":"10.10.2.107"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e23439de88077895887052701c7dec63b619185e269d84a48cbd8bf500f44e8d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lmxs6","generateName":"daemon-set-","namespace":"daemonsets-2954","uid":"ebee61bc-77f8-4994-b0ad-6587c8652e2c","resourceVersion":"36806","creationTimestamp":"2023-11-17T14:18:56Z","deletionTimestamp":"2023-11-17T14:19:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"2334f5c8-43e7-44ab-b2b1-e963c11c9845","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2334f5c8-43e7-44ab-b2b1-e963c11c9845\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-11-17T14:18:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hp4bf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hp4bf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker-2.c.operations-lab.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker-2.c.operations-lab.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-11-17T14:18:56Z"}],"hostIP":"172.16.0.4","podIP":"10.10.0.149","podIPs":[{"ip":"10.10.0.149"}],"startTime":"2023-11-17T14:18:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-11-17T14:18:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1b3a7aea8cc51f719c3fef0a93636cfb7973d4e6621a9396ee8e50f0b79f3dee","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:18:58.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2954" for this suite. 11/17/23 14:18:58.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:18:58.446
Nov 17 14:18:58.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:18:58.447
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:58.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:58.474
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 11/17/23 14:18:58.478
STEP: submitting the pod to kubernetes 11/17/23 14:18:58.478
Nov 17 14:18:58.488: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" in namespace "pods-4787" to be "running and ready"
Nov 17 14:18:58.492: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244803ms
Nov 17 14:18:58.492: INFO: The phase of Pod pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:19:00.496: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 2.007490051s
Nov 17 14:19:00.496: INFO: The phase of Pod pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851 is Running (Ready = true)
Nov 17 14:19:00.496: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/17/23 14:19:00.499
STEP: updating the pod 11/17/23 14:19:00.502
Nov 17 14:19:01.021: INFO: Successfully updated pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851"
Nov 17 14:19:01.021: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" in namespace "pods-4787" to be "terminated with reason DeadlineExceeded"
Nov 17 14:19:01.026: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 5.091731ms
Nov 17 14:19:03.032: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474167s
Nov 17 14:19:05.031: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009324549s
Nov 17 14:19:05.031: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:05.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4787" for this suite. 11/17/23 14:19:05.035
------------------------------
â€¢ [SLOW TEST] [6.600 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:18:58.446
    Nov 17 14:18:58.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:18:58.447
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:18:58.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:18:58.474
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 11/17/23 14:18:58.478
    STEP: submitting the pod to kubernetes 11/17/23 14:18:58.478
    Nov 17 14:18:58.488: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" in namespace "pods-4787" to be "running and ready"
    Nov 17 14:18:58.492: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244803ms
    Nov 17 14:18:58.492: INFO: The phase of Pod pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:19:00.496: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 2.007490051s
    Nov 17 14:19:00.496: INFO: The phase of Pod pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851 is Running (Ready = true)
    Nov 17 14:19:00.496: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/17/23 14:19:00.499
    STEP: updating the pod 11/17/23 14:19:00.502
    Nov 17 14:19:01.021: INFO: Successfully updated pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851"
    Nov 17 14:19:01.021: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" in namespace "pods-4787" to be "terminated with reason DeadlineExceeded"
    Nov 17 14:19:01.026: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 5.091731ms
    Nov 17 14:19:03.032: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474167s
    Nov 17 14:19:05.031: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009324549s
    Nov 17 14:19:05.031: INFO: Pod "pod-update-activedeadlineseconds-088b4ce1-0373-47f4-953b-6b7bc82d7851" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:05.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4787" for this suite. 11/17/23 14:19:05.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:05.046
Nov 17 14:19:05.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename init-container 11/17/23 14:19:05.047
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:05.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:05.074
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 11/17/23 14:19:05.078
Nov 17 14:19:05.078: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8017" for this suite. 11/17/23 14:19:10.832
------------------------------
â€¢ [SLOW TEST] [5.794 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:05.046
    Nov 17 14:19:05.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename init-container 11/17/23 14:19:05.047
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:05.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:05.074
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 11/17/23 14:19:05.078
    Nov 17 14:19:05.078: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8017" for this suite. 11/17/23 14:19:10.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:10.842
Nov 17 14:19:10.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:19:10.844
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:10.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:10.865
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 11/17/23 14:19:10.868
Nov 17 14:19:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7356 api-versions'
Nov 17 14:19:10.980: INFO: stderr: ""
Nov 17 14:19:10.980: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube-green.com/v1alpha1\nkyverno.io/v1\nkyverno.io/v1alpha2\nkyverno.io/v1beta1\nkyverno.io/v2alpha1\nkyverno.io/v2beta1\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrbacmanager.reactiveops.io/v1beta1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\nvelero.io/v1\nwgpolicyk8s.io/v1alpha2\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:10.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7356" for this suite. 11/17/23 14:19:10.985
------------------------------
â€¢ [0.150 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:10.842
    Nov 17 14:19:10.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:19:10.844
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:10.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:10.865
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 11/17/23 14:19:10.868
    Nov 17 14:19:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7356 api-versions'
    Nov 17 14:19:10.980: INFO: stderr: ""
    Nov 17 14:19:10.980: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nkube-green.com/v1alpha1\nkyverno.io/v1\nkyverno.io/v1alpha2\nkyverno.io/v1beta1\nkyverno.io/v2alpha1\nkyverno.io/v2beta1\nlogging-extensions.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1alpha1\nlogging.banzaicloud.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrbacmanager.reactiveops.io/v1beta1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\nvelero.io/v1\nwgpolicyk8s.io/v1alpha2\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:10.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7356" for this suite. 11/17/23 14:19:10.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:10.992
Nov 17 14:19:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:19:10.994
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:11.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:11.011
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1282 11/17/23 14:19:11.014
STEP: creating a selector 11/17/23 14:19:11.014
STEP: Creating the service pods in kubernetes 11/17/23 14:19:11.014
Nov 17 14:19:11.014: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 17 14:19:11.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1282" to be "running and ready"
Nov 17 14:19:11.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.988207ms
Nov 17 14:19:11.077: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:19:13.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949795s
Nov 17 14:19:13.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:19:15.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017309004s
Nov 17 14:19:15.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:19:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017771913s
Nov 17 14:19:17.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:19:19.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019020265s
Nov 17 14:19:19.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:19:21.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018004156s
Nov 17 14:19:21.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:19:23.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016421038s
Nov 17 14:19:23.080: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 17 14:19:23.080: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 17 14:19:23.083: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1282" to be "running and ready"
Nov 17 14:19:23.085: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.06849ms
Nov 17 14:19:23.085: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 17 14:19:23.085: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 17 14:19:23.087: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1282" to be "running and ready"
Nov 17 14:19:23.089: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.106694ms
Nov 17 14:19:23.089: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 17 14:19:23.089: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/17/23 14:19:23.091
Nov 17 14:19:23.097: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1282" to be "running"
Nov 17 14:19:23.111: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.410543ms
Nov 17 14:19:25.116: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018821192s
Nov 17 14:19:25.117: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 17 14:19:25.120: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 17 14:19:25.120: INFO: Breadth first check of 10.10.2.96 on host 172.16.0.5...
Nov 17 14:19:25.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.2.96&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:19:25.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:19:25.124: INFO: ExecWithOptions: Clientset creation
Nov 17 14:19:25.124: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.2.96%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:19:25.241: INFO: Waiting for responses: map[]
Nov 17 14:19:25.241: INFO: reached 10.10.2.96 after 0/1 tries
Nov 17 14:19:25.241: INFO: Breadth first check of 10.10.0.35 on host 172.16.0.4...
Nov 17 14:19:25.244: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.0.35&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:19:25.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:19:25.245: INFO: ExecWithOptions: Clientset creation
Nov 17 14:19:25.245: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.35%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:19:25.364: INFO: Waiting for responses: map[]
Nov 17 14:19:25.364: INFO: reached 10.10.0.35 after 0/1 tries
Nov 17 14:19:25.364: INFO: Breadth first check of 10.10.1.42 on host 172.16.0.3...
Nov 17 14:19:25.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.1.42&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:19:25.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:19:25.369: INFO: ExecWithOptions: Clientset creation
Nov 17 14:19:25.369: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Nov 17 14:19:25.479: INFO: Waiting for responses: map[]
Nov 17 14:19:25.479: INFO: reached 10.10.1.42 after 0/1 tries
Nov 17 14:19:25.479: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:25.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1282" for this suite. 11/17/23 14:19:25.484
------------------------------
â€¢ [SLOW TEST] [14.497 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:10.992
    Nov 17 14:19:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:19:10.994
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:11.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:11.011
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1282 11/17/23 14:19:11.014
    STEP: creating a selector 11/17/23 14:19:11.014
    STEP: Creating the service pods in kubernetes 11/17/23 14:19:11.014
    Nov 17 14:19:11.014: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 17 14:19:11.063: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1282" to be "running and ready"
    Nov 17 14:19:11.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.988207ms
    Nov 17 14:19:11.077: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:19:13.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949795s
    Nov 17 14:19:13.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:19:15.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017309004s
    Nov 17 14:19:15.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:19:17.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017771913s
    Nov 17 14:19:17.081: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:19:19.082: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.019020265s
    Nov 17 14:19:19.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:19:21.081: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018004156s
    Nov 17 14:19:21.082: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:19:23.080: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016421038s
    Nov 17 14:19:23.080: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 17 14:19:23.080: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 17 14:19:23.083: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1282" to be "running and ready"
    Nov 17 14:19:23.085: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.06849ms
    Nov 17 14:19:23.085: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 17 14:19:23.085: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 17 14:19:23.087: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1282" to be "running and ready"
    Nov 17 14:19:23.089: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.106694ms
    Nov 17 14:19:23.089: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 17 14:19:23.089: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/17/23 14:19:23.091
    Nov 17 14:19:23.097: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1282" to be "running"
    Nov 17 14:19:23.111: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.410543ms
    Nov 17 14:19:25.116: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018821192s
    Nov 17 14:19:25.117: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 17 14:19:25.120: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 17 14:19:25.120: INFO: Breadth first check of 10.10.2.96 on host 172.16.0.5...
    Nov 17 14:19:25.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.2.96&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:19:25.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:19:25.124: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:19:25.124: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.2.96%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:19:25.241: INFO: Waiting for responses: map[]
    Nov 17 14:19:25.241: INFO: reached 10.10.2.96 after 0/1 tries
    Nov 17 14:19:25.241: INFO: Breadth first check of 10.10.0.35 on host 172.16.0.4...
    Nov 17 14:19:25.244: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.0.35&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:19:25.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:19:25.245: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:19:25.245: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.35%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:19:25.364: INFO: Waiting for responses: map[]
    Nov 17 14:19:25.364: INFO: reached 10.10.0.35 after 0/1 tries
    Nov 17 14:19:25.364: INFO: Breadth first check of 10.10.1.42 on host 172.16.0.3...
    Nov 17 14:19:25.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.64:9080/dial?request=hostname&protocol=http&host=10.10.1.42&port=8083&tries=1'] Namespace:pod-network-test-1282 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:19:25.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:19:25.369: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:19:25.369: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1282/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Nov 17 14:19:25.479: INFO: Waiting for responses: map[]
    Nov 17 14:19:25.479: INFO: reached 10.10.1.42 after 0/1 tries
    Nov 17 14:19:25.479: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:25.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1282" for this suite. 11/17/23 14:19:25.484
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:25.491
Nov 17 14:19:25.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 14:19:25.492
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:25.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:25.512
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 11/17/23 14:19:25.52
Nov 17 14:19:25.520: INFO: Creating simple deployment test-deployment-gklp7
Nov 17 14:19:25.534: INFO: deployment "test-deployment-gklp7" doesn't have the required revision set
STEP: Getting /status 11/17/23 14:19:27.549
Nov 17 14:19:27.554: INFO: Deployment test-deployment-gklp7 has Conditions: [{Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 11/17/23 14:19:27.554
Nov 17 14:19:27.567: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 19, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gklp7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 11/17/23 14:19:27.567
Nov 17 14:19:27.569: INFO: Observed &Deployment event: ADDED
Nov 17 14:19:27.569: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
Nov 17 14:19:27.570: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 17 14:19:27.570: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gklp7-54bc444df" is progressing.}
Nov 17 14:19:27.571: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
Nov 17 14:19:27.571: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
Nov 17 14:19:27.571: INFO: Found Deployment test-deployment-gklp7 in namespace deployment-8469 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 17 14:19:27.571: INFO: Deployment test-deployment-gklp7 has an updated status
STEP: patching the Statefulset Status 11/17/23 14:19:27.571
Nov 17 14:19:27.571: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 17 14:19:27.581: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 11/17/23 14:19:27.581
Nov 17 14:19:27.586: INFO: Observed &Deployment event: ADDED
Nov 17 14:19:27.586: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gklp7-54bc444df" is progressing.}
Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
Nov 17 14:19:27.588: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 17 14:19:27.588: INFO: Observed &Deployment event: MODIFIED
Nov 17 14:19:27.588: INFO: Found deployment test-deployment-gklp7 in namespace deployment-8469 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 17 14:19:27.588: INFO: Deployment test-deployment-gklp7 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 14:19:27.592: INFO: Deployment "test-deployment-gklp7":
&Deployment{ObjectMeta:{test-deployment-gklp7  deployment-8469  7ef079bd-cec1-42da-a037-c100c551ed58 37181 1 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-17 14:19:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-17 14:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003230ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gklp7-54bc444df",LastUpdateTime:2023-11-17 14:19:27 +0000 UTC,LastTransitionTime:2023-11-17 14:19:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 17 14:19:27.596: INFO: New ReplicaSet "test-deployment-gklp7-54bc444df" of Deployment "test-deployment-gklp7":
&ReplicaSet{ObjectMeta:{test-deployment-gklp7-54bc444df  deployment-8469  178ad6a4-2c23-4330-8aef-453cbc5bad96 37171 1 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gklp7 7ef079bd-cec1-42da-a037-c100c551ed58 0xc0032317a0 0xc0032317a1}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef079bd-cec1-42da-a037-c100c551ed58\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:19:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003231848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:19:27.600: INFO: Pod "test-deployment-gklp7-54bc444df-kjmpd" is available:
&Pod{ObjectMeta:{test-deployment-gklp7-54bc444df-kjmpd test-deployment-gklp7-54bc444df- deployment-8469  eca016f2-1e5f-4837-86f0-fe25560bce55 37170 0 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-gklp7-54bc444df 178ad6a4-2c23-4330-8aef-453cbc5bad96 0xc003231bf0 0xc003231bf1}] [] [{kube-controller-manager Update v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178ad6a4-2c23-4330-8aef-453cbc5bad96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:19:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cd9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cd9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.135,StartTime:2023-11-17 14:19:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:19:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7a01b942b96cbfd8dc9d4fd2707c08d6cb8667886c0bc718e88b927a4f244de8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:27.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8469" for this suite. 11/17/23 14:19:27.604
------------------------------
â€¢ [2.123 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:25.491
    Nov 17 14:19:25.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 14:19:25.492
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:25.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:25.512
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 11/17/23 14:19:25.52
    Nov 17 14:19:25.520: INFO: Creating simple deployment test-deployment-gklp7
    Nov 17 14:19:25.534: INFO: deployment "test-deployment-gklp7" doesn't have the required revision set
    STEP: Getting /status 11/17/23 14:19:27.549
    Nov 17 14:19:27.554: INFO: Deployment test-deployment-gklp7 has Conditions: [{Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 11/17/23 14:19:27.554
    Nov 17 14:19:27.567: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 19, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 19, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gklp7-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 11/17/23 14:19:27.567
    Nov 17 14:19:27.569: INFO: Observed &Deployment event: ADDED
    Nov 17 14:19:27.569: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
    Nov 17 14:19:27.570: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
    Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 17 14:19:27.570: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 17 14:19:27.570: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gklp7-54bc444df" is progressing.}
    Nov 17 14:19:27.571: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
    Nov 17 14:19:27.571: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 17 14:19:27.571: INFO: Observed Deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
    Nov 17 14:19:27.571: INFO: Found Deployment test-deployment-gklp7 in namespace deployment-8469 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 17 14:19:27.571: INFO: Deployment test-deployment-gklp7 has an updated status
    STEP: patching the Statefulset Status 11/17/23 14:19:27.571
    Nov 17 14:19:27.571: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 17 14:19:27.581: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 11/17/23 14:19:27.581
    Nov 17 14:19:27.586: INFO: Observed &Deployment event: ADDED
    Nov 17 14:19:27.586: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
    Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gklp7-54bc444df"}
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:25 +0000 UTC 2023-11-17 14:19:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gklp7-54bc444df" is progressing.}
    Nov 17 14:19:27.587: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 17 14:19:27.587: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
    Nov 17 14:19:27.588: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-11-17 14:19:26 +0000 UTC 2023-11-17 14:19:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gklp7-54bc444df" has successfully progressed.}
    Nov 17 14:19:27.588: INFO: Observed deployment test-deployment-gklp7 in namespace deployment-8469 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 17 14:19:27.588: INFO: Observed &Deployment event: MODIFIED
    Nov 17 14:19:27.588: INFO: Found deployment test-deployment-gklp7 in namespace deployment-8469 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Nov 17 14:19:27.588: INFO: Deployment test-deployment-gklp7 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 14:19:27.592: INFO: Deployment "test-deployment-gklp7":
    &Deployment{ObjectMeta:{test-deployment-gklp7  deployment-8469  7ef079bd-cec1-42da-a037-c100c551ed58 37181 1 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-11-17 14:19:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-11-17 14:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003230ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gklp7-54bc444df",LastUpdateTime:2023-11-17 14:19:27 +0000 UTC,LastTransitionTime:2023-11-17 14:19:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Nov 17 14:19:27.596: INFO: New ReplicaSet "test-deployment-gklp7-54bc444df" of Deployment "test-deployment-gklp7":
    &ReplicaSet{ObjectMeta:{test-deployment-gklp7-54bc444df  deployment-8469  178ad6a4-2c23-4330-8aef-453cbc5bad96 37171 1 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gklp7 7ef079bd-cec1-42da-a037-c100c551ed58 0xc0032317a0 0xc0032317a1}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef079bd-cec1-42da-a037-c100c551ed58\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:19:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003231848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:19:27.600: INFO: Pod "test-deployment-gklp7-54bc444df-kjmpd" is available:
    &Pod{ObjectMeta:{test-deployment-gklp7-54bc444df-kjmpd test-deployment-gklp7-54bc444df- deployment-8469  eca016f2-1e5f-4837-86f0-fe25560bce55 37170 0 2023-11-17 14:19:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-gklp7-54bc444df 178ad6a4-2c23-4330-8aef-453cbc5bad96 0xc003231bf0 0xc003231bf1}] [] [{kube-controller-manager Update v1 2023-11-17 14:19:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"178ad6a4-2c23-4330-8aef-453cbc5bad96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:19:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cd9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cd9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.135,StartTime:2023-11-17 14:19:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:19:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7a01b942b96cbfd8dc9d4fd2707c08d6cb8667886c0bc718e88b927a4f244de8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:27.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8469" for this suite. 11/17/23 14:19:27.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:27.615
Nov 17 14:19:27.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 14:19:27.616
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:27.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:27.641
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-7715/configmap-test-8f2dabc3-ca5f-449f-ac62-afb5ca535655 11/17/23 14:19:27.644
STEP: Creating a pod to test consume configMaps 11/17/23 14:19:27.649
Nov 17 14:19:27.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa" in namespace "configmap-7715" to be "Succeeded or Failed"
Nov 17 14:19:27.661: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510895ms
Nov 17 14:19:29.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008774825s
Nov 17 14:19:31.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009202749s
STEP: Saw pod success 11/17/23 14:19:31.666
Nov 17 14:19:31.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa" satisfied condition "Succeeded or Failed"
Nov 17 14:19:31.669: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa container env-test: <nil>
STEP: delete the pod 11/17/23 14:19:31.685
Nov 17 14:19:31.695: INFO: Waiting for pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa to disappear
Nov 17 14:19:31.698: INFO: Pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7715" for this suite. 11/17/23 14:19:31.702
------------------------------
â€¢ [4.094 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:27.615
    Nov 17 14:19:27.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 14:19:27.616
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:27.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:27.641
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-7715/configmap-test-8f2dabc3-ca5f-449f-ac62-afb5ca535655 11/17/23 14:19:27.644
    STEP: Creating a pod to test consume configMaps 11/17/23 14:19:27.649
    Nov 17 14:19:27.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa" in namespace "configmap-7715" to be "Succeeded or Failed"
    Nov 17 14:19:27.661: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510895ms
    Nov 17 14:19:29.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008774825s
    Nov 17 14:19:31.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009202749s
    STEP: Saw pod success 11/17/23 14:19:31.666
    Nov 17 14:19:31.666: INFO: Pod "pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa" satisfied condition "Succeeded or Failed"
    Nov 17 14:19:31.669: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa container env-test: <nil>
    STEP: delete the pod 11/17/23 14:19:31.685
    Nov 17 14:19:31.695: INFO: Waiting for pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa to disappear
    Nov 17 14:19:31.698: INFO: Pod pod-configmaps-c95be45d-c095-4ca6-a467-fdc96e03c2fa no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7715" for this suite. 11/17/23 14:19:31.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:31.71
Nov 17 14:19:31.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:19:31.712
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:31.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:31.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:19:31.743
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:19:32.412
STEP: Deploying the webhook pod 11/17/23 14:19:32.428
STEP: Wait for the deployment to be ready 11/17/23 14:19:32.444
Nov 17 14:19:32.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:19:34.467
STEP: Verifying the service has paired with the endpoint 11/17/23 14:19:34.483
Nov 17 14:19:35.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 11/17/23 14:19:35.558
STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:35.592
STEP: Deleting the collection of validation webhooks 11/17/23 14:19:35.621
STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:35.668
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:35.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1258" for this suite. 11/17/23 14:19:35.769
STEP: Destroying namespace "webhook-1258-markers" for this suite. 11/17/23 14:19:35.792
------------------------------
â€¢ [4.109 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:31.71
    Nov 17 14:19:31.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:19:31.712
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:31.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:31.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:19:31.743
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:19:32.412
    STEP: Deploying the webhook pod 11/17/23 14:19:32.428
    STEP: Wait for the deployment to be ready 11/17/23 14:19:32.444
    Nov 17 14:19:32.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:19:34.467
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:19:34.483
    Nov 17 14:19:35.483: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 11/17/23 14:19:35.558
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:35.592
    STEP: Deleting the collection of validation webhooks 11/17/23 14:19:35.621
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:35.668
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:35.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1258" for this suite. 11/17/23 14:19:35.769
    STEP: Destroying namespace "webhook-1258-markers" for this suite. 11/17/23 14:19:35.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:35.821
Nov 17 14:19:35.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:19:35.822
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:35.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:35.878
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-6a0b7397-d387-4456-b412-93233fc1946d 11/17/23 14:19:35.883
STEP: Creating a pod to test consume secrets 11/17/23 14:19:35.892
Nov 17 14:19:35.904: INFO: Waiting up to 5m0s for pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d" in namespace "secrets-791" to be "Succeeded or Failed"
Nov 17 14:19:35.916: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.065083ms
Nov 17 14:19:37.920: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015961705s
Nov 17 14:19:39.921: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016331338s
STEP: Saw pod success 11/17/23 14:19:39.921
Nov 17 14:19:39.921: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d" satisfied condition "Succeeded or Failed"
Nov 17 14:19:39.925: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:19:39.932
Nov 17 14:19:39.947: INFO: Waiting for pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d to disappear
Nov 17 14:19:39.951: INFO: Pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:39.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-791" for this suite. 11/17/23 14:19:39.956
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:35.821
    Nov 17 14:19:35.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:19:35.822
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:35.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:35.878
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-6a0b7397-d387-4456-b412-93233fc1946d 11/17/23 14:19:35.883
    STEP: Creating a pod to test consume secrets 11/17/23 14:19:35.892
    Nov 17 14:19:35.904: INFO: Waiting up to 5m0s for pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d" in namespace "secrets-791" to be "Succeeded or Failed"
    Nov 17 14:19:35.916: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.065083ms
    Nov 17 14:19:37.920: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015961705s
    Nov 17 14:19:39.921: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016331338s
    STEP: Saw pod success 11/17/23 14:19:39.921
    Nov 17 14:19:39.921: INFO: Pod "pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d" satisfied condition "Succeeded or Failed"
    Nov 17 14:19:39.925: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:19:39.932
    Nov 17 14:19:39.947: INFO: Waiting for pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d to disappear
    Nov 17 14:19:39.951: INFO: Pod pod-secrets-0cc8abcf-c06c-4121-9384-7cd96afbc71d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:39.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-791" for this suite. 11/17/23 14:19:39.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:39.964
Nov 17 14:19:39.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:19:39.965
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:39.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:39.993
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:19:40.03
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:19:40.326
STEP: Deploying the webhook pod 11/17/23 14:19:40.333
STEP: Wait for the deployment to be ready 11/17/23 14:19:40.349
Nov 17 14:19:40.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:19:42.371
STEP: Verifying the service has paired with the endpoint 11/17/23 14:19:42.392
Nov 17 14:19:43.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 11/17/23 14:19:43.396
STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.416
STEP: Updating a validating webhook configuration's rules to not include the create operation 11/17/23 14:19:43.427
STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.439
STEP: Patching a validating webhook configuration's rules to include the create operation 11/17/23 14:19:43.449
STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.458
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:43.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7806" for this suite. 11/17/23 14:19:43.563
STEP: Destroying namespace "webhook-7806-markers" for this suite. 11/17/23 14:19:43.571
------------------------------
â€¢ [3.621 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:39.964
    Nov 17 14:19:39.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:19:39.965
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:39.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:39.993
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:19:40.03
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:19:40.326
    STEP: Deploying the webhook pod 11/17/23 14:19:40.333
    STEP: Wait for the deployment to be ready 11/17/23 14:19:40.349
    Nov 17 14:19:40.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:19:42.371
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:19:42.392
    Nov 17 14:19:43.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 11/17/23 14:19:43.396
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.416
    STEP: Updating a validating webhook configuration's rules to not include the create operation 11/17/23 14:19:43.427
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.439
    STEP: Patching a validating webhook configuration's rules to include the create operation 11/17/23 14:19:43.449
    STEP: Creating a configMap that does not comply to the validation webhook rules 11/17/23 14:19:43.458
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:43.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7806" for this suite. 11/17/23 14:19:43.563
    STEP: Destroying namespace "webhook-7806-markers" for this suite. 11/17/23 14:19:43.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:43.592
Nov 17 14:19:43.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 14:19:43.593
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:43.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:43.629
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Nov 17 14:19:43.646: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 17 14:19:48.651: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 14:19:48.651
Nov 17 14:19:48.651: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/17/23 14:19:48.664
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 14:19:48.680: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1394  34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba 37569 1 2023-11-17 14:19:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-17 14:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b976c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Nov 17 14:19:48.685: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Nov 17 14:19:48.685: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Nov 17 14:19:48.685: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1394  3160be02-9824-4b71-8f45-5b29130f5a4e 37571 1 2023-11-17 14:19:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba 0xc00297f777 0xc00297f778}] [] [{e2e.test Update apps/v1 2023-11-17 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:19:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 14:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00297f858 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 17 14:19:48.704: INFO: Pod "test-cleanup-controller-rkjzl" is available:
&Pod{ObjectMeta:{test-cleanup-controller-rkjzl test-cleanup-controller- deployment-1394  07c46827-516d-425f-a5cc-f74b4f10bfe0 37536 0 2023-11-17 14:19:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 3160be02-9824-4b71-8f45-5b29130f5a4e 0xc003b979f7 0xc003b979f8}] [] [{kube-controller-manager Update v1 2023-11-17 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3160be02-9824-4b71-8f45-5b29130f5a4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:19:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7t7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7t7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.192,StartTime:2023-11-17 14:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5664f5a7e9c11ec08e399e50c20840c2d0eb3e3f05d183ec9709dcf5343afeb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:48.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1394" for this suite. 11/17/23 14:19:48.719
------------------------------
â€¢ [SLOW TEST] [5.152 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:43.592
    Nov 17 14:19:43.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 14:19:43.593
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:43.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:43.629
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Nov 17 14:19:43.646: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Nov 17 14:19:48.651: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 14:19:48.651
    Nov 17 14:19:48.651: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 11/17/23 14:19:48.664
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 14:19:48.680: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1394  34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba 37569 1 2023-11-17 14:19:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-11-17 14:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b976c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 17 14:19:48.685: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Nov 17 14:19:48.685: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Nov 17 14:19:48.685: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1394  3160be02-9824-4b71-8f45-5b29130f5a4e 37571 1 2023-11-17 14:19:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba 0xc00297f777 0xc00297f778}] [] [{e2e.test Update apps/v1 2023-11-17 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:19:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 14:19:48 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"34e4dbdd-5993-4fff-b7f8-3bea66f3f3ba\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00297f858 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 14:19:48.704: INFO: Pod "test-cleanup-controller-rkjzl" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-rkjzl test-cleanup-controller- deployment-1394  07c46827-516d-425f-a5cc-f74b4f10bfe0 37536 0 2023-11-17 14:19:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 3160be02-9824-4b71-8f45-5b29130f5a4e 0xc003b979f7 0xc003b979f8}] [] [{kube-controller-manager Update v1 2023-11-17 14:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3160be02-9824-4b71-8f45-5b29130f5a4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:19:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r7t7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7t7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.192,StartTime:2023-11-17 14:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5664f5a7e9c11ec08e399e50c20840c2d0eb3e3f05d183ec9709dcf5343afeb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:48.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1394" for this suite. 11/17/23 14:19:48.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:48.748
Nov 17 14:19:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubelet-test 11/17/23 14:19:48.75
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:48.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:48.792
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:48.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8961" for this suite. 11/17/23 14:19:48.837
------------------------------
â€¢ [0.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:48.748
    Nov 17 14:19:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubelet-test 11/17/23 14:19:48.75
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:48.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:48.792
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:48.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8961" for this suite. 11/17/23 14:19:48.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:48.846
Nov 17 14:19:48.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 14:19:48.848
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:48.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:48.89
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/17/23 14:19:48.895
Nov 17 14:19:48.906: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9572" to be "running and ready"
Nov 17 14:19:48.911: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763628ms
Nov 17 14:19:48.911: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:19:50.915: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009050556s
Nov 17 14:19:50.915: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Nov 17 14:19:50.915: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 11/17/23 14:19:50.918
STEP: Then the orphan pod is adopted 11/17/23 14:19:50.924
STEP: When the matched label of one of its pods change 11/17/23 14:19:51.932
Nov 17 14:19:51.935: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 11/17/23 14:19:51.943
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:51.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9572" for this suite. 11/17/23 14:19:51.967
------------------------------
â€¢ [3.160 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:48.846
    Nov 17 14:19:48.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 14:19:48.848
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:48.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:48.89
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 11/17/23 14:19:48.895
    Nov 17 14:19:48.906: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9572" to be "running and ready"
    Nov 17 14:19:48.911: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763628ms
    Nov 17 14:19:48.911: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:19:50.915: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009050556s
    Nov 17 14:19:50.915: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Nov 17 14:19:50.915: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 11/17/23 14:19:50.918
    STEP: Then the orphan pod is adopted 11/17/23 14:19:50.924
    STEP: When the matched label of one of its pods change 11/17/23 14:19:51.932
    Nov 17 14:19:51.935: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/17/23 14:19:51.943
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:51.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9572" for this suite. 11/17/23 14:19:51.967
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:52.007
Nov 17 14:19:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename endpointslice 11/17/23 14:19:52.01
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:52.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:52.032
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:54.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7781" for this suite. 11/17/23 14:19:54.139
------------------------------
â€¢ [2.166 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:52.007
    Nov 17 14:19:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename endpointslice 11/17/23 14:19:52.01
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:52.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:52.032
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:54.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7781" for this suite. 11/17/23 14:19:54.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:54.176
Nov 17 14:19:54.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:19:54.177
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:54.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:54.209
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-1aa4dc73-4eb4-46ac-8774-7e9efec79718 11/17/23 14:19:54.213
STEP: Creating a pod to test consume configMaps 11/17/23 14:19:54.229
Nov 17 14:19:54.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db" in namespace "projected-9086" to be "Succeeded or Failed"
Nov 17 14:19:54.253: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.683787ms
Nov 17 14:19:56.258: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014972004s
Nov 17 14:19:58.259: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016212356s
STEP: Saw pod success 11/17/23 14:19:58.259
Nov 17 14:19:58.259: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db" satisfied condition "Succeeded or Failed"
Nov 17 14:19:58.263: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:19:58.274
Nov 17 14:19:58.288: INFO: Waiting for pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db to disappear
Nov 17 14:19:58.291: INFO: Pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:19:58.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9086" for this suite. 11/17/23 14:19:58.297
------------------------------
â€¢ [4.130 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:54.176
    Nov 17 14:19:54.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:19:54.177
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:54.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:54.209
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-1aa4dc73-4eb4-46ac-8774-7e9efec79718 11/17/23 14:19:54.213
    STEP: Creating a pod to test consume configMaps 11/17/23 14:19:54.229
    Nov 17 14:19:54.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db" in namespace "projected-9086" to be "Succeeded or Failed"
    Nov 17 14:19:54.253: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.683787ms
    Nov 17 14:19:56.258: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014972004s
    Nov 17 14:19:58.259: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016212356s
    STEP: Saw pod success 11/17/23 14:19:58.259
    Nov 17 14:19:58.259: INFO: Pod "pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db" satisfied condition "Succeeded or Failed"
    Nov 17 14:19:58.263: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:19:58.274
    Nov 17 14:19:58.288: INFO: Waiting for pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db to disappear
    Nov 17 14:19:58.291: INFO: Pod pod-projected-configmaps-e4b800a9-7fe7-4a66-b4d5-8408093f58db no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:19:58.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9086" for this suite. 11/17/23 14:19:58.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:19:58.306
Nov 17 14:19:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:19:58.308
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:58.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:58.332
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 11/17/23 14:19:58.336
Nov 17 14:19:58.336: INFO: namespace kubectl-6649
Nov 17 14:19:58.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 create -f -'
Nov 17 14:20:02.443: INFO: stderr: ""
Nov 17 14:20:02.443: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/17/23 14:20:02.443
Nov 17 14:20:03.450: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:20:03.450: INFO: Found 0 / 1
Nov 17 14:20:04.474: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:20:04.474: INFO: Found 1 / 1
Nov 17 14:20:04.474: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 17 14:20:04.496: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:20:04.496: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 17 14:20:04.496: INFO: wait on agnhost-primary startup in kubectl-6649 
Nov 17 14:20:04.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 logs agnhost-primary-xkchk agnhost-primary'
Nov 17 14:20:04.855: INFO: stderr: ""
Nov 17 14:20:04.855: INFO: stdout: "Paused\n"
STEP: exposing RC 11/17/23 14:20:04.855
Nov 17 14:20:04.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 17 14:20:05.160: INFO: stderr: ""
Nov 17 14:20:05.160: INFO: stdout: "service/rm2 exposed\n"
Nov 17 14:20:05.220: INFO: Service rm2 in namespace kubectl-6649 found.
STEP: exposing service 11/17/23 14:20:07.226
Nov 17 14:20:07.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 17 14:20:07.341: INFO: stderr: ""
Nov 17 14:20:07.341: INFO: stdout: "service/rm3 exposed\n"
Nov 17 14:20:07.349: INFO: Service rm3 in namespace kubectl-6649 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:09.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6649" for this suite. 11/17/23 14:20:09.36
------------------------------
â€¢ [SLOW TEST] [11.059 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:19:58.306
    Nov 17 14:19:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:19:58.308
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:19:58.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:19:58.332
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 11/17/23 14:19:58.336
    Nov 17 14:19:58.336: INFO: namespace kubectl-6649
    Nov 17 14:19:58.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 create -f -'
    Nov 17 14:20:02.443: INFO: stderr: ""
    Nov 17 14:20:02.443: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/17/23 14:20:02.443
    Nov 17 14:20:03.450: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:20:03.450: INFO: Found 0 / 1
    Nov 17 14:20:04.474: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:20:04.474: INFO: Found 1 / 1
    Nov 17 14:20:04.474: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 17 14:20:04.496: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:20:04.496: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 17 14:20:04.496: INFO: wait on agnhost-primary startup in kubectl-6649 
    Nov 17 14:20:04.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 logs agnhost-primary-xkchk agnhost-primary'
    Nov 17 14:20:04.855: INFO: stderr: ""
    Nov 17 14:20:04.855: INFO: stdout: "Paused\n"
    STEP: exposing RC 11/17/23 14:20:04.855
    Nov 17 14:20:04.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Nov 17 14:20:05.160: INFO: stderr: ""
    Nov 17 14:20:05.160: INFO: stdout: "service/rm2 exposed\n"
    Nov 17 14:20:05.220: INFO: Service rm2 in namespace kubectl-6649 found.
    STEP: exposing service 11/17/23 14:20:07.226
    Nov 17 14:20:07.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6649 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Nov 17 14:20:07.341: INFO: stderr: ""
    Nov 17 14:20:07.341: INFO: stdout: "service/rm3 exposed\n"
    Nov 17 14:20:07.349: INFO: Service rm3 in namespace kubectl-6649 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:09.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6649" for this suite. 11/17/23 14:20:09.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:09.365
Nov 17 14:20:09.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 14:20:09.366
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:09.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:09.386
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 11/17/23 14:20:09.389
STEP: Wait for the Deployment to create new ReplicaSet 11/17/23 14:20:09.394
STEP: delete the deployment 11/17/23 14:20:09.907
STEP: wait for all rs to be garbage collected 11/17/23 14:20:09.913
STEP: expected 0 rs, got 1 rs 11/17/23 14:20:09.943
STEP: expected 0 pods, got 2 pods 11/17/23 14:20:09.951
STEP: Gathering metrics 11/17/23 14:20:10.46
Nov 17 14:20:10.486: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 14:20:10.491: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.098387ms
Nov 17 14:20:10.491: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 14:20:10.491: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 14:20:10.588: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:10.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2509" for this suite. 11/17/23 14:20:10.593
------------------------------
â€¢ [1.237 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:09.365
    Nov 17 14:20:09.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 14:20:09.366
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:09.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:09.386
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 11/17/23 14:20:09.389
    STEP: Wait for the Deployment to create new ReplicaSet 11/17/23 14:20:09.394
    STEP: delete the deployment 11/17/23 14:20:09.907
    STEP: wait for all rs to be garbage collected 11/17/23 14:20:09.913
    STEP: expected 0 rs, got 1 rs 11/17/23 14:20:09.943
    STEP: expected 0 pods, got 2 pods 11/17/23 14:20:09.951
    STEP: Gathering metrics 11/17/23 14:20:10.46
    Nov 17 14:20:10.486: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 14:20:10.491: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.098387ms
    Nov 17 14:20:10.491: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 14:20:10.491: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 14:20:10.588: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:10.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2509" for this suite. 11/17/23 14:20:10.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:10.606
Nov 17 14:20:10.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:20:10.607
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:10.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:10.634
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 11/17/23 14:20:10.637
Nov 17 14:20:10.648: INFO: Waiting up to 5m0s for pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159" in namespace "downward-api-5690" to be "Succeeded or Failed"
Nov 17 14:20:10.656: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321362ms
Nov 17 14:20:12.660: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011873881s
Nov 17 14:20:14.663: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014432885s
STEP: Saw pod success 11/17/23 14:20:14.663
Nov 17 14:20:14.663: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159" satisfied condition "Succeeded or Failed"
Nov 17 14:20:14.672: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:20:14.69
Nov 17 14:20:14.719: INFO: Waiting for pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 to disappear
Nov 17 14:20:14.726: INFO: Pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:14.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5690" for this suite. 11/17/23 14:20:14.735
------------------------------
â€¢ [4.145 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:10.606
    Nov 17 14:20:10.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:20:10.607
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:10.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:10.634
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 11/17/23 14:20:10.637
    Nov 17 14:20:10.648: INFO: Waiting up to 5m0s for pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159" in namespace "downward-api-5690" to be "Succeeded or Failed"
    Nov 17 14:20:10.656: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Pending", Reason="", readiness=false. Elapsed: 7.321362ms
    Nov 17 14:20:12.660: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011873881s
    Nov 17 14:20:14.663: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014432885s
    STEP: Saw pod success 11/17/23 14:20:14.663
    Nov 17 14:20:14.663: INFO: Pod "downward-api-b0157103-341f-4601-83eb-d36af15a4159" satisfied condition "Succeeded or Failed"
    Nov 17 14:20:14.672: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:20:14.69
    Nov 17 14:20:14.719: INFO: Waiting for pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 to disappear
    Nov 17 14:20:14.726: INFO: Pod downward-api-b0157103-341f-4601-83eb-d36af15a4159 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:14.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5690" for this suite. 11/17/23 14:20:14.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:14.754
Nov 17 14:20:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:20:14.755
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:14.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:14.796
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 11/17/23 14:20:14.802
Nov 17 14:20:14.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 create -f -'
Nov 17 14:20:15.442: INFO: stderr: ""
Nov 17 14:20:15.442: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 11/17/23 14:20:15.442
Nov 17 14:20:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 diff -f -'
Nov 17 14:20:16.064: INFO: rc: 1
Nov 17 14:20:16.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 delete -f -'
Nov 17 14:20:16.186: INFO: stderr: ""
Nov 17 14:20:16.186: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:16.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9461" for this suite. 11/17/23 14:20:16.194
------------------------------
â€¢ [1.449 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:14.754
    Nov 17 14:20:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:20:14.755
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:14.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:14.796
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 11/17/23 14:20:14.802
    Nov 17 14:20:14.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 create -f -'
    Nov 17 14:20:15.442: INFO: stderr: ""
    Nov 17 14:20:15.442: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 11/17/23 14:20:15.442
    Nov 17 14:20:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 diff -f -'
    Nov 17 14:20:16.064: INFO: rc: 1
    Nov 17 14:20:16.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-9461 delete -f -'
    Nov 17 14:20:16.186: INFO: stderr: ""
    Nov 17 14:20:16.186: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:16.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9461" for this suite. 11/17/23 14:20:16.194
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:16.203
Nov 17 14:20:16.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 14:20:16.206
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:16.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:16.277
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 11/17/23 14:20:16.28
STEP: Ensuring active pods == parallelism 11/17/23 14:20:16.286
STEP: Orphaning one of the Job's Pods 11/17/23 14:20:18.29
Nov 17 14:20:18.810: INFO: Successfully updated pod "adopt-release-dlr95"
STEP: Checking that the Job readopts the Pod 11/17/23 14:20:18.81
Nov 17 14:20:18.810: INFO: Waiting up to 15m0s for pod "adopt-release-dlr95" in namespace "job-8156" to be "adopted"
Nov 17 14:20:18.814: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 3.72533ms
Nov 17 14:20:20.818: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 2.007680439s
Nov 17 14:20:20.818: INFO: Pod "adopt-release-dlr95" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 11/17/23 14:20:20.818
Nov 17 14:20:21.333: INFO: Successfully updated pod "adopt-release-dlr95"
STEP: Checking that the Job releases the Pod 11/17/23 14:20:21.333
Nov 17 14:20:21.333: INFO: Waiting up to 15m0s for pod "adopt-release-dlr95" in namespace "job-8156" to be "released"
Nov 17 14:20:21.346: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 12.716612ms
Nov 17 14:20:23.350: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 2.017175717s
Nov 17 14:20:23.350: INFO: Pod "adopt-release-dlr95" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:23.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8156" for this suite. 11/17/23 14:20:23.356
------------------------------
â€¢ [SLOW TEST] [7.158 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:16.203
    Nov 17 14:20:16.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 14:20:16.206
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:16.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:16.277
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 11/17/23 14:20:16.28
    STEP: Ensuring active pods == parallelism 11/17/23 14:20:16.286
    STEP: Orphaning one of the Job's Pods 11/17/23 14:20:18.29
    Nov 17 14:20:18.810: INFO: Successfully updated pod "adopt-release-dlr95"
    STEP: Checking that the Job readopts the Pod 11/17/23 14:20:18.81
    Nov 17 14:20:18.810: INFO: Waiting up to 15m0s for pod "adopt-release-dlr95" in namespace "job-8156" to be "adopted"
    Nov 17 14:20:18.814: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 3.72533ms
    Nov 17 14:20:20.818: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 2.007680439s
    Nov 17 14:20:20.818: INFO: Pod "adopt-release-dlr95" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 11/17/23 14:20:20.818
    Nov 17 14:20:21.333: INFO: Successfully updated pod "adopt-release-dlr95"
    STEP: Checking that the Job releases the Pod 11/17/23 14:20:21.333
    Nov 17 14:20:21.333: INFO: Waiting up to 15m0s for pod "adopt-release-dlr95" in namespace "job-8156" to be "released"
    Nov 17 14:20:21.346: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 12.716612ms
    Nov 17 14:20:23.350: INFO: Pod "adopt-release-dlr95": Phase="Running", Reason="", readiness=true. Elapsed: 2.017175717s
    Nov 17 14:20:23.350: INFO: Pod "adopt-release-dlr95" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:23.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8156" for this suite. 11/17/23 14:20:23.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:23.365
Nov 17 14:20:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:20:23.366
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:23.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:23.39
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 11/17/23 14:20:23.437
Nov 17 14:20:23.449: INFO: Waiting up to 5m0s for pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42" in namespace "emptydir-8581" to be "Succeeded or Failed"
Nov 17 14:20:23.455: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Pending", Reason="", readiness=false. Elapsed: 5.251938ms
Nov 17 14:20:25.459: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263917s
Nov 17 14:20:27.460: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01026707s
STEP: Saw pod success 11/17/23 14:20:27.46
Nov 17 14:20:27.460: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42" satisfied condition "Succeeded or Failed"
Nov 17 14:20:27.464: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 container test-container: <nil>
STEP: delete the pod 11/17/23 14:20:27.485
Nov 17 14:20:27.501: INFO: Waiting for pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 to disappear
Nov 17 14:20:27.505: INFO: Pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:27.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8581" for this suite. 11/17/23 14:20:27.51
------------------------------
â€¢ [4.151 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:23.365
    Nov 17 14:20:23.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:20:23.366
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:23.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:23.39
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/17/23 14:20:23.437
    Nov 17 14:20:23.449: INFO: Waiting up to 5m0s for pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42" in namespace "emptydir-8581" to be "Succeeded or Failed"
    Nov 17 14:20:23.455: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Pending", Reason="", readiness=false. Elapsed: 5.251938ms
    Nov 17 14:20:25.459: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009263917s
    Nov 17 14:20:27.460: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01026707s
    STEP: Saw pod success 11/17/23 14:20:27.46
    Nov 17 14:20:27.460: INFO: Pod "pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42" satisfied condition "Succeeded or Failed"
    Nov 17 14:20:27.464: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:20:27.485
    Nov 17 14:20:27.501: INFO: Waiting for pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 to disappear
    Nov 17 14:20:27.505: INFO: Pod pod-a28bc503-0d5f-44aa-9ec5-c08a52b59f42 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:27.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8581" for this suite. 11/17/23 14:20:27.51
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:27.516
Nov 17 14:20:27.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 14:20:27.518
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:27.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:27.56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-9db2533d-27b0-44c5-b041-621b6f15a028 11/17/23 14:20:27.563
STEP: Creating a pod to test consume configMaps 11/17/23 14:20:27.568
Nov 17 14:20:27.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5" in namespace "configmap-2556" to be "Succeeded or Failed"
Nov 17 14:20:27.580: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694731ms
Nov 17 14:20:29.584: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008822925s
Nov 17 14:20:31.585: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009626386s
STEP: Saw pod success 11/17/23 14:20:31.585
Nov 17 14:20:31.585: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5" satisfied condition "Succeeded or Failed"
Nov 17 14:20:31.588: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:20:31.595
Nov 17 14:20:31.609: INFO: Waiting for pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 to disappear
Nov 17 14:20:31.615: INFO: Pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:31.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2556" for this suite. 11/17/23 14:20:31.62
------------------------------
â€¢ [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:27.516
    Nov 17 14:20:27.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 14:20:27.518
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:27.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:27.56
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-9db2533d-27b0-44c5-b041-621b6f15a028 11/17/23 14:20:27.563
    STEP: Creating a pod to test consume configMaps 11/17/23 14:20:27.568
    Nov 17 14:20:27.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5" in namespace "configmap-2556" to be "Succeeded or Failed"
    Nov 17 14:20:27.580: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.694731ms
    Nov 17 14:20:29.584: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008822925s
    Nov 17 14:20:31.585: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009626386s
    STEP: Saw pod success 11/17/23 14:20:31.585
    Nov 17 14:20:31.585: INFO: Pod "pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5" satisfied condition "Succeeded or Failed"
    Nov 17 14:20:31.588: INFO: Trying to get logs from node k8s-worker-3.c.operations-lab.internal pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:20:31.595
    Nov 17 14:20:31.609: INFO: Waiting for pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 to disappear
    Nov 17 14:20:31.615: INFO: Pod pod-configmaps-34d06cb4-0902-4454-8de2-37877ad1b3b5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:31.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2556" for this suite. 11/17/23 14:20:31.62
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:31.629
Nov 17 14:20:31.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 14:20:31.631
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:31.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:31.652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 11/17/23 14:20:31.656
STEP: Creating a ResourceQuota 11/17/23 14:20:36.675
STEP: Ensuring resource quota status is calculated 11/17/23 14:20:36.685
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9923" for this suite. 11/17/23 14:20:38.694
------------------------------
â€¢ [SLOW TEST] [7.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:31.629
    Nov 17 14:20:31.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 14:20:31.631
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:31.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:31.652
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 11/17/23 14:20:31.656
    STEP: Creating a ResourceQuota 11/17/23 14:20:36.675
    STEP: Ensuring resource quota status is calculated 11/17/23 14:20:36.685
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9923" for this suite. 11/17/23 14:20:38.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:38.708
Nov 17 14:20:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename events 11/17/23 14:20:38.709
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:38.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:38.736
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 11/17/23 14:20:38.739
STEP: listing events in all namespaces 11/17/23 14:20:38.75
STEP: listing events in test namespace 11/17/23 14:20:38.789
STEP: listing events with field selection filtering on source 11/17/23 14:20:38.793
STEP: listing events with field selection filtering on reportingController 11/17/23 14:20:38.796
STEP: getting the test event 11/17/23 14:20:38.803
STEP: patching the test event 11/17/23 14:20:38.806
STEP: getting the test event 11/17/23 14:20:38.821
STEP: updating the test event 11/17/23 14:20:38.825
STEP: getting the test event 11/17/23 14:20:38.833
STEP: deleting the test event 11/17/23 14:20:38.837
STEP: listing events in all namespaces 11/17/23 14:20:38.845
STEP: listing events in test namespace 11/17/23 14:20:38.862
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:20:38.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7962" for this suite. 11/17/23 14:20:38.869
------------------------------
â€¢ [0.167 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:38.708
    Nov 17 14:20:38.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename events 11/17/23 14:20:38.709
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:38.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:38.736
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 11/17/23 14:20:38.739
    STEP: listing events in all namespaces 11/17/23 14:20:38.75
    STEP: listing events in test namespace 11/17/23 14:20:38.789
    STEP: listing events with field selection filtering on source 11/17/23 14:20:38.793
    STEP: listing events with field selection filtering on reportingController 11/17/23 14:20:38.796
    STEP: getting the test event 11/17/23 14:20:38.803
    STEP: patching the test event 11/17/23 14:20:38.806
    STEP: getting the test event 11/17/23 14:20:38.821
    STEP: updating the test event 11/17/23 14:20:38.825
    STEP: getting the test event 11/17/23 14:20:38.833
    STEP: deleting the test event 11/17/23 14:20:38.837
    STEP: listing events in all namespaces 11/17/23 14:20:38.845
    STEP: listing events in test namespace 11/17/23 14:20:38.862
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:20:38.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7962" for this suite. 11/17/23 14:20:38.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:20:38.876
Nov 17 14:20:38.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 14:20:38.877
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:38.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:38.895
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-krg2c" 11/17/23 14:20:38.9
Nov 17 14:20:38.908: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard cpu limit of 500m
Nov 17 14:20:38.908: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.908
STEP: Confirm /status for "e2e-rq-status-krg2c" resourceQuota via watch 11/17/23 14:20:38.916
Nov 17 14:20:38.918: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList(nil)
Nov 17 14:20:38.918: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 17 14:20:38.918: INFO: ResourceQuota "e2e-rq-status-krg2c" /status was updated
STEP: Patching hard spec values for cpu & memory 11/17/23 14:20:38.921
Nov 17 14:20:38.929: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard cpu limit of 1
Nov 17 14:20:38.929: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.929
STEP: Confirm /status for "e2e-rq-status-krg2c" resourceQuota via watch 11/17/23 14:20:38.936
Nov 17 14:20:38.938: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Nov 17 14:20:38.938: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Nov 17 14:20:38.938: INFO: ResourceQuota "e2e-rq-status-krg2c" /status was patched
STEP: Get "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.938
Nov 17 14:20:38.941: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard cpu of 1
Nov 17 14:20:38.941: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-krg2c" /status before checking Spec is unchanged 11/17/23 14:20:38.944
Nov 17 14:20:38.957: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard cpu of 2
Nov 17 14:20:38.957: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard memory of 2Gi
Nov 17 14:20:38.958: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Nov 17 14:20:38.959: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Nov 17 14:25:23.970: INFO: ResourceQuota "e2e-rq-status-krg2c" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 14:25:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8962" for this suite. 11/17/23 14:25:23.975
------------------------------
â€¢ [SLOW TEST] [285.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:20:38.876
    Nov 17 14:20:38.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 14:20:38.877
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:20:38.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:20:38.895
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-krg2c" 11/17/23 14:20:38.9
    Nov 17 14:20:38.908: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard cpu limit of 500m
    Nov 17 14:20:38.908: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.908
    STEP: Confirm /status for "e2e-rq-status-krg2c" resourceQuota via watch 11/17/23 14:20:38.916
    Nov 17 14:20:38.918: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList(nil)
    Nov 17 14:20:38.918: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 17 14:20:38.918: INFO: ResourceQuota "e2e-rq-status-krg2c" /status was updated
    STEP: Patching hard spec values for cpu & memory 11/17/23 14:20:38.921
    Nov 17 14:20:38.929: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard cpu limit of 1
    Nov 17 14:20:38.929: INFO: Resource quota "e2e-rq-status-krg2c" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.929
    STEP: Confirm /status for "e2e-rq-status-krg2c" resourceQuota via watch 11/17/23 14:20:38.936
    Nov 17 14:20:38.938: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Nov 17 14:20:38.938: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Nov 17 14:20:38.938: INFO: ResourceQuota "e2e-rq-status-krg2c" /status was patched
    STEP: Get "e2e-rq-status-krg2c" /status 11/17/23 14:20:38.938
    Nov 17 14:20:38.941: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard cpu of 1
    Nov 17 14:20:38.941: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-krg2c" /status before checking Spec is unchanged 11/17/23 14:20:38.944
    Nov 17 14:20:38.957: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard cpu of 2
    Nov 17 14:20:38.957: INFO: Resourcequota "e2e-rq-status-krg2c" reports status: hard memory of 2Gi
    Nov 17 14:20:38.958: INFO: observed resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Nov 17 14:20:38.959: INFO: Found resourceQuota "e2e-rq-status-krg2c" in namespace "resourcequota-8962" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Nov 17 14:25:23.970: INFO: ResourceQuota "e2e-rq-status-krg2c" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:25:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8962" for this suite. 11/17/23 14:25:23.975
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:25:23.983
Nov 17 14:25:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:25:23.985
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:24.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:24.013
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 11/17/23 14:25:24.016
Nov 17 14:25:24.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-442 cluster-info'
Nov 17 14:25:24.120: INFO: stderr: ""
Nov 17 14:25:24.120: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:25:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-442" for this suite. 11/17/23 14:25:24.125
------------------------------
â€¢ [0.150 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:25:23.983
    Nov 17 14:25:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:25:23.985
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:24.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:24.013
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 11/17/23 14:25:24.016
    Nov 17 14:25:24.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-442 cluster-info'
    Nov 17 14:25:24.120: INFO: stderr: ""
    Nov 17 14:25:24.120: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:25:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-442" for this suite. 11/17/23 14:25:24.125
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:25:24.133
Nov 17 14:25:24.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename controllerrevisions 11/17/23 14:25:24.135
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:24.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:24.161
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-z45dg-daemon-set" 11/17/23 14:25:24.187
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:25:24.196
Nov 17 14:25:24.208: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:25:24.216: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
Nov 17 14:25:24.216: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:25:25.221: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:25:25.226: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
Nov 17 14:25:25.226: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:25:26.222: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:25:26.228: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 3
Nov 17 14:25:26.228: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-z45dg-daemon-set
STEP: Confirm DaemonSet "e2e-z45dg-daemon-set" successfully created with "daemonset-name=e2e-z45dg-daemon-set" label 11/17/23 14:25:26.232
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z45dg-daemon-set" 11/17/23 14:25:26.244
Nov 17 14:25:26.248: INFO: Located ControllerRevision: "e2e-z45dg-daemon-set-5b8db6f8fc"
STEP: Patching ControllerRevision "e2e-z45dg-daemon-set-5b8db6f8fc" 11/17/23 14:25:26.252
Nov 17 14:25:26.261: INFO: e2e-z45dg-daemon-set-5b8db6f8fc has been patched
STEP: Create a new ControllerRevision 11/17/23 14:25:26.261
Nov 17 14:25:26.266: INFO: Created ControllerRevision: e2e-z45dg-daemon-set-56dcf7c877
STEP: Confirm that there are two ControllerRevisions 11/17/23 14:25:26.266
Nov 17 14:25:26.266: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 17 14:25:26.270: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-z45dg-daemon-set-5b8db6f8fc" 11/17/23 14:25:26.27
STEP: Confirm that there is only one ControllerRevision 11/17/23 14:25:26.278
Nov 17 14:25:26.278: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 17 14:25:26.282: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-z45dg-daemon-set-56dcf7c877" 11/17/23 14:25:26.285
Nov 17 14:25:26.297: INFO: e2e-z45dg-daemon-set-56dcf7c877 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 11/17/23 14:25:26.297
W1117 14:25:26.312884      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 11/17/23 14:25:26.313
Nov 17 14:25:26.313: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 17 14:25:27.317: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 17 14:25:27.322: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z45dg-daemon-set-56dcf7c877=updated" 11/17/23 14:25:27.322
STEP: Confirm that there is only one ControllerRevision 11/17/23 14:25:27.331
Nov 17 14:25:27.331: INFO: Requesting list of ControllerRevisions to confirm quantity
Nov 17 14:25:27.334: INFO: Found 1 ControllerRevisions
Nov 17 14:25:27.337: INFO: ControllerRevision "e2e-z45dg-daemon-set-5f67759db8" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-z45dg-daemon-set" 11/17/23 14:25:27.34
STEP: deleting DaemonSet.extensions e2e-z45dg-daemon-set in namespace controllerrevisions-7067, will wait for the garbage collector to delete the pods 11/17/23 14:25:27.341
Nov 17 14:25:27.402: INFO: Deleting DaemonSet.extensions e2e-z45dg-daemon-set took: 7.46588ms
Nov 17 14:25:27.503: INFO: Terminating DaemonSet.extensions e2e-z45dg-daemon-set pods took: 101.261479ms
Nov 17 14:25:29.207: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
Nov 17 14:25:29.207: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z45dg-daemon-set
Nov 17 14:25:29.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39998"},"items":null}

Nov 17 14:25:29.213: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39998"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:25:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7067" for this suite. 11/17/23 14:25:29.234
------------------------------
â€¢ [SLOW TEST] [5.121 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:25:24.133
    Nov 17 14:25:24.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename controllerrevisions 11/17/23 14:25:24.135
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:24.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:24.161
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-z45dg-daemon-set" 11/17/23 14:25:24.187
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:25:24.196
    Nov 17 14:25:24.208: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:25:24.216: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
    Nov 17 14:25:24.216: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:25:25.221: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:25:25.226: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
    Nov 17 14:25:25.226: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:25:26.222: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:25:26.228: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 3
    Nov 17 14:25:26.228: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-z45dg-daemon-set
    STEP: Confirm DaemonSet "e2e-z45dg-daemon-set" successfully created with "daemonset-name=e2e-z45dg-daemon-set" label 11/17/23 14:25:26.232
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z45dg-daemon-set" 11/17/23 14:25:26.244
    Nov 17 14:25:26.248: INFO: Located ControllerRevision: "e2e-z45dg-daemon-set-5b8db6f8fc"
    STEP: Patching ControllerRevision "e2e-z45dg-daemon-set-5b8db6f8fc" 11/17/23 14:25:26.252
    Nov 17 14:25:26.261: INFO: e2e-z45dg-daemon-set-5b8db6f8fc has been patched
    STEP: Create a new ControllerRevision 11/17/23 14:25:26.261
    Nov 17 14:25:26.266: INFO: Created ControllerRevision: e2e-z45dg-daemon-set-56dcf7c877
    STEP: Confirm that there are two ControllerRevisions 11/17/23 14:25:26.266
    Nov 17 14:25:26.266: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 17 14:25:26.270: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-z45dg-daemon-set-5b8db6f8fc" 11/17/23 14:25:26.27
    STEP: Confirm that there is only one ControllerRevision 11/17/23 14:25:26.278
    Nov 17 14:25:26.278: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 17 14:25:26.282: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-z45dg-daemon-set-56dcf7c877" 11/17/23 14:25:26.285
    Nov 17 14:25:26.297: INFO: e2e-z45dg-daemon-set-56dcf7c877 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 11/17/23 14:25:26.297
    W1117 14:25:26.312884      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 11/17/23 14:25:26.313
    Nov 17 14:25:26.313: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 17 14:25:27.317: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 17 14:25:27.322: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z45dg-daemon-set-56dcf7c877=updated" 11/17/23 14:25:27.322
    STEP: Confirm that there is only one ControllerRevision 11/17/23 14:25:27.331
    Nov 17 14:25:27.331: INFO: Requesting list of ControllerRevisions to confirm quantity
    Nov 17 14:25:27.334: INFO: Found 1 ControllerRevisions
    Nov 17 14:25:27.337: INFO: ControllerRevision "e2e-z45dg-daemon-set-5f67759db8" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-z45dg-daemon-set" 11/17/23 14:25:27.34
    STEP: deleting DaemonSet.extensions e2e-z45dg-daemon-set in namespace controllerrevisions-7067, will wait for the garbage collector to delete the pods 11/17/23 14:25:27.341
    Nov 17 14:25:27.402: INFO: Deleting DaemonSet.extensions e2e-z45dg-daemon-set took: 7.46588ms
    Nov 17 14:25:27.503: INFO: Terminating DaemonSet.extensions e2e-z45dg-daemon-set pods took: 101.261479ms
    Nov 17 14:25:29.207: INFO: Number of nodes with available pods controlled by daemonset e2e-z45dg-daemon-set: 0
    Nov 17 14:25:29.207: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z45dg-daemon-set
    Nov 17 14:25:29.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39998"},"items":null}

    Nov 17 14:25:29.213: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39998"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:25:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7067" for this suite. 11/17/23 14:25:29.234
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:25:29.254
Nov 17 14:25:29.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename containers 11/17/23 14:25:29.255
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:29.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:29.285
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 11/17/23 14:25:29.289
Nov 17 14:25:29.299: INFO: Waiting up to 5m0s for pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9" in namespace "containers-9737" to be "Succeeded or Failed"
Nov 17 14:25:29.309: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.108575ms
Nov 17 14:25:31.315: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015999816s
Nov 17 14:25:33.316: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017204264s
STEP: Saw pod success 11/17/23 14:25:33.316
Nov 17 14:25:33.316: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9" satisfied condition "Succeeded or Failed"
Nov 17 14:25:33.320: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:25:33.341
Nov 17 14:25:33.359: INFO: Waiting for pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 to disappear
Nov 17 14:25:33.363: INFO: Pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 17 14:25:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9737" for this suite. 11/17/23 14:25:33.367
------------------------------
â€¢ [4.120 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:25:29.254
    Nov 17 14:25:29.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename containers 11/17/23 14:25:29.255
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:29.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:29.285
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 11/17/23 14:25:29.289
    Nov 17 14:25:29.299: INFO: Waiting up to 5m0s for pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9" in namespace "containers-9737" to be "Succeeded or Failed"
    Nov 17 14:25:29.309: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.108575ms
    Nov 17 14:25:31.315: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015999816s
    Nov 17 14:25:33.316: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017204264s
    STEP: Saw pod success 11/17/23 14:25:33.316
    Nov 17 14:25:33.316: INFO: Pod "client-containers-34dab7b6-022e-464b-b405-b0add223f0d9" satisfied condition "Succeeded or Failed"
    Nov 17 14:25:33.320: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:25:33.341
    Nov 17 14:25:33.359: INFO: Waiting for pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 to disappear
    Nov 17 14:25:33.363: INFO: Pod client-containers-34dab7b6-022e-464b-b405-b0add223f0d9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:25:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9737" for this suite. 11/17/23 14:25:33.367
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:25:33.375
Nov 17 14:25:33.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 14:25:33.376
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:33.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:33.396
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4106 11/17/23 14:25:33.401
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4106 11/17/23 14:25:33.409
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4106 11/17/23 14:25:33.416
Nov 17 14:25:33.423: INFO: Found 0 stateful pods, waiting for 1
Nov 17 14:25:43.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/17/23 14:25:43.427
Nov 17 14:25:43.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:25:43.648: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:25:43.648: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:25:43.648: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 14:25:43.652: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 17 14:25:53.659: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 14:25:53.659: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 14:25:53.675: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Nov 17 14:25:53.676: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
Nov 17 14:25:53.676: INFO: 
Nov 17 14:25:53.676: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 17 14:25:54.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989882023s
Nov 17 14:25:55.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985210078s
Nov 17 14:25:56.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981417094s
Nov 17 14:25:57.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977311095s
Nov 17 14:25:58.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972532005s
Nov 17 14:25:59.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966955043s
Nov 17 14:26:00.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960989649s
Nov 17 14:26:01.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956750997s
Nov 17 14:26:02.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.739644ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4106 11/17/23 14:26:03.721
Nov 17 14:26:03.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 14:26:03.930: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 14:26:03.930: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 14:26:03.930: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 14:26:03.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 14:26:04.140: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 17 14:26:04.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 14:26:04.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 14:26:04.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 14:26:04.372: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 17 14:26:04.372: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 14:26:04.372: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 17 14:26:04.376: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 17 14:26:14.381: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:26:14.381: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:26:14.381: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 11/17/23 14:26:14.381
Nov 17 14:26:14.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:26:14.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:26:14.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:26:14.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 14:26:14.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:26:14.789: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:26:14.789: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:26:14.789: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 14:26:14.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:26:14.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:26:14.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:26:14.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 14:26:14.995: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 14:26:15.000: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 17 14:26:25.010: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 14:26:25.010: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 14:26:25.010: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 17 14:26:25.026: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Nov 17 14:26:25.026: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
Nov 17 14:26:25.026: INFO: ss-1  k8s-worker-3.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
Nov 17 14:26:25.026: INFO: ss-2  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
Nov 17 14:26:25.026: INFO: 
Nov 17 14:26:25.026: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 17 14:26:26.030: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Nov 17 14:26:26.030: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
Nov 17 14:26:26.030: INFO: ss-1  k8s-worker-3.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
Nov 17 14:26:26.030: INFO: ss-2  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
Nov 17 14:26:26.030: INFO: 
Nov 17 14:26:26.030: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 17 14:26:27.034: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989543219s
Nov 17 14:26:28.038: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98533759s
Nov 17 14:26:29.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981614056s
Nov 17 14:26:30.045: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97761621s
Nov 17 14:26:31.050: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974159511s
Nov 17 14:26:32.053: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969849905s
Nov 17 14:26:33.057: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966423077s
Nov 17 14:26:34.061: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.991593ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4106 11/17/23 14:26:35.061
Nov 17 14:26:35.065: INFO: Scaling statefulset ss to 0
Nov 17 14:26:35.074: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 14:26:35.076: INFO: Deleting all statefulset in ns statefulset-4106
Nov 17 14:26:35.078: INFO: Scaling statefulset ss to 0
Nov 17 14:26:35.086: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 14:26:35.088: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:26:35.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4106" for this suite. 11/17/23 14:26:35.109
------------------------------
â€¢ [SLOW TEST] [61.750 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:25:33.375
    Nov 17 14:25:33.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 14:25:33.376
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:25:33.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:25:33.396
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4106 11/17/23 14:25:33.401
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4106 11/17/23 14:25:33.409
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4106 11/17/23 14:25:33.416
    Nov 17 14:25:33.423: INFO: Found 0 stateful pods, waiting for 1
    Nov 17 14:25:43.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 11/17/23 14:25:43.427
    Nov 17 14:25:43.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:25:43.648: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:25:43.648: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:25:43.648: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 14:25:43.652: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Nov 17 14:25:53.659: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 14:25:53.659: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 14:25:53.675: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Nov 17 14:25:53.676: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
    Nov 17 14:25:53.676: INFO: 
    Nov 17 14:25:53.676: INFO: StatefulSet ss has not reached scale 3, at 1
    Nov 17 14:25:54.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989882023s
    Nov 17 14:25:55.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985210078s
    Nov 17 14:25:56.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981417094s
    Nov 17 14:25:57.696: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977311095s
    Nov 17 14:25:58.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972532005s
    Nov 17 14:25:59.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966955043s
    Nov 17 14:26:00.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960989649s
    Nov 17 14:26:01.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956750997s
    Nov 17 14:26:02.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.739644ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4106 11/17/23 14:26:03.721
    Nov 17 14:26:03.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 14:26:03.930: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 14:26:03.930: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 14:26:03.930: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 14:26:03.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 14:26:04.140: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 17 14:26:04.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 14:26:04.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 14:26:04.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 14:26:04.372: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Nov 17 14:26:04.372: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 14:26:04.372: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Nov 17 14:26:04.376: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Nov 17 14:26:14.381: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:26:14.381: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:26:14.381: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 11/17/23 14:26:14.381
    Nov 17 14:26:14.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:26:14.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:26:14.582: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:26:14.582: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 14:26:14.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:26:14.789: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:26:14.789: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:26:14.789: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 14:26:14.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-4106 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:26:14.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:26:14.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:26:14.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 14:26:14.995: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 14:26:15.000: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Nov 17 14:26:25.010: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 14:26:25.010: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 14:26:25.010: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Nov 17 14:26:25.026: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Nov 17 14:26:25.026: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
    Nov 17 14:26:25.026: INFO: ss-1  k8s-worker-3.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
    Nov 17 14:26:25.026: INFO: ss-2  k8s-worker-1.c.operations-lab.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
    Nov 17 14:26:25.026: INFO: 
    Nov 17 14:26:25.026: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 17 14:26:26.030: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Nov 17 14:26:26.030: INFO: ss-0  k8s-worker-2.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:33 +0000 UTC  }]
    Nov 17 14:26:26.030: INFO: ss-1  k8s-worker-3.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
    Nov 17 14:26:26.030: INFO: ss-2  k8s-worker-1.c.operations-lab.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:26:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:25:53 +0000 UTC  }]
    Nov 17 14:26:26.030: INFO: 
    Nov 17 14:26:26.030: INFO: StatefulSet ss has not reached scale 0, at 3
    Nov 17 14:26:27.034: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989543219s
    Nov 17 14:26:28.038: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.98533759s
    Nov 17 14:26:29.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981614056s
    Nov 17 14:26:30.045: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97761621s
    Nov 17 14:26:31.050: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974159511s
    Nov 17 14:26:32.053: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969849905s
    Nov 17 14:26:33.057: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966423077s
    Nov 17 14:26:34.061: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.991593ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4106 11/17/23 14:26:35.061
    Nov 17 14:26:35.065: INFO: Scaling statefulset ss to 0
    Nov 17 14:26:35.074: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 14:26:35.076: INFO: Deleting all statefulset in ns statefulset-4106
    Nov 17 14:26:35.078: INFO: Scaling statefulset ss to 0
    Nov 17 14:26:35.086: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 14:26:35.088: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:26:35.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4106" for this suite. 11/17/23 14:26:35.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:26:35.126
Nov 17 14:26:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename init-container 11/17/23 14:26:35.127
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:35.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:35.151
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 11/17/23 14:26:35.154
Nov 17 14:26:35.154: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:26:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7419" for this suite. 11/17/23 14:26:39.36
------------------------------
â€¢ [4.241 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:26:35.126
    Nov 17 14:26:35.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename init-container 11/17/23 14:26:35.127
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:35.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:35.151
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 11/17/23 14:26:35.154
    Nov 17 14:26:35.154: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:26:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7419" for this suite. 11/17/23 14:26:39.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:26:39.368
Nov 17 14:26:39.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:26:39.369
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:39.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:39.393
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:26:39.396
Nov 17 14:26:39.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46" in namespace "downward-api-9677" to be "Succeeded or Failed"
Nov 17 14:26:39.416: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Pending", Reason="", readiness=false. Elapsed: 7.558872ms
Nov 17 14:26:41.422: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013412992s
Nov 17 14:26:43.423: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013902231s
STEP: Saw pod success 11/17/23 14:26:43.423
Nov 17 14:26:43.423: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46" satisfied condition "Succeeded or Failed"
Nov 17 14:26:43.443: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 container client-container: <nil>
STEP: delete the pod 11/17/23 14:26:43.451
Nov 17 14:26:43.464: INFO: Waiting for pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 to disappear
Nov 17 14:26:43.467: INFO: Pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:26:43.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9677" for this suite. 11/17/23 14:26:43.471
------------------------------
â€¢ [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:26:39.368
    Nov 17 14:26:39.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:26:39.369
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:39.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:39.393
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:26:39.396
    Nov 17 14:26:39.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46" in namespace "downward-api-9677" to be "Succeeded or Failed"
    Nov 17 14:26:39.416: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Pending", Reason="", readiness=false. Elapsed: 7.558872ms
    Nov 17 14:26:41.422: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013412992s
    Nov 17 14:26:43.423: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013902231s
    STEP: Saw pod success 11/17/23 14:26:43.423
    Nov 17 14:26:43.423: INFO: Pod "downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46" satisfied condition "Succeeded or Failed"
    Nov 17 14:26:43.443: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:26:43.451
    Nov 17 14:26:43.464: INFO: Waiting for pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 to disappear
    Nov 17 14:26:43.467: INFO: Pod downwardapi-volume-530f45d3-3b03-40fa-92ba-55f01e2e4e46 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:26:43.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9677" for this suite. 11/17/23 14:26:43.471
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:26:43.479
Nov 17 14:26:43.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:26:43.481
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:43.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:43.497
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 in namespace container-probe-1494 11/17/23 14:26:43.501
Nov 17 14:26:43.514: INFO: Waiting up to 5m0s for pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9" in namespace "container-probe-1494" to be "not pending"
Nov 17 14:26:43.518: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372919ms
Nov 17 14:26:45.522: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007839359s
Nov 17 14:26:45.522: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9" satisfied condition "not pending"
Nov 17 14:26:45.522: INFO: Started pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 in namespace container-probe-1494
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:26:45.522
Nov 17 14:26:45.526: INFO: Initial restart count of pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 is 0
STEP: deleting the pod 11/17/23 14:30:46.183
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:30:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1494" for this suite. 11/17/23 14:30:46.208
------------------------------
â€¢ [SLOW TEST] [242.767 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:26:43.479
    Nov 17 14:26:43.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:26:43.481
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:26:43.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:26:43.497
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 in namespace container-probe-1494 11/17/23 14:26:43.501
    Nov 17 14:26:43.514: INFO: Waiting up to 5m0s for pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9" in namespace "container-probe-1494" to be "not pending"
    Nov 17 14:26:43.518: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372919ms
    Nov 17 14:26:45.522: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007839359s
    Nov 17 14:26:45.522: INFO: Pod "busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9" satisfied condition "not pending"
    Nov 17 14:26:45.522: INFO: Started pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 in namespace container-probe-1494
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:26:45.522
    Nov 17 14:26:45.526: INFO: Initial restart count of pod busybox-c69f0c38-886e-4d5e-aa44-57e51db30cc9 is 0
    STEP: deleting the pod 11/17/23 14:30:46.183
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:30:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1494" for this suite. 11/17/23 14:30:46.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:30:46.251
Nov 17 14:30:46.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename subpath 11/17/23 14:30:46.253
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:30:46.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:30:46.286
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/17/23 14:30:46.292
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-zdv8 11/17/23 14:30:46.306
STEP: Creating a pod to test atomic-volume-subpath 11/17/23 14:30:46.307
Nov 17 14:30:46.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zdv8" in namespace "subpath-5157" to be "Succeeded or Failed"
Nov 17 14:30:46.344: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665897ms
Nov 17 14:30:48.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012473864s
Nov 17 14:30:50.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 4.012229015s
Nov 17 14:30:52.350: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 6.012750242s
Nov 17 14:30:54.347: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 8.010220762s
Nov 17 14:30:56.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 10.011027095s
Nov 17 14:30:58.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 12.011825126s
Nov 17 14:31:00.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 14.011557755s
Nov 17 14:31:02.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 16.011569038s
Nov 17 14:31:04.350: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 18.013100937s
Nov 17 14:31:06.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 20.010922453s
Nov 17 14:31:08.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=false. Elapsed: 22.010973003s
Nov 17 14:31:10.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011451702s
STEP: Saw pod success 11/17/23 14:31:10.349
Nov 17 14:31:10.349: INFO: Pod "pod-subpath-test-projected-zdv8" satisfied condition "Succeeded or Failed"
Nov 17 14:31:10.353: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-projected-zdv8 container test-container-subpath-projected-zdv8: <nil>
STEP: delete the pod 11/17/23 14:31:10.375
Nov 17 14:31:10.389: INFO: Waiting for pod pod-subpath-test-projected-zdv8 to disappear
Nov 17 14:31:10.394: INFO: Pod pod-subpath-test-projected-zdv8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-zdv8 11/17/23 14:31:10.394
Nov 17 14:31:10.394: INFO: Deleting pod "pod-subpath-test-projected-zdv8" in namespace "subpath-5157"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 17 14:31:10.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5157" for this suite. 11/17/23 14:31:10.402
------------------------------
â€¢ [SLOW TEST] [24.160 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:30:46.251
    Nov 17 14:30:46.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename subpath 11/17/23 14:30:46.253
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:30:46.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:30:46.286
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/17/23 14:30:46.292
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-zdv8 11/17/23 14:30:46.306
    STEP: Creating a pod to test atomic-volume-subpath 11/17/23 14:30:46.307
    Nov 17 14:30:46.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zdv8" in namespace "subpath-5157" to be "Succeeded or Failed"
    Nov 17 14:30:46.344: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665897ms
    Nov 17 14:30:48.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012473864s
    Nov 17 14:30:50.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 4.012229015s
    Nov 17 14:30:52.350: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 6.012750242s
    Nov 17 14:30:54.347: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 8.010220762s
    Nov 17 14:30:56.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 10.011027095s
    Nov 17 14:30:58.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 12.011825126s
    Nov 17 14:31:00.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 14.011557755s
    Nov 17 14:31:02.349: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 16.011569038s
    Nov 17 14:31:04.350: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 18.013100937s
    Nov 17 14:31:06.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=true. Elapsed: 20.010922453s
    Nov 17 14:31:08.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Running", Reason="", readiness=false. Elapsed: 22.010973003s
    Nov 17 14:31:10.348: INFO: Pod "pod-subpath-test-projected-zdv8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011451702s
    STEP: Saw pod success 11/17/23 14:31:10.349
    Nov 17 14:31:10.349: INFO: Pod "pod-subpath-test-projected-zdv8" satisfied condition "Succeeded or Failed"
    Nov 17 14:31:10.353: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-projected-zdv8 container test-container-subpath-projected-zdv8: <nil>
    STEP: delete the pod 11/17/23 14:31:10.375
    Nov 17 14:31:10.389: INFO: Waiting for pod pod-subpath-test-projected-zdv8 to disappear
    Nov 17 14:31:10.394: INFO: Pod pod-subpath-test-projected-zdv8 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-zdv8 11/17/23 14:31:10.394
    Nov 17 14:31:10.394: INFO: Deleting pod "pod-subpath-test-projected-zdv8" in namespace "subpath-5157"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:31:10.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5157" for this suite. 11/17/23 14:31:10.402
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:31:10.412
Nov 17 14:31:10.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 14:31:10.414
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:10.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:10.439
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 11/17/23 14:31:10.443
Nov 17 14:31:10.453: INFO: Waiting up to 5m0s for pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3" in namespace "var-expansion-5311" to be "Succeeded or Failed"
Nov 17 14:31:10.460: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668709ms
Nov 17 14:31:12.465: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011652377s
Nov 17 14:31:14.465: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011922344s
STEP: Saw pod success 11/17/23 14:31:14.466
Nov 17 14:31:14.466: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3" satisfied condition "Succeeded or Failed"
Nov 17 14:31:14.470: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:31:14.476
Nov 17 14:31:14.493: INFO: Waiting for pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 to disappear
Nov 17 14:31:14.497: INFO: Pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 14:31:14.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5311" for this suite. 11/17/23 14:31:14.503
------------------------------
â€¢ [4.098 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:31:10.412
    Nov 17 14:31:10.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 14:31:10.414
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:10.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:10.439
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 11/17/23 14:31:10.443
    Nov 17 14:31:10.453: INFO: Waiting up to 5m0s for pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3" in namespace "var-expansion-5311" to be "Succeeded or Failed"
    Nov 17 14:31:10.460: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668709ms
    Nov 17 14:31:12.465: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011652377s
    Nov 17 14:31:14.465: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011922344s
    STEP: Saw pod success 11/17/23 14:31:14.466
    Nov 17 14:31:14.466: INFO: Pod "var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3" satisfied condition "Succeeded or Failed"
    Nov 17 14:31:14.470: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:31:14.476
    Nov 17 14:31:14.493: INFO: Waiting for pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 to disappear
    Nov 17 14:31:14.497: INFO: Pod var-expansion-81bc95e0-8ac9-4f52-946a-325625775fd3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:31:14.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5311" for this suite. 11/17/23 14:31:14.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:31:14.519
Nov 17 14:31:14.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 14:31:14.521
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:14.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:14.545
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 11/17/23 14:31:14.554
STEP: waiting for Deployment to be created 11/17/23 14:31:14.56
STEP: waiting for all Replicas to be Ready 11/17/23 14:31:14.562
Nov 17 14:31:14.564: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.564: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.590: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.590: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.621: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.621: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.701: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:14.701: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 17 14:31:15.935: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 17 14:31:15.935: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 17 14:31:16.218: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 11/17/23 14:31:16.218
W1117 14:31:16.230259      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 17 14:31:16.237: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 11/17/23 14:31:16.237
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.252: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.253: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.278: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.278: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:16.311: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:16.312: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:16.327: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:16.327: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:17.239: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:17.239: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:17.280: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
STEP: listing Deployments 11/17/23 14:31:17.28
Nov 17 14:31:17.286: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 11/17/23 14:31:17.286
Nov 17 14:31:17.301: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 11/17/23 14:31:17.301
Nov 17 14:31:17.311: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:17.320: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:17.364: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:17.390: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:17.400: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:18.962: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:19.297: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:19.373: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:19.384: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 17 14:31:20.969: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 11/17/23 14:31:21.02
STEP: fetching the DeploymentStatus 11/17/23 14:31:21.029
Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3
Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3
STEP: deleting the Deployment 11/17/23 14:31:21.037
Nov 17 14:31:21.047: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.048: INFO: observed event type MODIFIED
Nov 17 14:31:21.049: INFO: observed event type MODIFIED
Nov 17 14:31:21.049: INFO: observed event type MODIFIED
Nov 17 14:31:21.049: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 14:31:21.056: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 17 14:31:21.086: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3745  de40e1f7-3022-41ba-ad8c-e94e74fb922c 42465 2 2023-11-17 14:31:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4877 0xc000ed4878}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Nov 17 14:31:21.095: INFO: pod: "test-deployment-7b7876f9d6-6k6rd":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-6k6rd test-deployment-7b7876f9d6- deployment-3745  e1cde30d-43ad-408a-b62e-945cf6540e68 42464 0 2023-11-17 14:31:19 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de40e1f7-3022-41ba-ad8c-e94e74fb922c 0xc0037863e7 0xc0037863e8}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de40e1f7-3022-41ba-ad8c-e94e74fb922c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbnpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbnpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.83,StartTime:2023-11-17 14:31:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6328e4a8a031004e2214aa36b79ca6e094c07e602cdc7fe26f374ba4f9227111,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 17 14:31:21.095: INFO: pod: "test-deployment-7b7876f9d6-sl7h7":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-sl7h7 test-deployment-7b7876f9d6- deployment-3745  c55503aa-4456-4dd2-904d-587855306c95 42416 0 2023-11-17 14:31:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de40e1f7-3022-41ba-ad8c-e94e74fb922c 0xc0037865c7 0xc0037865c8}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de40e1f7-3022-41ba-ad8c-e94e74fb922c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pj78t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pj78t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.68,StartTime:2023-11-17 14:31:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65d0a4525cccad0b9fd0fd32a5b36603025d5568bbc93d7dd5c47543361c9fab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 17 14:31:21.095: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3745  55ded911-6940-4695-8182-6b914f57b782 42473 4 2023-11-17 14:31:16 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4967 0xc000ed4968}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4ba0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 17 14:31:21.105: INFO: pod: "test-deployment-7df74c55ff-bfzzj":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-bfzzj test-deployment-7df74c55ff- deployment-3745  540ae857-f3db-4029-8437-a79f630720ba 42469 0 2023-11-17 14:31:17 +0000 UTC 2023-11-17 14:31:21 +0000 UTC 0xc000ed53d8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 55ded911-6940-4695-8182-6b914f57b782 0xc000ed5467 0xc000ed5468}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55ded911-6940-4695-8182-6b914f57b782\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xlb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xlb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.154,StartTime:2023-11-17 14:31:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://a9e8604374994a81736d44cdd634c266e420871d4ea93a04b444967b8262d826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 17 14:31:21.105: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3745  4f504431-f3d6-41fd-8bd7-c4fa5a39c27a 42359 3 2023-11-17 14:31:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4c67 0xc000ed4c68}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4d90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 14:31:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3745" for this suite. 11/17/23 14:31:21.135
------------------------------
â€¢ [SLOW TEST] [6.631 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:31:14.519
    Nov 17 14:31:14.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 14:31:14.521
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:14.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:14.545
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 11/17/23 14:31:14.554
    STEP: waiting for Deployment to be created 11/17/23 14:31:14.56
    STEP: waiting for all Replicas to be Ready 11/17/23 14:31:14.562
    Nov 17 14:31:14.564: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.564: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.590: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.590: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.621: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.621: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.701: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:14.701: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Nov 17 14:31:15.935: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 17 14:31:15.935: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Nov 17 14:31:16.218: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 11/17/23 14:31:16.218
    W1117 14:31:16.230259      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 17 14:31:16.237: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 11/17/23 14:31:16.237
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 0
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.240: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.252: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.253: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.278: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.278: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:16.311: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:16.312: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:16.327: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:16.327: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:17.239: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:17.239: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:17.280: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    STEP: listing Deployments 11/17/23 14:31:17.28
    Nov 17 14:31:17.286: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 11/17/23 14:31:17.286
    Nov 17 14:31:17.301: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 11/17/23 14:31:17.301
    Nov 17 14:31:17.311: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:17.320: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:17.364: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:17.390: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:17.400: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:18.962: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:19.297: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:19.373: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:19.384: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Nov 17 14:31:20.969: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 11/17/23 14:31:21.02
    STEP: fetching the DeploymentStatus 11/17/23 14:31:21.029
    Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:21.036: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 1
    Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3
    Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 2
    Nov 17 14:31:21.037: INFO: observed Deployment test-deployment in namespace deployment-3745 with ReadyReplicas 3
    STEP: deleting the Deployment 11/17/23 14:31:21.037
    Nov 17 14:31:21.047: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.048: INFO: observed event type MODIFIED
    Nov 17 14:31:21.049: INFO: observed event type MODIFIED
    Nov 17 14:31:21.049: INFO: observed event type MODIFIED
    Nov 17 14:31:21.049: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 14:31:21.056: INFO: Log out all the ReplicaSets if there is no deployment created
    Nov 17 14:31:21.086: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3745  de40e1f7-3022-41ba-ad8c-e94e74fb922c 42465 2 2023-11-17 14:31:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4877 0xc000ed4878}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4900 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Nov 17 14:31:21.095: INFO: pod: "test-deployment-7b7876f9d6-6k6rd":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-6k6rd test-deployment-7b7876f9d6- deployment-3745  e1cde30d-43ad-408a-b62e-945cf6540e68 42464 0 2023-11-17 14:31:19 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de40e1f7-3022-41ba-ad8c-e94e74fb922c 0xc0037863e7 0xc0037863e8}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de40e1f7-3022-41ba-ad8c-e94e74fb922c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbnpz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbnpz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.83,StartTime:2023-11-17 14:31:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6328e4a8a031004e2214aa36b79ca6e094c07e602cdc7fe26f374ba4f9227111,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 17 14:31:21.095: INFO: pod: "test-deployment-7b7876f9d6-sl7h7":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-sl7h7 test-deployment-7b7876f9d6- deployment-3745  c55503aa-4456-4dd2-904d-587855306c95 42416 0 2023-11-17 14:31:17 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de40e1f7-3022-41ba-ad8c-e94e74fb922c 0xc0037865c7 0xc0037865c8}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de40e1f7-3022-41ba-ad8c-e94e74fb922c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pj78t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pj78t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.68,StartTime:2023-11-17 14:31:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65d0a4525cccad0b9fd0fd32a5b36603025d5568bbc93d7dd5c47543361c9fab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 17 14:31:21.095: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3745  55ded911-6940-4695-8182-6b914f57b782 42473 4 2023-11-17 14:31:16 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4967 0xc000ed4968}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4ba0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Nov 17 14:31:21.105: INFO: pod: "test-deployment-7df74c55ff-bfzzj":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-bfzzj test-deployment-7df74c55ff- deployment-3745  540ae857-f3db-4029-8437-a79f630720ba 42469 0 2023-11-17 14:31:17 +0000 UTC 2023-11-17 14:31:21 +0000 UTC 0xc000ed53d8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 55ded911-6940-4695-8182-6b914f57b782 0xc000ed5467 0xc000ed5468}] [] [{kube-controller-manager Update v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55ded911-6940-4695-8182-6b914f57b782\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 14:31:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xlb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xlb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 14:31:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.154,StartTime:2023-11-17 14:31:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 14:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://a9e8604374994a81736d44cdd634c266e420871d4ea93a04b444967b8262d826,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Nov 17 14:31:21.105: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3745  4f504431-f3d6-41fd-8bd7-c4fa5a39c27a 42359 3 2023-11-17 14:31:14 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e322fbbf-44c3-41ff-857f-556f9156c579 0xc000ed4c67 0xc000ed4c68}] [] [{kube-controller-manager Update apps/v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e322fbbf-44c3-41ff-857f-556f9156c579\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 14:31:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ed4d90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:31:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3745" for this suite. 11/17/23 14:31:21.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:31:21.151
Nov 17 14:31:21.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 14:31:21.153
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:21.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:21.203
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 11/17/23 14:31:21.211
Nov 17 14:31:21.228: INFO: Waiting up to 5m0s for pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754" in namespace "var-expansion-3486" to be "Succeeded or Failed"
Nov 17 14:31:21.235: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Pending", Reason="", readiness=false. Elapsed: 7.449826ms
Nov 17 14:31:23.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013269585s
Nov 17 14:31:25.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013145402s
STEP: Saw pod success 11/17/23 14:31:25.241
Nov 17 14:31:25.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754" satisfied condition "Succeeded or Failed"
Nov 17 14:31:25.244: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:31:25.251
Nov 17 14:31:25.264: INFO: Waiting for pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 to disappear
Nov 17 14:31:25.267: INFO: Pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 14:31:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3486" for this suite. 11/17/23 14:31:25.272
------------------------------
â€¢ [4.127 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:31:21.151
    Nov 17 14:31:21.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 14:31:21.153
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:21.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:21.203
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 11/17/23 14:31:21.211
    Nov 17 14:31:21.228: INFO: Waiting up to 5m0s for pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754" in namespace "var-expansion-3486" to be "Succeeded or Failed"
    Nov 17 14:31:21.235: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Pending", Reason="", readiness=false. Elapsed: 7.449826ms
    Nov 17 14:31:23.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013269585s
    Nov 17 14:31:25.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013145402s
    STEP: Saw pod success 11/17/23 14:31:25.241
    Nov 17 14:31:25.241: INFO: Pod "var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754" satisfied condition "Succeeded or Failed"
    Nov 17 14:31:25.244: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:31:25.251
    Nov 17 14:31:25.264: INFO: Waiting for pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 to disappear
    Nov 17 14:31:25.267: INFO: Pod var-expansion-5fc84b14-ef62-4bc5-824d-fb21866bb754 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:31:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3486" for this suite. 11/17/23 14:31:25.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:31:25.28
Nov 17 14:31:25.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename taint-multiple-pods 11/17/23 14:31:25.281
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:25.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:25.307
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Nov 17 14:31:25.310: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:32:25.371: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Nov 17 14:32:25.374: INFO: Starting informer...
STEP: Starting pods... 11/17/23 14:32:25.374
Nov 17 14:32:25.600: INFO: Pod1 is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
Nov 17 14:32:25.826: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6812" to be "running"
Nov 17 14:32:25.832: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230865ms
Nov 17 14:32:27.836: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009467935s
Nov 17 14:32:27.836: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Nov 17 14:32:27.836: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6812" to be "running"
Nov 17 14:32:27.839: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.662781ms
Nov 17 14:32:27.839: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Nov 17 14:32:27.839: INFO: Pod2 is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
STEP: Trying to apply a taint on the Node 11/17/23 14:32:27.839
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:32:27.854
STEP: Waiting for Pod1 and Pod2 to be deleted 11/17/23 14:32:27.862
Nov 17 14:32:33.535: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 17 14:32:53.641: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:32:53.662
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:32:53.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-6812" for this suite. 11/17/23 14:32:53.679
------------------------------
â€¢ [SLOW TEST] [88.420 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:31:25.28
    Nov 17 14:31:25.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename taint-multiple-pods 11/17/23 14:31:25.281
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:31:25.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:31:25.307
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Nov 17 14:31:25.310: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:32:25.371: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Nov 17 14:32:25.374: INFO: Starting informer...
    STEP: Starting pods... 11/17/23 14:32:25.374
    Nov 17 14:32:25.600: INFO: Pod1 is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
    Nov 17 14:32:25.826: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6812" to be "running"
    Nov 17 14:32:25.832: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230865ms
    Nov 17 14:32:27.836: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009467935s
    Nov 17 14:32:27.836: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Nov 17 14:32:27.836: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6812" to be "running"
    Nov 17 14:32:27.839: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.662781ms
    Nov 17 14:32:27.839: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Nov 17 14:32:27.839: INFO: Pod2 is running on k8s-worker-2.c.operations-lab.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 11/17/23 14:32:27.839
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:32:27.854
    STEP: Waiting for Pod1 and Pod2 to be deleted 11/17/23 14:32:27.862
    Nov 17 14:32:33.535: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Nov 17 14:32:53.641: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 11/17/23 14:32:53.662
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:32:53.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-6812" for this suite. 11/17/23 14:32:53.679
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:32:53.7
Nov 17 14:32:53.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename cronjob 11/17/23 14:32:53.705
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:32:53.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:32:53.729
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 11/17/23 14:32:53.734
STEP: creating 11/17/23 14:32:53.734
STEP: getting 11/17/23 14:32:53.742
STEP: listing 11/17/23 14:32:53.746
STEP: watching 11/17/23 14:32:53.749
Nov 17 14:32:53.749: INFO: starting watch
STEP: cluster-wide listing 11/17/23 14:32:53.751
STEP: cluster-wide watching 11/17/23 14:32:53.755
Nov 17 14:32:53.755: INFO: starting watch
STEP: patching 11/17/23 14:32:53.757
STEP: updating 11/17/23 14:32:53.764
Nov 17 14:32:53.776: INFO: waiting for watch events with expected annotations
Nov 17 14:32:53.776: INFO: saw patched and updated annotations
STEP: patching /status 11/17/23 14:32:53.776
STEP: updating /status 11/17/23 14:32:53.784
STEP: get /status 11/17/23 14:32:53.793
STEP: deleting 11/17/23 14:32:53.798
STEP: deleting a collection 11/17/23 14:32:53.815
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 17 14:32:53.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7557" for this suite. 11/17/23 14:32:53.832
------------------------------
â€¢ [0.141 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:32:53.7
    Nov 17 14:32:53.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename cronjob 11/17/23 14:32:53.705
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:32:53.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:32:53.729
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 11/17/23 14:32:53.734
    STEP: creating 11/17/23 14:32:53.734
    STEP: getting 11/17/23 14:32:53.742
    STEP: listing 11/17/23 14:32:53.746
    STEP: watching 11/17/23 14:32:53.749
    Nov 17 14:32:53.749: INFO: starting watch
    STEP: cluster-wide listing 11/17/23 14:32:53.751
    STEP: cluster-wide watching 11/17/23 14:32:53.755
    Nov 17 14:32:53.755: INFO: starting watch
    STEP: patching 11/17/23 14:32:53.757
    STEP: updating 11/17/23 14:32:53.764
    Nov 17 14:32:53.776: INFO: waiting for watch events with expected annotations
    Nov 17 14:32:53.776: INFO: saw patched and updated annotations
    STEP: patching /status 11/17/23 14:32:53.776
    STEP: updating /status 11/17/23 14:32:53.784
    STEP: get /status 11/17/23 14:32:53.793
    STEP: deleting 11/17/23 14:32:53.798
    STEP: deleting a collection 11/17/23 14:32:53.815
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:32:53.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7557" for this suite. 11/17/23 14:32:53.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:32:53.843
Nov 17 14:32:53.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename limitrange 11/17/23 14:32:53.846
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:32:53.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:32:53.868
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 11/17/23 14:32:53.872
STEP: Setting up watch 11/17/23 14:32:53.872
STEP: Submitting a LimitRange 11/17/23 14:32:53.976
STEP: Verifying LimitRange creation was observed 11/17/23 14:32:53.985
STEP: Fetching the LimitRange to ensure it has proper values 11/17/23 14:32:53.986
Nov 17 14:32:53.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 17 14:32:53.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 11/17/23 14:32:53.99
STEP: Ensuring Pod has resource requirements applied from LimitRange 11/17/23 14:32:53.997
Nov 17 14:32:54.004: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 17 14:32:54.004: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 11/17/23 14:32:54.004
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/17/23 14:32:54.011
Nov 17 14:32:54.023: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 17 14:32:54.023: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 11/17/23 14:32:54.023
STEP: Failing to create a Pod with more than max resources 11/17/23 14:32:54.027
STEP: Updating a LimitRange 11/17/23 14:32:54.031
STEP: Verifying LimitRange updating is effective 11/17/23 14:32:54.038
STEP: Creating a Pod with less than former min resources 11/17/23 14:32:56.043
STEP: Failing to create a Pod with more than max resources 11/17/23 14:32:56.051
STEP: Deleting a LimitRange 11/17/23 14:32:56.055
STEP: Verifying the LimitRange was deleted 11/17/23 14:32:56.062
Nov 17 14:33:01.066: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 11/17/23 14:33:01.067
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Nov 17 14:33:01.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8585" for this suite. 11/17/23 14:33:01.086
------------------------------
â€¢ [SLOW TEST] [7.252 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:32:53.843
    Nov 17 14:32:53.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename limitrange 11/17/23 14:32:53.846
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:32:53.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:32:53.868
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 11/17/23 14:32:53.872
    STEP: Setting up watch 11/17/23 14:32:53.872
    STEP: Submitting a LimitRange 11/17/23 14:32:53.976
    STEP: Verifying LimitRange creation was observed 11/17/23 14:32:53.985
    STEP: Fetching the LimitRange to ensure it has proper values 11/17/23 14:32:53.986
    Nov 17 14:32:53.990: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 17 14:32:53.990: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 11/17/23 14:32:53.99
    STEP: Ensuring Pod has resource requirements applied from LimitRange 11/17/23 14:32:53.997
    Nov 17 14:32:54.004: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Nov 17 14:32:54.004: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 11/17/23 14:32:54.004
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 11/17/23 14:32:54.011
    Nov 17 14:32:54.023: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Nov 17 14:32:54.023: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 11/17/23 14:32:54.023
    STEP: Failing to create a Pod with more than max resources 11/17/23 14:32:54.027
    STEP: Updating a LimitRange 11/17/23 14:32:54.031
    STEP: Verifying LimitRange updating is effective 11/17/23 14:32:54.038
    STEP: Creating a Pod with less than former min resources 11/17/23 14:32:56.043
    STEP: Failing to create a Pod with more than max resources 11/17/23 14:32:56.051
    STEP: Deleting a LimitRange 11/17/23 14:32:56.055
    STEP: Verifying the LimitRange was deleted 11/17/23 14:32:56.062
    Nov 17 14:33:01.066: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 11/17/23 14:33:01.067
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:33:01.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8585" for this suite. 11/17/23 14:33:01.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:33:01.097
Nov 17 14:33:01.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:33:01.098
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:01.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:01.121
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:33:01.125
Nov 17 14:33:01.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a" in namespace "projected-3753" to be "Succeeded or Failed"
Nov 17 14:33:01.141: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693043ms
Nov 17 14:33:03.146: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033592s
Nov 17 14:33:05.149: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01120006s
STEP: Saw pod success 11/17/23 14:33:05.149
Nov 17 14:33:05.149: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a" satisfied condition "Succeeded or Failed"
Nov 17 14:33:05.155: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a container client-container: <nil>
STEP: delete the pod 11/17/23 14:33:05.175
Nov 17 14:33:05.189: INFO: Waiting for pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a to disappear
Nov 17 14:33:05.194: INFO: Pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 14:33:05.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3753" for this suite. 11/17/23 14:33:05.201
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:33:01.097
    Nov 17 14:33:01.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:33:01.098
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:01.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:01.121
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:33:01.125
    Nov 17 14:33:01.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a" in namespace "projected-3753" to be "Succeeded or Failed"
    Nov 17 14:33:01.141: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693043ms
    Nov 17 14:33:03.146: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008033592s
    Nov 17 14:33:05.149: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01120006s
    STEP: Saw pod success 11/17/23 14:33:05.149
    Nov 17 14:33:05.149: INFO: Pod "downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a" satisfied condition "Succeeded or Failed"
    Nov 17 14:33:05.155: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a container client-container: <nil>
    STEP: delete the pod 11/17/23 14:33:05.175
    Nov 17 14:33:05.189: INFO: Waiting for pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a to disappear
    Nov 17 14:33:05.194: INFO: Pod downwardapi-volume-02b217b4-5bc2-4327-9ed0-edb1b9e0333a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:33:05.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3753" for this suite. 11/17/23 14:33:05.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:33:05.213
Nov 17 14:33:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:33:05.216
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:05.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:05.242
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:33:05.248
Nov 17 14:33:05.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056" in namespace "projected-5045" to be "Succeeded or Failed"
Nov 17 14:33:05.266: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028826ms
Nov 17 14:33:07.271: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020132s
Nov 17 14:33:09.272: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Running", Reason="", readiness=false. Elapsed: 4.01160732s
Nov 17 14:33:11.271: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011245031s
STEP: Saw pod success 11/17/23 14:33:11.271
Nov 17 14:33:11.272: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056" satisfied condition "Succeeded or Failed"
Nov 17 14:33:11.274: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 container client-container: <nil>
STEP: delete the pod 11/17/23 14:33:11.28
Nov 17 14:33:11.294: INFO: Waiting for pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 to disappear
Nov 17 14:33:11.296: INFO: Pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 14:33:11.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5045" for this suite. 11/17/23 14:33:11.301
------------------------------
â€¢ [SLOW TEST] [6.095 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:33:05.213
    Nov 17 14:33:05.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:33:05.216
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:05.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:05.242
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:33:05.248
    Nov 17 14:33:05.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056" in namespace "projected-5045" to be "Succeeded or Failed"
    Nov 17 14:33:05.266: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028826ms
    Nov 17 14:33:07.271: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020132s
    Nov 17 14:33:09.272: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Running", Reason="", readiness=false. Elapsed: 4.01160732s
    Nov 17 14:33:11.271: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011245031s
    STEP: Saw pod success 11/17/23 14:33:11.271
    Nov 17 14:33:11.272: INFO: Pod "downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056" satisfied condition "Succeeded or Failed"
    Nov 17 14:33:11.274: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:33:11.28
    Nov 17 14:33:11.294: INFO: Waiting for pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 to disappear
    Nov 17 14:33:11.296: INFO: Pod downwardapi-volume-2bb77de3-ba1c-47cc-8900-f583ee407056 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:33:11.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5045" for this suite. 11/17/23 14:33:11.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:33:11.308
Nov 17 14:33:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename cronjob 11/17/23 14:33:11.309
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:11.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:11.331
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 11/17/23 14:33:11.335
STEP: Ensuring no jobs are scheduled 11/17/23 14:33:11.341
STEP: Ensuring no job exists by listing jobs explicitly 11/17/23 14:38:11.347
STEP: Removing cronjob 11/17/23 14:38:11.35
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 17 14:38:11.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7376" for this suite. 11/17/23 14:38:11.36
------------------------------
â€¢ [SLOW TEST] [300.057 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:33:11.308
    Nov 17 14:33:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename cronjob 11/17/23 14:33:11.309
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:33:11.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:33:11.331
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 11/17/23 14:33:11.335
    STEP: Ensuring no jobs are scheduled 11/17/23 14:33:11.341
    STEP: Ensuring no job exists by listing jobs explicitly 11/17/23 14:38:11.347
    STEP: Removing cronjob 11/17/23 14:38:11.35
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:38:11.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7376" for this suite. 11/17/23 14:38:11.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:38:11.37
Nov 17 14:38:11.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:38:11.371
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:11.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:11.391
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:38:11.397
Nov 17 14:38:11.407: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2878" to be "running and ready"
Nov 17 14:38:11.411: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.523698ms
Nov 17 14:38:11.411: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:38:13.415: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008565717s
Nov 17 14:38:13.415: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:38:15.416: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.008893382s
Nov 17 14:38:15.416: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 17 14:38:15.416: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 11/17/23 14:38:15.418
Nov 17 14:38:15.424: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2878" to be "running and ready"
Nov 17 14:38:15.427: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949067ms
Nov 17 14:38:15.427: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:38:17.431: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006712302s
Nov 17 14:38:17.431: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Nov 17 14:38:17.431: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 11/17/23 14:38:17.433
STEP: delete the pod with lifecycle hook 11/17/23 14:38:17.452
Nov 17 14:38:17.460: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 17 14:38:17.463: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 17 14:38:19.464: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 17 14:38:19.468: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 17 14:38:21.464: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 17 14:38:21.468: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 17 14:38:21.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2878" for this suite. 11/17/23 14:38:21.472
------------------------------
â€¢ [SLOW TEST] [10.111 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:38:11.37
    Nov 17 14:38:11.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:38:11.371
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:11.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:11.391
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:38:11.397
    Nov 17 14:38:11.407: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2878" to be "running and ready"
    Nov 17 14:38:11.411: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.523698ms
    Nov 17 14:38:11.411: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:38:13.415: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008565717s
    Nov 17 14:38:13.415: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:38:15.416: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.008893382s
    Nov 17 14:38:15.416: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 17 14:38:15.416: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 11/17/23 14:38:15.418
    Nov 17 14:38:15.424: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2878" to be "running and ready"
    Nov 17 14:38:15.427: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949067ms
    Nov 17 14:38:15.427: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:38:17.431: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006712302s
    Nov 17 14:38:17.431: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Nov 17 14:38:17.431: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 11/17/23 14:38:17.433
    STEP: delete the pod with lifecycle hook 11/17/23 14:38:17.452
    Nov 17 14:38:17.460: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 17 14:38:17.463: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 17 14:38:19.464: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 17 14:38:19.468: INFO: Pod pod-with-poststart-exec-hook still exists
    Nov 17 14:38:21.464: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Nov 17 14:38:21.468: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:38:21.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2878" for this suite. 11/17/23 14:38:21.472
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:38:21.482
Nov 17 14:38:21.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:38:21.483
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:21.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:21.504
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-ec3ae1d2-2a40-4156-afbb-444693c99482 11/17/23 14:38:21.506
STEP: Creating a pod to test consume secrets 11/17/23 14:38:21.51
Nov 17 14:38:21.522: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489" in namespace "projected-1662" to be "Succeeded or Failed"
Nov 17 14:38:21.530: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826905ms
Nov 17 14:38:23.535: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012339381s
Nov 17 14:38:25.534: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012087738s
STEP: Saw pod success 11/17/23 14:38:25.534
Nov 17 14:38:25.534: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489" satisfied condition "Succeeded or Failed"
Nov 17 14:38:25.537: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:38:25.558
Nov 17 14:38:25.575: INFO: Waiting for pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 to disappear
Nov 17 14:38:25.578: INFO: Pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 14:38:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1662" for this suite. 11/17/23 14:38:25.582
------------------------------
â€¢ [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:38:21.482
    Nov 17 14:38:21.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:38:21.483
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:21.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:21.504
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-ec3ae1d2-2a40-4156-afbb-444693c99482 11/17/23 14:38:21.506
    STEP: Creating a pod to test consume secrets 11/17/23 14:38:21.51
    Nov 17 14:38:21.522: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489" in namespace "projected-1662" to be "Succeeded or Failed"
    Nov 17 14:38:21.530: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826905ms
    Nov 17 14:38:23.535: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012339381s
    Nov 17 14:38:25.534: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012087738s
    STEP: Saw pod success 11/17/23 14:38:25.534
    Nov 17 14:38:25.534: INFO: Pod "pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489" satisfied condition "Succeeded or Failed"
    Nov 17 14:38:25.537: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:38:25.558
    Nov 17 14:38:25.575: INFO: Waiting for pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 to disappear
    Nov 17 14:38:25.578: INFO: Pod pod-projected-secrets-7137a81a-6dfa-4cd9-b6ec-3537957fb489 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:38:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1662" for this suite. 11/17/23 14:38:25.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:38:25.591
Nov 17 14:38:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 14:38:25.593
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:25.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:25.616
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-f883f092-086c-471e-b8a9-062144658255 11/17/23 14:38:25.624
STEP: Creating the pod 11/17/23 14:38:25.63
Nov 17 14:38:25.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65" in namespace "configmap-4682" to be "running and ready"
Nov 17 14:38:25.647: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157319ms
Nov 17 14:38:25.647: INFO: The phase of Pod pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:38:27.651: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65": Phase="Running", Reason="", readiness=true. Elapsed: 2.00889174s
Nov 17 14:38:27.651: INFO: The phase of Pod pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65 is Running (Ready = true)
Nov 17 14:38:27.651: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-f883f092-086c-471e-b8a9-062144658255 11/17/23 14:38:27.663
STEP: waiting to observe update in volume 11/17/23 14:38:27.671
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:39:37.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4682" for this suite. 11/17/23 14:39:37.993
------------------------------
â€¢ [SLOW TEST] [72.411 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:38:25.591
    Nov 17 14:38:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 14:38:25.593
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:38:25.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:38:25.616
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-f883f092-086c-471e-b8a9-062144658255 11/17/23 14:38:25.624
    STEP: Creating the pod 11/17/23 14:38:25.63
    Nov 17 14:38:25.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65" in namespace "configmap-4682" to be "running and ready"
    Nov 17 14:38:25.647: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157319ms
    Nov 17 14:38:25.647: INFO: The phase of Pod pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:38:27.651: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65": Phase="Running", Reason="", readiness=true. Elapsed: 2.00889174s
    Nov 17 14:38:27.651: INFO: The phase of Pod pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65 is Running (Ready = true)
    Nov 17 14:38:27.651: INFO: Pod "pod-configmaps-412d7ca1-5b52-4e00-bdf4-1e3424df7c65" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-f883f092-086c-471e-b8a9-062144658255 11/17/23 14:38:27.663
    STEP: waiting to observe update in volume 11/17/23 14:38:27.671
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:39:37.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4682" for this suite. 11/17/23 14:39:37.993
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:39:38.002
Nov 17 14:39:38.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:39:38.004
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:39:38.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:39:38.04
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:39:38.077
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:39:38.315
STEP: Deploying the webhook pod 11/17/23 14:39:38.353
STEP: Wait for the deployment to be ready 11/17/23 14:39:38.394
Nov 17 14:39:38.442: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:39:40.453
STEP: Verifying the service has paired with the endpoint 11/17/23 14:39:40.467
Nov 17 14:39:41.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/17/23 14:39:41.471
STEP: create a configmap that should be updated by the webhook 11/17/23 14:39:41.49
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:39:41.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1149" for this suite. 11/17/23 14:39:41.641
STEP: Destroying namespace "webhook-1149-markers" for this suite. 11/17/23 14:39:41.649
------------------------------
â€¢ [3.663 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:39:38.002
    Nov 17 14:39:38.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:39:38.004
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:39:38.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:39:38.04
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:39:38.077
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:39:38.315
    STEP: Deploying the webhook pod 11/17/23 14:39:38.353
    STEP: Wait for the deployment to be ready 11/17/23 14:39:38.394
    Nov 17 14:39:38.442: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:39:40.453
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:39:40.467
    Nov 17 14:39:41.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 11/17/23 14:39:41.471
    STEP: create a configmap that should be updated by the webhook 11/17/23 14:39:41.49
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:39:41.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1149" for this suite. 11/17/23 14:39:41.641
    STEP: Destroying namespace "webhook-1149-markers" for this suite. 11/17/23 14:39:41.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:39:41.666
Nov 17 14:39:41.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:39:41.668
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:39:41.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:39:41.696
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 11/17/23 14:39:41.701
Nov 17 14:39:41.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: rename a version 11/17/23 14:39:49.97
STEP: check the new version name is served 11/17/23 14:39:49.991
STEP: check the old version name is removed 11/17/23 14:39:54.159
STEP: check the other version is not changed 11/17/23 14:39:55.304
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:40:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7496" for this suite. 11/17/23 14:40:03.119
------------------------------
â€¢ [SLOW TEST] [21.461 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:39:41.666
    Nov 17 14:39:41.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:39:41.668
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:39:41.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:39:41.696
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 11/17/23 14:39:41.701
    Nov 17 14:39:41.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: rename a version 11/17/23 14:39:49.97
    STEP: check the new version name is served 11/17/23 14:39:49.991
    STEP: check the old version name is removed 11/17/23 14:39:54.159
    STEP: check the other version is not changed 11/17/23 14:39:55.304
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:40:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7496" for this suite. 11/17/23 14:40:03.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:40:03.128
Nov 17 14:40:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:40:03.129
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:40:03.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:40:03.149
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/17/23 14:40:03.155
Nov 17 14:40:03.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:40:09.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:40:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8474" for this suite. 11/17/23 14:40:26.278
------------------------------
â€¢ [SLOW TEST] [23.156 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:40:03.128
    Nov 17 14:40:03.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:40:03.129
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:40:03.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:40:03.149
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 11/17/23 14:40:03.155
    Nov 17 14:40:03.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:40:09.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:40:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8474" for this suite. 11/17/23 14:40:26.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:40:26.285
Nov 17 14:40:26.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-pred 11/17/23 14:40:26.286
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:40:26.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:40:26.306
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 17 14:40:26.310: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 17 14:40:26.317: INFO: Waiting for terminating namespaces to be deleted...
Nov 17 14:40:26.320: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Nov 17 14:40:26.336: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container cert-manager ready: true, restart count 0
Nov 17 14:40:26.336: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container cainjector ready: true, restart count 0
Nov 17 14:40:26.336: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container webhook ready: true, restart count 0
Nov 17 14:40:26.336: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container minio ready: true, restart count 0
Nov 17 14:40:26.336: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container velero ready: true, restart count 0
Nov 17 14:40:26.336: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container traefik ready: true, restart count 0
Nov 17 14:40:26.336: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.336: INFO: 	Container kube-green ready: true, restart count 0
Nov 17 14:40:26.337: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:40:26.337: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container coredns ready: true, restart count 0
Nov 17 14:40:26.337: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container coredns ready: true, restart count 0
Nov 17 14:40:26.337: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container hubble-relay ready: true, restart count 0
Nov 17 14:40:26.337: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container backend ready: true, restart count 0
Nov 17 14:40:26.337: INFO: 	Container frontend ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kyverno ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kyverno-background ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kyverno-reports ready: true, restart count 0
Nov 17 14:40:26.337: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container local-path-provisioner ready: true, restart count 0
Nov 17 14:40:26.337: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:40:26.337: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container logging-operator ready: true, restart count 0
Nov 17 14:40:26.337: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 17 14:40:26.337: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:40:26.337: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 17 14:40:26.337: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container rbac-manager ready: true, restart count 0
Nov 17 14:40:26.337: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.337: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:40:26.337: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 14:40:26.337: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Nov 17 14:40:26.351: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:40:26.351: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 14:40:26.351: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:40:26.351: INFO: kyverno-cleanup-reports-28337200-69678 from kyverno-system started at 2023-11-17 14:40:00 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container cleanup ready: false, restart count 0
Nov 17 14:40:26.351: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:40:26.351: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:40:26.351: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 17 14:40:26.351: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container e2e ready: true, restart count 0
Nov 17 14:40:26.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:40:26.351: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:40:26.351: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 14:40:26.351: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
Nov 17 14:40:26.362: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 14:40:26.362: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 14:40:26.362: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 14:40:26.362: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 14:40:26.362: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:40:26.362: INFO: 	Container fluentd ready: true, restart count 5
Nov 17 14:40:26.362: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container alertmanager ready: true, restart count 0
Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:40:26.362: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 14:40:26.362: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 14:40:26.362: INFO: 	Container prometheus ready: true, restart count 0
Nov 17 14:40:26.362: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 14:40:26.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 14:40:26.362: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:40:26.362
Nov 17 14:40:26.371: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3947" to be "running"
Nov 17 14:40:26.375: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470431ms
Nov 17 14:40:28.380: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00833412s
Nov 17 14:40:28.380: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:40:28.384
STEP: Trying to apply a random label on the found node. 11/17/23 14:40:28.405
STEP: verifying the node has the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 95 11/17/23 14:40:28.422
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/17/23 14:40:28.436
Nov 17 14:40:28.451: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3947" to be "not pending"
Nov 17 14:40:28.458: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.120141ms
Nov 17 14:40:30.461: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009772661s
Nov 17 14:40:30.461: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.4 on the node which pod4 resides and expect not scheduled 11/17/23 14:40:30.461
Nov 17 14:40:30.470: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3947" to be "not pending"
Nov 17 14:40:30.473: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.437821ms
Nov 17 14:40:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008964256s
Nov 17 14:40:34.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007433356s
Nov 17 14:40:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008897489s
Nov 17 14:40:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009693279s
Nov 17 14:40:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007712972s
Nov 17 14:40:42.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007393174s
Nov 17 14:40:44.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010856749s
Nov 17 14:40:46.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008477998s
Nov 17 14:40:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007343467s
Nov 17 14:40:50.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006757529s
Nov 17 14:40:52.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008068288s
Nov 17 14:40:54.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00834721s
Nov 17 14:40:56.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009372747s
Nov 17 14:40:58.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008087553s
Nov 17 14:41:00.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008686991s
Nov 17 14:41:02.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007356856s
Nov 17 14:41:04.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009399867s
Nov 17 14:41:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008063447s
Nov 17 14:41:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008875973s
Nov 17 14:41:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007576713s
Nov 17 14:41:12.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006884483s
Nov 17 14:41:14.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009909971s
Nov 17 14:41:16.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007885764s
Nov 17 14:41:18.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008176063s
Nov 17 14:41:20.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007558886s
Nov 17 14:41:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007682032s
Nov 17 14:41:24.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007091027s
Nov 17 14:41:26.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009365751s
Nov 17 14:41:28.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008152869s
Nov 17 14:41:30.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008905497s
Nov 17 14:41:32.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00819147s
Nov 17 14:41:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007919211s
Nov 17 14:41:36.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008048165s
Nov 17 14:41:38.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008061308s
Nov 17 14:41:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007744461s
Nov 17 14:41:42.487: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01703603s
Nov 17 14:41:44.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007990361s
Nov 17 14:41:46.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007702185s
Nov 17 14:41:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007678418s
Nov 17 14:41:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008080935s
Nov 17 14:41:52.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007599047s
Nov 17 14:41:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007719025s
Nov 17 14:41:56.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008389657s
Nov 17 14:41:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007774212s
Nov 17 14:42:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007371053s
Nov 17 14:42:02.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007021608s
Nov 17 14:42:04.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009944051s
Nov 17 14:42:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008166107s
Nov 17 14:42:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008297585s
Nov 17 14:42:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007866564s
Nov 17 14:42:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008157176s
Nov 17 14:42:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008915943s
Nov 17 14:42:16.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009418238s
Nov 17 14:42:18.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007141184s
Nov 17 14:42:20.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00921347s
Nov 17 14:42:22.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006863751s
Nov 17 14:42:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00791622s
Nov 17 14:42:26.481: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010951832s
Nov 17 14:42:28.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007865456s
Nov 17 14:42:30.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007674784s
Nov 17 14:42:32.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007912112s
Nov 17 14:42:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008664996s
Nov 17 14:42:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009267556s
Nov 17 14:42:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009834802s
Nov 17 14:42:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007296827s
Nov 17 14:42:42.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007272113s
Nov 17 14:42:44.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008114191s
Nov 17 14:42:46.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009701228s
Nov 17 14:42:48.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010098441s
Nov 17 14:42:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00796111s
Nov 17 14:42:52.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007461048s
Nov 17 14:42:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007437944s
Nov 17 14:42:56.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006974202s
Nov 17 14:42:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007202108s
Nov 17 14:43:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007617626s
Nov 17 14:43:02.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007977257s
Nov 17 14:43:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007696069s
Nov 17 14:43:06.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00904744s
Nov 17 14:43:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008152124s
Nov 17 14:43:10.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009798377s
Nov 17 14:43:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008013249s
Nov 17 14:43:14.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007120491s
Nov 17 14:43:16.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007376993s
Nov 17 14:43:18.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007480926s
Nov 17 14:43:20.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006820519s
Nov 17 14:43:22.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008876352s
Nov 17 14:43:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008645259s
Nov 17 14:43:26.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008481949s
Nov 17 14:43:28.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.007460196s
Nov 17 14:43:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008022946s
Nov 17 14:43:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009297925s
Nov 17 14:43:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008145777s
Nov 17 14:43:36.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007381973s
Nov 17 14:43:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008950498s
Nov 17 14:43:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.00729818s
Nov 17 14:43:42.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008055145s
Nov 17 14:43:44.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009323786s
Nov 17 14:43:46.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008736107s
Nov 17 14:43:48.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007939213s
Nov 17 14:43:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008401916s
Nov 17 14:43:52.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009289869s
Nov 17 14:43:54.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008843938s
Nov 17 14:43:56.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007735244s
Nov 17 14:43:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007025911s
Nov 17 14:44:00.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008521245s
Nov 17 14:44:02.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008947567s
Nov 17 14:44:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007733355s
Nov 17 14:44:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007909077s
Nov 17 14:44:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008565278s
Nov 17 14:44:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00702467s
Nov 17 14:44:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008379128s
Nov 17 14:44:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00919235s
Nov 17 14:44:16.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008618594s
Nov 17 14:44:18.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010716242s
Nov 17 14:44:20.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008539562s
Nov 17 14:44:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007535895s
Nov 17 14:44:24.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007283414s
Nov 17 14:44:26.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008183795s
Nov 17 14:44:28.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009103245s
Nov 17 14:44:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008054147s
Nov 17 14:44:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009061873s
Nov 17 14:44:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00831999s
Nov 17 14:44:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009570297s
Nov 17 14:44:38.483: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013665391s
Nov 17 14:44:40.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009320412s
Nov 17 14:44:42.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008070794s
Nov 17 14:44:44.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009001516s
Nov 17 14:44:46.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007447876s
Nov 17 14:44:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007820372s
Nov 17 14:44:50.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007849027s
Nov 17 14:44:52.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008525931s
Nov 17 14:44:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007211694s
Nov 17 14:44:56.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008806159s
Nov 17 14:44:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007112554s
Nov 17 14:45:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007580348s
Nov 17 14:45:02.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008380524s
Nov 17 14:45:04.484: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013924338s
Nov 17 14:45:06.569: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.09905568s
Nov 17 14:45:08.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009008254s
Nov 17 14:45:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007114146s
Nov 17 14:45:12.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007364621s
Nov 17 14:45:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.00891451s
Nov 17 14:45:16.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008091136s
Nov 17 14:45:18.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008572014s
Nov 17 14:45:20.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008160061s
Nov 17 14:45:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007849356s
Nov 17 14:45:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008286996s
Nov 17 14:45:26.487: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017740006s
Nov 17 14:45:28.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00889832s
Nov 17 14:45:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008035673s
Nov 17 14:45:30.481: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01124591s
STEP: removing the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 off the node k8s-worker-2.c.operations-lab.internal 11/17/23 14:45:30.481
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 11/17/23 14:45:30.5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:45:30.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3947" for this suite. 11/17/23 14:45:30.511
------------------------------
â€¢ [SLOW TEST] [304.236 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:40:26.285
    Nov 17 14:40:26.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-pred 11/17/23 14:40:26.286
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:40:26.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:40:26.306
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 17 14:40:26.310: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 17 14:40:26.317: INFO: Waiting for terminating namespaces to be deleted...
    Nov 17 14:40:26.320: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Nov 17 14:40:26.336: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container cert-manager ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container cainjector ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container webhook ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container minio ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container velero ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container traefik ready: true, restart count 0
    Nov 17 14:40:26.336: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.336: INFO: 	Container kube-green ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container hubble-relay ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container backend ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: 	Container frontend ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kyverno ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kyverno-background ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kyverno-reports ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container logging-operator ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container rbac-manager ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.337: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 14:40:26.337: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Nov 17 14:40:26.351: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: kyverno-cleanup-reports-28337200-69678 from kyverno-system started at 2023-11-17 14:40:00 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container cleanup ready: false, restart count 0
    Nov 17 14:40:26.351: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container e2e ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 14:40:26.351: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
    Nov 17 14:40:26.362: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: 	Container fluentd ready: true, restart count 5
    Nov 17 14:40:26.362: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container alertmanager ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: 	Container prometheus ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 14:40:26.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 14:40:26.362: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 11/17/23 14:40:26.362
    Nov 17 14:40:26.371: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3947" to be "running"
    Nov 17 14:40:26.375: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470431ms
    Nov 17 14:40:28.380: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00833412s
    Nov 17 14:40:28.380: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 11/17/23 14:40:28.384
    STEP: Trying to apply a random label on the found node. 11/17/23 14:40:28.405
    STEP: verifying the node has the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 95 11/17/23 14:40:28.422
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 11/17/23 14:40:28.436
    Nov 17 14:40:28.451: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3947" to be "not pending"
    Nov 17 14:40:28.458: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.120141ms
    Nov 17 14:40:30.461: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009772661s
    Nov 17 14:40:30.461: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.4 on the node which pod4 resides and expect not scheduled 11/17/23 14:40:30.461
    Nov 17 14:40:30.470: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3947" to be "not pending"
    Nov 17 14:40:30.473: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.437821ms
    Nov 17 14:40:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008964256s
    Nov 17 14:40:34.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007433356s
    Nov 17 14:40:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008897489s
    Nov 17 14:40:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009693279s
    Nov 17 14:40:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007712972s
    Nov 17 14:40:42.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007393174s
    Nov 17 14:40:44.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010856749s
    Nov 17 14:40:46.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008477998s
    Nov 17 14:40:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007343467s
    Nov 17 14:40:50.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006757529s
    Nov 17 14:40:52.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008068288s
    Nov 17 14:40:54.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00834721s
    Nov 17 14:40:56.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009372747s
    Nov 17 14:40:58.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008087553s
    Nov 17 14:41:00.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008686991s
    Nov 17 14:41:02.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007356856s
    Nov 17 14:41:04.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009399867s
    Nov 17 14:41:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008063447s
    Nov 17 14:41:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008875973s
    Nov 17 14:41:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007576713s
    Nov 17 14:41:12.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006884483s
    Nov 17 14:41:14.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009909971s
    Nov 17 14:41:16.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007885764s
    Nov 17 14:41:18.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008176063s
    Nov 17 14:41:20.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007558886s
    Nov 17 14:41:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007682032s
    Nov 17 14:41:24.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007091027s
    Nov 17 14:41:26.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009365751s
    Nov 17 14:41:28.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008152869s
    Nov 17 14:41:30.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008905497s
    Nov 17 14:41:32.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00819147s
    Nov 17 14:41:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007919211s
    Nov 17 14:41:36.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008048165s
    Nov 17 14:41:38.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008061308s
    Nov 17 14:41:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007744461s
    Nov 17 14:41:42.487: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01703603s
    Nov 17 14:41:44.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007990361s
    Nov 17 14:41:46.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007702185s
    Nov 17 14:41:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007678418s
    Nov 17 14:41:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008080935s
    Nov 17 14:41:52.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007599047s
    Nov 17 14:41:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007719025s
    Nov 17 14:41:56.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008389657s
    Nov 17 14:41:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007774212s
    Nov 17 14:42:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007371053s
    Nov 17 14:42:02.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007021608s
    Nov 17 14:42:04.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009944051s
    Nov 17 14:42:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008166107s
    Nov 17 14:42:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008297585s
    Nov 17 14:42:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007866564s
    Nov 17 14:42:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008157176s
    Nov 17 14:42:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008915943s
    Nov 17 14:42:16.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009418238s
    Nov 17 14:42:18.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007141184s
    Nov 17 14:42:20.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00921347s
    Nov 17 14:42:22.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006863751s
    Nov 17 14:42:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00791622s
    Nov 17 14:42:26.481: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010951832s
    Nov 17 14:42:28.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007865456s
    Nov 17 14:42:30.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007674784s
    Nov 17 14:42:32.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007912112s
    Nov 17 14:42:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008664996s
    Nov 17 14:42:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009267556s
    Nov 17 14:42:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009834802s
    Nov 17 14:42:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007296827s
    Nov 17 14:42:42.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007272113s
    Nov 17 14:42:44.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008114191s
    Nov 17 14:42:46.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009701228s
    Nov 17 14:42:48.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010098441s
    Nov 17 14:42:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.00796111s
    Nov 17 14:42:52.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007461048s
    Nov 17 14:42:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007437944s
    Nov 17 14:42:56.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006974202s
    Nov 17 14:42:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007202108s
    Nov 17 14:43:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007617626s
    Nov 17 14:43:02.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007977257s
    Nov 17 14:43:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007696069s
    Nov 17 14:43:06.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00904744s
    Nov 17 14:43:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.008152124s
    Nov 17 14:43:10.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009798377s
    Nov 17 14:43:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008013249s
    Nov 17 14:43:14.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007120491s
    Nov 17 14:43:16.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007376993s
    Nov 17 14:43:18.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007480926s
    Nov 17 14:43:20.476: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006820519s
    Nov 17 14:43:22.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008876352s
    Nov 17 14:43:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008645259s
    Nov 17 14:43:26.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008481949s
    Nov 17 14:43:28.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.007460196s
    Nov 17 14:43:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008022946s
    Nov 17 14:43:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009297925s
    Nov 17 14:43:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008145777s
    Nov 17 14:43:36.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007381973s
    Nov 17 14:43:38.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008950498s
    Nov 17 14:43:40.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.00729818s
    Nov 17 14:43:42.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008055145s
    Nov 17 14:43:44.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009323786s
    Nov 17 14:43:46.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008736107s
    Nov 17 14:43:48.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007939213s
    Nov 17 14:43:50.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008401916s
    Nov 17 14:43:52.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.009289869s
    Nov 17 14:43:54.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008843938s
    Nov 17 14:43:56.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007735244s
    Nov 17 14:43:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007025911s
    Nov 17 14:44:00.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008521245s
    Nov 17 14:44:02.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008947567s
    Nov 17 14:44:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007733355s
    Nov 17 14:44:06.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007909077s
    Nov 17 14:44:08.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008565278s
    Nov 17 14:44:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.00702467s
    Nov 17 14:44:12.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008379128s
    Nov 17 14:44:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00919235s
    Nov 17 14:44:16.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008618594s
    Nov 17 14:44:18.480: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010716242s
    Nov 17 14:44:20.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008539562s
    Nov 17 14:44:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007535895s
    Nov 17 14:44:24.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007283414s
    Nov 17 14:44:26.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008183795s
    Nov 17 14:44:28.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009103245s
    Nov 17 14:44:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008054147s
    Nov 17 14:44:32.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009061873s
    Nov 17 14:44:34.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00831999s
    Nov 17 14:44:36.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009570297s
    Nov 17 14:44:38.483: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013665391s
    Nov 17 14:44:40.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009320412s
    Nov 17 14:44:42.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008070794s
    Nov 17 14:44:44.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009001516s
    Nov 17 14:44:46.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007447876s
    Nov 17 14:44:48.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007820372s
    Nov 17 14:44:50.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.007849027s
    Nov 17 14:44:52.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008525931s
    Nov 17 14:44:54.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007211694s
    Nov 17 14:44:56.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008806159s
    Nov 17 14:44:58.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007112554s
    Nov 17 14:45:00.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007580348s
    Nov 17 14:45:02.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008380524s
    Nov 17 14:45:04.484: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013924338s
    Nov 17 14:45:06.569: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.09905568s
    Nov 17 14:45:08.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009008254s
    Nov 17 14:45:10.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007114146s
    Nov 17 14:45:12.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007364621s
    Nov 17 14:45:14.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.00891451s
    Nov 17 14:45:16.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008091136s
    Nov 17 14:45:18.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008572014s
    Nov 17 14:45:20.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008160061s
    Nov 17 14:45:22.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007849356s
    Nov 17 14:45:24.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008286996s
    Nov 17 14:45:26.487: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017740006s
    Nov 17 14:45:28.479: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00889832s
    Nov 17 14:45:30.478: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008035673s
    Nov 17 14:45:30.481: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01124591s
    STEP: removing the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 off the node k8s-worker-2.c.operations-lab.internal 11/17/23 14:45:30.481
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-fd3cc6e1-485d-4ab8-93be-17a1d71fc390 11/17/23 14:45:30.5
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:45:30.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3947" for this suite. 11/17/23 14:45:30.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:45:30.535
Nov 17 14:45:30.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:45:30.536
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:30.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:30.563
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:45:30.568
Nov 17 14:45:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 17 14:45:30.676: INFO: stderr: ""
Nov 17 14:45:30.676: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 11/17/23 14:45:30.676
Nov 17 14:45:30.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Nov 17 14:45:31.405: INFO: stderr: ""
Nov 17 14:45:31.405: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:45:31.405
Nov 17 14:45:31.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 delete pods e2e-test-httpd-pod'
Nov 17 14:45:32.828: INFO: stderr: ""
Nov 17 14:45:32.828: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:45:32.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7331" for this suite. 11/17/23 14:45:32.836
------------------------------
â€¢ [2.313 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:45:30.535
    Nov 17 14:45:30.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:45:30.536
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:30.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:30.563
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:45:30.568
    Nov 17 14:45:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Nov 17 14:45:30.676: INFO: stderr: ""
    Nov 17 14:45:30.676: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 11/17/23 14:45:30.676
    Nov 17 14:45:30.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Nov 17 14:45:31.405: INFO: stderr: ""
    Nov 17 14:45:31.405: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:45:31.405
    Nov 17 14:45:31.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-7331 delete pods e2e-test-httpd-pod'
    Nov 17 14:45:32.828: INFO: stderr: ""
    Nov 17 14:45:32.828: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:45:32.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7331" for this suite. 11/17/23 14:45:32.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:45:32.849
Nov 17 14:45:32.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 14:45:32.85
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:32.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:32.874
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-gkjcl" 11/17/23 14:45:32.878
Nov 17 14:45:32.896: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-gkjcl-8297" 11/17/23 14:45:32.896
Nov 17 14:45:32.908: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-gkjcl-8297" 11/17/23 14:45:32.908
Nov 17 14:45:32.919: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:45:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1136" for this suite. 11/17/23 14:45:32.924
STEP: Destroying namespace "e2e-ns-gkjcl-8297" for this suite. 11/17/23 14:45:32.931
------------------------------
â€¢ [0.095 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:45:32.849
    Nov 17 14:45:32.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 14:45:32.85
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:32.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:32.874
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-gkjcl" 11/17/23 14:45:32.878
    Nov 17 14:45:32.896: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-gkjcl-8297" 11/17/23 14:45:32.896
    Nov 17 14:45:32.908: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-gkjcl-8297" 11/17/23 14:45:32.908
    Nov 17 14:45:32.919: INFO: Namespace "e2e-ns-gkjcl-8297" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:45:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1136" for this suite. 11/17/23 14:45:32.924
    STEP: Destroying namespace "e2e-ns-gkjcl-8297" for this suite. 11/17/23 14:45:32.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:45:32.947
Nov 17 14:45:32.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 14:45:32.948
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:32.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:32.973
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7536 11/17/23 14:45:32.976
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 11/17/23 14:45:32.99
Nov 17 14:45:33.008: INFO: Found 0 stateful pods, waiting for 3
Nov 17 14:45:43.014: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:45:43.014: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:45:43.014: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:45:43.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:45:43.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:45:43.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:45:43.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/17/23 14:45:53.27
Nov 17 14:45:53.289: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/17/23 14:45:53.289
STEP: Updating Pods in reverse ordinal order 11/17/23 14:46:03.306
Nov 17 14:46:03.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 14:46:03.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 14:46:03.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 14:46:03.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 11/17/23 14:46:23.533
Nov 17 14:46:23.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 17 14:46:23.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 17 14:46:23.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 17 14:46:23.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 17 14:46:33.806: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 11/17/23 14:46:43.825
Nov 17 14:46:43.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 17 14:46:44.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 17 14:46:44.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 17 14:46:44.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 14:46:54.069: INFO: Deleting all statefulset in ns statefulset-7536
Nov 17 14:46:54.072: INFO: Scaling statefulset ss2 to 0
Nov 17 14:47:04.100: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 14:47:04.103: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7536" for this suite. 11/17/23 14:47:04.138
------------------------------
â€¢ [SLOW TEST] [91.200 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:45:32.947
    Nov 17 14:45:32.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 14:45:32.948
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:45:32.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:45:32.973
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7536 11/17/23 14:45:32.976
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 11/17/23 14:45:32.99
    Nov 17 14:45:33.008: INFO: Found 0 stateful pods, waiting for 3
    Nov 17 14:45:43.014: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:45:43.014: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:45:43.014: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:45:43.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:45:43.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:45:43.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:45:43.255: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/17/23 14:45:53.27
    Nov 17 14:45:53.289: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/17/23 14:45:53.289
    STEP: Updating Pods in reverse ordinal order 11/17/23 14:46:03.306
    Nov 17 14:46:03.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 14:46:03.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 14:46:03.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 14:46:03.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 11/17/23 14:46:23.533
    Nov 17 14:46:23.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Nov 17 14:46:23.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Nov 17 14:46:23.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Nov 17 14:46:23.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Nov 17 14:46:33.806: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 11/17/23 14:46:43.825
    Nov 17 14:46:43.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=statefulset-7536 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Nov 17 14:46:44.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Nov 17 14:46:44.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Nov 17 14:46:44.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 14:46:54.069: INFO: Deleting all statefulset in ns statefulset-7536
    Nov 17 14:46:54.072: INFO: Scaling statefulset ss2 to 0
    Nov 17 14:47:04.100: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 14:47:04.103: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7536" for this suite. 11/17/23 14:47:04.138
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:04.147
Nov 17 14:47:04.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 14:47:04.149
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:04.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:04.168
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 11/17/23 14:47:04.172
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/17/23 14:47:04.173
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/17/23 14:47:04.173
STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/17/23 14:47:04.174
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/17/23 14:47:04.174
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/17/23 14:47:04.174
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/17/23 14:47:04.175
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:04.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9959" for this suite. 11/17/23 14:47:04.179
------------------------------
â€¢ [0.038 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:04.147
    Nov 17 14:47:04.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 14:47:04.149
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:04.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:04.168
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 11/17/23 14:47:04.172
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 11/17/23 14:47:04.173
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 11/17/23 14:47:04.173
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 11/17/23 14:47:04.174
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 11/17/23 14:47:04.174
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 11/17/23 14:47:04.174
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 11/17/23 14:47:04.175
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:04.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9959" for this suite. 11/17/23 14:47:04.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:04.186
Nov 17 14:47:04.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:47:04.187
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:04.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:04.202
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:47:04.205
Nov 17 14:47:04.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141" in namespace "downward-api-7784" to be "Succeeded or Failed"
Nov 17 14:47:04.216: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333079ms
Nov 17 14:47:06.221: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Running", Reason="", readiness=false. Elapsed: 2.008343877s
Nov 17 14:47:08.220: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007734261s
STEP: Saw pod success 11/17/23 14:47:08.22
Nov 17 14:47:08.221: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141" satisfied condition "Succeeded or Failed"
Nov 17 14:47:08.224: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 container client-container: <nil>
STEP: delete the pod 11/17/23 14:47:08.244
Nov 17 14:47:08.254: INFO: Waiting for pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 to disappear
Nov 17 14:47:08.258: INFO: Pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7784" for this suite. 11/17/23 14:47:08.262
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:04.186
    Nov 17 14:47:04.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:47:04.187
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:04.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:04.202
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:47:04.205
    Nov 17 14:47:04.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141" in namespace "downward-api-7784" to be "Succeeded or Failed"
    Nov 17 14:47:04.216: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333079ms
    Nov 17 14:47:06.221: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Running", Reason="", readiness=false. Elapsed: 2.008343877s
    Nov 17 14:47:08.220: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007734261s
    STEP: Saw pod success 11/17/23 14:47:08.22
    Nov 17 14:47:08.221: INFO: Pod "downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141" satisfied condition "Succeeded or Failed"
    Nov 17 14:47:08.224: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:47:08.244
    Nov 17 14:47:08.254: INFO: Waiting for pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 to disappear
    Nov 17 14:47:08.258: INFO: Pod downwardapi-volume-d2b02e91-d331-4013-b665-aa26bbbd9141 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:08.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7784" for this suite. 11/17/23 14:47:08.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:08.272
Nov 17 14:47:08.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:47:08.274
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:08.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:08.293
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 11/17/23 14:47:08.296
Nov 17 14:47:08.297: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 17 14:47:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:09.010: INFO: stderr: ""
Nov 17 14:47:09.010: INFO: stdout: "service/agnhost-replica created\n"
Nov 17 14:47:09.010: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 17 14:47:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:09.661: INFO: stderr: ""
Nov 17 14:47:09.661: INFO: stdout: "service/agnhost-primary created\n"
Nov 17 14:47:09.661: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 17 14:47:09.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:10.228: INFO: stderr: ""
Nov 17 14:47:10.228: INFO: stdout: "service/frontend created\n"
Nov 17 14:47:10.228: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 17 14:47:10.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:10.808: INFO: stderr: ""
Nov 17 14:47:10.808: INFO: stdout: "deployment.apps/frontend created\n"
Nov 17 14:47:10.809: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 17 14:47:10.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:11.483: INFO: stderr: ""
Nov 17 14:47:11.483: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 17 14:47:11.483: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 17 14:47:11.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
Nov 17 14:47:12.200: INFO: stderr: ""
Nov 17 14:47:12.200: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 11/17/23 14:47:12.2
Nov 17 14:47:12.201: INFO: Waiting for all frontend pods to be Running.
Nov 17 14:47:17.252: INFO: Waiting for frontend to serve content.
Nov 17 14:47:17.265: INFO: Trying to add a new entry to the guestbook.
Nov 17 14:47:17.280: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 11/17/23 14:47:17.293
Nov 17 14:47:17.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:17.409: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:17.409: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 11/17/23 14:47:17.41
Nov 17 14:47:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:17.550: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:17.550: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/17/23 14:47:17.55
Nov 17 14:47:17.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:17.709: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:17.709: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/17/23 14:47:17.709
Nov 17 14:47:17.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:17.818: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:17.818: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 11/17/23 14:47:17.818
Nov 17 14:47:17.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:17.939: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:17.939: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 11/17/23 14:47:17.939
Nov 17 14:47:17.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
Nov 17 14:47:18.083: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:47:18.083: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:18.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4968" for this suite. 11/17/23 14:47:18.096
------------------------------
â€¢ [SLOW TEST] [9.859 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:08.272
    Nov 17 14:47:08.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:47:08.274
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:08.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:08.293
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 11/17/23 14:47:08.296
    Nov 17 14:47:08.297: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Nov 17 14:47:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:09.010: INFO: stderr: ""
    Nov 17 14:47:09.010: INFO: stdout: "service/agnhost-replica created\n"
    Nov 17 14:47:09.010: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Nov 17 14:47:09.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:09.661: INFO: stderr: ""
    Nov 17 14:47:09.661: INFO: stdout: "service/agnhost-primary created\n"
    Nov 17 14:47:09.661: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Nov 17 14:47:09.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:10.228: INFO: stderr: ""
    Nov 17 14:47:10.228: INFO: stdout: "service/frontend created\n"
    Nov 17 14:47:10.228: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Nov 17 14:47:10.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:10.808: INFO: stderr: ""
    Nov 17 14:47:10.808: INFO: stdout: "deployment.apps/frontend created\n"
    Nov 17 14:47:10.809: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 17 14:47:10.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:11.483: INFO: stderr: ""
    Nov 17 14:47:11.483: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Nov 17 14:47:11.483: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Nov 17 14:47:11.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 create -f -'
    Nov 17 14:47:12.200: INFO: stderr: ""
    Nov 17 14:47:12.200: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 11/17/23 14:47:12.2
    Nov 17 14:47:12.201: INFO: Waiting for all frontend pods to be Running.
    Nov 17 14:47:17.252: INFO: Waiting for frontend to serve content.
    Nov 17 14:47:17.265: INFO: Trying to add a new entry to the guestbook.
    Nov 17 14:47:17.280: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 11/17/23 14:47:17.293
    Nov 17 14:47:17.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:17.409: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:17.409: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 11/17/23 14:47:17.41
    Nov 17 14:47:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:17.550: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:17.550: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/17/23 14:47:17.55
    Nov 17 14:47:17.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:17.709: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:17.709: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/17/23 14:47:17.709
    Nov 17 14:47:17.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:17.818: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:17.818: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 11/17/23 14:47:17.818
    Nov 17 14:47:17.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:17.939: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:17.939: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 11/17/23 14:47:17.939
    Nov 17 14:47:17.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-4968 delete --grace-period=0 --force -f -'
    Nov 17 14:47:18.083: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:47:18.083: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:18.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4968" for this suite. 11/17/23 14:47:18.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:18.134
Nov 17 14:47:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 14:47:18.136
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.214
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-2386" 11/17/23 14:47:18.218
Nov 17 14:47:18.233: INFO: Namespace "namespaces-2386" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bfdd49f-be19-486b-977c-05f3d6ba48c3", "kubernetes.io/metadata.name":"namespaces-2386", "namespaces-2386":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:18.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2386" for this suite. 11/17/23 14:47:18.258
------------------------------
â€¢ [0.135 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:18.134
    Nov 17 14:47:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 14:47:18.136
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.214
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-2386" 11/17/23 14:47:18.218
    Nov 17 14:47:18.233: INFO: Namespace "namespaces-2386" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bfdd49f-be19-486b-977c-05f3d6ba48c3", "kubernetes.io/metadata.name":"namespaces-2386", "namespaces-2386":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:18.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2386" for this suite. 11/17/23 14:47:18.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:18.27
Nov 17 14:47:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 14:47:18.271
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.307
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 11/17/23 14:47:18.312
STEP: patching the Namespace 11/17/23 14:47:18.343
STEP: get the Namespace and ensuring it has the label 11/17/23 14:47:18.355
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:18.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4098" for this suite. 11/17/23 14:47:18.376
STEP: Destroying namespace "nspatchtest-ecf5b8b6-8e7b-4e45-ab70-18244927babd-2162" for this suite. 11/17/23 14:47:18.401
------------------------------
â€¢ [0.158 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:18.27
    Nov 17 14:47:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 14:47:18.271
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.307
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 11/17/23 14:47:18.312
    STEP: patching the Namespace 11/17/23 14:47:18.343
    STEP: get the Namespace and ensuring it has the label 11/17/23 14:47:18.355
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:18.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4098" for this suite. 11/17/23 14:47:18.376
    STEP: Destroying namespace "nspatchtest-ecf5b8b6-8e7b-4e45-ab70-18244927babd-2162" for this suite. 11/17/23 14:47:18.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:18.43
Nov 17 14:47:18.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:47:18.433
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.475
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:47:18.499
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:47:18.751
STEP: Deploying the webhook pod 11/17/23 14:47:18.768
STEP: Wait for the deployment to be ready 11/17/23 14:47:18.796
Nov 17 14:47:18.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 17 14:47:20.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 11/17/23 14:47:22.832
STEP: Verifying the service has paired with the endpoint 11/17/23 14:47:22.846
Nov 17 14:47:23.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 11/17/23 14:47:23.856
STEP: create a pod 11/17/23 14:47:23.879
Nov 17 14:47:23.890: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7933" to be "running"
Nov 17 14:47:23.900: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.935298ms
Nov 17 14:47:25.904: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013909517s
Nov 17 14:47:25.904: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 11/17/23 14:47:25.904
Nov 17 14:47:25.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=webhook-7933 attach --namespace=webhook-7933 to-be-attached-pod -i -c=container1'
Nov 17 14:47:26.016: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7933" for this suite. 11/17/23 14:47:26.081
STEP: Destroying namespace "webhook-7933-markers" for this suite. 11/17/23 14:47:26.098
------------------------------
â€¢ [SLOW TEST] [7.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:18.43
    Nov 17 14:47:18.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:47:18.433
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:18.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:18.475
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:47:18.499
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:47:18.751
    STEP: Deploying the webhook pod 11/17/23 14:47:18.768
    STEP: Wait for the deployment to be ready 11/17/23 14:47:18.796
    Nov 17 14:47:18.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Nov 17 14:47:20.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 14, 47, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 11/17/23 14:47:22.832
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:47:22.846
    Nov 17 14:47:23.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 11/17/23 14:47:23.856
    STEP: create a pod 11/17/23 14:47:23.879
    Nov 17 14:47:23.890: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7933" to be "running"
    Nov 17 14:47:23.900: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.935298ms
    Nov 17 14:47:25.904: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013909517s
    Nov 17 14:47:25.904: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 11/17/23 14:47:25.904
    Nov 17 14:47:25.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=webhook-7933 attach --namespace=webhook-7933 to-be-attached-pod -i -c=container1'
    Nov 17 14:47:26.016: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7933" for this suite. 11/17/23 14:47:26.081
    STEP: Destroying namespace "webhook-7933-markers" for this suite. 11/17/23 14:47:26.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:26.116
Nov 17 14:47:26.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption 11/17/23 14:47:26.117
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:26.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:26.162
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 11/17/23 14:47:26.179
STEP: Updating PodDisruptionBudget status 11/17/23 14:47:28.189
STEP: Waiting for all pods to be running 11/17/23 14:47:28.196
Nov 17 14:47:28.201: INFO: running pods: 0 < 1
STEP: locating a running pod 11/17/23 14:47:30.205
STEP: Waiting for the pdb to be processed 11/17/23 14:47:30.216
STEP: Patching PodDisruptionBudget status 11/17/23 14:47:30.224
STEP: Waiting for the pdb to be processed 11/17/23 14:47:30.234
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:30.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2799" for this suite. 11/17/23 14:47:30.244
------------------------------
â€¢ [4.133 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:26.116
    Nov 17 14:47:26.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption 11/17/23 14:47:26.117
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:26.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:26.162
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 11/17/23 14:47:26.179
    STEP: Updating PodDisruptionBudget status 11/17/23 14:47:28.189
    STEP: Waiting for all pods to be running 11/17/23 14:47:28.196
    Nov 17 14:47:28.201: INFO: running pods: 0 < 1
    STEP: locating a running pod 11/17/23 14:47:30.205
    STEP: Waiting for the pdb to be processed 11/17/23 14:47:30.216
    STEP: Patching PodDisruptionBudget status 11/17/23 14:47:30.224
    STEP: Waiting for the pdb to be processed 11/17/23 14:47:30.234
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:30.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2799" for this suite. 11/17/23 14:47:30.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:30.25
Nov 17 14:47:30.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:47:30.251
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:30.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:30.272
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 11/17/23 14:47:30.275
Nov 17 14:47:30.284: INFO: Waiting up to 5m0s for pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4" in namespace "emptydir-7356" to be "Succeeded or Failed"
Nov 17 14:47:30.289: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479044ms
Nov 17 14:47:32.295: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010832629s
Nov 17 14:47:34.296: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489415s
STEP: Saw pod success 11/17/23 14:47:34.296
Nov 17 14:47:34.296: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4" satisfied condition "Succeeded or Failed"
Nov 17 14:47:34.299: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 container test-container: <nil>
STEP: delete the pod 11/17/23 14:47:34.307
Nov 17 14:47:34.324: INFO: Waiting for pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 to disappear
Nov 17 14:47:34.330: INFO: Pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:34.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7356" for this suite. 11/17/23 14:47:34.335
------------------------------
â€¢ [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:30.25
    Nov 17 14:47:30.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:47:30.251
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:30.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:30.272
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 11/17/23 14:47:30.275
    Nov 17 14:47:30.284: INFO: Waiting up to 5m0s for pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4" in namespace "emptydir-7356" to be "Succeeded or Failed"
    Nov 17 14:47:30.289: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479044ms
    Nov 17 14:47:32.295: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Running", Reason="", readiness=false. Elapsed: 2.010832629s
    Nov 17 14:47:34.296: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489415s
    STEP: Saw pod success 11/17/23 14:47:34.296
    Nov 17 14:47:34.296: INFO: Pod "pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4" satisfied condition "Succeeded or Failed"
    Nov 17 14:47:34.299: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:47:34.307
    Nov 17 14:47:34.324: INFO: Waiting for pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 to disappear
    Nov 17 14:47:34.330: INFO: Pod pod-78cd4fcf-1d44-41cd-8b02-5d68b06e20e4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:34.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7356" for this suite. 11/17/23 14:47:34.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:34.347
Nov 17 14:47:34.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:47:34.348
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:34.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:34.384
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 11/17/23 14:47:34.388
Nov 17 14:47:34.388: INFO: Creating e2e-svc-a-bvqzx
Nov 17 14:47:34.415: INFO: Creating e2e-svc-b-tpcrb
Nov 17 14:47:34.453: INFO: Creating e2e-svc-c-rxqsj
STEP: deleting service collection 11/17/23 14:47:34.495
Nov 17 14:47:34.562: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:34.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3081" for this suite. 11/17/23 14:47:34.573
------------------------------
â€¢ [0.236 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:34.347
    Nov 17 14:47:34.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:47:34.348
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:34.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:34.384
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 11/17/23 14:47:34.388
    Nov 17 14:47:34.388: INFO: Creating e2e-svc-a-bvqzx
    Nov 17 14:47:34.415: INFO: Creating e2e-svc-b-tpcrb
    Nov 17 14:47:34.453: INFO: Creating e2e-svc-c-rxqsj
    STEP: deleting service collection 11/17/23 14:47:34.495
    Nov 17 14:47:34.562: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:34.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3081" for this suite. 11/17/23 14:47:34.573
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:34.584
Nov 17 14:47:34.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:47:34.586
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:34.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:34.62
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Nov 17 14:47:34.628: INFO: Got root ca configmap in namespace "svcaccounts-5249"
Nov 17 14:47:34.634: INFO: Deleted root ca configmap in namespace "svcaccounts-5249"
STEP: waiting for a new root ca configmap created 11/17/23 14:47:35.135
Nov 17 14:47:35.140: INFO: Recreated root ca configmap in namespace "svcaccounts-5249"
Nov 17 14:47:35.147: INFO: Updated root ca configmap in namespace "svcaccounts-5249"
STEP: waiting for the root ca configmap reconciled 11/17/23 14:47:35.647
Nov 17 14:47:35.653: INFO: Reconciled root ca configmap in namespace "svcaccounts-5249"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5249" for this suite. 11/17/23 14:47:35.66
------------------------------
â€¢ [1.087 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:34.584
    Nov 17 14:47:34.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:47:34.586
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:34.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:34.62
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Nov 17 14:47:34.628: INFO: Got root ca configmap in namespace "svcaccounts-5249"
    Nov 17 14:47:34.634: INFO: Deleted root ca configmap in namespace "svcaccounts-5249"
    STEP: waiting for a new root ca configmap created 11/17/23 14:47:35.135
    Nov 17 14:47:35.140: INFO: Recreated root ca configmap in namespace "svcaccounts-5249"
    Nov 17 14:47:35.147: INFO: Updated root ca configmap in namespace "svcaccounts-5249"
    STEP: waiting for the root ca configmap reconciled 11/17/23 14:47:35.647
    Nov 17 14:47:35.653: INFO: Reconciled root ca configmap in namespace "svcaccounts-5249"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5249" for this suite. 11/17/23 14:47:35.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:35.673
Nov 17 14:47:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename events 11/17/23 14:47:35.674
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:35.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:35.72
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 11/17/23 14:47:35.731
STEP: listing all events in all namespaces 11/17/23 14:47:35.74
STEP: patching the test event 11/17/23 14:47:35.748
STEP: fetching the test event 11/17/23 14:47:35.762
STEP: updating the test event 11/17/23 14:47:35.765
STEP: getting the test event 11/17/23 14:47:35.776
STEP: deleting the test event 11/17/23 14:47:35.78
STEP: listing all events in all namespaces 11/17/23 14:47:35.788
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:35.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6144" for this suite. 11/17/23 14:47:35.809
------------------------------
â€¢ [0.144 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:35.673
    Nov 17 14:47:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename events 11/17/23 14:47:35.674
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:35.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:35.72
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 11/17/23 14:47:35.731
    STEP: listing all events in all namespaces 11/17/23 14:47:35.74
    STEP: patching the test event 11/17/23 14:47:35.748
    STEP: fetching the test event 11/17/23 14:47:35.762
    STEP: updating the test event 11/17/23 14:47:35.765
    STEP: getting the test event 11/17/23 14:47:35.776
    STEP: deleting the test event 11/17/23 14:47:35.78
    STEP: listing all events in all namespaces 11/17/23 14:47:35.788
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:35.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6144" for this suite. 11/17/23 14:47:35.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:35.818
Nov 17 14:47:35.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:47:35.82
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:35.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:35.845
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  11/17/23 14:47:35.849
Nov 17 14:47:35.861: INFO: Waiting up to 5m0s for pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09" in namespace "svcaccounts-7449" to be "Succeeded or Failed"
Nov 17 14:47:35.866: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37083ms
Nov 17 14:47:37.871: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009995208s
Nov 17 14:47:39.870: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009930891s
STEP: Saw pod success 11/17/23 14:47:39.871
Nov 17 14:47:39.871: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09" satisfied condition "Succeeded or Failed"
Nov 17 14:47:39.876: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:47:39.885
Nov 17 14:47:39.907: INFO: Waiting for pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 to disappear
Nov 17 14:47:39.912: INFO: Pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7449" for this suite. 11/17/23 14:47:39.92
------------------------------
â€¢ [4.113 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:35.818
    Nov 17 14:47:35.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 14:47:35.82
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:35.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:35.845
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  11/17/23 14:47:35.849
    Nov 17 14:47:35.861: INFO: Waiting up to 5m0s for pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09" in namespace "svcaccounts-7449" to be "Succeeded or Failed"
    Nov 17 14:47:35.866: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37083ms
    Nov 17 14:47:37.871: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009995208s
    Nov 17 14:47:39.870: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009930891s
    STEP: Saw pod success 11/17/23 14:47:39.871
    Nov 17 14:47:39.871: INFO: Pod "test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09" satisfied condition "Succeeded or Failed"
    Nov 17 14:47:39.876: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:47:39.885
    Nov 17 14:47:39.907: INFO: Waiting for pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 to disappear
    Nov 17 14:47:39.912: INFO: Pod test-pod-8052e809-1ca7-4e30-912a-d709d3b5df09 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7449" for this suite. 11/17/23 14:47:39.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:39.934
Nov 17 14:47:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:47:39.935
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:39.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:39.971
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 11/17/23 14:47:39.975
Nov 17 14:47:39.989: INFO: Waiting up to 5m0s for pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb" in namespace "downward-api-3458" to be "Succeeded or Failed"
Nov 17 14:47:39.997: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.710021ms
Nov 17 14:47:42.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01176332s
Nov 17 14:47:44.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011588716s
STEP: Saw pod success 11/17/23 14:47:44.001
Nov 17 14:47:44.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb" satisfied condition "Succeeded or Failed"
Nov 17 14:47:44.004: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb container dapi-container: <nil>
STEP: delete the pod 11/17/23 14:47:44.012
Nov 17 14:47:44.031: INFO: Waiting for pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb to disappear
Nov 17 14:47:44.033: INFO: Pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3458" for this suite. 11/17/23 14:47:44.038
------------------------------
â€¢ [4.110 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:39.934
    Nov 17 14:47:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:47:39.935
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:39.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:39.971
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 11/17/23 14:47:39.975
    Nov 17 14:47:39.989: INFO: Waiting up to 5m0s for pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb" in namespace "downward-api-3458" to be "Succeeded or Failed"
    Nov 17 14:47:39.997: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.710021ms
    Nov 17 14:47:42.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01176332s
    Nov 17 14:47:44.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011588716s
    STEP: Saw pod success 11/17/23 14:47:44.001
    Nov 17 14:47:44.001: INFO: Pod "downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb" satisfied condition "Succeeded or Failed"
    Nov 17 14:47:44.004: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb container dapi-container: <nil>
    STEP: delete the pod 11/17/23 14:47:44.012
    Nov 17 14:47:44.031: INFO: Waiting for pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb to disappear
    Nov 17 14:47:44.033: INFO: Pod downward-api-d3961f69-a41f-4d1a-ab60-9c3e60e142bb no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3458" for this suite. 11/17/23 14:47:44.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:44.044
Nov 17 14:47:44.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:47:44.046
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:44.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:44.067
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Nov 17 14:47:44.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 14:47:48.966
Nov 17 14:47:48.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 create -f -'
Nov 17 14:47:52.277: INFO: stderr: ""
Nov 17 14:47:52.277: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 17 14:47:52.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 delete e2e-test-crd-publish-openapi-3387-crds test-cr'
Nov 17 14:47:52.379: INFO: stderr: ""
Nov 17 14:47:52.379: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 17 14:47:52.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 apply -f -'
Nov 17 14:47:52.966: INFO: stderr: ""
Nov 17 14:47:52.966: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 17 14:47:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 delete e2e-test-crd-publish-openapi-3387-crds test-cr'
Nov 17 14:47:53.064: INFO: stderr: ""
Nov 17 14:47:53.064: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 11/17/23 14:47:53.064
Nov 17 14:47:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 explain e2e-test-crd-publish-openapi-3387-crds'
Nov 17 14:47:53.631: INFO: stderr: ""
Nov 17 14:47:53.631: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3387-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:47:58.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7091" for this suite. 11/17/23 14:47:58.588
------------------------------
â€¢ [SLOW TEST] [14.551 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:44.044
    Nov 17 14:47:44.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 14:47:44.046
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:44.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:44.067
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Nov 17 14:47:44.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 14:47:48.966
    Nov 17 14:47:48.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 create -f -'
    Nov 17 14:47:52.277: INFO: stderr: ""
    Nov 17 14:47:52.277: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 17 14:47:52.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 delete e2e-test-crd-publish-openapi-3387-crds test-cr'
    Nov 17 14:47:52.379: INFO: stderr: ""
    Nov 17 14:47:52.379: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Nov 17 14:47:52.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 apply -f -'
    Nov 17 14:47:52.966: INFO: stderr: ""
    Nov 17 14:47:52.966: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Nov 17 14:47:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 --namespace=crd-publish-openapi-7091 delete e2e-test-crd-publish-openapi-3387-crds test-cr'
    Nov 17 14:47:53.064: INFO: stderr: ""
    Nov 17 14:47:53.064: INFO: stdout: "e2e-test-crd-publish-openapi-3387-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 11/17/23 14:47:53.064
    Nov 17 14:47:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-7091 explain e2e-test-crd-publish-openapi-3387-crds'
    Nov 17 14:47:53.631: INFO: stderr: ""
    Nov 17 14:47:53.631: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3387-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:47:58.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7091" for this suite. 11/17/23 14:47:58.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:47:58.597
Nov 17 14:47:58.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 14:47:58.598
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:58.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:58.619
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6482 11/17/23 14:47:58.623
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Nov 17 14:47:58.650: INFO: Found 0 stateful pods, waiting for 1
Nov 17 14:48:08.654: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 11/17/23 14:48:08.66
W1117 14:48:08.675400      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 17 14:48:08.691: INFO: Found 1 stateful pods, waiting for 2
Nov 17 14:48:18.697: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:48:18.697: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 11/17/23 14:48:18.704
STEP: Delete all of the StatefulSets 11/17/23 14:48:18.708
STEP: Verify that StatefulSets have been deleted 11/17/23 14:48:18.716
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 14:48:18.722: INFO: Deleting all statefulset in ns statefulset-6482
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:18.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6482" for this suite. 11/17/23 14:48:18.779
------------------------------
â€¢ [SLOW TEST] [20.202 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:47:58.597
    Nov 17 14:47:58.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 14:47:58.598
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:47:58.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:47:58.619
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6482 11/17/23 14:47:58.623
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Nov 17 14:47:58.650: INFO: Found 0 stateful pods, waiting for 1
    Nov 17 14:48:08.654: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 11/17/23 14:48:08.66
    W1117 14:48:08.675400      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 17 14:48:08.691: INFO: Found 1 stateful pods, waiting for 2
    Nov 17 14:48:18.697: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:48:18.697: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 11/17/23 14:48:18.704
    STEP: Delete all of the StatefulSets 11/17/23 14:48:18.708
    STEP: Verify that StatefulSets have been deleted 11/17/23 14:48:18.716
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 14:48:18.722: INFO: Deleting all statefulset in ns statefulset-6482
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:18.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6482" for this suite. 11/17/23 14:48:18.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:18.8
Nov 17 14:48:18.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 14:48:18.802
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:18.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:18.836
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 11/17/23 14:48:18.84
STEP: Ensuring job reaches completions 11/17/23 14:48:18.849
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6192" for this suite. 11/17/23 14:48:30.859
------------------------------
â€¢ [SLOW TEST] [12.064 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:18.8
    Nov 17 14:48:18.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 14:48:18.802
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:18.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:18.836
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 11/17/23 14:48:18.84
    STEP: Ensuring job reaches completions 11/17/23 14:48:18.849
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6192" for this suite. 11/17/23 14:48:30.859
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:30.866
Nov 17 14:48:30.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:48:30.867
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:30.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:30.893
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:30.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7570" for this suite. 11/17/23 14:48:30.937
------------------------------
â€¢ [0.079 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:30.866
    Nov 17 14:48:30.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:48:30.867
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:30.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:30.893
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:30.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7570" for this suite. 11/17/23 14:48:30.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:30.948
Nov 17 14:48:30.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:48:30.949
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:30.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:30.971
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:48:30.978
Nov 17 14:48:30.987: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6588" to be "running and ready"
Nov 17 14:48:30.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35319ms
Nov 17 14:48:30.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:48:32.995: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007660496s
Nov 17 14:48:32.995: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Nov 17 14:48:32.995: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 11/17/23 14:48:32.997
Nov 17 14:48:33.003: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6588" to be "running and ready"
Nov 17 14:48:33.006: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358898ms
Nov 17 14:48:33.006: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:48:35.012: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009218346s
Nov 17 14:48:35.012: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Nov 17 14:48:35.012: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 11/17/23 14:48:35.016
Nov 17 14:48:35.025: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 17 14:48:35.029: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 17 14:48:37.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 17 14:48:37.034: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 17 14:48:39.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 17 14:48:39.036: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 11/17/23 14:48:39.036
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6588" for this suite. 11/17/23 14:48:39.048
------------------------------
â€¢ [SLOW TEST] [8.108 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:30.948
    Nov 17 14:48:30.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-lifecycle-hook 11/17/23 14:48:30.949
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:30.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:30.971
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 11/17/23 14:48:30.978
    Nov 17 14:48:30.987: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6588" to be "running and ready"
    Nov 17 14:48:30.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35319ms
    Nov 17 14:48:30.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:48:32.995: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007660496s
    Nov 17 14:48:32.995: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Nov 17 14:48:32.995: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 11/17/23 14:48:32.997
    Nov 17 14:48:33.003: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6588" to be "running and ready"
    Nov 17 14:48:33.006: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358898ms
    Nov 17 14:48:33.006: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:48:35.012: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009218346s
    Nov 17 14:48:35.012: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Nov 17 14:48:35.012: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 11/17/23 14:48:35.016
    Nov 17 14:48:35.025: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 17 14:48:35.029: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 17 14:48:37.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 17 14:48:37.034: INFO: Pod pod-with-prestop-exec-hook still exists
    Nov 17 14:48:39.031: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Nov 17 14:48:39.036: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 11/17/23 14:48:39.036
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:39.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6588" for this suite. 11/17/23 14:48:39.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:39.059
Nov 17 14:48:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename ingressclass 11/17/23 14:48:39.061
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:39.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:39.09
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 11/17/23 14:48:39.094
STEP: getting /apis/networking.k8s.io 11/17/23 14:48:39.097
STEP: getting /apis/networking.k8s.iov1 11/17/23 14:48:39.098
STEP: creating 11/17/23 14:48:39.099
STEP: getting 11/17/23 14:48:39.114
STEP: listing 11/17/23 14:48:39.117
STEP: watching 11/17/23 14:48:39.121
Nov 17 14:48:39.121: INFO: starting watch
STEP: patching 11/17/23 14:48:39.122
STEP: updating 11/17/23 14:48:39.128
Nov 17 14:48:39.133: INFO: waiting for watch events with expected annotations
Nov 17 14:48:39.134: INFO: saw patched and updated annotations
STEP: deleting 11/17/23 14:48:39.134
STEP: deleting a collection 11/17/23 14:48:39.148
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:39.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-5333" for this suite. 11/17/23 14:48:39.166
------------------------------
â€¢ [0.113 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:39.059
    Nov 17 14:48:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename ingressclass 11/17/23 14:48:39.061
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:39.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:39.09
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 11/17/23 14:48:39.094
    STEP: getting /apis/networking.k8s.io 11/17/23 14:48:39.097
    STEP: getting /apis/networking.k8s.iov1 11/17/23 14:48:39.098
    STEP: creating 11/17/23 14:48:39.099
    STEP: getting 11/17/23 14:48:39.114
    STEP: listing 11/17/23 14:48:39.117
    STEP: watching 11/17/23 14:48:39.121
    Nov 17 14:48:39.121: INFO: starting watch
    STEP: patching 11/17/23 14:48:39.122
    STEP: updating 11/17/23 14:48:39.128
    Nov 17 14:48:39.133: INFO: waiting for watch events with expected annotations
    Nov 17 14:48:39.134: INFO: saw patched and updated annotations
    STEP: deleting 11/17/23 14:48:39.134
    STEP: deleting a collection 11/17/23 14:48:39.148
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:39.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-5333" for this suite. 11/17/23 14:48:39.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:39.174
Nov 17 14:48:39.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:48:39.175
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:39.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:39.197
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 11/17/23 14:48:39.2
STEP: setting up watch 11/17/23 14:48:39.201
STEP: submitting the pod to kubernetes 11/17/23 14:48:39.304
STEP: verifying the pod is in kubernetes 11/17/23 14:48:39.316
STEP: verifying pod creation was observed 11/17/23 14:48:39.319
Nov 17 14:48:39.320: INFO: Waiting up to 5m0s for pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a" in namespace "pods-7377" to be "running"
Nov 17 14:48:39.323: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703266ms
Nov 17 14:48:41.328: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008777413s
Nov 17 14:48:41.328: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a" satisfied condition "running"
STEP: deleting the pod gracefully 11/17/23 14:48:41.332
STEP: verifying pod deletion was observed 11/17/23 14:48:41.341
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7377" for this suite. 11/17/23 14:48:43.558
------------------------------
â€¢ [4.391 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:39.174
    Nov 17 14:48:39.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:48:39.175
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:39.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:39.197
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 11/17/23 14:48:39.2
    STEP: setting up watch 11/17/23 14:48:39.201
    STEP: submitting the pod to kubernetes 11/17/23 14:48:39.304
    STEP: verifying the pod is in kubernetes 11/17/23 14:48:39.316
    STEP: verifying pod creation was observed 11/17/23 14:48:39.319
    Nov 17 14:48:39.320: INFO: Waiting up to 5m0s for pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a" in namespace "pods-7377" to be "running"
    Nov 17 14:48:39.323: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703266ms
    Nov 17 14:48:41.328: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008777413s
    Nov 17 14:48:41.328: INFO: Pod "pod-submit-remove-9c9303be-288d-45ee-8182-ea164967fa2a" satisfied condition "running"
    STEP: deleting the pod gracefully 11/17/23 14:48:41.332
    STEP: verifying pod deletion was observed 11/17/23 14:48:41.341
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7377" for this suite. 11/17/23 14:48:43.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:43.568
Nov 17 14:48:43.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 14:48:43.569
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:43.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:43.591
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 11/17/23 14:48:43.594
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 11/17/23 14:48:43.599
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 11/17/23 14:48:43.6
STEP: creating a pod to probe DNS 11/17/23 14:48:43.6
STEP: submitting the pod to kubernetes 11/17/23 14:48:43.6
Nov 17 14:48:43.617: INFO: Waiting up to 15m0s for pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249" in namespace "dns-6124" to be "running"
Nov 17 14:48:43.625: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04761ms
Nov 17 14:48:45.633: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249": Phase="Running", Reason="", readiness=true. Elapsed: 2.01551931s
Nov 17 14:48:45.633: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249" satisfied condition "running"
STEP: retrieving the pod 11/17/23 14:48:45.633
STEP: looking for the results for each expected name from probers 11/17/23 14:48:45.637
Nov 17 14:48:45.661: INFO: DNS probes using dns-6124/dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249 succeeded

STEP: deleting the pod 11/17/23 14:48:45.661
STEP: deleting the test headless service 11/17/23 14:48:45.689
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:45.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6124" for this suite. 11/17/23 14:48:45.776
------------------------------
â€¢ [2.225 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:43.568
    Nov 17 14:48:43.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 14:48:43.569
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:43.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:43.591
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 11/17/23 14:48:43.594
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     11/17/23 14:48:43.599
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6124.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     11/17/23 14:48:43.6
    STEP: creating a pod to probe DNS 11/17/23 14:48:43.6
    STEP: submitting the pod to kubernetes 11/17/23 14:48:43.6
    Nov 17 14:48:43.617: INFO: Waiting up to 15m0s for pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249" in namespace "dns-6124" to be "running"
    Nov 17 14:48:43.625: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04761ms
    Nov 17 14:48:45.633: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249": Phase="Running", Reason="", readiness=true. Elapsed: 2.01551931s
    Nov 17 14:48:45.633: INFO: Pod "dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 14:48:45.633
    STEP: looking for the results for each expected name from probers 11/17/23 14:48:45.637
    Nov 17 14:48:45.661: INFO: DNS probes using dns-6124/dns-test-2d59ab12-b10c-441a-a23b-03c8080bd249 succeeded

    STEP: deleting the pod 11/17/23 14:48:45.661
    STEP: deleting the test headless service 11/17/23 14:48:45.689
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:45.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6124" for this suite. 11/17/23 14:48:45.776
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:45.794
Nov 17 14:48:45.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 14:48:45.799
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:45.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:45.829
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-9lzck" 11/17/23 14:48:45.833
Nov 17 14:48:45.839: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
Nov 17 14:48:46.844: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
Nov 17 14:48:46.848: INFO: Found 1 replicas for "e2e-rc-9lzck" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-9lzck" 11/17/23 14:48:46.849
STEP: Updating a scale subresource 11/17/23 14:48:46.854
STEP: Verifying replicas where modified for replication controller "e2e-rc-9lzck" 11/17/23 14:48:46.862
Nov 17 14:48:46.862: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
Nov 17 14:48:47.868: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
Nov 17 14:48:47.872: INFO: Found 2 replicas for "e2e-rc-9lzck" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6302" for this suite. 11/17/23 14:48:47.877
------------------------------
â€¢ [2.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:45.794
    Nov 17 14:48:45.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 14:48:45.799
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:45.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:45.829
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-9lzck" 11/17/23 14:48:45.833
    Nov 17 14:48:45.839: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
    Nov 17 14:48:46.844: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
    Nov 17 14:48:46.848: INFO: Found 1 replicas for "e2e-rc-9lzck" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-9lzck" 11/17/23 14:48:46.849
    STEP: Updating a scale subresource 11/17/23 14:48:46.854
    STEP: Verifying replicas where modified for replication controller "e2e-rc-9lzck" 11/17/23 14:48:46.862
    Nov 17 14:48:46.862: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
    Nov 17 14:48:47.868: INFO: Get Replication Controller "e2e-rc-9lzck" to confirm replicas
    Nov 17 14:48:47.872: INFO: Found 2 replicas for "e2e-rc-9lzck" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:47.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6302" for this suite. 11/17/23 14:48:47.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:47.887
Nov 17 14:48:47.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:48:47.888
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:47.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:47.905
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 11/17/23 14:48:47.909
Nov 17 14:48:47.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 create -f -'
Nov 17 14:48:48.675: INFO: stderr: ""
Nov 17 14:48:48.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 14:48:48.675
Nov 17 14:48:48.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 14:48:48.807: INFO: stderr: ""
Nov 17 14:48:48.807: INFO: stdout: "update-demo-nautilus-lx299 update-demo-nautilus-sd5d7 "
Nov 17 14:48:48.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 14:48:48.911: INFO: stderr: ""
Nov 17 14:48:48.911: INFO: stdout: ""
Nov 17 14:48:48.911: INFO: update-demo-nautilus-lx299 is created but not running
Nov 17 14:48:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 14:48:54.010: INFO: stderr: ""
Nov 17 14:48:54.010: INFO: stdout: "update-demo-nautilus-lx299 update-demo-nautilus-sd5d7 "
Nov 17 14:48:54.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 14:48:54.107: INFO: stderr: ""
Nov 17 14:48:54.107: INFO: stdout: "true"
Nov 17 14:48:54.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 14:48:54.212: INFO: stderr: ""
Nov 17 14:48:54.212: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 14:48:54.212: INFO: validating pod update-demo-nautilus-lx299
Nov 17 14:48:54.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 14:48:54.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 14:48:54.220: INFO: update-demo-nautilus-lx299 is verified up and running
Nov 17 14:48:54.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-sd5d7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 14:48:54.313: INFO: stderr: ""
Nov 17 14:48:54.313: INFO: stdout: "true"
Nov 17 14:48:54.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-sd5d7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 14:48:54.406: INFO: stderr: ""
Nov 17 14:48:54.406: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 14:48:54.406: INFO: validating pod update-demo-nautilus-sd5d7
Nov 17 14:48:54.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 14:48:54.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 14:48:54.411: INFO: update-demo-nautilus-sd5d7 is verified up and running
STEP: using delete to clean up resources 11/17/23 14:48:54.411
Nov 17 14:48:54.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 delete --grace-period=0 --force -f -'
Nov 17 14:48:54.506: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 14:48:54.506: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 17 14:48:54.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get rc,svc -l name=update-demo --no-headers'
Nov 17 14:48:54.609: INFO: stderr: "No resources found in kubectl-5469 namespace.\n"
Nov 17 14:48:54.609: INFO: stdout: ""
Nov 17 14:48:54.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 17 14:48:54.718: INFO: stderr: ""
Nov 17 14:48:54.718: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:54.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5469" for this suite. 11/17/23 14:48:54.723
------------------------------
â€¢ [SLOW TEST] [6.842 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:47.887
    Nov 17 14:48:47.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:48:47.888
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:47.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:47.905
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 11/17/23 14:48:47.909
    Nov 17 14:48:47.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 create -f -'
    Nov 17 14:48:48.675: INFO: stderr: ""
    Nov 17 14:48:48.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 14:48:48.675
    Nov 17 14:48:48.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 14:48:48.807: INFO: stderr: ""
    Nov 17 14:48:48.807: INFO: stdout: "update-demo-nautilus-lx299 update-demo-nautilus-sd5d7 "
    Nov 17 14:48:48.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 14:48:48.911: INFO: stderr: ""
    Nov 17 14:48:48.911: INFO: stdout: ""
    Nov 17 14:48:48.911: INFO: update-demo-nautilus-lx299 is created but not running
    Nov 17 14:48:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 14:48:54.010: INFO: stderr: ""
    Nov 17 14:48:54.010: INFO: stdout: "update-demo-nautilus-lx299 update-demo-nautilus-sd5d7 "
    Nov 17 14:48:54.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 14:48:54.107: INFO: stderr: ""
    Nov 17 14:48:54.107: INFO: stdout: "true"
    Nov 17 14:48:54.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-lx299 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 14:48:54.212: INFO: stderr: ""
    Nov 17 14:48:54.212: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 14:48:54.212: INFO: validating pod update-demo-nautilus-lx299
    Nov 17 14:48:54.220: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 14:48:54.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 14:48:54.220: INFO: update-demo-nautilus-lx299 is verified up and running
    Nov 17 14:48:54.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-sd5d7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 14:48:54.313: INFO: stderr: ""
    Nov 17 14:48:54.313: INFO: stdout: "true"
    Nov 17 14:48:54.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods update-demo-nautilus-sd5d7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 14:48:54.406: INFO: stderr: ""
    Nov 17 14:48:54.406: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 14:48:54.406: INFO: validating pod update-demo-nautilus-sd5d7
    Nov 17 14:48:54.411: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 14:48:54.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 14:48:54.411: INFO: update-demo-nautilus-sd5d7 is verified up and running
    STEP: using delete to clean up resources 11/17/23 14:48:54.411
    Nov 17 14:48:54.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 delete --grace-period=0 --force -f -'
    Nov 17 14:48:54.506: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 14:48:54.506: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 17 14:48:54.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get rc,svc -l name=update-demo --no-headers'
    Nov 17 14:48:54.609: INFO: stderr: "No resources found in kubectl-5469 namespace.\n"
    Nov 17 14:48:54.609: INFO: stdout: ""
    Nov 17 14:48:54.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-5469 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 17 14:48:54.718: INFO: stderr: ""
    Nov 17 14:48:54.718: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:54.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5469" for this suite. 11/17/23 14:48:54.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:54.73
Nov 17 14:48:54.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context-test 11/17/23 14:48:54.732
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:54.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:54.756
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Nov 17 14:48:54.767: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5" in namespace "security-context-test-179" to be "Succeeded or Failed"
Nov 17 14:48:54.775: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.166683ms
Nov 17 14:48:56.780: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012145679s
Nov 17 14:48:58.779: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011887459s
Nov 17 14:48:58.779: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 14:48:58.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-179" for this suite. 11/17/23 14:48:58.794
------------------------------
â€¢ [4.073 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:54.73
    Nov 17 14:48:54.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context-test 11/17/23 14:48:54.732
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:54.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:54.756
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Nov 17 14:48:54.767: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5" in namespace "security-context-test-179" to be "Succeeded or Failed"
    Nov 17 14:48:54.775: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.166683ms
    Nov 17 14:48:56.780: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012145679s
    Nov 17 14:48:58.779: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011887459s
    Nov 17 14:48:58.779: INFO: Pod "alpine-nnp-false-23b2a88f-55b5-4639-a661-aca3f483e2c5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:48:58.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-179" for this suite. 11/17/23 14:48:58.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:48:58.806
Nov 17 14:48:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:48:58.807
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:58.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:58.832
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-ea64677e-4d97-44b8-8244-cd45c1a54077 11/17/23 14:48:58.836
STEP: Creating a pod to test consume configMaps 11/17/23 14:48:58.845
Nov 17 14:48:58.855: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31" in namespace "projected-9130" to be "Succeeded or Failed"
Nov 17 14:48:58.859: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608911ms
Nov 17 14:49:00.863: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007827845s
Nov 17 14:49:02.865: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010494566s
STEP: Saw pod success 11/17/23 14:49:02.865
Nov 17 14:49:02.866: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31" satisfied condition "Succeeded or Failed"
Nov 17 14:49:02.870: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:49:02.878
Nov 17 14:49:02.900: INFO: Waiting for pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 to disappear
Nov 17 14:49:02.904: INFO: Pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 14:49:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9130" for this suite. 11/17/23 14:49:02.91
------------------------------
â€¢ [4.112 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:48:58.806
    Nov 17 14:48:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:48:58.807
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:48:58.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:48:58.832
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-ea64677e-4d97-44b8-8244-cd45c1a54077 11/17/23 14:48:58.836
    STEP: Creating a pod to test consume configMaps 11/17/23 14:48:58.845
    Nov 17 14:48:58.855: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31" in namespace "projected-9130" to be "Succeeded or Failed"
    Nov 17 14:48:58.859: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608911ms
    Nov 17 14:49:00.863: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007827845s
    Nov 17 14:49:02.865: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010494566s
    STEP: Saw pod success 11/17/23 14:49:02.865
    Nov 17 14:49:02.866: INFO: Pod "pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31" satisfied condition "Succeeded or Failed"
    Nov 17 14:49:02.870: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:49:02.878
    Nov 17 14:49:02.900: INFO: Waiting for pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 to disappear
    Nov 17 14:49:02.904: INFO: Pod pod-projected-configmaps-4b197dce-381e-4463-bf55-604d253f9a31 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:49:02.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9130" for this suite. 11/17/23 14:49:02.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:49:02.92
Nov 17 14:49:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:49:02.922
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:02.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:02.951
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-5749 11/17/23 14:49:02.955
STEP: creating service affinity-nodeport in namespace services-5749 11/17/23 14:49:02.956
STEP: creating replication controller affinity-nodeport in namespace services-5749 11/17/23 14:49:02.981
I1117 14:49:02.991947      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5749, replica count: 3
I1117 14:49:06.043157      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 14:49:06.052: INFO: Creating new exec pod
Nov 17 14:49:06.057: INFO: Waiting up to 5m0s for pod "execpod-affinitylznqz" in namespace "services-5749" to be "running"
Nov 17 14:49:06.061: INFO: Pod "execpod-affinitylznqz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272288ms
Nov 17 14:49:08.067: INFO: Pod "execpod-affinitylznqz": Phase="Running", Reason="", readiness=true. Elapsed: 2.00939888s
Nov 17 14:49:08.067: INFO: Pod "execpod-affinitylznqz" satisfied condition "running"
Nov 17 14:49:09.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Nov 17 14:49:09.287: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 17 14:49:09.287: INFO: stdout: ""
Nov 17 14:49:09.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 10.107.119.90 80'
Nov 17 14:49:09.494: INFO: stderr: "+ nc -v -z -w 2 10.107.119.90 80\nConnection to 10.107.119.90 80 port [tcp/http] succeeded!\n"
Nov 17 14:49:09.494: INFO: stdout: ""
Nov 17 14:49:09.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32585'
Nov 17 14:49:09.684: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32585\nConnection to 172.16.0.4 32585 port [tcp/*] succeeded!\n"
Nov 17 14:49:09.684: INFO: stdout: ""
Nov 17 14:49:09.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 32585'
Nov 17 14:49:09.860: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 32585\nConnection to 172.16.0.5 32585 port [tcp/*] succeeded!\n"
Nov 17 14:49:09.860: INFO: stdout: ""
Nov 17 14:49:09.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32585/ ; done'
Nov 17 14:49:10.153: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n"
Nov 17 14:49:10.153: INFO: stdout: "\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v"
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
Nov 17 14:49:10.153: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5749, will wait for the garbage collector to delete the pods 11/17/23 14:49:10.167
Nov 17 14:49:10.227: INFO: Deleting ReplicationController affinity-nodeport took: 6.039628ms
Nov 17 14:49:10.327: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.496217ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:49:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5749" for this suite. 11/17/23 14:49:12.261
------------------------------
â€¢ [SLOW TEST] [9.349 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:49:02.92
    Nov 17 14:49:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:49:02.922
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:02.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:02.951
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-5749 11/17/23 14:49:02.955
    STEP: creating service affinity-nodeport in namespace services-5749 11/17/23 14:49:02.956
    STEP: creating replication controller affinity-nodeport in namespace services-5749 11/17/23 14:49:02.981
    I1117 14:49:02.991947      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5749, replica count: 3
    I1117 14:49:06.043157      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 14:49:06.052: INFO: Creating new exec pod
    Nov 17 14:49:06.057: INFO: Waiting up to 5m0s for pod "execpod-affinitylznqz" in namespace "services-5749" to be "running"
    Nov 17 14:49:06.061: INFO: Pod "execpod-affinitylznqz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.272288ms
    Nov 17 14:49:08.067: INFO: Pod "execpod-affinitylznqz": Phase="Running", Reason="", readiness=true. Elapsed: 2.00939888s
    Nov 17 14:49:08.067: INFO: Pod "execpod-affinitylznqz" satisfied condition "running"
    Nov 17 14:49:09.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Nov 17 14:49:09.287: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Nov 17 14:49:09.287: INFO: stdout: ""
    Nov 17 14:49:09.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 10.107.119.90 80'
    Nov 17 14:49:09.494: INFO: stderr: "+ nc -v -z -w 2 10.107.119.90 80\nConnection to 10.107.119.90 80 port [tcp/http] succeeded!\n"
    Nov 17 14:49:09.494: INFO: stdout: ""
    Nov 17 14:49:09.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32585'
    Nov 17 14:49:09.684: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32585\nConnection to 172.16.0.4 32585 port [tcp/*] succeeded!\n"
    Nov 17 14:49:09.684: INFO: stdout: ""
    Nov 17 14:49:09.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 32585'
    Nov 17 14:49:09.860: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 32585\nConnection to 172.16.0.5 32585 port [tcp/*] succeeded!\n"
    Nov 17 14:49:09.860: INFO: stdout: ""
    Nov 17 14:49:09.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-5749 exec execpod-affinitylznqz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32585/ ; done'
    Nov 17 14:49:10.153: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32585/\n"
    Nov 17 14:49:10.153: INFO: stdout: "\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v\naffinity-nodeport-kxs6v"
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Received response from host: affinity-nodeport-kxs6v
    Nov 17 14:49:10.153: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-5749, will wait for the garbage collector to delete the pods 11/17/23 14:49:10.167
    Nov 17 14:49:10.227: INFO: Deleting ReplicationController affinity-nodeport took: 6.039628ms
    Nov 17 14:49:10.327: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.496217ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:49:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5749" for this suite. 11/17/23 14:49:12.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:49:12.27
Nov 17 14:49:12.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 14:49:12.271
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:12.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:12.289
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 11/17/23 14:49:12.292
Nov 17 14:49:12.299: INFO: Waiting up to 5m0s for pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9" in namespace "emptydir-2895" to be "Succeeded or Failed"
Nov 17 14:49:12.306: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.326868ms
Nov 17 14:49:14.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921761s
Nov 17 14:49:16.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01052519s
STEP: Saw pod success 11/17/23 14:49:16.31
Nov 17 14:49:16.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9" satisfied condition "Succeeded or Failed"
Nov 17 14:49:16.313: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 container test-container: <nil>
STEP: delete the pod 11/17/23 14:49:16.322
Nov 17 14:49:16.343: INFO: Waiting for pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 to disappear
Nov 17 14:49:16.349: INFO: Pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 14:49:16.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2895" for this suite. 11/17/23 14:49:16.355
------------------------------
â€¢ [4.100 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:49:12.27
    Nov 17 14:49:12.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 14:49:12.271
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:12.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:12.289
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 11/17/23 14:49:12.292
    Nov 17 14:49:12.299: INFO: Waiting up to 5m0s for pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9" in namespace "emptydir-2895" to be "Succeeded or Failed"
    Nov 17 14:49:12.306: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.326868ms
    Nov 17 14:49:14.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921761s
    Nov 17 14:49:16.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01052519s
    STEP: Saw pod success 11/17/23 14:49:16.31
    Nov 17 14:49:16.310: INFO: Pod "pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9" satisfied condition "Succeeded or Failed"
    Nov 17 14:49:16.313: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 container test-container: <nil>
    STEP: delete the pod 11/17/23 14:49:16.322
    Nov 17 14:49:16.343: INFO: Waiting for pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 to disappear
    Nov 17 14:49:16.349: INFO: Pod pod-80ef9bd5-d9b5-464e-bb1b-94223ee9a6a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:49:16.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2895" for this suite. 11/17/23 14:49:16.355
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:49:16.37
Nov 17 14:49:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-runtime 11/17/23 14:49:16.372
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:16.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:16.407
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/17/23 14:49:16.425
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/17/23 14:49:32.538
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/17/23 14:49:32.541
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/17/23 14:49:32.548
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/17/23 14:49:32.548
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/17/23 14:49:32.57
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/17/23 14:49:35.589
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/17/23 14:49:37.6
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/17/23 14:49:37.607
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/17/23 14:49:37.608
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/17/23 14:49:37.629
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/17/23 14:49:38.639
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/17/23 14:49:41.653
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/17/23 14:49:41.659
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/17/23 14:49:41.659
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 17 14:49:41.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2910" for this suite. 11/17/23 14:49:41.682
------------------------------
â€¢ [SLOW TEST] [25.317 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:49:16.37
    Nov 17 14:49:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-runtime 11/17/23 14:49:16.372
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:16.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:16.407
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 11/17/23 14:49:16.425
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 11/17/23 14:49:32.538
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 11/17/23 14:49:32.541
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 11/17/23 14:49:32.548
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 11/17/23 14:49:32.548
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 11/17/23 14:49:32.57
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 11/17/23 14:49:35.589
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 11/17/23 14:49:37.6
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 11/17/23 14:49:37.607
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 11/17/23 14:49:37.608
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 11/17/23 14:49:37.629
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 11/17/23 14:49:38.639
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 11/17/23 14:49:41.653
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 11/17/23 14:49:41.659
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 11/17/23 14:49:41.659
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:49:41.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2910" for this suite. 11/17/23 14:49:41.682
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:49:41.689
Nov 17 14:49:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:49:41.69
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:41.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:41.713
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:50:41.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1271" for this suite. 11/17/23 14:50:41.764
------------------------------
â€¢ [SLOW TEST] [60.138 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:49:41.689
    Nov 17 14:49:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:49:41.69
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:49:41.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:49:41.713
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:50:41.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1271" for this suite. 11/17/23 14:50:41.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:50:41.828
Nov 17 14:50:41.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:50:41.829
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:50:41.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:50:41.907
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:50:41.921
Nov 17 14:50:41.963: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3" in namespace "downward-api-8164" to be "Succeeded or Failed"
Nov 17 14:50:41.974: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.178344ms
Nov 17 14:50:43.985: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021759773s
Nov 17 14:50:45.983: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019757462s
STEP: Saw pod success 11/17/23 14:50:45.983
Nov 17 14:50:45.983: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3" satisfied condition "Succeeded or Failed"
Nov 17 14:50:45.991: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 container client-container: <nil>
STEP: delete the pod 11/17/23 14:50:46.01
Nov 17 14:50:46.056: INFO: Waiting for pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 to disappear
Nov 17 14:50:46.063: INFO: Pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:50:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8164" for this suite. 11/17/23 14:50:46.078
------------------------------
â€¢ [4.293 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:50:41.828
    Nov 17 14:50:41.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:50:41.829
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:50:41.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:50:41.907
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:50:41.921
    Nov 17 14:50:41.963: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3" in namespace "downward-api-8164" to be "Succeeded or Failed"
    Nov 17 14:50:41.974: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.178344ms
    Nov 17 14:50:43.985: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021759773s
    Nov 17 14:50:45.983: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019757462s
    STEP: Saw pod success 11/17/23 14:50:45.983
    Nov 17 14:50:45.983: INFO: Pod "downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3" satisfied condition "Succeeded or Failed"
    Nov 17 14:50:45.991: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:50:46.01
    Nov 17 14:50:46.056: INFO: Waiting for pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 to disappear
    Nov 17 14:50:46.063: INFO: Pod downwardapi-volume-00406490-afde-4e9b-a7df-3f15ef3ccdd3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:50:46.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8164" for this suite. 11/17/23 14:50:46.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:50:46.123
Nov 17 14:50:46.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 14:50:46.125
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:50:46.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:50:46.183
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a in namespace container-probe-7750 11/17/23 14:50:46.237
Nov 17 14:50:46.280: INFO: Waiting up to 5m0s for pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a" in namespace "container-probe-7750" to be "not pending"
Nov 17 14:50:46.304: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.210927ms
Nov 17 14:50:48.314: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033863102s
Nov 17 14:50:48.314: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a" satisfied condition "not pending"
Nov 17 14:50:48.314: INFO: Started pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a in namespace container-probe-7750
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:50:48.314
Nov 17 14:50:48.329: INFO: Initial restart count of pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a is 0
Nov 17 14:51:38.499: INFO: Restart count of pod container-probe-7750/busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a is now 1 (50.17046216s elapsed)
STEP: deleting the pod 11/17/23 14:51:38.499
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 14:51:38.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7750" for this suite. 11/17/23 14:51:38.527
------------------------------
â€¢ [SLOW TEST] [52.430 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:50:46.123
    Nov 17 14:50:46.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 14:50:46.125
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:50:46.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:50:46.183
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a in namespace container-probe-7750 11/17/23 14:50:46.237
    Nov 17 14:50:46.280: INFO: Waiting up to 5m0s for pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a" in namespace "container-probe-7750" to be "not pending"
    Nov 17 14:50:46.304: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.210927ms
    Nov 17 14:50:48.314: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033863102s
    Nov 17 14:50:48.314: INFO: Pod "busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a" satisfied condition "not pending"
    Nov 17 14:50:48.314: INFO: Started pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a in namespace container-probe-7750
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 14:50:48.314
    Nov 17 14:50:48.329: INFO: Initial restart count of pod busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a is 0
    Nov 17 14:51:38.499: INFO: Restart count of pod container-probe-7750/busybox-41c7b159-efd0-4db4-85d5-9bba52bc4a4a is now 1 (50.17046216s elapsed)
    STEP: deleting the pod 11/17/23 14:51:38.499
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:51:38.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7750" for this suite. 11/17/23 14:51:38.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:51:38.556
Nov 17 14:51:38.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:51:38.558
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:51:38.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:51:38.583
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-245 11/17/23 14:51:38.586
STEP: creating a selector 11/17/23 14:51:38.586
STEP: Creating the service pods in kubernetes 11/17/23 14:51:38.587
Nov 17 14:51:38.587: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 17 14:51:38.617: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-245" to be "running and ready"
Nov 17 14:51:38.627: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.999828ms
Nov 17 14:51:38.627: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:51:40.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017411079s
Nov 17 14:51:40.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:42.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015655752s
Nov 17 14:51:42.633: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:44.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014810115s
Nov 17 14:51:44.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:46.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014526891s
Nov 17 14:51:46.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:48.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01481603s
Nov 17 14:51:48.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:50.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014479303s
Nov 17 14:51:50.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:52.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01883375s
Nov 17 14:51:52.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:54.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01432585s
Nov 17 14:51:54.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:56.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014063216s
Nov 17 14:51:56.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:51:58.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016114787s
Nov 17 14:51:58.633: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Nov 17 14:52:00.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015586818s
Nov 17 14:52:00.633: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Nov 17 14:52:00.633: INFO: Pod "netserver-0" satisfied condition "running and ready"
Nov 17 14:52:00.636: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-245" to be "running and ready"
Nov 17 14:52:00.639: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.11843ms
Nov 17 14:52:00.639: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Nov 17 14:52:00.639: INFO: Pod "netserver-1" satisfied condition "running and ready"
Nov 17 14:52:00.643: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-245" to be "running and ready"
Nov 17 14:52:00.645: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.816538ms
Nov 17 14:52:00.645: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Nov 17 14:52:00.645: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 11/17/23 14:52:00.648
Nov 17 14:52:00.660: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-245" to be "running"
Nov 17 14:52:00.669: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.967663ms
Nov 17 14:52:02.673: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01264836s
Nov 17 14:52:02.673: INFO: Pod "test-container-pod" satisfied condition "running"
Nov 17 14:52:02.676: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-245" to be "running"
Nov 17 14:52:02.679: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.951332ms
Nov 17 14:52:02.680: INFO: Pod "host-test-container-pod" satisfied condition "running"
Nov 17 14:52:02.683: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 17 14:52:02.683: INFO: Going to poll 10.10.2.218 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:52:02.685: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.2.218:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:52:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:52:02.686: INFO: ExecWithOptions: Clientset creation
Nov 17 14:52:02.687: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.2.218%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:52:02.815: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 17 14:52:02.815: INFO: Going to poll 10.10.0.34 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:52:02.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.34:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:52:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:52:02.819: INFO: ExecWithOptions: Clientset creation
Nov 17 14:52:02.819: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.34%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:52:02.929: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 17 14:52:02.929: INFO: Going to poll 10.10.1.137 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 17 14:52:02.934: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.137:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:52:02.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:52:02.935: INFO: ExecWithOptions: Clientset creation
Nov 17 14:52:02.935: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.137%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:52:03.068: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:03.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-245" for this suite. 11/17/23 14:52:03.073
------------------------------
â€¢ [SLOW TEST] [24.522 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:51:38.556
    Nov 17 14:51:38.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pod-network-test 11/17/23 14:51:38.558
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:51:38.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:51:38.583
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-245 11/17/23 14:51:38.586
    STEP: creating a selector 11/17/23 14:51:38.586
    STEP: Creating the service pods in kubernetes 11/17/23 14:51:38.587
    Nov 17 14:51:38.587: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Nov 17 14:51:38.617: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-245" to be "running and ready"
    Nov 17 14:51:38.627: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.999828ms
    Nov 17 14:51:38.627: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:51:40.635: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017411079s
    Nov 17 14:51:40.635: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:42.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015655752s
    Nov 17 14:51:42.633: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:44.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014810115s
    Nov 17 14:51:44.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:46.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014526891s
    Nov 17 14:51:46.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:48.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01481603s
    Nov 17 14:51:48.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:50.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014479303s
    Nov 17 14:51:50.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:52.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01883375s
    Nov 17 14:51:52.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:54.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01432585s
    Nov 17 14:51:54.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:56.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014063216s
    Nov 17 14:51:56.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:51:58.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016114787s
    Nov 17 14:51:58.633: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Nov 17 14:52:00.633: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015586818s
    Nov 17 14:52:00.633: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Nov 17 14:52:00.633: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Nov 17 14:52:00.636: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-245" to be "running and ready"
    Nov 17 14:52:00.639: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.11843ms
    Nov 17 14:52:00.639: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Nov 17 14:52:00.639: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Nov 17 14:52:00.643: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-245" to be "running and ready"
    Nov 17 14:52:00.645: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.816538ms
    Nov 17 14:52:00.645: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Nov 17 14:52:00.645: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 11/17/23 14:52:00.648
    Nov 17 14:52:00.660: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-245" to be "running"
    Nov 17 14:52:00.669: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.967663ms
    Nov 17 14:52:02.673: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01264836s
    Nov 17 14:52:02.673: INFO: Pod "test-container-pod" satisfied condition "running"
    Nov 17 14:52:02.676: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-245" to be "running"
    Nov 17 14:52:02.679: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.951332ms
    Nov 17 14:52:02.680: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Nov 17 14:52:02.683: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Nov 17 14:52:02.683: INFO: Going to poll 10.10.2.218 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:52:02.685: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.2.218:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:52:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:52:02.686: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:52:02.687: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.2.218%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:52:02.815: INFO: Found all 1 expected endpoints: [netserver-0]
    Nov 17 14:52:02.815: INFO: Going to poll 10.10.0.34 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:52:02.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.34:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:52:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:52:02.819: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:52:02.819: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.34%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:52:02.929: INFO: Found all 1 expected endpoints: [netserver-1]
    Nov 17 14:52:02.929: INFO: Going to poll 10.10.1.137 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Nov 17 14:52:02.934: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.137:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-245 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:52:02.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:52:02.935: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:52:02.935: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-245/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.137%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:52:03.068: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:03.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-245" for this suite. 11/17/23 14:52:03.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:03.081
Nov 17 14:52:03.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sysctl 11/17/23 14:52:03.082
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:03.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:03.104
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/17/23 14:52:03.108
STEP: Watching for error events or started pod 11/17/23 14:52:03.116
STEP: Waiting for pod completion 11/17/23 14:52:05.121
Nov 17 14:52:05.121: INFO: Waiting up to 3m0s for pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048" in namespace "sysctl-8317" to be "completed"
Nov 17 14:52:05.126: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660078ms
Nov 17 14:52:07.130: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009155314s
Nov 17 14:52:07.130: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048" satisfied condition "completed"
STEP: Checking that the pod succeeded 11/17/23 14:52:07.133
STEP: Getting logs from the pod 11/17/23 14:52:07.134
STEP: Checking that the sysctl is actually updated 11/17/23 14:52:07.14
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:07.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8317" for this suite. 11/17/23 14:52:07.145
------------------------------
â€¢ [4.070 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:03.081
    Nov 17 14:52:03.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sysctl 11/17/23 14:52:03.082
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:03.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:03.104
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 11/17/23 14:52:03.108
    STEP: Watching for error events or started pod 11/17/23 14:52:03.116
    STEP: Waiting for pod completion 11/17/23 14:52:05.121
    Nov 17 14:52:05.121: INFO: Waiting up to 3m0s for pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048" in namespace "sysctl-8317" to be "completed"
    Nov 17 14:52:05.126: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660078ms
    Nov 17 14:52:07.130: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009155314s
    Nov 17 14:52:07.130: INFO: Pod "sysctl-28921da4-a19c-4f52-9268-42148fa41048" satisfied condition "completed"
    STEP: Checking that the pod succeeded 11/17/23 14:52:07.133
    STEP: Getting logs from the pod 11/17/23 14:52:07.134
    STEP: Checking that the sysctl is actually updated 11/17/23 14:52:07.14
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:07.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8317" for this suite. 11/17/23 14:52:07.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:07.157
Nov 17 14:52:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename init-container 11/17/23 14:52:07.158
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:07.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:07.177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 11/17/23 14:52:07.18
Nov 17 14:52:07.180: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4688" for this suite. 11/17/23 14:52:10.469
------------------------------
â€¢ [3.319 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:07.157
    Nov 17 14:52:07.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename init-container 11/17/23 14:52:07.158
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:07.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:07.177
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 11/17/23 14:52:07.18
    Nov 17 14:52:07.180: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:10.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4688" for this suite. 11/17/23 14:52:10.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:10.485
Nov 17 14:52:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 14:52:10.487
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:10.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:10.509
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 11/17/23 14:52:10.542
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:52:10.55
Nov 17 14:52:10.555: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:10.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 14:52:10.559: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:11.565: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:11.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 14:52:11.569: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:12.565: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:12.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 14:52:12.574: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 11/17/23 14:52:12.581
Nov 17 14:52:12.632: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:12.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 14:52:12.637: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:13.656: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 14:52:13.684: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:14.642: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:14.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 14:52:14.646: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:15.642: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:15.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 14:52:15.648: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 14:52:16.643: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 14:52:16.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 14:52:16.652: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 14:52:16.68
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8770, will wait for the garbage collector to delete the pods 11/17/23 14:52:16.681
Nov 17 14:52:16.747: INFO: Deleting DaemonSet.extensions daemon-set took: 9.543538ms
Nov 17 14:52:16.948: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.401602ms
Nov 17 14:52:18.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 14:52:18.753: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 14:52:18.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52314"},"items":null}

Nov 17 14:52:18.759: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52314"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:18.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8770" for this suite. 11/17/23 14:52:18.779
------------------------------
â€¢ [SLOW TEST] [8.304 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:10.485
    Nov 17 14:52:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 14:52:10.487
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:10.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:10.509
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 11/17/23 14:52:10.542
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 14:52:10.55
    Nov 17 14:52:10.555: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:10.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 14:52:10.559: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:11.565: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:11.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 14:52:11.569: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:12.565: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:12.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 14:52:12.574: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 11/17/23 14:52:12.581
    Nov 17 14:52:12.632: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:12.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 14:52:12.637: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:13.656: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:13.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 14:52:13.684: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:14.642: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:14.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 14:52:14.646: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:15.642: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:15.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 14:52:15.648: INFO: Node k8s-worker-3.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 14:52:16.643: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 14:52:16.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 14:52:16.652: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 14:52:16.68
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8770, will wait for the garbage collector to delete the pods 11/17/23 14:52:16.681
    Nov 17 14:52:16.747: INFO: Deleting DaemonSet.extensions daemon-set took: 9.543538ms
    Nov 17 14:52:16.948: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.401602ms
    Nov 17 14:52:18.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 14:52:18.753: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 14:52:18.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52314"},"items":null}

    Nov 17 14:52:18.759: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52314"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:18.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8770" for this suite. 11/17/23 14:52:18.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:18.792
Nov 17 14:52:18.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:52:18.794
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:18.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:18.827
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 11/17/23 14:52:18.831
Nov 17 14:52:18.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8230 create -f -'
Nov 17 14:52:19.413: INFO: stderr: ""
Nov 17 14:52:19.414: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/17/23 14:52:19.414
Nov 17 14:52:20.418: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:52:20.418: INFO: Found 1 / 1
Nov 17 14:52:20.418: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 11/17/23 14:52:20.418
Nov 17 14:52:20.421: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:52:20.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 17 14:52:20.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8230 patch pod agnhost-primary-phmng -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 17 14:52:20.523: INFO: stderr: ""
Nov 17 14:52:20.523: INFO: stdout: "pod/agnhost-primary-phmng patched\n"
STEP: checking annotations 11/17/23 14:52:20.523
Nov 17 14:52:20.527: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 14:52:20.527: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:20.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8230" for this suite. 11/17/23 14:52:20.531
------------------------------
â€¢ [1.745 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:18.792
    Nov 17 14:52:18.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:52:18.794
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:18.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:18.827
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 11/17/23 14:52:18.831
    Nov 17 14:52:18.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8230 create -f -'
    Nov 17 14:52:19.413: INFO: stderr: ""
    Nov 17 14:52:19.414: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/17/23 14:52:19.414
    Nov 17 14:52:20.418: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:52:20.418: INFO: Found 1 / 1
    Nov 17 14:52:20.418: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 11/17/23 14:52:20.418
    Nov 17 14:52:20.421: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:52:20.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 17 14:52:20.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-8230 patch pod agnhost-primary-phmng -p {"metadata":{"annotations":{"x":"y"}}}'
    Nov 17 14:52:20.523: INFO: stderr: ""
    Nov 17 14:52:20.523: INFO: stdout: "pod/agnhost-primary-phmng patched\n"
    STEP: checking annotations 11/17/23 14:52:20.523
    Nov 17 14:52:20.527: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 14:52:20.527: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:20.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8230" for this suite. 11/17/23 14:52:20.531
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:20.538
Nov 17 14:52:20.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:52:20.54
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:20.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:20.559
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 11/17/23 14:52:20.562
STEP: submitting the pod to kubernetes 11/17/23 14:52:20.562
STEP: verifying QOS class is set on the pod 11/17/23 14:52:20.57
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:20.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6510" for this suite. 11/17/23 14:52:20.587
------------------------------
â€¢ [0.057 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:20.538
    Nov 17 14:52:20.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:52:20.54
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:20.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:20.559
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 11/17/23 14:52:20.562
    STEP: submitting the pod to kubernetes 11/17/23 14:52:20.562
    STEP: verifying QOS class is set on the pod 11/17/23 14:52:20.57
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:20.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6510" for this suite. 11/17/23 14:52:20.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:20.597
Nov 17 14:52:20.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 14:52:20.599
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:20.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:20.668
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:52:20.671
Nov 17 14:52:20.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-1036 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Nov 17 14:52:20.763: INFO: stderr: ""
Nov 17 14:52:20.763: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 11/17/23 14:52:20.763
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Nov 17 14:52:20.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-1036 delete pods e2e-test-httpd-pod'
Nov 17 14:52:23.438: INFO: stderr: ""
Nov 17 14:52:23.439: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:23.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1036" for this suite. 11/17/23 14:52:23.444
------------------------------
â€¢ [2.857 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:20.597
    Nov 17 14:52:20.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 14:52:20.599
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:20.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:20.668
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 11/17/23 14:52:20.671
    Nov 17 14:52:20.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-1036 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Nov 17 14:52:20.763: INFO: stderr: ""
    Nov 17 14:52:20.763: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 11/17/23 14:52:20.763
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Nov 17 14:52:20.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-1036 delete pods e2e-test-httpd-pod'
    Nov 17 14:52:23.438: INFO: stderr: ""
    Nov 17 14:52:23.439: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:23.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1036" for this suite. 11/17/23 14:52:23.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:23.456
Nov 17 14:52:23.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 14:52:23.457
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:23.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:23.486
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-943.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 11/17/23 14:52:23.491
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-943.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 11/17/23 14:52:23.491
STEP: creating a pod to probe /etc/hosts 11/17/23 14:52:23.491
STEP: submitting the pod to kubernetes 11/17/23 14:52:23.491
Nov 17 14:52:23.504: INFO: Waiting up to 15m0s for pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7" in namespace "dns-943" to be "running"
Nov 17 14:52:23.511: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737776ms
Nov 17 14:52:25.515: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01102414s
Nov 17 14:52:25.516: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7" satisfied condition "running"
STEP: retrieving the pod 11/17/23 14:52:25.516
STEP: looking for the results for each expected name from probers 11/17/23 14:52:25.52
Nov 17 14:52:25.542: INFO: DNS probes using dns-943/dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7 succeeded

STEP: deleting the pod 11/17/23 14:52:25.542
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:25.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-943" for this suite. 11/17/23 14:52:25.574
------------------------------
â€¢ [2.135 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:23.456
    Nov 17 14:52:23.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 14:52:23.457
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:23.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:23.486
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-943.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     11/17/23 14:52:23.491
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-943.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-943.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     11/17/23 14:52:23.491
    STEP: creating a pod to probe /etc/hosts 11/17/23 14:52:23.491
    STEP: submitting the pod to kubernetes 11/17/23 14:52:23.491
    Nov 17 14:52:23.504: INFO: Waiting up to 15m0s for pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7" in namespace "dns-943" to be "running"
    Nov 17 14:52:23.511: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737776ms
    Nov 17 14:52:25.515: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01102414s
    Nov 17 14:52:25.516: INFO: Pod "dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 14:52:25.516
    STEP: looking for the results for each expected name from probers 11/17/23 14:52:25.52
    Nov 17 14:52:25.542: INFO: DNS probes using dns-943/dns-test-8d2a56f7-b44a-455f-aa16-77f4d9e95be7 succeeded

    STEP: deleting the pod 11/17/23 14:52:25.542
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:25.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-943" for this suite. 11/17/23 14:52:25.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:25.594
Nov 17 14:52:25.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename containers 11/17/23 14:52:25.595
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:25.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:25.635
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 11/17/23 14:52:25.637
Nov 17 14:52:25.649: INFO: Waiting up to 5m0s for pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359" in namespace "containers-2956" to be "Succeeded or Failed"
Nov 17 14:52:25.655: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081822ms
Nov 17 14:52:27.661: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011820959s
Nov 17 14:52:29.659: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009916523s
STEP: Saw pod success 11/17/23 14:52:29.659
Nov 17 14:52:29.659: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359" satisfied condition "Succeeded or Failed"
Nov 17 14:52:29.662: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 14:52:29.671
Nov 17 14:52:29.681: INFO: Waiting for pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 to disappear
Nov 17 14:52:29.684: INFO: Pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:29.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2956" for this suite. 11/17/23 14:52:29.688
------------------------------
â€¢ [4.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:25.594
    Nov 17 14:52:25.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename containers 11/17/23 14:52:25.595
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:25.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:25.635
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 11/17/23 14:52:25.637
    Nov 17 14:52:25.649: INFO: Waiting up to 5m0s for pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359" in namespace "containers-2956" to be "Succeeded or Failed"
    Nov 17 14:52:25.655: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081822ms
    Nov 17 14:52:27.661: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011820959s
    Nov 17 14:52:29.659: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009916523s
    STEP: Saw pod success 11/17/23 14:52:29.659
    Nov 17 14:52:29.659: INFO: Pod "client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359" satisfied condition "Succeeded or Failed"
    Nov 17 14:52:29.662: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 14:52:29.671
    Nov 17 14:52:29.681: INFO: Waiting for pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 to disappear
    Nov 17 14:52:29.684: INFO: Pod client-containers-d3b72c66-40e9-4a41-b86e-1869f33e7359 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:29.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2956" for this suite. 11/17/23 14:52:29.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:29.696
Nov 17 14:52:29.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 14:52:29.697
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:29.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:29.72
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Nov 17 14:52:29.723: INFO: Creating ReplicaSet my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce
Nov 17 14:52:29.734: INFO: Pod name my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Found 0 pods out of 1
Nov 17 14:52:34.739: INFO: Pod name my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Found 1 pods out of 1
Nov 17 14:52:34.739: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce" is running
Nov 17 14:52:34.739: INFO: Waiting up to 5m0s for pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" in namespace "replicaset-1757" to be "running"
Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf": Phase="Running", Reason="", readiness=true. Elapsed: 2.793503ms
Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" satisfied condition "running"
Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:29 +0000 UTC Reason: Message:}])
Nov 17 14:52:34.742: INFO: Trying to dial the pod
Nov 17 14:52:39.751: INFO: Controller my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Got expected result from replica 1 [my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf]: "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:39.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1757" for this suite. 11/17/23 14:52:39.757
------------------------------
â€¢ [SLOW TEST] [10.068 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:29.696
    Nov 17 14:52:29.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 14:52:29.697
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:29.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:29.72
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Nov 17 14:52:29.723: INFO: Creating ReplicaSet my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce
    Nov 17 14:52:29.734: INFO: Pod name my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Found 0 pods out of 1
    Nov 17 14:52:34.739: INFO: Pod name my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Found 1 pods out of 1
    Nov 17 14:52:34.739: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce" is running
    Nov 17 14:52:34.739: INFO: Waiting up to 5m0s for pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" in namespace "replicaset-1757" to be "running"
    Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf": Phase="Running", Reason="", readiness=true. Elapsed: 2.793503ms
    Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" satisfied condition "running"
    Nov 17 14:52:34.742: INFO: Pod "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-11-17 14:52:29 +0000 UTC Reason: Message:}])
    Nov 17 14:52:34.742: INFO: Trying to dial the pod
    Nov 17 14:52:39.751: INFO: Controller my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce: Got expected result from replica 1 [my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf]: "my-hostname-basic-807b3e57-b80a-4faf-b421-0d109cca7dce-95xjf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:39.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1757" for this suite. 11/17/23 14:52:39.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:39.765
Nov 17 14:52:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:52:39.767
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:39.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:39.787
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9568 11/17/23 14:52:39.79
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/17/23 14:52:39.811
STEP: creating service externalsvc in namespace services-9568 11/17/23 14:52:39.812
STEP: creating replication controller externalsvc in namespace services-9568 11/17/23 14:52:39.838
I1117 14:52:39.848274      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9568, replica count: 2
I1117 14:52:42.898975      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 11/17/23 14:52:42.902
Nov 17 14:52:42.923: INFO: Creating new exec pod
Nov 17 14:52:42.932: INFO: Waiting up to 5m0s for pod "execpod78ft5" in namespace "services-9568" to be "running"
Nov 17 14:52:42.936: INFO: Pod "execpod78ft5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865673ms
Nov 17 14:52:44.942: INFO: Pod "execpod78ft5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010376208s
Nov 17 14:52:44.942: INFO: Pod "execpod78ft5" satisfied condition "running"
Nov 17 14:52:44.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9568 exec execpod78ft5 -- /bin/sh -x -c nslookup nodeport-service.services-9568.svc.cluster.local'
Nov 17 14:52:45.234: INFO: stderr: "+ nslookup nodeport-service.services-9568.svc.cluster.local\n"
Nov 17 14:52:45.234: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9568.svc.cluster.local\tcanonical name = externalsvc.services-9568.svc.cluster.local.\nName:\texternalsvc.services-9568.svc.cluster.local\nAddress: 10.98.177.8\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9568, will wait for the garbage collector to delete the pods 11/17/23 14:52:45.234
Nov 17 14:52:45.297: INFO: Deleting ReplicationController externalsvc took: 8.02834ms
Nov 17 14:52:45.397: INFO: Terminating ReplicationController externalsvc pods took: 100.934699ms
Nov 17 14:52:47.634: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:47.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9568" for this suite. 11/17/23 14:52:47.666
------------------------------
â€¢ [SLOW TEST] [7.909 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:39.765
    Nov 17 14:52:39.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:52:39.767
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:39.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:39.787
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9568 11/17/23 14:52:39.79
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 11/17/23 14:52:39.811
    STEP: creating service externalsvc in namespace services-9568 11/17/23 14:52:39.812
    STEP: creating replication controller externalsvc in namespace services-9568 11/17/23 14:52:39.838
    I1117 14:52:39.848274      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9568, replica count: 2
    I1117 14:52:42.898975      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 11/17/23 14:52:42.902
    Nov 17 14:52:42.923: INFO: Creating new exec pod
    Nov 17 14:52:42.932: INFO: Waiting up to 5m0s for pod "execpod78ft5" in namespace "services-9568" to be "running"
    Nov 17 14:52:42.936: INFO: Pod "execpod78ft5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865673ms
    Nov 17 14:52:44.942: INFO: Pod "execpod78ft5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010376208s
    Nov 17 14:52:44.942: INFO: Pod "execpod78ft5" satisfied condition "running"
    Nov 17 14:52:44.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9568 exec execpod78ft5 -- /bin/sh -x -c nslookup nodeport-service.services-9568.svc.cluster.local'
    Nov 17 14:52:45.234: INFO: stderr: "+ nslookup nodeport-service.services-9568.svc.cluster.local\n"
    Nov 17 14:52:45.234: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9568.svc.cluster.local\tcanonical name = externalsvc.services-9568.svc.cluster.local.\nName:\texternalsvc.services-9568.svc.cluster.local\nAddress: 10.98.177.8\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9568, will wait for the garbage collector to delete the pods 11/17/23 14:52:45.234
    Nov 17 14:52:45.297: INFO: Deleting ReplicationController externalsvc took: 8.02834ms
    Nov 17 14:52:45.397: INFO: Terminating ReplicationController externalsvc pods took: 100.934699ms
    Nov 17 14:52:47.634: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:47.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9568" for this suite. 11/17/23 14:52:47.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:47.677
Nov 17 14:52:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 14:52:47.679
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:47.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:47.704
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Nov 17 14:52:47.745: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bbeba0d7-da4a-4c11-921e-2ce6ffc480ff", Controller:(*bool)(0xc0037864e6), BlockOwnerDeletion:(*bool)(0xc0037864e7)}}
Nov 17 14:52:47.759: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dd2c06c0-4d4c-4c35-96f4-acd27bf30e5f", Controller:(*bool)(0xc00378670a), BlockOwnerDeletion:(*bool)(0xc00378670b)}}
Nov 17 14:52:47.769: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8a31af79-9bca-41ae-b1c9-802e9b7b238f", Controller:(*bool)(0xc00d0a912a), BlockOwnerDeletion:(*bool)(0xc00d0a912b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 14:52:52.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5063" for this suite. 11/17/23 14:52:52.789
------------------------------
â€¢ [SLOW TEST] [5.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:47.677
    Nov 17 14:52:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 14:52:47.679
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:47.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:47.704
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Nov 17 14:52:47.745: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bbeba0d7-da4a-4c11-921e-2ce6ffc480ff", Controller:(*bool)(0xc0037864e6), BlockOwnerDeletion:(*bool)(0xc0037864e7)}}
    Nov 17 14:52:47.759: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dd2c06c0-4d4c-4c35-96f4-acd27bf30e5f", Controller:(*bool)(0xc00378670a), BlockOwnerDeletion:(*bool)(0xc00378670b)}}
    Nov 17 14:52:47.769: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8a31af79-9bca-41ae-b1c9-802e9b7b238f", Controller:(*bool)(0xc00d0a912a), BlockOwnerDeletion:(*bool)(0xc00d0a912b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:52:52.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5063" for this suite. 11/17/23 14:52:52.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:52:52.803
Nov 17 14:52:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 14:52:52.805
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:52.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:52.832
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5718 11/17/23 14:52:52.837
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 11/17/23 14:52:52.849
Nov 17 14:52:52.869: INFO: Found 0 stateful pods, waiting for 3
Nov 17 14:53:02.875: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:53:02.875: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:53:02.875: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/17/23 14:53:02.884
Nov 17 14:53:02.906: INFO: Updating stateful set ss2
STEP: Creating a new revision 11/17/23 14:53:02.906
STEP: Not applying an update when the partition is greater than the number of replicas 11/17/23 14:53:12.927
STEP: Performing a canary update 11/17/23 14:53:12.927
Nov 17 14:53:12.953: INFO: Updating stateful set ss2
Nov 17 14:53:12.968: INFO: Waiting for Pod statefulset-5718/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 11/17/23 14:53:22.983
Nov 17 14:53:23.106: INFO: Found 1 stateful pods, waiting for 3
Nov 17 14:53:33.110: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:53:33.110: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 17 14:53:33.110: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 11/17/23 14:53:33.118
Nov 17 14:53:33.139: INFO: Updating stateful set ss2
Nov 17 14:53:33.147: INFO: Waiting for Pod statefulset-5718/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Nov 17 14:53:43.182: INFO: Updating stateful set ss2
Nov 17 14:53:43.189: INFO: Waiting for StatefulSet statefulset-5718/ss2 to complete update
Nov 17 14:53:43.189: INFO: Waiting for Pod statefulset-5718/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 14:53:53.209: INFO: Deleting all statefulset in ns statefulset-5718
Nov 17 14:53:53.225: INFO: Scaling statefulset ss2 to 0
Nov 17 14:54:03.350: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 14:54:03.353: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:54:03.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5718" for this suite. 11/17/23 14:54:03.406
------------------------------
â€¢ [SLOW TEST] [70.618 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:52:52.803
    Nov 17 14:52:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 14:52:52.805
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:52:52.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:52:52.832
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5718 11/17/23 14:52:52.837
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 11/17/23 14:52:52.849
    Nov 17 14:52:52.869: INFO: Found 0 stateful pods, waiting for 3
    Nov 17 14:53:02.875: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:53:02.875: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:53:02.875: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 11/17/23 14:53:02.884
    Nov 17 14:53:02.906: INFO: Updating stateful set ss2
    STEP: Creating a new revision 11/17/23 14:53:02.906
    STEP: Not applying an update when the partition is greater than the number of replicas 11/17/23 14:53:12.927
    STEP: Performing a canary update 11/17/23 14:53:12.927
    Nov 17 14:53:12.953: INFO: Updating stateful set ss2
    Nov 17 14:53:12.968: INFO: Waiting for Pod statefulset-5718/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 11/17/23 14:53:22.983
    Nov 17 14:53:23.106: INFO: Found 1 stateful pods, waiting for 3
    Nov 17 14:53:33.110: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:53:33.110: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Nov 17 14:53:33.110: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 11/17/23 14:53:33.118
    Nov 17 14:53:33.139: INFO: Updating stateful set ss2
    Nov 17 14:53:33.147: INFO: Waiting for Pod statefulset-5718/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Nov 17 14:53:43.182: INFO: Updating stateful set ss2
    Nov 17 14:53:43.189: INFO: Waiting for StatefulSet statefulset-5718/ss2 to complete update
    Nov 17 14:53:43.189: INFO: Waiting for Pod statefulset-5718/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 14:53:53.209: INFO: Deleting all statefulset in ns statefulset-5718
    Nov 17 14:53:53.225: INFO: Scaling statefulset ss2 to 0
    Nov 17 14:54:03.350: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 14:54:03.353: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:54:03.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5718" for this suite. 11/17/23 14:54:03.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:54:03.424
Nov 17 14:54:03.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svc-latency 11/17/23 14:54:03.425
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:03.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:03.46
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Nov 17 14:54:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8433 11/17/23 14:54:03.464
I1117 14:54:03.474290      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8433, replica count: 1
I1117 14:54:04.525729      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1117 14:54:05.526128      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 14:54:05.653: INFO: Created: latency-svc-dlh2f
Nov 17 14:54:05.666: INFO: Got endpoints: latency-svc-dlh2f [39.298977ms]
Nov 17 14:54:05.693: INFO: Created: latency-svc-c7tk8
Nov 17 14:54:05.710: INFO: Created: latency-svc-9bg2b
Nov 17 14:54:05.711: INFO: Got endpoints: latency-svc-c7tk8 [44.791664ms]
Nov 17 14:54:05.727: INFO: Got endpoints: latency-svc-9bg2b [60.041163ms]
Nov 17 14:54:05.735: INFO: Created: latency-svc-w9sz9
Nov 17 14:54:05.740: INFO: Got endpoints: latency-svc-w9sz9 [73.897273ms]
Nov 17 14:54:05.770: INFO: Created: latency-svc-9f9xh
Nov 17 14:54:05.784: INFO: Got endpoints: latency-svc-9f9xh [116.934447ms]
Nov 17 14:54:05.799: INFO: Created: latency-svc-pq7c5
Nov 17 14:54:05.815: INFO: Got endpoints: latency-svc-pq7c5 [148.179743ms]
Nov 17 14:54:05.817: INFO: Created: latency-svc-9mchs
Nov 17 14:54:05.829: INFO: Got endpoints: latency-svc-9mchs [162.283666ms]
Nov 17 14:54:05.841: INFO: Created: latency-svc-v8tnp
Nov 17 14:54:05.850: INFO: Got endpoints: latency-svc-v8tnp [182.907163ms]
Nov 17 14:54:05.866: INFO: Created: latency-svc-snwkf
Nov 17 14:54:05.875: INFO: Got endpoints: latency-svc-snwkf [207.744325ms]
Nov 17 14:54:05.888: INFO: Created: latency-svc-mkbkn
Nov 17 14:54:05.899: INFO: Got endpoints: latency-svc-mkbkn [231.870488ms]
Nov 17 14:54:05.916: INFO: Created: latency-svc-jh2f5
Nov 17 14:54:05.933: INFO: Got endpoints: latency-svc-jh2f5 [267.037987ms]
Nov 17 14:54:05.947: INFO: Created: latency-svc-5rhmr
Nov 17 14:54:05.956: INFO: Got endpoints: latency-svc-5rhmr [288.798051ms]
Nov 17 14:54:05.967: INFO: Created: latency-svc-hg2d7
Nov 17 14:54:05.985: INFO: Got endpoints: latency-svc-hg2d7 [318.016686ms]
Nov 17 14:54:05.991: INFO: Created: latency-svc-g8r54
Nov 17 14:54:06.005: INFO: Got endpoints: latency-svc-g8r54 [337.150199ms]
Nov 17 14:54:06.039: INFO: Created: latency-svc-dwsh2
Nov 17 14:54:06.054: INFO: Got endpoints: latency-svc-dwsh2 [388.124056ms]
Nov 17 14:54:06.081: INFO: Created: latency-svc-jsdkj
Nov 17 14:54:06.104: INFO: Created: latency-svc-d28sf
Nov 17 14:54:06.107: INFO: Got endpoints: latency-svc-jsdkj [438.980034ms]
Nov 17 14:54:06.132: INFO: Got endpoints: latency-svc-d28sf [420.459874ms]
Nov 17 14:54:06.157: INFO: Created: latency-svc-vzxxg
Nov 17 14:54:06.174: INFO: Got endpoints: latency-svc-vzxxg [447.537792ms]
Nov 17 14:54:06.176: INFO: Created: latency-svc-426gq
Nov 17 14:54:06.191: INFO: Got endpoints: latency-svc-426gq [450.782696ms]
Nov 17 14:54:06.205: INFO: Created: latency-svc-ndpq2
Nov 17 14:54:06.216: INFO: Got endpoints: latency-svc-ndpq2 [431.596101ms]
Nov 17 14:54:06.232: INFO: Created: latency-svc-mskhh
Nov 17 14:54:06.241: INFO: Got endpoints: latency-svc-mskhh [426.044772ms]
Nov 17 14:54:06.254: INFO: Created: latency-svc-z5djr
Nov 17 14:54:06.276: INFO: Got endpoints: latency-svc-z5djr [446.709067ms]
Nov 17 14:54:06.286: INFO: Created: latency-svc-qjj9p
Nov 17 14:54:06.313: INFO: Got endpoints: latency-svc-qjj9p [463.535742ms]
Nov 17 14:54:06.315: INFO: Created: latency-svc-m7dq8
Nov 17 14:54:06.343: INFO: Got endpoints: latency-svc-m7dq8 [468.074459ms]
Nov 17 14:54:06.351: INFO: Created: latency-svc-jdjsz
Nov 17 14:54:06.369: INFO: Got endpoints: latency-svc-jdjsz [469.400165ms]
Nov 17 14:54:06.375: INFO: Created: latency-svc-sdhvq
Nov 17 14:54:06.396: INFO: Got endpoints: latency-svc-sdhvq [462.86009ms]
Nov 17 14:54:06.400: INFO: Created: latency-svc-4tlcw
Nov 17 14:54:06.420: INFO: Got endpoints: latency-svc-4tlcw [464.335003ms]
Nov 17 14:54:06.438: INFO: Created: latency-svc-794m6
Nov 17 14:54:06.450: INFO: Got endpoints: latency-svc-794m6 [464.513956ms]
Nov 17 14:54:06.461: INFO: Created: latency-svc-wml4m
Nov 17 14:54:06.476: INFO: Got endpoints: latency-svc-wml4m [471.492939ms]
Nov 17 14:54:06.487: INFO: Created: latency-svc-sk8rj
Nov 17 14:54:06.500: INFO: Got endpoints: latency-svc-sk8rj [445.501167ms]
Nov 17 14:54:06.506: INFO: Created: latency-svc-gwkfr
Nov 17 14:54:06.519: INFO: Got endpoints: latency-svc-gwkfr [412.876371ms]
Nov 17 14:54:06.534: INFO: Created: latency-svc-mlrwz
Nov 17 14:54:06.547: INFO: Created: latency-svc-22xbd
Nov 17 14:54:06.548: INFO: Got endpoints: latency-svc-mlrwz [415.989407ms]
Nov 17 14:54:06.572: INFO: Got endpoints: latency-svc-22xbd [397.241283ms]
Nov 17 14:54:06.590: INFO: Created: latency-svc-7ttkm
Nov 17 14:54:06.598: INFO: Got endpoints: latency-svc-7ttkm [406.683958ms]
Nov 17 14:54:06.609: INFO: Created: latency-svc-vpkcf
Nov 17 14:54:06.624: INFO: Got endpoints: latency-svc-vpkcf [407.953286ms]
Nov 17 14:54:06.633: INFO: Created: latency-svc-w5p5b
Nov 17 14:54:06.649: INFO: Got endpoints: latency-svc-w5p5b [407.740821ms]
Nov 17 14:54:06.657: INFO: Created: latency-svc-kv885
Nov 17 14:54:06.698: INFO: Got endpoints: latency-svc-kv885 [422.047394ms]
Nov 17 14:54:06.726: INFO: Created: latency-svc-ch97h
Nov 17 14:54:06.741: INFO: Got endpoints: latency-svc-ch97h [427.292788ms]
Nov 17 14:54:06.753: INFO: Created: latency-svc-99qxh
Nov 17 14:54:06.768: INFO: Got endpoints: latency-svc-99qxh [424.265581ms]
Nov 17 14:54:06.790: INFO: Created: latency-svc-rcgph
Nov 17 14:54:06.794: INFO: Got endpoints: latency-svc-rcgph [425.121458ms]
Nov 17 14:54:06.839: INFO: Created: latency-svc-pk6r6
Nov 17 14:54:06.847: INFO: Got endpoints: latency-svc-pk6r6 [450.33966ms]
Nov 17 14:54:06.868: INFO: Created: latency-svc-5cqx4
Nov 17 14:54:06.881: INFO: Created: latency-svc-44sxc
Nov 17 14:54:06.907: INFO: Got endpoints: latency-svc-5cqx4 [486.145704ms]
Nov 17 14:54:06.918: INFO: Got endpoints: latency-svc-44sxc [467.833253ms]
Nov 17 14:54:06.928: INFO: Created: latency-svc-bkzbw
Nov 17 14:54:06.945: INFO: Created: latency-svc-xcckb
Nov 17 14:54:06.951: INFO: Got endpoints: latency-svc-bkzbw [474.661576ms]
Nov 17 14:54:06.960: INFO: Got endpoints: latency-svc-xcckb [460.133091ms]
Nov 17 14:54:06.971: INFO: Created: latency-svc-cwtzz
Nov 17 14:54:06.978: INFO: Got endpoints: latency-svc-cwtzz [458.269901ms]
Nov 17 14:54:06.990: INFO: Created: latency-svc-cxj4g
Nov 17 14:54:06.996: INFO: Got endpoints: latency-svc-cxj4g [447.649107ms]
Nov 17 14:54:07.004: INFO: Created: latency-svc-zkkm2
Nov 17 14:54:07.013: INFO: Got endpoints: latency-svc-zkkm2 [440.76535ms]
Nov 17 14:54:07.024: INFO: Created: latency-svc-hxd4d
Nov 17 14:54:07.044: INFO: Created: latency-svc-6fkxg
Nov 17 14:54:07.044: INFO: Got endpoints: latency-svc-hxd4d [446.391506ms]
Nov 17 14:54:07.052: INFO: Got endpoints: latency-svc-6fkxg [428.513961ms]
Nov 17 14:54:07.062: INFO: Created: latency-svc-9nss7
Nov 17 14:54:07.075: INFO: Got endpoints: latency-svc-9nss7 [425.874242ms]
Nov 17 14:54:07.080: INFO: Created: latency-svc-bs2fc
Nov 17 14:54:07.090: INFO: Got endpoints: latency-svc-bs2fc [391.523478ms]
Nov 17 14:54:07.098: INFO: Created: latency-svc-5krn7
Nov 17 14:54:07.109: INFO: Got endpoints: latency-svc-5krn7 [368.478124ms]
Nov 17 14:54:07.118: INFO: Created: latency-svc-lpxfp
Nov 17 14:54:07.129: INFO: Got endpoints: latency-svc-lpxfp [361.320028ms]
Nov 17 14:54:07.133: INFO: Created: latency-svc-7p9vp
Nov 17 14:54:07.140: INFO: Got endpoints: latency-svc-7p9vp [346.51892ms]
Nov 17 14:54:07.148: INFO: Created: latency-svc-vxlms
Nov 17 14:54:07.162: INFO: Got endpoints: latency-svc-vxlms [315.412117ms]
Nov 17 14:54:07.170: INFO: Created: latency-svc-2zn5t
Nov 17 14:54:07.181: INFO: Got endpoints: latency-svc-2zn5t [274.74983ms]
Nov 17 14:54:07.190: INFO: Created: latency-svc-wxk9b
Nov 17 14:54:07.201: INFO: Got endpoints: latency-svc-wxk9b [282.600789ms]
Nov 17 14:54:07.216: INFO: Created: latency-svc-sm5rh
Nov 17 14:54:07.227: INFO: Got endpoints: latency-svc-sm5rh [275.59054ms]
Nov 17 14:54:07.244: INFO: Created: latency-svc-ctsd4
Nov 17 14:54:07.251: INFO: Got endpoints: latency-svc-ctsd4 [291.24369ms]
Nov 17 14:54:07.257: INFO: Created: latency-svc-jp7q5
Nov 17 14:54:07.271: INFO: Got endpoints: latency-svc-jp7q5 [293.552656ms]
Nov 17 14:54:07.281: INFO: Created: latency-svc-2t6w7
Nov 17 14:54:07.290: INFO: Got endpoints: latency-svc-2t6w7 [294.079601ms]
Nov 17 14:54:07.300: INFO: Created: latency-svc-qkz5w
Nov 17 14:54:07.315: INFO: Got endpoints: latency-svc-qkz5w [302.414306ms]
Nov 17 14:54:07.457: INFO: Created: latency-svc-tr9pq
Nov 17 14:54:07.466: INFO: Created: latency-svc-cz2dv
Nov 17 14:54:07.467: INFO: Created: latency-svc-qx442
Nov 17 14:54:07.467: INFO: Created: latency-svc-s5pql
Nov 17 14:54:07.467: INFO: Created: latency-svc-xs5mw
Nov 17 14:54:07.468: INFO: Created: latency-svc-dhwfm
Nov 17 14:54:07.468: INFO: Created: latency-svc-dbzsw
Nov 17 14:54:07.468: INFO: Created: latency-svc-22fq7
Nov 17 14:54:07.473: INFO: Created: latency-svc-2kkwq
Nov 17 14:54:07.486: INFO: Created: latency-svc-b7d7q
Nov 17 14:54:07.488: INFO: Created: latency-svc-xt5ln
Nov 17 14:54:07.488: INFO: Created: latency-svc-sqdmt
Nov 17 14:54:07.489: INFO: Created: latency-svc-vgphp
Nov 17 14:54:07.491: INFO: Created: latency-svc-zt6bx
Nov 17 14:54:07.491: INFO: Created: latency-svc-jnnlz
Nov 17 14:54:07.497: INFO: Got endpoints: latency-svc-tr9pq [387.451485ms]
Nov 17 14:54:07.503: INFO: Got endpoints: latency-svc-cz2dv [362.836254ms]
Nov 17 14:54:07.504: INFO: Got endpoints: latency-svc-s5pql [374.47674ms]
Nov 17 14:54:07.521: INFO: Got endpoints: latency-svc-22fq7 [205.822456ms]
Nov 17 14:54:07.531: INFO: Created: latency-svc-4rj8m
Nov 17 14:54:07.546: INFO: Created: latency-svc-d8wkp
Nov 17 14:54:07.562: INFO: Got endpoints: latency-svc-dbzsw [399.161254ms]
Nov 17 14:54:07.569: INFO: Created: latency-svc-mpd7x
Nov 17 14:54:07.603: INFO: Created: latency-svc-fk99z
Nov 17 14:54:07.614: INFO: Got endpoints: latency-svc-xs5mw [324.567811ms]
Nov 17 14:54:07.623: INFO: Created: latency-svc-qddzr
Nov 17 14:54:07.639: INFO: Created: latency-svc-jbpqj
Nov 17 14:54:07.662: INFO: Got endpoints: latency-svc-dhwfm [617.841452ms]
Nov 17 14:54:07.682: INFO: Created: latency-svc-jbj5j
Nov 17 14:54:07.716: INFO: Got endpoints: latency-svc-2kkwq [534.070143ms]
Nov 17 14:54:07.743: INFO: Created: latency-svc-szrlh
Nov 17 14:54:07.761: INFO: Got endpoints: latency-svc-qx442 [509.255464ms]
Nov 17 14:54:07.785: INFO: Created: latency-svc-4mcb7
Nov 17 14:54:07.819: INFO: Got endpoints: latency-svc-xt5ln [744.173628ms]
Nov 17 14:54:07.844: INFO: Created: latency-svc-8p2ln
Nov 17 14:54:07.871: INFO: Got endpoints: latency-svc-vgphp [781.689261ms]
Nov 17 14:54:07.891: INFO: Created: latency-svc-7x27n
Nov 17 14:54:07.912: INFO: Got endpoints: latency-svc-zt6bx [860.117998ms]
Nov 17 14:54:07.937: INFO: Created: latency-svc-d46cq
Nov 17 14:54:07.963: INFO: Got endpoints: latency-svc-jnnlz [691.87614ms]
Nov 17 14:54:07.986: INFO: Created: latency-svc-4xcdd
Nov 17 14:54:08.017: INFO: Got endpoints: latency-svc-b7d7q [789.862872ms]
Nov 17 14:54:08.053: INFO: Created: latency-svc-tpktj
Nov 17 14:54:08.065: INFO: Got endpoints: latency-svc-sqdmt [864.624335ms]
Nov 17 14:54:08.089: INFO: Created: latency-svc-97sq2
Nov 17 14:54:08.112: INFO: Got endpoints: latency-svc-4rj8m [615.688199ms]
Nov 17 14:54:08.145: INFO: Created: latency-svc-kpbbm
Nov 17 14:54:08.162: INFO: Got endpoints: latency-svc-d8wkp [658.720393ms]
Nov 17 14:54:08.197: INFO: Created: latency-svc-mkb6r
Nov 17 14:54:08.218: INFO: Got endpoints: latency-svc-mpd7x [714.292642ms]
Nov 17 14:54:08.249: INFO: Created: latency-svc-9hrnk
Nov 17 14:54:08.264: INFO: Got endpoints: latency-svc-fk99z [742.85775ms]
Nov 17 14:54:08.287: INFO: Created: latency-svc-vzqnd
Nov 17 14:54:08.314: INFO: Got endpoints: latency-svc-qddzr [752.015022ms]
Nov 17 14:54:08.344: INFO: Created: latency-svc-xnpqk
Nov 17 14:54:08.366: INFO: Got endpoints: latency-svc-jbpqj [751.76893ms]
Nov 17 14:54:08.388: INFO: Created: latency-svc-24zm2
Nov 17 14:54:08.411: INFO: Got endpoints: latency-svc-jbj5j [748.925914ms]
Nov 17 14:54:08.432: INFO: Created: latency-svc-958c5
Nov 17 14:54:08.475: INFO: Got endpoints: latency-svc-szrlh [758.913323ms]
Nov 17 14:54:08.502: INFO: Created: latency-svc-lpgrn
Nov 17 14:54:08.514: INFO: Got endpoints: latency-svc-4mcb7 [752.769325ms]
Nov 17 14:54:08.540: INFO: Created: latency-svc-q94qk
Nov 17 14:54:08.570: INFO: Got endpoints: latency-svc-8p2ln [750.990564ms]
Nov 17 14:54:08.599: INFO: Created: latency-svc-srqvz
Nov 17 14:54:08.620: INFO: Got endpoints: latency-svc-7x27n [748.144892ms]
Nov 17 14:54:08.647: INFO: Created: latency-svc-qfpch
Nov 17 14:54:08.665: INFO: Got endpoints: latency-svc-d46cq [752.41699ms]
Nov 17 14:54:08.698: INFO: Created: latency-svc-6md25
Nov 17 14:54:08.722: INFO: Got endpoints: latency-svc-4xcdd [758.922724ms]
Nov 17 14:54:08.818: INFO: Got endpoints: latency-svc-tpktj [801.098485ms]
Nov 17 14:54:08.830: INFO: Got endpoints: latency-svc-97sq2 [764.698881ms]
Nov 17 14:54:08.840: INFO: Created: latency-svc-lqhtp
Nov 17 14:54:08.876: INFO: Got endpoints: latency-svc-kpbbm [763.194677ms]
Nov 17 14:54:08.895: INFO: Created: latency-svc-7fpq9
Nov 17 14:54:08.935: INFO: Got endpoints: latency-svc-mkb6r [772.181357ms]
Nov 17 14:54:08.947: INFO: Created: latency-svc-2sg7h
Nov 17 14:54:08.981: INFO: Got endpoints: latency-svc-9hrnk [763.475804ms]
Nov 17 14:54:08.990: INFO: Created: latency-svc-jx72t
Nov 17 14:54:09.018: INFO: Created: latency-svc-45hz8
Nov 17 14:54:09.034: INFO: Got endpoints: latency-svc-vzqnd [770.121901ms]
Nov 17 14:54:09.049: INFO: Created: latency-svc-tpqkf
Nov 17 14:54:09.064: INFO: Got endpoints: latency-svc-xnpqk [750.293594ms]
Nov 17 14:54:09.080: INFO: Created: latency-svc-rk7r9
Nov 17 14:54:09.093: INFO: Created: latency-svc-x2vj5
Nov 17 14:54:09.113: INFO: Got endpoints: latency-svc-24zm2 [746.393142ms]
Nov 17 14:54:09.139: INFO: Created: latency-svc-bljf7
Nov 17 14:54:09.162: INFO: Got endpoints: latency-svc-958c5 [750.83377ms]
Nov 17 14:54:09.193: INFO: Created: latency-svc-lggh2
Nov 17 14:54:09.214: INFO: Got endpoints: latency-svc-lpgrn [739.11649ms]
Nov 17 14:54:09.238: INFO: Created: latency-svc-lqv7d
Nov 17 14:54:09.262: INFO: Got endpoints: latency-svc-q94qk [748.283201ms]
Nov 17 14:54:09.304: INFO: Created: latency-svc-xwr24
Nov 17 14:54:09.319: INFO: Got endpoints: latency-svc-srqvz [749.056592ms]
Nov 17 14:54:09.344: INFO: Created: latency-svc-wfcbb
Nov 17 14:54:09.363: INFO: Got endpoints: latency-svc-qfpch [743.55829ms]
Nov 17 14:54:09.390: INFO: Created: latency-svc-85bpj
Nov 17 14:54:09.420: INFO: Got endpoints: latency-svc-6md25 [754.705261ms]
Nov 17 14:54:09.440: INFO: Created: latency-svc-2qwrg
Nov 17 14:54:09.464: INFO: Got endpoints: latency-svc-lqhtp [741.562284ms]
Nov 17 14:54:09.485: INFO: Created: latency-svc-4xf4d
Nov 17 14:54:09.514: INFO: Got endpoints: latency-svc-7fpq9 [696.264916ms]
Nov 17 14:54:09.555: INFO: Created: latency-svc-5rnml
Nov 17 14:54:09.604: INFO: Got endpoints: latency-svc-2sg7h [773.666404ms]
Nov 17 14:54:09.630: INFO: Got endpoints: latency-svc-jx72t [753.660382ms]
Nov 17 14:54:09.655: INFO: Created: latency-svc-f2kxd
Nov 17 14:54:09.680: INFO: Got endpoints: latency-svc-45hz8 [745.542509ms]
Nov 17 14:54:09.691: INFO: Created: latency-svc-vvcwj
Nov 17 14:54:09.724: INFO: Got endpoints: latency-svc-tpqkf [742.395112ms]
Nov 17 14:54:09.724: INFO: Created: latency-svc-88hrb
Nov 17 14:54:09.768: INFO: Got endpoints: latency-svc-rk7r9 [733.592756ms]
Nov 17 14:54:09.768: INFO: Created: latency-svc-mg9jw
Nov 17 14:54:09.812: INFO: Created: latency-svc-qwj95
Nov 17 14:54:09.833: INFO: Got endpoints: latency-svc-x2vj5 [768.300289ms]
Nov 17 14:54:09.874: INFO: Got endpoints: latency-svc-bljf7 [760.865415ms]
Nov 17 14:54:09.879: INFO: Created: latency-svc-qcphv
Nov 17 14:54:09.903: INFO: Created: latency-svc-r8jn9
Nov 17 14:54:09.920: INFO: Got endpoints: latency-svc-lggh2 [757.75701ms]
Nov 17 14:54:09.950: INFO: Created: latency-svc-qf7h9
Nov 17 14:54:09.967: INFO: Got endpoints: latency-svc-lqv7d [752.775152ms]
Nov 17 14:54:09.998: INFO: Created: latency-svc-bs7jq
Nov 17 14:54:10.017: INFO: Got endpoints: latency-svc-xwr24 [754.560553ms]
Nov 17 14:54:10.057: INFO: Created: latency-svc-wpq77
Nov 17 14:54:10.065: INFO: Got endpoints: latency-svc-wfcbb [746.236176ms]
Nov 17 14:54:10.086: INFO: Created: latency-svc-l5st4
Nov 17 14:54:10.114: INFO: Got endpoints: latency-svc-85bpj [750.688226ms]
Nov 17 14:54:10.139: INFO: Created: latency-svc-6l5ks
Nov 17 14:54:10.160: INFO: Got endpoints: latency-svc-2qwrg [740.456936ms]
Nov 17 14:54:10.180: INFO: Created: latency-svc-t49tf
Nov 17 14:54:10.214: INFO: Got endpoints: latency-svc-4xf4d [750.334857ms]
Nov 17 14:54:10.244: INFO: Created: latency-svc-z4s2m
Nov 17 14:54:10.261: INFO: Got endpoints: latency-svc-5rnml [746.943223ms]
Nov 17 14:54:10.289: INFO: Created: latency-svc-mthdn
Nov 17 14:54:10.310: INFO: Got endpoints: latency-svc-f2kxd [706.738111ms]
Nov 17 14:54:10.333: INFO: Created: latency-svc-l7k52
Nov 17 14:54:10.366: INFO: Got endpoints: latency-svc-vvcwj [736.11215ms]
Nov 17 14:54:10.387: INFO: Created: latency-svc-wwxrr
Nov 17 14:54:10.413: INFO: Got endpoints: latency-svc-88hrb [732.356457ms]
Nov 17 14:54:10.433: INFO: Created: latency-svc-twdlj
Nov 17 14:54:10.468: INFO: Got endpoints: latency-svc-mg9jw [743.840213ms]
Nov 17 14:54:10.491: INFO: Created: latency-svc-b8f6z
Nov 17 14:54:10.510: INFO: Got endpoints: latency-svc-qwj95 [742.426322ms]
Nov 17 14:54:10.535: INFO: Created: latency-svc-xxp6c
Nov 17 14:54:10.565: INFO: Got endpoints: latency-svc-qcphv [732.08822ms]
Nov 17 14:54:10.588: INFO: Created: latency-svc-tbrzp
Nov 17 14:54:10.614: INFO: Got endpoints: latency-svc-r8jn9 [740.596797ms]
Nov 17 14:54:10.637: INFO: Created: latency-svc-jg2nb
Nov 17 14:54:10.661: INFO: Got endpoints: latency-svc-qf7h9 [740.694764ms]
Nov 17 14:54:10.686: INFO: Created: latency-svc-w4659
Nov 17 14:54:10.716: INFO: Got endpoints: latency-svc-bs7jq [749.125778ms]
Nov 17 14:54:10.758: INFO: Created: latency-svc-6tnjq
Nov 17 14:54:10.771: INFO: Got endpoints: latency-svc-wpq77 [754.796844ms]
Nov 17 14:54:10.794: INFO: Created: latency-svc-8ndv8
Nov 17 14:54:10.815: INFO: Got endpoints: latency-svc-l5st4 [749.99359ms]
Nov 17 14:54:10.838: INFO: Created: latency-svc-w4wwv
Nov 17 14:54:10.865: INFO: Got endpoints: latency-svc-6l5ks [750.826824ms]
Nov 17 14:54:10.890: INFO: Created: latency-svc-2lc82
Nov 17 14:54:10.912: INFO: Got endpoints: latency-svc-t49tf [751.527321ms]
Nov 17 14:54:10.935: INFO: Created: latency-svc-n6z88
Nov 17 14:54:10.964: INFO: Got endpoints: latency-svc-z4s2m [749.863418ms]
Nov 17 14:54:11.017: INFO: Created: latency-svc-n8vbz
Nov 17 14:54:11.019: INFO: Got endpoints: latency-svc-mthdn [757.995695ms]
Nov 17 14:54:11.082: INFO: Created: latency-svc-8t7z8
Nov 17 14:54:11.091: INFO: Got endpoints: latency-svc-l7k52 [780.601074ms]
Nov 17 14:54:11.130: INFO: Got endpoints: latency-svc-wwxrr [763.685597ms]
Nov 17 14:54:11.176: INFO: Created: latency-svc-7pvzz
Nov 17 14:54:11.259: INFO: Created: latency-svc-ljb7q
Nov 17 14:54:11.260: INFO: Got endpoints: latency-svc-twdlj [847.051449ms]
Nov 17 14:54:11.282: INFO: Got endpoints: latency-svc-b8f6z [814.543229ms]
Nov 17 14:54:11.336: INFO: Got endpoints: latency-svc-xxp6c [825.905534ms]
Nov 17 14:54:11.338: INFO: Created: latency-svc-ltvsf
Nov 17 14:54:11.383: INFO: Created: latency-svc-4jc29
Nov 17 14:54:11.383: INFO: Got endpoints: latency-svc-tbrzp [818.626088ms]
Nov 17 14:54:11.405: INFO: Created: latency-svc-zrpdd
Nov 17 14:54:11.447: INFO: Got endpoints: latency-svc-jg2nb [832.350266ms]
Nov 17 14:54:11.476: INFO: Created: latency-svc-qj8kl
Nov 17 14:54:11.497: INFO: Created: latency-svc-44rzl
Nov 17 14:54:11.498: INFO: Got endpoints: latency-svc-w4659 [836.665282ms]
Nov 17 14:54:11.528: INFO: Got endpoints: latency-svc-6tnjq [811.708292ms]
Nov 17 14:54:11.573: INFO: Created: latency-svc-9v7qc
Nov 17 14:54:11.591: INFO: Got endpoints: latency-svc-8ndv8 [819.434075ms]
Nov 17 14:54:11.628: INFO: Created: latency-svc-969ct
Nov 17 14:54:11.638: INFO: Got endpoints: latency-svc-w4wwv [822.147287ms]
Nov 17 14:54:11.663: INFO: Created: latency-svc-cjtmt
Nov 17 14:54:11.703: INFO: Got endpoints: latency-svc-2lc82 [838.137166ms]
Nov 17 14:54:11.755: INFO: Created: latency-svc-klwlc
Nov 17 14:54:11.773: INFO: Got endpoints: latency-svc-n6z88 [860.752014ms]
Nov 17 14:54:11.808: INFO: Got endpoints: latency-svc-n8vbz [843.547066ms]
Nov 17 14:54:11.848: INFO: Created: latency-svc-fdgk8
Nov 17 14:54:11.849: INFO: Got endpoints: latency-svc-8t7z8 [830.067187ms]
Nov 17 14:54:11.896: INFO: Got endpoints: latency-svc-7pvzz [804.850989ms]
Nov 17 14:54:11.911: INFO: Created: latency-svc-xf6fn
Nov 17 14:54:11.957: INFO: Got endpoints: latency-svc-ljb7q [827.134679ms]
Nov 17 14:54:11.972: INFO: Created: latency-svc-t28xc
Nov 17 14:54:11.989: INFO: Got endpoints: latency-svc-ltvsf [729.041735ms]
Nov 17 14:54:12.023: INFO: Got endpoints: latency-svc-4jc29 [740.976854ms]
Nov 17 14:54:12.044: INFO: Created: latency-svc-b6wkc
Nov 17 14:54:12.084: INFO: Got endpoints: latency-svc-zrpdd [747.76066ms]
Nov 17 14:54:12.102: INFO: Created: latency-svc-sln29
Nov 17 14:54:12.135: INFO: Got endpoints: latency-svc-qj8kl [751.42155ms]
Nov 17 14:54:12.136: INFO: Created: latency-svc-2v8jz
Nov 17 14:54:12.151: INFO: Created: latency-svc-t4sjp
Nov 17 14:54:12.192: INFO: Got endpoints: latency-svc-44rzl [744.648975ms]
Nov 17 14:54:12.204: INFO: Created: latency-svc-zp8ls
Nov 17 14:54:12.231: INFO: Got endpoints: latency-svc-9v7qc [733.356855ms]
Nov 17 14:54:12.263: INFO: Created: latency-svc-hv6j6
Nov 17 14:54:12.276: INFO: Got endpoints: latency-svc-969ct [747.925896ms]
Nov 17 14:54:12.287: INFO: Created: latency-svc-2fkqg
Nov 17 14:54:12.303: INFO: Created: latency-svc-49w4b
Nov 17 14:54:12.317: INFO: Got endpoints: latency-svc-cjtmt [725.516431ms]
Nov 17 14:54:12.328: INFO: Created: latency-svc-tz9z9
Nov 17 14:54:12.343: INFO: Created: latency-svc-5g5nx
Nov 17 14:54:12.355: INFO: Created: latency-svc-xspb5
Nov 17 14:54:12.365: INFO: Got endpoints: latency-svc-klwlc [727.014568ms]
Nov 17 14:54:12.384: INFO: Created: latency-svc-6lkqn
Nov 17 14:54:12.416: INFO: Got endpoints: latency-svc-fdgk8 [713.203804ms]
Nov 17 14:54:12.436: INFO: Created: latency-svc-s7v9f
Nov 17 14:54:12.465: INFO: Got endpoints: latency-svc-xf6fn [692.761582ms]
Nov 17 14:54:12.484: INFO: Created: latency-svc-zmtmm
Nov 17 14:54:12.510: INFO: Got endpoints: latency-svc-t28xc [701.810683ms]
Nov 17 14:54:12.534: INFO: Created: latency-svc-2d7tv
Nov 17 14:54:12.566: INFO: Got endpoints: latency-svc-b6wkc [716.638486ms]
Nov 17 14:54:12.580: INFO: Created: latency-svc-qdfjf
Nov 17 14:54:12.617: INFO: Got endpoints: latency-svc-sln29 [720.70607ms]
Nov 17 14:54:12.633: INFO: Created: latency-svc-rlhrv
Nov 17 14:54:12.664: INFO: Got endpoints: latency-svc-2v8jz [706.660206ms]
Nov 17 14:54:12.683: INFO: Created: latency-svc-5fbnc
Nov 17 14:54:12.718: INFO: Got endpoints: latency-svc-t4sjp [728.565659ms]
Nov 17 14:54:12.739: INFO: Created: latency-svc-4mk5l
Nov 17 14:54:12.763: INFO: Got endpoints: latency-svc-zp8ls [739.306586ms]
Nov 17 14:54:12.783: INFO: Created: latency-svc-b2sv9
Nov 17 14:54:12.818: INFO: Got endpoints: latency-svc-hv6j6 [734.1367ms]
Nov 17 14:54:12.839: INFO: Created: latency-svc-k9682
Nov 17 14:54:12.867: INFO: Got endpoints: latency-svc-2fkqg [732.29694ms]
Nov 17 14:54:12.886: INFO: Created: latency-svc-j4n4z
Nov 17 14:54:12.915: INFO: Got endpoints: latency-svc-49w4b [723.092121ms]
Nov 17 14:54:12.933: INFO: Created: latency-svc-dc6w2
Nov 17 14:54:12.961: INFO: Got endpoints: latency-svc-tz9z9 [729.315689ms]
Nov 17 14:54:12.982: INFO: Created: latency-svc-b54hk
Nov 17 14:54:13.012: INFO: Got endpoints: latency-svc-5g5nx [735.823844ms]
Nov 17 14:54:13.028: INFO: Created: latency-svc-79wdr
Nov 17 14:54:13.064: INFO: Got endpoints: latency-svc-xspb5 [746.808059ms]
Nov 17 14:54:13.083: INFO: Created: latency-svc-gl5kd
Nov 17 14:54:13.114: INFO: Got endpoints: latency-svc-6lkqn [749.406432ms]
Nov 17 14:54:13.133: INFO: Created: latency-svc-kvfjk
Nov 17 14:54:13.166: INFO: Got endpoints: latency-svc-s7v9f [749.404479ms]
Nov 17 14:54:13.186: INFO: Created: latency-svc-lfskv
Nov 17 14:54:13.213: INFO: Got endpoints: latency-svc-zmtmm [747.950132ms]
Nov 17 14:54:13.233: INFO: Created: latency-svc-qhmvx
Nov 17 14:54:13.263: INFO: Got endpoints: latency-svc-2d7tv [753.158936ms]
Nov 17 14:54:13.284: INFO: Created: latency-svc-whjld
Nov 17 14:54:13.314: INFO: Got endpoints: latency-svc-qdfjf [748.225463ms]
Nov 17 14:54:13.333: INFO: Created: latency-svc-c5kd6
Nov 17 14:54:13.369: INFO: Got endpoints: latency-svc-rlhrv [752.47905ms]
Nov 17 14:54:13.394: INFO: Created: latency-svc-vwzln
Nov 17 14:54:13.413: INFO: Got endpoints: latency-svc-5fbnc [749.710411ms]
Nov 17 14:54:13.435: INFO: Created: latency-svc-8cv2z
Nov 17 14:54:13.464: INFO: Got endpoints: latency-svc-4mk5l [746.118806ms]
Nov 17 14:54:13.488: INFO: Created: latency-svc-sgb92
Nov 17 14:54:13.518: INFO: Got endpoints: latency-svc-b2sv9 [755.449043ms]
Nov 17 14:54:13.538: INFO: Created: latency-svc-qq896
Nov 17 14:54:13.565: INFO: Got endpoints: latency-svc-k9682 [746.520376ms]
Nov 17 14:54:13.612: INFO: Got endpoints: latency-svc-j4n4z [744.370222ms]
Nov 17 14:54:13.664: INFO: Got endpoints: latency-svc-dc6w2 [749.383444ms]
Nov 17 14:54:13.718: INFO: Got endpoints: latency-svc-b54hk [757.014168ms]
Nov 17 14:54:13.772: INFO: Got endpoints: latency-svc-79wdr [759.577765ms]
Nov 17 14:54:13.823: INFO: Got endpoints: latency-svc-gl5kd [759.801553ms]
Nov 17 14:54:13.864: INFO: Got endpoints: latency-svc-kvfjk [749.835678ms]
Nov 17 14:54:13.915: INFO: Got endpoints: latency-svc-lfskv [749.108267ms]
Nov 17 14:54:13.962: INFO: Got endpoints: latency-svc-qhmvx [748.732169ms]
Nov 17 14:54:14.014: INFO: Got endpoints: latency-svc-whjld [751.162014ms]
Nov 17 14:54:14.062: INFO: Got endpoints: latency-svc-c5kd6 [747.221002ms]
Nov 17 14:54:14.113: INFO: Got endpoints: latency-svc-vwzln [743.099588ms]
Nov 17 14:54:14.165: INFO: Got endpoints: latency-svc-8cv2z [752.069838ms]
Nov 17 14:54:14.213: INFO: Got endpoints: latency-svc-sgb92 [749.655944ms]
Nov 17 14:54:14.263: INFO: Got endpoints: latency-svc-qq896 [744.525559ms]
Nov 17 14:54:14.263: INFO: Latencies: [44.791664ms 60.041163ms 73.897273ms 116.934447ms 148.179743ms 162.283666ms 182.907163ms 205.822456ms 207.744325ms 231.870488ms 267.037987ms 274.74983ms 275.59054ms 282.600789ms 288.798051ms 291.24369ms 293.552656ms 294.079601ms 302.414306ms 315.412117ms 318.016686ms 324.567811ms 337.150199ms 346.51892ms 361.320028ms 362.836254ms 368.478124ms 374.47674ms 387.451485ms 388.124056ms 391.523478ms 397.241283ms 399.161254ms 406.683958ms 407.740821ms 407.953286ms 412.876371ms 415.989407ms 420.459874ms 422.047394ms 424.265581ms 425.121458ms 425.874242ms 426.044772ms 427.292788ms 428.513961ms 431.596101ms 438.980034ms 440.76535ms 445.501167ms 446.391506ms 446.709067ms 447.537792ms 447.649107ms 450.33966ms 450.782696ms 458.269901ms 460.133091ms 462.86009ms 463.535742ms 464.335003ms 464.513956ms 467.833253ms 468.074459ms 469.400165ms 471.492939ms 474.661576ms 486.145704ms 509.255464ms 534.070143ms 615.688199ms 617.841452ms 658.720393ms 691.87614ms 692.761582ms 696.264916ms 701.810683ms 706.660206ms 706.738111ms 713.203804ms 714.292642ms 716.638486ms 720.70607ms 723.092121ms 725.516431ms 727.014568ms 728.565659ms 729.041735ms 729.315689ms 732.08822ms 732.29694ms 732.356457ms 733.356855ms 733.592756ms 734.1367ms 735.823844ms 736.11215ms 739.11649ms 739.306586ms 740.456936ms 740.596797ms 740.694764ms 740.976854ms 741.562284ms 742.395112ms 742.426322ms 742.85775ms 743.099588ms 743.55829ms 743.840213ms 744.173628ms 744.370222ms 744.525559ms 744.648975ms 745.542509ms 746.118806ms 746.236176ms 746.393142ms 746.520376ms 746.808059ms 746.943223ms 747.221002ms 747.76066ms 747.925896ms 747.950132ms 748.144892ms 748.225463ms 748.283201ms 748.732169ms 748.925914ms 749.056592ms 749.108267ms 749.125778ms 749.383444ms 749.404479ms 749.406432ms 749.655944ms 749.710411ms 749.835678ms 749.863418ms 749.99359ms 750.293594ms 750.334857ms 750.688226ms 750.826824ms 750.83377ms 750.990564ms 751.162014ms 751.42155ms 751.527321ms 751.76893ms 752.015022ms 752.069838ms 752.41699ms 752.47905ms 752.769325ms 752.775152ms 753.158936ms 753.660382ms 754.560553ms 754.705261ms 754.796844ms 755.449043ms 757.014168ms 757.75701ms 757.995695ms 758.913323ms 758.922724ms 759.577765ms 759.801553ms 760.865415ms 763.194677ms 763.475804ms 763.685597ms 764.698881ms 768.300289ms 770.121901ms 772.181357ms 773.666404ms 780.601074ms 781.689261ms 789.862872ms 801.098485ms 804.850989ms 811.708292ms 814.543229ms 818.626088ms 819.434075ms 822.147287ms 825.905534ms 827.134679ms 830.067187ms 832.350266ms 836.665282ms 838.137166ms 843.547066ms 847.051449ms 860.117998ms 860.752014ms 864.624335ms]
Nov 17 14:54:14.263: INFO: 50 %ile: 740.596797ms
Nov 17 14:54:14.263: INFO: 90 %ile: 781.689261ms
Nov 17 14:54:14.263: INFO: 99 %ile: 860.752014ms
Nov 17 14:54:14.263: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Nov 17 14:54:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-8433" for this suite. 11/17/23 14:54:14.27
------------------------------
â€¢ [SLOW TEST] [10.858 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:54:03.424
    Nov 17 14:54:03.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svc-latency 11/17/23 14:54:03.425
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:03.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:03.46
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Nov 17 14:54:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8433 11/17/23 14:54:03.464
    I1117 14:54:03.474290      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8433, replica count: 1
    I1117 14:54:04.525729      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1117 14:54:05.526128      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 14:54:05.653: INFO: Created: latency-svc-dlh2f
    Nov 17 14:54:05.666: INFO: Got endpoints: latency-svc-dlh2f [39.298977ms]
    Nov 17 14:54:05.693: INFO: Created: latency-svc-c7tk8
    Nov 17 14:54:05.710: INFO: Created: latency-svc-9bg2b
    Nov 17 14:54:05.711: INFO: Got endpoints: latency-svc-c7tk8 [44.791664ms]
    Nov 17 14:54:05.727: INFO: Got endpoints: latency-svc-9bg2b [60.041163ms]
    Nov 17 14:54:05.735: INFO: Created: latency-svc-w9sz9
    Nov 17 14:54:05.740: INFO: Got endpoints: latency-svc-w9sz9 [73.897273ms]
    Nov 17 14:54:05.770: INFO: Created: latency-svc-9f9xh
    Nov 17 14:54:05.784: INFO: Got endpoints: latency-svc-9f9xh [116.934447ms]
    Nov 17 14:54:05.799: INFO: Created: latency-svc-pq7c5
    Nov 17 14:54:05.815: INFO: Got endpoints: latency-svc-pq7c5 [148.179743ms]
    Nov 17 14:54:05.817: INFO: Created: latency-svc-9mchs
    Nov 17 14:54:05.829: INFO: Got endpoints: latency-svc-9mchs [162.283666ms]
    Nov 17 14:54:05.841: INFO: Created: latency-svc-v8tnp
    Nov 17 14:54:05.850: INFO: Got endpoints: latency-svc-v8tnp [182.907163ms]
    Nov 17 14:54:05.866: INFO: Created: latency-svc-snwkf
    Nov 17 14:54:05.875: INFO: Got endpoints: latency-svc-snwkf [207.744325ms]
    Nov 17 14:54:05.888: INFO: Created: latency-svc-mkbkn
    Nov 17 14:54:05.899: INFO: Got endpoints: latency-svc-mkbkn [231.870488ms]
    Nov 17 14:54:05.916: INFO: Created: latency-svc-jh2f5
    Nov 17 14:54:05.933: INFO: Got endpoints: latency-svc-jh2f5 [267.037987ms]
    Nov 17 14:54:05.947: INFO: Created: latency-svc-5rhmr
    Nov 17 14:54:05.956: INFO: Got endpoints: latency-svc-5rhmr [288.798051ms]
    Nov 17 14:54:05.967: INFO: Created: latency-svc-hg2d7
    Nov 17 14:54:05.985: INFO: Got endpoints: latency-svc-hg2d7 [318.016686ms]
    Nov 17 14:54:05.991: INFO: Created: latency-svc-g8r54
    Nov 17 14:54:06.005: INFO: Got endpoints: latency-svc-g8r54 [337.150199ms]
    Nov 17 14:54:06.039: INFO: Created: latency-svc-dwsh2
    Nov 17 14:54:06.054: INFO: Got endpoints: latency-svc-dwsh2 [388.124056ms]
    Nov 17 14:54:06.081: INFO: Created: latency-svc-jsdkj
    Nov 17 14:54:06.104: INFO: Created: latency-svc-d28sf
    Nov 17 14:54:06.107: INFO: Got endpoints: latency-svc-jsdkj [438.980034ms]
    Nov 17 14:54:06.132: INFO: Got endpoints: latency-svc-d28sf [420.459874ms]
    Nov 17 14:54:06.157: INFO: Created: latency-svc-vzxxg
    Nov 17 14:54:06.174: INFO: Got endpoints: latency-svc-vzxxg [447.537792ms]
    Nov 17 14:54:06.176: INFO: Created: latency-svc-426gq
    Nov 17 14:54:06.191: INFO: Got endpoints: latency-svc-426gq [450.782696ms]
    Nov 17 14:54:06.205: INFO: Created: latency-svc-ndpq2
    Nov 17 14:54:06.216: INFO: Got endpoints: latency-svc-ndpq2 [431.596101ms]
    Nov 17 14:54:06.232: INFO: Created: latency-svc-mskhh
    Nov 17 14:54:06.241: INFO: Got endpoints: latency-svc-mskhh [426.044772ms]
    Nov 17 14:54:06.254: INFO: Created: latency-svc-z5djr
    Nov 17 14:54:06.276: INFO: Got endpoints: latency-svc-z5djr [446.709067ms]
    Nov 17 14:54:06.286: INFO: Created: latency-svc-qjj9p
    Nov 17 14:54:06.313: INFO: Got endpoints: latency-svc-qjj9p [463.535742ms]
    Nov 17 14:54:06.315: INFO: Created: latency-svc-m7dq8
    Nov 17 14:54:06.343: INFO: Got endpoints: latency-svc-m7dq8 [468.074459ms]
    Nov 17 14:54:06.351: INFO: Created: latency-svc-jdjsz
    Nov 17 14:54:06.369: INFO: Got endpoints: latency-svc-jdjsz [469.400165ms]
    Nov 17 14:54:06.375: INFO: Created: latency-svc-sdhvq
    Nov 17 14:54:06.396: INFO: Got endpoints: latency-svc-sdhvq [462.86009ms]
    Nov 17 14:54:06.400: INFO: Created: latency-svc-4tlcw
    Nov 17 14:54:06.420: INFO: Got endpoints: latency-svc-4tlcw [464.335003ms]
    Nov 17 14:54:06.438: INFO: Created: latency-svc-794m6
    Nov 17 14:54:06.450: INFO: Got endpoints: latency-svc-794m6 [464.513956ms]
    Nov 17 14:54:06.461: INFO: Created: latency-svc-wml4m
    Nov 17 14:54:06.476: INFO: Got endpoints: latency-svc-wml4m [471.492939ms]
    Nov 17 14:54:06.487: INFO: Created: latency-svc-sk8rj
    Nov 17 14:54:06.500: INFO: Got endpoints: latency-svc-sk8rj [445.501167ms]
    Nov 17 14:54:06.506: INFO: Created: latency-svc-gwkfr
    Nov 17 14:54:06.519: INFO: Got endpoints: latency-svc-gwkfr [412.876371ms]
    Nov 17 14:54:06.534: INFO: Created: latency-svc-mlrwz
    Nov 17 14:54:06.547: INFO: Created: latency-svc-22xbd
    Nov 17 14:54:06.548: INFO: Got endpoints: latency-svc-mlrwz [415.989407ms]
    Nov 17 14:54:06.572: INFO: Got endpoints: latency-svc-22xbd [397.241283ms]
    Nov 17 14:54:06.590: INFO: Created: latency-svc-7ttkm
    Nov 17 14:54:06.598: INFO: Got endpoints: latency-svc-7ttkm [406.683958ms]
    Nov 17 14:54:06.609: INFO: Created: latency-svc-vpkcf
    Nov 17 14:54:06.624: INFO: Got endpoints: latency-svc-vpkcf [407.953286ms]
    Nov 17 14:54:06.633: INFO: Created: latency-svc-w5p5b
    Nov 17 14:54:06.649: INFO: Got endpoints: latency-svc-w5p5b [407.740821ms]
    Nov 17 14:54:06.657: INFO: Created: latency-svc-kv885
    Nov 17 14:54:06.698: INFO: Got endpoints: latency-svc-kv885 [422.047394ms]
    Nov 17 14:54:06.726: INFO: Created: latency-svc-ch97h
    Nov 17 14:54:06.741: INFO: Got endpoints: latency-svc-ch97h [427.292788ms]
    Nov 17 14:54:06.753: INFO: Created: latency-svc-99qxh
    Nov 17 14:54:06.768: INFO: Got endpoints: latency-svc-99qxh [424.265581ms]
    Nov 17 14:54:06.790: INFO: Created: latency-svc-rcgph
    Nov 17 14:54:06.794: INFO: Got endpoints: latency-svc-rcgph [425.121458ms]
    Nov 17 14:54:06.839: INFO: Created: latency-svc-pk6r6
    Nov 17 14:54:06.847: INFO: Got endpoints: latency-svc-pk6r6 [450.33966ms]
    Nov 17 14:54:06.868: INFO: Created: latency-svc-5cqx4
    Nov 17 14:54:06.881: INFO: Created: latency-svc-44sxc
    Nov 17 14:54:06.907: INFO: Got endpoints: latency-svc-5cqx4 [486.145704ms]
    Nov 17 14:54:06.918: INFO: Got endpoints: latency-svc-44sxc [467.833253ms]
    Nov 17 14:54:06.928: INFO: Created: latency-svc-bkzbw
    Nov 17 14:54:06.945: INFO: Created: latency-svc-xcckb
    Nov 17 14:54:06.951: INFO: Got endpoints: latency-svc-bkzbw [474.661576ms]
    Nov 17 14:54:06.960: INFO: Got endpoints: latency-svc-xcckb [460.133091ms]
    Nov 17 14:54:06.971: INFO: Created: latency-svc-cwtzz
    Nov 17 14:54:06.978: INFO: Got endpoints: latency-svc-cwtzz [458.269901ms]
    Nov 17 14:54:06.990: INFO: Created: latency-svc-cxj4g
    Nov 17 14:54:06.996: INFO: Got endpoints: latency-svc-cxj4g [447.649107ms]
    Nov 17 14:54:07.004: INFO: Created: latency-svc-zkkm2
    Nov 17 14:54:07.013: INFO: Got endpoints: latency-svc-zkkm2 [440.76535ms]
    Nov 17 14:54:07.024: INFO: Created: latency-svc-hxd4d
    Nov 17 14:54:07.044: INFO: Created: latency-svc-6fkxg
    Nov 17 14:54:07.044: INFO: Got endpoints: latency-svc-hxd4d [446.391506ms]
    Nov 17 14:54:07.052: INFO: Got endpoints: latency-svc-6fkxg [428.513961ms]
    Nov 17 14:54:07.062: INFO: Created: latency-svc-9nss7
    Nov 17 14:54:07.075: INFO: Got endpoints: latency-svc-9nss7 [425.874242ms]
    Nov 17 14:54:07.080: INFO: Created: latency-svc-bs2fc
    Nov 17 14:54:07.090: INFO: Got endpoints: latency-svc-bs2fc [391.523478ms]
    Nov 17 14:54:07.098: INFO: Created: latency-svc-5krn7
    Nov 17 14:54:07.109: INFO: Got endpoints: latency-svc-5krn7 [368.478124ms]
    Nov 17 14:54:07.118: INFO: Created: latency-svc-lpxfp
    Nov 17 14:54:07.129: INFO: Got endpoints: latency-svc-lpxfp [361.320028ms]
    Nov 17 14:54:07.133: INFO: Created: latency-svc-7p9vp
    Nov 17 14:54:07.140: INFO: Got endpoints: latency-svc-7p9vp [346.51892ms]
    Nov 17 14:54:07.148: INFO: Created: latency-svc-vxlms
    Nov 17 14:54:07.162: INFO: Got endpoints: latency-svc-vxlms [315.412117ms]
    Nov 17 14:54:07.170: INFO: Created: latency-svc-2zn5t
    Nov 17 14:54:07.181: INFO: Got endpoints: latency-svc-2zn5t [274.74983ms]
    Nov 17 14:54:07.190: INFO: Created: latency-svc-wxk9b
    Nov 17 14:54:07.201: INFO: Got endpoints: latency-svc-wxk9b [282.600789ms]
    Nov 17 14:54:07.216: INFO: Created: latency-svc-sm5rh
    Nov 17 14:54:07.227: INFO: Got endpoints: latency-svc-sm5rh [275.59054ms]
    Nov 17 14:54:07.244: INFO: Created: latency-svc-ctsd4
    Nov 17 14:54:07.251: INFO: Got endpoints: latency-svc-ctsd4 [291.24369ms]
    Nov 17 14:54:07.257: INFO: Created: latency-svc-jp7q5
    Nov 17 14:54:07.271: INFO: Got endpoints: latency-svc-jp7q5 [293.552656ms]
    Nov 17 14:54:07.281: INFO: Created: latency-svc-2t6w7
    Nov 17 14:54:07.290: INFO: Got endpoints: latency-svc-2t6w7 [294.079601ms]
    Nov 17 14:54:07.300: INFO: Created: latency-svc-qkz5w
    Nov 17 14:54:07.315: INFO: Got endpoints: latency-svc-qkz5w [302.414306ms]
    Nov 17 14:54:07.457: INFO: Created: latency-svc-tr9pq
    Nov 17 14:54:07.466: INFO: Created: latency-svc-cz2dv
    Nov 17 14:54:07.467: INFO: Created: latency-svc-qx442
    Nov 17 14:54:07.467: INFO: Created: latency-svc-s5pql
    Nov 17 14:54:07.467: INFO: Created: latency-svc-xs5mw
    Nov 17 14:54:07.468: INFO: Created: latency-svc-dhwfm
    Nov 17 14:54:07.468: INFO: Created: latency-svc-dbzsw
    Nov 17 14:54:07.468: INFO: Created: latency-svc-22fq7
    Nov 17 14:54:07.473: INFO: Created: latency-svc-2kkwq
    Nov 17 14:54:07.486: INFO: Created: latency-svc-b7d7q
    Nov 17 14:54:07.488: INFO: Created: latency-svc-xt5ln
    Nov 17 14:54:07.488: INFO: Created: latency-svc-sqdmt
    Nov 17 14:54:07.489: INFO: Created: latency-svc-vgphp
    Nov 17 14:54:07.491: INFO: Created: latency-svc-zt6bx
    Nov 17 14:54:07.491: INFO: Created: latency-svc-jnnlz
    Nov 17 14:54:07.497: INFO: Got endpoints: latency-svc-tr9pq [387.451485ms]
    Nov 17 14:54:07.503: INFO: Got endpoints: latency-svc-cz2dv [362.836254ms]
    Nov 17 14:54:07.504: INFO: Got endpoints: latency-svc-s5pql [374.47674ms]
    Nov 17 14:54:07.521: INFO: Got endpoints: latency-svc-22fq7 [205.822456ms]
    Nov 17 14:54:07.531: INFO: Created: latency-svc-4rj8m
    Nov 17 14:54:07.546: INFO: Created: latency-svc-d8wkp
    Nov 17 14:54:07.562: INFO: Got endpoints: latency-svc-dbzsw [399.161254ms]
    Nov 17 14:54:07.569: INFO: Created: latency-svc-mpd7x
    Nov 17 14:54:07.603: INFO: Created: latency-svc-fk99z
    Nov 17 14:54:07.614: INFO: Got endpoints: latency-svc-xs5mw [324.567811ms]
    Nov 17 14:54:07.623: INFO: Created: latency-svc-qddzr
    Nov 17 14:54:07.639: INFO: Created: latency-svc-jbpqj
    Nov 17 14:54:07.662: INFO: Got endpoints: latency-svc-dhwfm [617.841452ms]
    Nov 17 14:54:07.682: INFO: Created: latency-svc-jbj5j
    Nov 17 14:54:07.716: INFO: Got endpoints: latency-svc-2kkwq [534.070143ms]
    Nov 17 14:54:07.743: INFO: Created: latency-svc-szrlh
    Nov 17 14:54:07.761: INFO: Got endpoints: latency-svc-qx442 [509.255464ms]
    Nov 17 14:54:07.785: INFO: Created: latency-svc-4mcb7
    Nov 17 14:54:07.819: INFO: Got endpoints: latency-svc-xt5ln [744.173628ms]
    Nov 17 14:54:07.844: INFO: Created: latency-svc-8p2ln
    Nov 17 14:54:07.871: INFO: Got endpoints: latency-svc-vgphp [781.689261ms]
    Nov 17 14:54:07.891: INFO: Created: latency-svc-7x27n
    Nov 17 14:54:07.912: INFO: Got endpoints: latency-svc-zt6bx [860.117998ms]
    Nov 17 14:54:07.937: INFO: Created: latency-svc-d46cq
    Nov 17 14:54:07.963: INFO: Got endpoints: latency-svc-jnnlz [691.87614ms]
    Nov 17 14:54:07.986: INFO: Created: latency-svc-4xcdd
    Nov 17 14:54:08.017: INFO: Got endpoints: latency-svc-b7d7q [789.862872ms]
    Nov 17 14:54:08.053: INFO: Created: latency-svc-tpktj
    Nov 17 14:54:08.065: INFO: Got endpoints: latency-svc-sqdmt [864.624335ms]
    Nov 17 14:54:08.089: INFO: Created: latency-svc-97sq2
    Nov 17 14:54:08.112: INFO: Got endpoints: latency-svc-4rj8m [615.688199ms]
    Nov 17 14:54:08.145: INFO: Created: latency-svc-kpbbm
    Nov 17 14:54:08.162: INFO: Got endpoints: latency-svc-d8wkp [658.720393ms]
    Nov 17 14:54:08.197: INFO: Created: latency-svc-mkb6r
    Nov 17 14:54:08.218: INFO: Got endpoints: latency-svc-mpd7x [714.292642ms]
    Nov 17 14:54:08.249: INFO: Created: latency-svc-9hrnk
    Nov 17 14:54:08.264: INFO: Got endpoints: latency-svc-fk99z [742.85775ms]
    Nov 17 14:54:08.287: INFO: Created: latency-svc-vzqnd
    Nov 17 14:54:08.314: INFO: Got endpoints: latency-svc-qddzr [752.015022ms]
    Nov 17 14:54:08.344: INFO: Created: latency-svc-xnpqk
    Nov 17 14:54:08.366: INFO: Got endpoints: latency-svc-jbpqj [751.76893ms]
    Nov 17 14:54:08.388: INFO: Created: latency-svc-24zm2
    Nov 17 14:54:08.411: INFO: Got endpoints: latency-svc-jbj5j [748.925914ms]
    Nov 17 14:54:08.432: INFO: Created: latency-svc-958c5
    Nov 17 14:54:08.475: INFO: Got endpoints: latency-svc-szrlh [758.913323ms]
    Nov 17 14:54:08.502: INFO: Created: latency-svc-lpgrn
    Nov 17 14:54:08.514: INFO: Got endpoints: latency-svc-4mcb7 [752.769325ms]
    Nov 17 14:54:08.540: INFO: Created: latency-svc-q94qk
    Nov 17 14:54:08.570: INFO: Got endpoints: latency-svc-8p2ln [750.990564ms]
    Nov 17 14:54:08.599: INFO: Created: latency-svc-srqvz
    Nov 17 14:54:08.620: INFO: Got endpoints: latency-svc-7x27n [748.144892ms]
    Nov 17 14:54:08.647: INFO: Created: latency-svc-qfpch
    Nov 17 14:54:08.665: INFO: Got endpoints: latency-svc-d46cq [752.41699ms]
    Nov 17 14:54:08.698: INFO: Created: latency-svc-6md25
    Nov 17 14:54:08.722: INFO: Got endpoints: latency-svc-4xcdd [758.922724ms]
    Nov 17 14:54:08.818: INFO: Got endpoints: latency-svc-tpktj [801.098485ms]
    Nov 17 14:54:08.830: INFO: Got endpoints: latency-svc-97sq2 [764.698881ms]
    Nov 17 14:54:08.840: INFO: Created: latency-svc-lqhtp
    Nov 17 14:54:08.876: INFO: Got endpoints: latency-svc-kpbbm [763.194677ms]
    Nov 17 14:54:08.895: INFO: Created: latency-svc-7fpq9
    Nov 17 14:54:08.935: INFO: Got endpoints: latency-svc-mkb6r [772.181357ms]
    Nov 17 14:54:08.947: INFO: Created: latency-svc-2sg7h
    Nov 17 14:54:08.981: INFO: Got endpoints: latency-svc-9hrnk [763.475804ms]
    Nov 17 14:54:08.990: INFO: Created: latency-svc-jx72t
    Nov 17 14:54:09.018: INFO: Created: latency-svc-45hz8
    Nov 17 14:54:09.034: INFO: Got endpoints: latency-svc-vzqnd [770.121901ms]
    Nov 17 14:54:09.049: INFO: Created: latency-svc-tpqkf
    Nov 17 14:54:09.064: INFO: Got endpoints: latency-svc-xnpqk [750.293594ms]
    Nov 17 14:54:09.080: INFO: Created: latency-svc-rk7r9
    Nov 17 14:54:09.093: INFO: Created: latency-svc-x2vj5
    Nov 17 14:54:09.113: INFO: Got endpoints: latency-svc-24zm2 [746.393142ms]
    Nov 17 14:54:09.139: INFO: Created: latency-svc-bljf7
    Nov 17 14:54:09.162: INFO: Got endpoints: latency-svc-958c5 [750.83377ms]
    Nov 17 14:54:09.193: INFO: Created: latency-svc-lggh2
    Nov 17 14:54:09.214: INFO: Got endpoints: latency-svc-lpgrn [739.11649ms]
    Nov 17 14:54:09.238: INFO: Created: latency-svc-lqv7d
    Nov 17 14:54:09.262: INFO: Got endpoints: latency-svc-q94qk [748.283201ms]
    Nov 17 14:54:09.304: INFO: Created: latency-svc-xwr24
    Nov 17 14:54:09.319: INFO: Got endpoints: latency-svc-srqvz [749.056592ms]
    Nov 17 14:54:09.344: INFO: Created: latency-svc-wfcbb
    Nov 17 14:54:09.363: INFO: Got endpoints: latency-svc-qfpch [743.55829ms]
    Nov 17 14:54:09.390: INFO: Created: latency-svc-85bpj
    Nov 17 14:54:09.420: INFO: Got endpoints: latency-svc-6md25 [754.705261ms]
    Nov 17 14:54:09.440: INFO: Created: latency-svc-2qwrg
    Nov 17 14:54:09.464: INFO: Got endpoints: latency-svc-lqhtp [741.562284ms]
    Nov 17 14:54:09.485: INFO: Created: latency-svc-4xf4d
    Nov 17 14:54:09.514: INFO: Got endpoints: latency-svc-7fpq9 [696.264916ms]
    Nov 17 14:54:09.555: INFO: Created: latency-svc-5rnml
    Nov 17 14:54:09.604: INFO: Got endpoints: latency-svc-2sg7h [773.666404ms]
    Nov 17 14:54:09.630: INFO: Got endpoints: latency-svc-jx72t [753.660382ms]
    Nov 17 14:54:09.655: INFO: Created: latency-svc-f2kxd
    Nov 17 14:54:09.680: INFO: Got endpoints: latency-svc-45hz8 [745.542509ms]
    Nov 17 14:54:09.691: INFO: Created: latency-svc-vvcwj
    Nov 17 14:54:09.724: INFO: Got endpoints: latency-svc-tpqkf [742.395112ms]
    Nov 17 14:54:09.724: INFO: Created: latency-svc-88hrb
    Nov 17 14:54:09.768: INFO: Got endpoints: latency-svc-rk7r9 [733.592756ms]
    Nov 17 14:54:09.768: INFO: Created: latency-svc-mg9jw
    Nov 17 14:54:09.812: INFO: Created: latency-svc-qwj95
    Nov 17 14:54:09.833: INFO: Got endpoints: latency-svc-x2vj5 [768.300289ms]
    Nov 17 14:54:09.874: INFO: Got endpoints: latency-svc-bljf7 [760.865415ms]
    Nov 17 14:54:09.879: INFO: Created: latency-svc-qcphv
    Nov 17 14:54:09.903: INFO: Created: latency-svc-r8jn9
    Nov 17 14:54:09.920: INFO: Got endpoints: latency-svc-lggh2 [757.75701ms]
    Nov 17 14:54:09.950: INFO: Created: latency-svc-qf7h9
    Nov 17 14:54:09.967: INFO: Got endpoints: latency-svc-lqv7d [752.775152ms]
    Nov 17 14:54:09.998: INFO: Created: latency-svc-bs7jq
    Nov 17 14:54:10.017: INFO: Got endpoints: latency-svc-xwr24 [754.560553ms]
    Nov 17 14:54:10.057: INFO: Created: latency-svc-wpq77
    Nov 17 14:54:10.065: INFO: Got endpoints: latency-svc-wfcbb [746.236176ms]
    Nov 17 14:54:10.086: INFO: Created: latency-svc-l5st4
    Nov 17 14:54:10.114: INFO: Got endpoints: latency-svc-85bpj [750.688226ms]
    Nov 17 14:54:10.139: INFO: Created: latency-svc-6l5ks
    Nov 17 14:54:10.160: INFO: Got endpoints: latency-svc-2qwrg [740.456936ms]
    Nov 17 14:54:10.180: INFO: Created: latency-svc-t49tf
    Nov 17 14:54:10.214: INFO: Got endpoints: latency-svc-4xf4d [750.334857ms]
    Nov 17 14:54:10.244: INFO: Created: latency-svc-z4s2m
    Nov 17 14:54:10.261: INFO: Got endpoints: latency-svc-5rnml [746.943223ms]
    Nov 17 14:54:10.289: INFO: Created: latency-svc-mthdn
    Nov 17 14:54:10.310: INFO: Got endpoints: latency-svc-f2kxd [706.738111ms]
    Nov 17 14:54:10.333: INFO: Created: latency-svc-l7k52
    Nov 17 14:54:10.366: INFO: Got endpoints: latency-svc-vvcwj [736.11215ms]
    Nov 17 14:54:10.387: INFO: Created: latency-svc-wwxrr
    Nov 17 14:54:10.413: INFO: Got endpoints: latency-svc-88hrb [732.356457ms]
    Nov 17 14:54:10.433: INFO: Created: latency-svc-twdlj
    Nov 17 14:54:10.468: INFO: Got endpoints: latency-svc-mg9jw [743.840213ms]
    Nov 17 14:54:10.491: INFO: Created: latency-svc-b8f6z
    Nov 17 14:54:10.510: INFO: Got endpoints: latency-svc-qwj95 [742.426322ms]
    Nov 17 14:54:10.535: INFO: Created: latency-svc-xxp6c
    Nov 17 14:54:10.565: INFO: Got endpoints: latency-svc-qcphv [732.08822ms]
    Nov 17 14:54:10.588: INFO: Created: latency-svc-tbrzp
    Nov 17 14:54:10.614: INFO: Got endpoints: latency-svc-r8jn9 [740.596797ms]
    Nov 17 14:54:10.637: INFO: Created: latency-svc-jg2nb
    Nov 17 14:54:10.661: INFO: Got endpoints: latency-svc-qf7h9 [740.694764ms]
    Nov 17 14:54:10.686: INFO: Created: latency-svc-w4659
    Nov 17 14:54:10.716: INFO: Got endpoints: latency-svc-bs7jq [749.125778ms]
    Nov 17 14:54:10.758: INFO: Created: latency-svc-6tnjq
    Nov 17 14:54:10.771: INFO: Got endpoints: latency-svc-wpq77 [754.796844ms]
    Nov 17 14:54:10.794: INFO: Created: latency-svc-8ndv8
    Nov 17 14:54:10.815: INFO: Got endpoints: latency-svc-l5st4 [749.99359ms]
    Nov 17 14:54:10.838: INFO: Created: latency-svc-w4wwv
    Nov 17 14:54:10.865: INFO: Got endpoints: latency-svc-6l5ks [750.826824ms]
    Nov 17 14:54:10.890: INFO: Created: latency-svc-2lc82
    Nov 17 14:54:10.912: INFO: Got endpoints: latency-svc-t49tf [751.527321ms]
    Nov 17 14:54:10.935: INFO: Created: latency-svc-n6z88
    Nov 17 14:54:10.964: INFO: Got endpoints: latency-svc-z4s2m [749.863418ms]
    Nov 17 14:54:11.017: INFO: Created: latency-svc-n8vbz
    Nov 17 14:54:11.019: INFO: Got endpoints: latency-svc-mthdn [757.995695ms]
    Nov 17 14:54:11.082: INFO: Created: latency-svc-8t7z8
    Nov 17 14:54:11.091: INFO: Got endpoints: latency-svc-l7k52 [780.601074ms]
    Nov 17 14:54:11.130: INFO: Got endpoints: latency-svc-wwxrr [763.685597ms]
    Nov 17 14:54:11.176: INFO: Created: latency-svc-7pvzz
    Nov 17 14:54:11.259: INFO: Created: latency-svc-ljb7q
    Nov 17 14:54:11.260: INFO: Got endpoints: latency-svc-twdlj [847.051449ms]
    Nov 17 14:54:11.282: INFO: Got endpoints: latency-svc-b8f6z [814.543229ms]
    Nov 17 14:54:11.336: INFO: Got endpoints: latency-svc-xxp6c [825.905534ms]
    Nov 17 14:54:11.338: INFO: Created: latency-svc-ltvsf
    Nov 17 14:54:11.383: INFO: Created: latency-svc-4jc29
    Nov 17 14:54:11.383: INFO: Got endpoints: latency-svc-tbrzp [818.626088ms]
    Nov 17 14:54:11.405: INFO: Created: latency-svc-zrpdd
    Nov 17 14:54:11.447: INFO: Got endpoints: latency-svc-jg2nb [832.350266ms]
    Nov 17 14:54:11.476: INFO: Created: latency-svc-qj8kl
    Nov 17 14:54:11.497: INFO: Created: latency-svc-44rzl
    Nov 17 14:54:11.498: INFO: Got endpoints: latency-svc-w4659 [836.665282ms]
    Nov 17 14:54:11.528: INFO: Got endpoints: latency-svc-6tnjq [811.708292ms]
    Nov 17 14:54:11.573: INFO: Created: latency-svc-9v7qc
    Nov 17 14:54:11.591: INFO: Got endpoints: latency-svc-8ndv8 [819.434075ms]
    Nov 17 14:54:11.628: INFO: Created: latency-svc-969ct
    Nov 17 14:54:11.638: INFO: Got endpoints: latency-svc-w4wwv [822.147287ms]
    Nov 17 14:54:11.663: INFO: Created: latency-svc-cjtmt
    Nov 17 14:54:11.703: INFO: Got endpoints: latency-svc-2lc82 [838.137166ms]
    Nov 17 14:54:11.755: INFO: Created: latency-svc-klwlc
    Nov 17 14:54:11.773: INFO: Got endpoints: latency-svc-n6z88 [860.752014ms]
    Nov 17 14:54:11.808: INFO: Got endpoints: latency-svc-n8vbz [843.547066ms]
    Nov 17 14:54:11.848: INFO: Created: latency-svc-fdgk8
    Nov 17 14:54:11.849: INFO: Got endpoints: latency-svc-8t7z8 [830.067187ms]
    Nov 17 14:54:11.896: INFO: Got endpoints: latency-svc-7pvzz [804.850989ms]
    Nov 17 14:54:11.911: INFO: Created: latency-svc-xf6fn
    Nov 17 14:54:11.957: INFO: Got endpoints: latency-svc-ljb7q [827.134679ms]
    Nov 17 14:54:11.972: INFO: Created: latency-svc-t28xc
    Nov 17 14:54:11.989: INFO: Got endpoints: latency-svc-ltvsf [729.041735ms]
    Nov 17 14:54:12.023: INFO: Got endpoints: latency-svc-4jc29 [740.976854ms]
    Nov 17 14:54:12.044: INFO: Created: latency-svc-b6wkc
    Nov 17 14:54:12.084: INFO: Got endpoints: latency-svc-zrpdd [747.76066ms]
    Nov 17 14:54:12.102: INFO: Created: latency-svc-sln29
    Nov 17 14:54:12.135: INFO: Got endpoints: latency-svc-qj8kl [751.42155ms]
    Nov 17 14:54:12.136: INFO: Created: latency-svc-2v8jz
    Nov 17 14:54:12.151: INFO: Created: latency-svc-t4sjp
    Nov 17 14:54:12.192: INFO: Got endpoints: latency-svc-44rzl [744.648975ms]
    Nov 17 14:54:12.204: INFO: Created: latency-svc-zp8ls
    Nov 17 14:54:12.231: INFO: Got endpoints: latency-svc-9v7qc [733.356855ms]
    Nov 17 14:54:12.263: INFO: Created: latency-svc-hv6j6
    Nov 17 14:54:12.276: INFO: Got endpoints: latency-svc-969ct [747.925896ms]
    Nov 17 14:54:12.287: INFO: Created: latency-svc-2fkqg
    Nov 17 14:54:12.303: INFO: Created: latency-svc-49w4b
    Nov 17 14:54:12.317: INFO: Got endpoints: latency-svc-cjtmt [725.516431ms]
    Nov 17 14:54:12.328: INFO: Created: latency-svc-tz9z9
    Nov 17 14:54:12.343: INFO: Created: latency-svc-5g5nx
    Nov 17 14:54:12.355: INFO: Created: latency-svc-xspb5
    Nov 17 14:54:12.365: INFO: Got endpoints: latency-svc-klwlc [727.014568ms]
    Nov 17 14:54:12.384: INFO: Created: latency-svc-6lkqn
    Nov 17 14:54:12.416: INFO: Got endpoints: latency-svc-fdgk8 [713.203804ms]
    Nov 17 14:54:12.436: INFO: Created: latency-svc-s7v9f
    Nov 17 14:54:12.465: INFO: Got endpoints: latency-svc-xf6fn [692.761582ms]
    Nov 17 14:54:12.484: INFO: Created: latency-svc-zmtmm
    Nov 17 14:54:12.510: INFO: Got endpoints: latency-svc-t28xc [701.810683ms]
    Nov 17 14:54:12.534: INFO: Created: latency-svc-2d7tv
    Nov 17 14:54:12.566: INFO: Got endpoints: latency-svc-b6wkc [716.638486ms]
    Nov 17 14:54:12.580: INFO: Created: latency-svc-qdfjf
    Nov 17 14:54:12.617: INFO: Got endpoints: latency-svc-sln29 [720.70607ms]
    Nov 17 14:54:12.633: INFO: Created: latency-svc-rlhrv
    Nov 17 14:54:12.664: INFO: Got endpoints: latency-svc-2v8jz [706.660206ms]
    Nov 17 14:54:12.683: INFO: Created: latency-svc-5fbnc
    Nov 17 14:54:12.718: INFO: Got endpoints: latency-svc-t4sjp [728.565659ms]
    Nov 17 14:54:12.739: INFO: Created: latency-svc-4mk5l
    Nov 17 14:54:12.763: INFO: Got endpoints: latency-svc-zp8ls [739.306586ms]
    Nov 17 14:54:12.783: INFO: Created: latency-svc-b2sv9
    Nov 17 14:54:12.818: INFO: Got endpoints: latency-svc-hv6j6 [734.1367ms]
    Nov 17 14:54:12.839: INFO: Created: latency-svc-k9682
    Nov 17 14:54:12.867: INFO: Got endpoints: latency-svc-2fkqg [732.29694ms]
    Nov 17 14:54:12.886: INFO: Created: latency-svc-j4n4z
    Nov 17 14:54:12.915: INFO: Got endpoints: latency-svc-49w4b [723.092121ms]
    Nov 17 14:54:12.933: INFO: Created: latency-svc-dc6w2
    Nov 17 14:54:12.961: INFO: Got endpoints: latency-svc-tz9z9 [729.315689ms]
    Nov 17 14:54:12.982: INFO: Created: latency-svc-b54hk
    Nov 17 14:54:13.012: INFO: Got endpoints: latency-svc-5g5nx [735.823844ms]
    Nov 17 14:54:13.028: INFO: Created: latency-svc-79wdr
    Nov 17 14:54:13.064: INFO: Got endpoints: latency-svc-xspb5 [746.808059ms]
    Nov 17 14:54:13.083: INFO: Created: latency-svc-gl5kd
    Nov 17 14:54:13.114: INFO: Got endpoints: latency-svc-6lkqn [749.406432ms]
    Nov 17 14:54:13.133: INFO: Created: latency-svc-kvfjk
    Nov 17 14:54:13.166: INFO: Got endpoints: latency-svc-s7v9f [749.404479ms]
    Nov 17 14:54:13.186: INFO: Created: latency-svc-lfskv
    Nov 17 14:54:13.213: INFO: Got endpoints: latency-svc-zmtmm [747.950132ms]
    Nov 17 14:54:13.233: INFO: Created: latency-svc-qhmvx
    Nov 17 14:54:13.263: INFO: Got endpoints: latency-svc-2d7tv [753.158936ms]
    Nov 17 14:54:13.284: INFO: Created: latency-svc-whjld
    Nov 17 14:54:13.314: INFO: Got endpoints: latency-svc-qdfjf [748.225463ms]
    Nov 17 14:54:13.333: INFO: Created: latency-svc-c5kd6
    Nov 17 14:54:13.369: INFO: Got endpoints: latency-svc-rlhrv [752.47905ms]
    Nov 17 14:54:13.394: INFO: Created: latency-svc-vwzln
    Nov 17 14:54:13.413: INFO: Got endpoints: latency-svc-5fbnc [749.710411ms]
    Nov 17 14:54:13.435: INFO: Created: latency-svc-8cv2z
    Nov 17 14:54:13.464: INFO: Got endpoints: latency-svc-4mk5l [746.118806ms]
    Nov 17 14:54:13.488: INFO: Created: latency-svc-sgb92
    Nov 17 14:54:13.518: INFO: Got endpoints: latency-svc-b2sv9 [755.449043ms]
    Nov 17 14:54:13.538: INFO: Created: latency-svc-qq896
    Nov 17 14:54:13.565: INFO: Got endpoints: latency-svc-k9682 [746.520376ms]
    Nov 17 14:54:13.612: INFO: Got endpoints: latency-svc-j4n4z [744.370222ms]
    Nov 17 14:54:13.664: INFO: Got endpoints: latency-svc-dc6w2 [749.383444ms]
    Nov 17 14:54:13.718: INFO: Got endpoints: latency-svc-b54hk [757.014168ms]
    Nov 17 14:54:13.772: INFO: Got endpoints: latency-svc-79wdr [759.577765ms]
    Nov 17 14:54:13.823: INFO: Got endpoints: latency-svc-gl5kd [759.801553ms]
    Nov 17 14:54:13.864: INFO: Got endpoints: latency-svc-kvfjk [749.835678ms]
    Nov 17 14:54:13.915: INFO: Got endpoints: latency-svc-lfskv [749.108267ms]
    Nov 17 14:54:13.962: INFO: Got endpoints: latency-svc-qhmvx [748.732169ms]
    Nov 17 14:54:14.014: INFO: Got endpoints: latency-svc-whjld [751.162014ms]
    Nov 17 14:54:14.062: INFO: Got endpoints: latency-svc-c5kd6 [747.221002ms]
    Nov 17 14:54:14.113: INFO: Got endpoints: latency-svc-vwzln [743.099588ms]
    Nov 17 14:54:14.165: INFO: Got endpoints: latency-svc-8cv2z [752.069838ms]
    Nov 17 14:54:14.213: INFO: Got endpoints: latency-svc-sgb92 [749.655944ms]
    Nov 17 14:54:14.263: INFO: Got endpoints: latency-svc-qq896 [744.525559ms]
    Nov 17 14:54:14.263: INFO: Latencies: [44.791664ms 60.041163ms 73.897273ms 116.934447ms 148.179743ms 162.283666ms 182.907163ms 205.822456ms 207.744325ms 231.870488ms 267.037987ms 274.74983ms 275.59054ms 282.600789ms 288.798051ms 291.24369ms 293.552656ms 294.079601ms 302.414306ms 315.412117ms 318.016686ms 324.567811ms 337.150199ms 346.51892ms 361.320028ms 362.836254ms 368.478124ms 374.47674ms 387.451485ms 388.124056ms 391.523478ms 397.241283ms 399.161254ms 406.683958ms 407.740821ms 407.953286ms 412.876371ms 415.989407ms 420.459874ms 422.047394ms 424.265581ms 425.121458ms 425.874242ms 426.044772ms 427.292788ms 428.513961ms 431.596101ms 438.980034ms 440.76535ms 445.501167ms 446.391506ms 446.709067ms 447.537792ms 447.649107ms 450.33966ms 450.782696ms 458.269901ms 460.133091ms 462.86009ms 463.535742ms 464.335003ms 464.513956ms 467.833253ms 468.074459ms 469.400165ms 471.492939ms 474.661576ms 486.145704ms 509.255464ms 534.070143ms 615.688199ms 617.841452ms 658.720393ms 691.87614ms 692.761582ms 696.264916ms 701.810683ms 706.660206ms 706.738111ms 713.203804ms 714.292642ms 716.638486ms 720.70607ms 723.092121ms 725.516431ms 727.014568ms 728.565659ms 729.041735ms 729.315689ms 732.08822ms 732.29694ms 732.356457ms 733.356855ms 733.592756ms 734.1367ms 735.823844ms 736.11215ms 739.11649ms 739.306586ms 740.456936ms 740.596797ms 740.694764ms 740.976854ms 741.562284ms 742.395112ms 742.426322ms 742.85775ms 743.099588ms 743.55829ms 743.840213ms 744.173628ms 744.370222ms 744.525559ms 744.648975ms 745.542509ms 746.118806ms 746.236176ms 746.393142ms 746.520376ms 746.808059ms 746.943223ms 747.221002ms 747.76066ms 747.925896ms 747.950132ms 748.144892ms 748.225463ms 748.283201ms 748.732169ms 748.925914ms 749.056592ms 749.108267ms 749.125778ms 749.383444ms 749.404479ms 749.406432ms 749.655944ms 749.710411ms 749.835678ms 749.863418ms 749.99359ms 750.293594ms 750.334857ms 750.688226ms 750.826824ms 750.83377ms 750.990564ms 751.162014ms 751.42155ms 751.527321ms 751.76893ms 752.015022ms 752.069838ms 752.41699ms 752.47905ms 752.769325ms 752.775152ms 753.158936ms 753.660382ms 754.560553ms 754.705261ms 754.796844ms 755.449043ms 757.014168ms 757.75701ms 757.995695ms 758.913323ms 758.922724ms 759.577765ms 759.801553ms 760.865415ms 763.194677ms 763.475804ms 763.685597ms 764.698881ms 768.300289ms 770.121901ms 772.181357ms 773.666404ms 780.601074ms 781.689261ms 789.862872ms 801.098485ms 804.850989ms 811.708292ms 814.543229ms 818.626088ms 819.434075ms 822.147287ms 825.905534ms 827.134679ms 830.067187ms 832.350266ms 836.665282ms 838.137166ms 843.547066ms 847.051449ms 860.117998ms 860.752014ms 864.624335ms]
    Nov 17 14:54:14.263: INFO: 50 %ile: 740.596797ms
    Nov 17 14:54:14.263: INFO: 90 %ile: 781.689261ms
    Nov 17 14:54:14.263: INFO: 99 %ile: 860.752014ms
    Nov 17 14:54:14.263: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:54:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-8433" for this suite. 11/17/23 14:54:14.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:54:14.283
Nov 17 14:54:14.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:54:14.284
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:14.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:14.317
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 11/17/23 14:54:14.321
Nov 17 14:54:14.332: INFO: created test-pod-1
Nov 17 14:54:14.360: INFO: created test-pod-2
Nov 17 14:54:14.407: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 11/17/23 14:54:14.407
Nov 17 14:54:14.407: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4582' to be running and ready
Nov 17 14:54:14.429: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 17 14:54:14.429: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 17 14:54:14.429: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Nov 17 14:54:14.429: INFO: 0 / 3 pods in namespace 'pods-4582' are running and ready (0 seconds elapsed)
Nov 17 14:54:14.429: INFO: expected 0 pod replicas in namespace 'pods-4582', 0 are Running and Ready.
Nov 17 14:54:14.429: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
Nov 17 14:54:14.429: INFO: test-pod-1  k8s-worker-2.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
Nov 17 14:54:14.429: INFO: test-pod-2  k8s-worker-2.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
Nov 17 14:54:14.429: INFO: test-pod-3  k8s-worker-2.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
Nov 17 14:54:14.429: INFO: 
Nov 17 14:54:16.440: INFO: 3 / 3 pods in namespace 'pods-4582' are running and ready (2 seconds elapsed)
Nov 17 14:54:16.440: INFO: expected 0 pod replicas in namespace 'pods-4582', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 11/17/23 14:54:16.464
Nov 17 14:54:16.469: INFO: Pod quantity 3 is different from expected quantity 0
Nov 17 14:54:17.473: INFO: Pod quantity 3 is different from expected quantity 0
Nov 17 14:54:18.473: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:54:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4582" for this suite. 11/17/23 14:54:19.501
------------------------------
â€¢ [SLOW TEST] [5.245 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:54:14.283
    Nov 17 14:54:14.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:54:14.284
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:14.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:14.317
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 11/17/23 14:54:14.321
    Nov 17 14:54:14.332: INFO: created test-pod-1
    Nov 17 14:54:14.360: INFO: created test-pod-2
    Nov 17 14:54:14.407: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 11/17/23 14:54:14.407
    Nov 17 14:54:14.407: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4582' to be running and ready
    Nov 17 14:54:14.429: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 17 14:54:14.429: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 17 14:54:14.429: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Nov 17 14:54:14.429: INFO: 0 / 3 pods in namespace 'pods-4582' are running and ready (0 seconds elapsed)
    Nov 17 14:54:14.429: INFO: expected 0 pod replicas in namespace 'pods-4582', 0 are Running and Ready.
    Nov 17 14:54:14.429: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
    Nov 17 14:54:14.429: INFO: test-pod-1  k8s-worker-2.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
    Nov 17 14:54:14.429: INFO: test-pod-2  k8s-worker-2.c.operations-lab.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
    Nov 17 14:54:14.429: INFO: test-pod-3  k8s-worker-2.c.operations-lab.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 14:54:14 +0000 UTC  }]
    Nov 17 14:54:14.429: INFO: 
    Nov 17 14:54:16.440: INFO: 3 / 3 pods in namespace 'pods-4582' are running and ready (2 seconds elapsed)
    Nov 17 14:54:16.440: INFO: expected 0 pod replicas in namespace 'pods-4582', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 11/17/23 14:54:16.464
    Nov 17 14:54:16.469: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 17 14:54:17.473: INFO: Pod quantity 3 is different from expected quantity 0
    Nov 17 14:54:18.473: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:54:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4582" for this suite. 11/17/23 14:54:19.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:54:19.536
Nov 17 14:54:19.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:54:19.538
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:19.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:19.626
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-4583cc02-5fa4-462a-80eb-2e48a83cef3c 11/17/23 14:54:19.639
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:54:19.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5748" for this suite. 11/17/23 14:54:19.663
------------------------------
â€¢ [0.149 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:54:19.536
    Nov 17 14:54:19.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:54:19.538
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:19.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:19.626
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-4583cc02-5fa4-462a-80eb-2e48a83cef3c 11/17/23 14:54:19.639
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:54:19.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5748" for this suite. 11/17/23 14:54:19.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:54:19.69
Nov 17 14:54:19.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 14:54:19.692
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:19.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:19.755
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 11/17/23 14:54:19.762
STEP: waiting for pod running 11/17/23 14:54:19.791
Nov 17 14:54:19.791: INFO: Waiting up to 2m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935" to be "running"
Nov 17 14:54:19.805: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866722ms
Nov 17 14:54:21.813: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021758658s
Nov 17 14:54:21.813: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" satisfied condition "running"
STEP: creating a file in subpath 11/17/23 14:54:21.813
Nov 17 14:54:21.821: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6935 PodName:var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:54:21.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:54:21.823: INFO: ExecWithOptions: Clientset creation
Nov 17 14:54:21.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6935/pods/var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 11/17/23 14:54:21.992
Nov 17 14:54:21.997: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6935 PodName:var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:54:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:54:21.998: INFO: ExecWithOptions: Clientset creation
Nov 17 14:54:21.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6935/pods/var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 11/17/23 14:54:22.116
Nov 17 14:54:22.655: INFO: Successfully updated pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c"
STEP: waiting for annotated pod running 11/17/23 14:54:22.655
Nov 17 14:54:22.656: INFO: Waiting up to 2m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935" to be "running"
Nov 17 14:54:22.670: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Running", Reason="", readiness=true. Elapsed: 14.896662ms
Nov 17 14:54:22.670: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" satisfied condition "running"
STEP: deleting the pod gracefully 11/17/23 14:54:22.67
Nov 17 14:54:22.671: INFO: Deleting pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935"
Nov 17 14:54:22.699: INFO: Wait up to 5m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 14:54:56.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6935" for this suite. 11/17/23 14:54:56.742
------------------------------
â€¢ [SLOW TEST] [37.060 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:54:19.69
    Nov 17 14:54:19.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 14:54:19.692
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:19.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:19.755
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 11/17/23 14:54:19.762
    STEP: waiting for pod running 11/17/23 14:54:19.791
    Nov 17 14:54:19.791: INFO: Waiting up to 2m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935" to be "running"
    Nov 17 14:54:19.805: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866722ms
    Nov 17 14:54:21.813: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Running", Reason="", readiness=true. Elapsed: 2.021758658s
    Nov 17 14:54:21.813: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" satisfied condition "running"
    STEP: creating a file in subpath 11/17/23 14:54:21.813
    Nov 17 14:54:21.821: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6935 PodName:var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:54:21.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:54:21.823: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:54:21.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6935/pods/var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 11/17/23 14:54:21.992
    Nov 17 14:54:21.997: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6935 PodName:var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:54:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:54:21.998: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:54:21.998: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6935/pods/var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 11/17/23 14:54:22.116
    Nov 17 14:54:22.655: INFO: Successfully updated pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c"
    STEP: waiting for annotated pod running 11/17/23 14:54:22.655
    Nov 17 14:54:22.656: INFO: Waiting up to 2m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935" to be "running"
    Nov 17 14:54:22.670: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c": Phase="Running", Reason="", readiness=true. Elapsed: 14.896662ms
    Nov 17 14:54:22.670: INFO: Pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" satisfied condition "running"
    STEP: deleting the pod gracefully 11/17/23 14:54:22.67
    Nov 17 14:54:22.671: INFO: Deleting pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" in namespace "var-expansion-6935"
    Nov 17 14:54:22.699: INFO: Wait up to 5m0s for pod "var-expansion-8cfd954a-b022-4dcb-9a65-85fd9591c41c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:54:56.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6935" for this suite. 11/17/23 14:54:56.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:54:56.751
Nov 17 14:54:56.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 14:54:56.752
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:56.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:56.776
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 11/17/23 14:54:56.78
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:56.8
STEP: Creating a pod in the namespace 11/17/23 14:54:56.809
STEP: Waiting for the pod to have running status 11/17/23 14:54:56.826
Nov 17 14:54:56.827: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-958" to be "running"
Nov 17 14:54:56.840: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.752297ms
Nov 17 14:54:58.843: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016554834s
Nov 17 14:54:58.843: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 11/17/23 14:54:58.843
STEP: Waiting for the namespace to be removed. 11/17/23 14:54:58.856
STEP: Recreating the namespace 11/17/23 14:55:10.86
STEP: Verifying there are no pods in the namespace 11/17/23 14:55:10.883
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8880" for this suite. 11/17/23 14:55:10.892
STEP: Destroying namespace "nsdeletetest-958" for this suite. 11/17/23 14:55:10.899
Nov 17 14:55:10.903: INFO: Namespace nsdeletetest-958 was already deleted
STEP: Destroying namespace "nsdeletetest-8514" for this suite. 11/17/23 14:55:10.903
------------------------------
â€¢ [SLOW TEST] [14.164 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:54:56.751
    Nov 17 14:54:56.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 14:54:56.752
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:56.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:54:56.776
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 11/17/23 14:54:56.78
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:54:56.8
    STEP: Creating a pod in the namespace 11/17/23 14:54:56.809
    STEP: Waiting for the pod to have running status 11/17/23 14:54:56.826
    Nov 17 14:54:56.827: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-958" to be "running"
    Nov 17 14:54:56.840: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.752297ms
    Nov 17 14:54:58.843: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016554834s
    Nov 17 14:54:58.843: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 11/17/23 14:54:58.843
    STEP: Waiting for the namespace to be removed. 11/17/23 14:54:58.856
    STEP: Recreating the namespace 11/17/23 14:55:10.86
    STEP: Verifying there are no pods in the namespace 11/17/23 14:55:10.883
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:10.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8880" for this suite. 11/17/23 14:55:10.892
    STEP: Destroying namespace "nsdeletetest-958" for this suite. 11/17/23 14:55:10.899
    Nov 17 14:55:10.903: INFO: Namespace nsdeletetest-958 was already deleted
    STEP: Destroying namespace "nsdeletetest-8514" for this suite. 11/17/23 14:55:10.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:10.916
Nov 17 14:55:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:55:10.917
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:10.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:10.942
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-24571917-3b21-4bec-af21-2b3fa786221a 11/17/23 14:55:10.946
STEP: Creating a pod to test consume secrets 11/17/23 14:55:10.951
Nov 17 14:55:10.964: INFO: Waiting up to 5m0s for pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58" in namespace "secrets-7212" to be "Succeeded or Failed"
Nov 17 14:55:10.971: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632236ms
Nov 17 14:55:12.976: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011392852s
Nov 17 14:55:14.975: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010847757s
STEP: Saw pod success 11/17/23 14:55:14.975
Nov 17 14:55:14.976: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58" satisfied condition "Succeeded or Failed"
Nov 17 14:55:14.979: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:55:14.993
Nov 17 14:55:15.009: INFO: Waiting for pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 to disappear
Nov 17 14:55:15.016: INFO: Pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:15.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7212" for this suite. 11/17/23 14:55:15.028
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:10.916
    Nov 17 14:55:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:55:10.917
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:10.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:10.942
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-24571917-3b21-4bec-af21-2b3fa786221a 11/17/23 14:55:10.946
    STEP: Creating a pod to test consume secrets 11/17/23 14:55:10.951
    Nov 17 14:55:10.964: INFO: Waiting up to 5m0s for pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58" in namespace "secrets-7212" to be "Succeeded or Failed"
    Nov 17 14:55:10.971: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632236ms
    Nov 17 14:55:12.976: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011392852s
    Nov 17 14:55:14.975: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010847757s
    STEP: Saw pod success 11/17/23 14:55:14.975
    Nov 17 14:55:14.976: INFO: Pod "pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58" satisfied condition "Succeeded or Failed"
    Nov 17 14:55:14.979: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:55:14.993
    Nov 17 14:55:15.009: INFO: Waiting for pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 to disappear
    Nov 17 14:55:15.016: INFO: Pod pod-secrets-0086dc75-1e70-457a-ab88-1e3ca5addc58 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:15.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7212" for this suite. 11/17/23 14:55:15.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:15.046
Nov 17 14:55:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:55:15.047
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:15.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:15.069
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 11/17/23 14:55:15.079
STEP: waiting for available Endpoint 11/17/23 14:55:15.084
STEP: listing all Endpoints 11/17/23 14:55:15.086
STEP: updating the Endpoint 11/17/23 14:55:15.09
STEP: fetching the Endpoint 11/17/23 14:55:15.097
STEP: patching the Endpoint 11/17/23 14:55:15.101
STEP: fetching the Endpoint 11/17/23 14:55:15.112
STEP: deleting the Endpoint by Collection 11/17/23 14:55:15.115
STEP: waiting for Endpoint deletion 11/17/23 14:55:15.121
STEP: fetching the Endpoint 11/17/23 14:55:15.123
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:15.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-157" for this suite. 11/17/23 14:55:15.129
------------------------------
â€¢ [0.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:15.046
    Nov 17 14:55:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:55:15.047
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:15.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:15.069
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 11/17/23 14:55:15.079
    STEP: waiting for available Endpoint 11/17/23 14:55:15.084
    STEP: listing all Endpoints 11/17/23 14:55:15.086
    STEP: updating the Endpoint 11/17/23 14:55:15.09
    STEP: fetching the Endpoint 11/17/23 14:55:15.097
    STEP: patching the Endpoint 11/17/23 14:55:15.101
    STEP: fetching the Endpoint 11/17/23 14:55:15.112
    STEP: deleting the Endpoint by Collection 11/17/23 14:55:15.115
    STEP: waiting for Endpoint deletion 11/17/23 14:55:15.121
    STEP: fetching the Endpoint 11/17/23 14:55:15.123
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:15.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-157" for this suite. 11/17/23 14:55:15.129
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:15.136
Nov 17 14:55:15.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 14:55:15.138
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:15.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:15.154
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-5763178c-71e6-42be-87e7-c78f40f41af5 11/17/23 14:55:15.158
STEP: Creating a pod to test consume secrets 11/17/23 14:55:15.164
Nov 17 14:55:15.172: INFO: Waiting up to 5m0s for pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b" in namespace "secrets-4383" to be "Succeeded or Failed"
Nov 17 14:55:15.176: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024041ms
Nov 17 14:55:17.182: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009429609s
Nov 17 14:55:19.181: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008630898s
STEP: Saw pod success 11/17/23 14:55:19.181
Nov 17 14:55:19.181: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b" satisfied condition "Succeeded or Failed"
Nov 17 14:55:19.184: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 14:55:19.189
Nov 17 14:55:19.201: INFO: Waiting for pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b to disappear
Nov 17 14:55:19.204: INFO: Pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:19.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4383" for this suite. 11/17/23 14:55:19.208
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:15.136
    Nov 17 14:55:15.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 14:55:15.138
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:15.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:15.154
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-5763178c-71e6-42be-87e7-c78f40f41af5 11/17/23 14:55:15.158
    STEP: Creating a pod to test consume secrets 11/17/23 14:55:15.164
    Nov 17 14:55:15.172: INFO: Waiting up to 5m0s for pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b" in namespace "secrets-4383" to be "Succeeded or Failed"
    Nov 17 14:55:15.176: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024041ms
    Nov 17 14:55:17.182: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009429609s
    Nov 17 14:55:19.181: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008630898s
    STEP: Saw pod success 11/17/23 14:55:19.181
    Nov 17 14:55:19.181: INFO: Pod "pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b" satisfied condition "Succeeded or Failed"
    Nov 17 14:55:19.184: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 14:55:19.189
    Nov 17 14:55:19.201: INFO: Waiting for pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b to disappear
    Nov 17 14:55:19.204: INFO: Pod pod-secrets-8e47f82e-0688-4acd-8ad8-00e2629b164b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:19.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4383" for this suite. 11/17/23 14:55:19.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:19.215
Nov 17 14:55:19.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 14:55:19.217
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:19.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:19.233
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Nov 17 14:55:19.252: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 17 14:55:24.257: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 14:55:24.257
STEP: Scaling up "test-rs" replicaset  11/17/23 14:55:24.257
Nov 17 14:55:24.265: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 11/17/23 14:55:24.265
W1117 14:55:24.279948      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Nov 17 14:55:24.285: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
Nov 17 14:55:24.305: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
Nov 17 14:55:24.328: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
Nov 17 14:55:24.359: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
Nov 17 14:55:25.276: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 2, AvailableReplicas 2
Nov 17 14:55:26.045: INFO: observed Replicaset test-rs in namespace replicaset-6784 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:26.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6784" for this suite. 11/17/23 14:55:26.049
------------------------------
â€¢ [SLOW TEST] [6.841 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:19.215
    Nov 17 14:55:19.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 14:55:19.217
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:19.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:19.233
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Nov 17 14:55:19.252: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 17 14:55:24.257: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 14:55:24.257
    STEP: Scaling up "test-rs" replicaset  11/17/23 14:55:24.257
    Nov 17 14:55:24.265: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 11/17/23 14:55:24.265
    W1117 14:55:24.279948      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Nov 17 14:55:24.285: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
    Nov 17 14:55:24.305: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
    Nov 17 14:55:24.328: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
    Nov 17 14:55:24.359: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 1, AvailableReplicas 1
    Nov 17 14:55:25.276: INFO: observed ReplicaSet test-rs in namespace replicaset-6784 with ReadyReplicas 2, AvailableReplicas 2
    Nov 17 14:55:26.045: INFO: observed Replicaset test-rs in namespace replicaset-6784 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:26.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6784" for this suite. 11/17/23 14:55:26.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:26.056
Nov 17 14:55:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename runtimeclass 11/17/23 14:55:26.058
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:26.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:26.081
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 11/17/23 14:55:26.085
STEP: getting /apis/node.k8s.io 11/17/23 14:55:26.088
STEP: getting /apis/node.k8s.io/v1 11/17/23 14:55:26.089
STEP: creating 11/17/23 14:55:26.09
STEP: watching 11/17/23 14:55:26.108
Nov 17 14:55:26.108: INFO: starting watch
STEP: getting 11/17/23 14:55:26.117
STEP: listing 11/17/23 14:55:26.121
STEP: patching 11/17/23 14:55:26.127
STEP: updating 11/17/23 14:55:26.133
Nov 17 14:55:26.140: INFO: waiting for watch events with expected annotations
STEP: deleting 11/17/23 14:55:26.14
STEP: deleting a collection 11/17/23 14:55:26.153
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6117" for this suite. 11/17/23 14:55:26.175
------------------------------
â€¢ [0.127 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:26.056
    Nov 17 14:55:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename runtimeclass 11/17/23 14:55:26.058
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:26.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:26.081
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 11/17/23 14:55:26.085
    STEP: getting /apis/node.k8s.io 11/17/23 14:55:26.088
    STEP: getting /apis/node.k8s.io/v1 11/17/23 14:55:26.089
    STEP: creating 11/17/23 14:55:26.09
    STEP: watching 11/17/23 14:55:26.108
    Nov 17 14:55:26.108: INFO: starting watch
    STEP: getting 11/17/23 14:55:26.117
    STEP: listing 11/17/23 14:55:26.121
    STEP: patching 11/17/23 14:55:26.127
    STEP: updating 11/17/23 14:55:26.133
    Nov 17 14:55:26.140: INFO: waiting for watch events with expected annotations
    STEP: deleting 11/17/23 14:55:26.14
    STEP: deleting a collection 11/17/23 14:55:26.153
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6117" for this suite. 11/17/23 14:55:26.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:26.189
Nov 17 14:55:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 14:55:26.191
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:26.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:26.214
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 11/17/23 14:55:26.217
STEP: When the matched label of one of its pods change 11/17/23 14:55:26.23
Nov 17 14:55:26.234: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 17 14:55:31.328: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 11/17/23 14:55:31.411
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6512" for this suite. 11/17/23 14:55:32.452
------------------------------
â€¢ [SLOW TEST] [6.276 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:26.189
    Nov 17 14:55:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 14:55:26.191
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:26.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:26.214
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 11/17/23 14:55:26.217
    STEP: When the matched label of one of its pods change 11/17/23 14:55:26.23
    Nov 17 14:55:26.234: INFO: Pod name pod-release: Found 0 pods out of 1
    Nov 17 14:55:31.328: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 11/17/23 14:55:31.411
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6512" for this suite. 11/17/23 14:55:32.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:32.466
Nov 17 14:55:32.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption 11/17/23 14:55:32.468
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:32.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:32.502
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 11/17/23 14:55:32.505
STEP: Waiting for the pdb to be processed 11/17/23 14:55:32.513
STEP: First trying to evict a pod which shouldn't be evictable 11/17/23 14:55:32.533
STEP: Waiting for all pods to be running 11/17/23 14:55:32.533
Nov 17 14:55:32.539: INFO: pods: 0 < 3
STEP: locating a running pod 11/17/23 14:55:34.544
STEP: Updating the pdb to allow a pod to be evicted 11/17/23 14:55:34.554
STEP: Waiting for the pdb to be processed 11/17/23 14:55:34.563
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/17/23 14:55:36.572
STEP: Waiting for all pods to be running 11/17/23 14:55:36.572
STEP: Waiting for the pdb to observed all healthy pods 11/17/23 14:55:36.576
STEP: Patching the pdb to disallow a pod to be evicted 11/17/23 14:55:36.606
STEP: Waiting for the pdb to be processed 11/17/23 14:55:36.651
STEP: Waiting for all pods to be running 11/17/23 14:55:38.667
STEP: locating a running pod 11/17/23 14:55:38.671
STEP: Deleting the pdb to allow a pod to be evicted 11/17/23 14:55:38.681
STEP: Waiting for the pdb to be deleted 11/17/23 14:55:38.686
STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/17/23 14:55:38.689
STEP: Waiting for all pods to be running 11/17/23 14:55:38.689
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:38.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6034" for this suite. 11/17/23 14:55:38.717
------------------------------
â€¢ [SLOW TEST] [6.268 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:32.466
    Nov 17 14:55:32.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption 11/17/23 14:55:32.468
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:32.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:32.502
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 11/17/23 14:55:32.505
    STEP: Waiting for the pdb to be processed 11/17/23 14:55:32.513
    STEP: First trying to evict a pod which shouldn't be evictable 11/17/23 14:55:32.533
    STEP: Waiting for all pods to be running 11/17/23 14:55:32.533
    Nov 17 14:55:32.539: INFO: pods: 0 < 3
    STEP: locating a running pod 11/17/23 14:55:34.544
    STEP: Updating the pdb to allow a pod to be evicted 11/17/23 14:55:34.554
    STEP: Waiting for the pdb to be processed 11/17/23 14:55:34.563
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/17/23 14:55:36.572
    STEP: Waiting for all pods to be running 11/17/23 14:55:36.572
    STEP: Waiting for the pdb to observed all healthy pods 11/17/23 14:55:36.576
    STEP: Patching the pdb to disallow a pod to be evicted 11/17/23 14:55:36.606
    STEP: Waiting for the pdb to be processed 11/17/23 14:55:36.651
    STEP: Waiting for all pods to be running 11/17/23 14:55:38.667
    STEP: locating a running pod 11/17/23 14:55:38.671
    STEP: Deleting the pdb to allow a pod to be evicted 11/17/23 14:55:38.681
    STEP: Waiting for the pdb to be deleted 11/17/23 14:55:38.686
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 11/17/23 14:55:38.689
    STEP: Waiting for all pods to be running 11/17/23 14:55:38.689
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:38.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6034" for this suite. 11/17/23 14:55:38.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:38.736
Nov 17 14:55:38.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 14:55:38.737
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:38.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:38.776
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:55:38.78
Nov 17 14:55:38.790: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9" in namespace "downward-api-7479" to be "Succeeded or Failed"
Nov 17 14:55:38.797: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427336ms
Nov 17 14:55:40.804: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014402104s
Nov 17 14:55:42.806: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016466526s
STEP: Saw pod success 11/17/23 14:55:42.806
Nov 17 14:55:42.806: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9" satisfied condition "Succeeded or Failed"
Nov 17 14:55:42.811: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 container client-container: <nil>
STEP: delete the pod 11/17/23 14:55:42.818
Nov 17 14:55:42.836: INFO: Waiting for pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 to disappear
Nov 17 14:55:42.840: INFO: Pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:42.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7479" for this suite. 11/17/23 14:55:42.846
------------------------------
â€¢ [4.119 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:38.736
    Nov 17 14:55:38.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 14:55:38.737
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:38.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:38.776
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:55:38.78
    Nov 17 14:55:38.790: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9" in namespace "downward-api-7479" to be "Succeeded or Failed"
    Nov 17 14:55:38.797: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427336ms
    Nov 17 14:55:40.804: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014402104s
    Nov 17 14:55:42.806: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016466526s
    STEP: Saw pod success 11/17/23 14:55:42.806
    Nov 17 14:55:42.806: INFO: Pod "downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9" satisfied condition "Succeeded or Failed"
    Nov 17 14:55:42.811: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:55:42.818
    Nov 17 14:55:42.836: INFO: Waiting for pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 to disappear
    Nov 17 14:55:42.840: INFO: Pod downwardapi-volume-c41e298d-5b4d-4a73-8d52-cc34eb3eecb9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:42.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7479" for this suite. 11/17/23 14:55:42.846
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:42.855
Nov 17 14:55:42.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:55:42.856
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:42.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:42.883
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-aaff51ae-4099-4ecd-a67e-d2f23a6fa0b7 11/17/23 14:55:42.891
STEP: Creating secret with name s-test-opt-upd-f76a9414-43ec-4987-8435-ee4640159397 11/17/23 14:55:42.897
STEP: Creating the pod 11/17/23 14:55:42.903
Nov 17 14:55:42.917: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56" in namespace "projected-7876" to be "running and ready"
Nov 17 14:55:42.923: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20829ms
Nov 17 14:55:42.923: INFO: The phase of Pod pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:55:44.928: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56": Phase="Running", Reason="", readiness=true. Elapsed: 2.010707557s
Nov 17 14:55:44.928: INFO: The phase of Pod pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56 is Running (Ready = true)
Nov 17 14:55:44.928: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-aaff51ae-4099-4ecd-a67e-d2f23a6fa0b7 11/17/23 14:55:44.95
STEP: Updating secret s-test-opt-upd-f76a9414-43ec-4987-8435-ee4640159397 11/17/23 14:55:44.959
STEP: Creating secret with name s-test-opt-create-aae5c088-6804-44e0-892d-d2f1ef2a03c8 11/17/23 14:55:44.965
STEP: waiting to observe update in volume 11/17/23 14:55:44.972
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 14:55:46.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7876" for this suite. 11/17/23 14:55:47.004
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:42.855
    Nov 17 14:55:42.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:55:42.856
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:42.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:42.883
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-aaff51ae-4099-4ecd-a67e-d2f23a6fa0b7 11/17/23 14:55:42.891
    STEP: Creating secret with name s-test-opt-upd-f76a9414-43ec-4987-8435-ee4640159397 11/17/23 14:55:42.897
    STEP: Creating the pod 11/17/23 14:55:42.903
    Nov 17 14:55:42.917: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56" in namespace "projected-7876" to be "running and ready"
    Nov 17 14:55:42.923: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20829ms
    Nov 17 14:55:42.923: INFO: The phase of Pod pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:55:44.928: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56": Phase="Running", Reason="", readiness=true. Elapsed: 2.010707557s
    Nov 17 14:55:44.928: INFO: The phase of Pod pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56 is Running (Ready = true)
    Nov 17 14:55:44.928: INFO: Pod "pod-projected-secrets-2c493abd-a4db-4331-b913-b20459d2fa56" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-aaff51ae-4099-4ecd-a67e-d2f23a6fa0b7 11/17/23 14:55:44.95
    STEP: Updating secret s-test-opt-upd-f76a9414-43ec-4987-8435-ee4640159397 11/17/23 14:55:44.959
    STEP: Creating secret with name s-test-opt-create-aae5c088-6804-44e0-892d-d2f1ef2a03c8 11/17/23 14:55:44.965
    STEP: waiting to observe update in volume 11/17/23 14:55:44.972
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:55:46.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7876" for this suite. 11/17/23 14:55:47.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:55:47.014
Nov 17 14:55:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename subpath 11/17/23 14:55:47.016
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:47.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:47.038
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/17/23 14:55:47.041
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-qct9 11/17/23 14:55:47.051
STEP: Creating a pod to test atomic-volume-subpath 11/17/23 14:55:47.051
Nov 17 14:55:47.059: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qct9" in namespace "subpath-1334" to be "Succeeded or Failed"
Nov 17 14:55:47.062: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299541ms
Nov 17 14:55:49.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00863889s
Nov 17 14:55:51.068: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009358266s
Nov 17 14:55:53.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 6.008235854s
Nov 17 14:55:55.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008445432s
Nov 17 14:55:57.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 10.00742723s
Nov 17 14:55:59.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 12.007859683s
Nov 17 14:56:01.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 14.007260773s
Nov 17 14:56:03.071: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012332707s
Nov 17 14:56:05.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008096117s
Nov 17 14:56:07.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 20.008165803s
Nov 17 14:56:09.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=false. Elapsed: 22.007947963s
Nov 17 14:56:11.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006889012s
STEP: Saw pod success 11/17/23 14:56:11.066
Nov 17 14:56:11.066: INFO: Pod "pod-subpath-test-configmap-qct9" satisfied condition "Succeeded or Failed"
Nov 17 14:56:11.069: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-configmap-qct9 container test-container-subpath-configmap-qct9: <nil>
STEP: delete the pod 11/17/23 14:56:11.076
Nov 17 14:56:11.087: INFO: Waiting for pod pod-subpath-test-configmap-qct9 to disappear
Nov 17 14:56:11.089: INFO: Pod pod-subpath-test-configmap-qct9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qct9 11/17/23 14:56:11.089
Nov 17 14:56:11.090: INFO: Deleting pod "pod-subpath-test-configmap-qct9" in namespace "subpath-1334"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 17 14:56:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1334" for this suite. 11/17/23 14:56:11.096
------------------------------
â€¢ [SLOW TEST] [24.089 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:55:47.014
    Nov 17 14:55:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename subpath 11/17/23 14:55:47.016
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:55:47.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:55:47.038
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/17/23 14:55:47.041
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-qct9 11/17/23 14:55:47.051
    STEP: Creating a pod to test atomic-volume-subpath 11/17/23 14:55:47.051
    Nov 17 14:55:47.059: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qct9" in namespace "subpath-1334" to be "Succeeded or Failed"
    Nov 17 14:55:47.062: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299541ms
    Nov 17 14:55:49.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00863889s
    Nov 17 14:55:51.068: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009358266s
    Nov 17 14:55:53.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 6.008235854s
    Nov 17 14:55:55.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008445432s
    Nov 17 14:55:57.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 10.00742723s
    Nov 17 14:55:59.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 12.007859683s
    Nov 17 14:56:01.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 14.007260773s
    Nov 17 14:56:03.071: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012332707s
    Nov 17 14:56:05.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008096117s
    Nov 17 14:56:07.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=true. Elapsed: 20.008165803s
    Nov 17 14:56:09.067: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Running", Reason="", readiness=false. Elapsed: 22.007947963s
    Nov 17 14:56:11.066: INFO: Pod "pod-subpath-test-configmap-qct9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006889012s
    STEP: Saw pod success 11/17/23 14:56:11.066
    Nov 17 14:56:11.066: INFO: Pod "pod-subpath-test-configmap-qct9" satisfied condition "Succeeded or Failed"
    Nov 17 14:56:11.069: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-configmap-qct9 container test-container-subpath-configmap-qct9: <nil>
    STEP: delete the pod 11/17/23 14:56:11.076
    Nov 17 14:56:11.087: INFO: Waiting for pod pod-subpath-test-configmap-qct9 to disappear
    Nov 17 14:56:11.089: INFO: Pod pod-subpath-test-configmap-qct9 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-qct9 11/17/23 14:56:11.089
    Nov 17 14:56:11.090: INFO: Deleting pod "pod-subpath-test-configmap-qct9" in namespace "subpath-1334"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:56:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1334" for this suite. 11/17/23 14:56:11.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:56:11.106
Nov 17 14:56:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 14:56:11.108
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:56:11.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:56:11.13
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Nov 17 14:56:11.143: INFO: Waiting up to 2m0s for pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" in namespace "var-expansion-4496" to be "container 0 failed with reason CreateContainerConfigError"
Nov 17 14:56:11.147: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420069ms
Nov 17 14:56:13.152: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009186503s
Nov 17 14:56:13.152: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 17 14:56:13.152: INFO: Deleting pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" in namespace "var-expansion-4496"
Nov 17 14:56:13.159: INFO: Wait up to 5m0s for pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 14:56:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4496" for this suite. 11/17/23 14:56:15.172
------------------------------
â€¢ [4.074 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:56:11.106
    Nov 17 14:56:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 14:56:11.108
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:56:11.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:56:11.13
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Nov 17 14:56:11.143: INFO: Waiting up to 2m0s for pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" in namespace "var-expansion-4496" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 17 14:56:11.147: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420069ms
    Nov 17 14:56:13.152: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009186503s
    Nov 17 14:56:13.152: INFO: Pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 17 14:56:13.152: INFO: Deleting pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" in namespace "var-expansion-4496"
    Nov 17 14:56:13.159: INFO: Wait up to 5m0s for pod "var-expansion-f144634d-d5ab-4c92-9974-acc0c68b4e6d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:56:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4496" for this suite. 11/17/23 14:56:15.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:56:15.181
Nov 17 14:56:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:56:15.183
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:56:15.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:56:15.205
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 17 14:56:15.222: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:57:15.280: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 11/17/23 14:57:15.284
Nov 17 14:57:15.312: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 17 14:57:15.319: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 17 14:57:15.351: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 17 14:57:15.367: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 17 14:57:15.399: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 17 14:57:15.411: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/17/23 14:57:15.411
Nov 17 14:57:15.411: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:15.425: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.710071ms
Nov 17 14:57:17.430: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018789062s
Nov 17 14:57:17.430: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 17 14:57:17.430: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:17.434: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.763815ms
Nov 17 14:57:17.434: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:57:17.434: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:17.438: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.294728ms
Nov 17 14:57:17.438: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:57:17.438: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:17.442: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.793082ms
Nov 17 14:57:17.442: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:57:17.442: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:17.445: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.289949ms
Nov 17 14:57:17.445: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:57:17.445: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
Nov 17 14:57:17.455: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.697724ms
Nov 17 14:57:17.455: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 11/17/23 14:57:17.455
Nov 17 14:57:17.468: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Nov 17 14:57:17.474: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493819ms
Nov 17 14:57:19.478: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01042852s
Nov 17 14:57:21.479: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011204959s
Nov 17 14:57:23.481: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013369215s
Nov 17 14:57:23.481: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:57:23.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8605" for this suite. 11/17/23 14:57:23.599
------------------------------
â€¢ [SLOW TEST] [68.427 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:56:15.181
    Nov 17 14:56:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:56:15.183
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:56:15.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:56:15.205
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 17 14:56:15.222: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:57:15.280: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 11/17/23 14:57:15.284
    Nov 17 14:57:15.312: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 17 14:57:15.319: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 17 14:57:15.351: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 17 14:57:15.367: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 17 14:57:15.399: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 17 14:57:15.411: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/17/23 14:57:15.411
    Nov 17 14:57:15.411: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:15.425: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.710071ms
    Nov 17 14:57:17.430: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018789062s
    Nov 17 14:57:17.430: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 17 14:57:17.430: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:17.434: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.763815ms
    Nov 17 14:57:17.434: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:57:17.434: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:17.438: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.294728ms
    Nov 17 14:57:17.438: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:57:17.438: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:17.442: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.793082ms
    Nov 17 14:57:17.442: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:57:17.442: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:17.445: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.289949ms
    Nov 17 14:57:17.445: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:57:17.445: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8605" to be "running"
    Nov 17 14:57:17.455: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.697724ms
    Nov 17 14:57:17.455: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 11/17/23 14:57:17.455
    Nov 17 14:57:17.468: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Nov 17 14:57:17.474: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493819ms
    Nov 17 14:57:19.478: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01042852s
    Nov 17 14:57:21.479: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011204959s
    Nov 17 14:57:23.481: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.013369215s
    Nov 17 14:57:23.481: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:57:23.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8605" for this suite. 11/17/23 14:57:23.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:57:23.608
Nov 17 14:57:23.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:57:23.61
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:57:23.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:57:23.637
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Nov 17 14:57:23.656: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 17 14:58:23.721: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 11/17/23 14:58:23.724
Nov 17 14:58:23.744: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 17 14:58:23.756: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 17 14:58:23.785: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 17 14:58:23.804: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 17 14:58:23.836: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 17 14:58:23.844: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 11/17/23 14:58:23.844
Nov 17 14:58:23.844: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:23.853: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619624ms
Nov 17 14:58:25.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015096946s
Nov 17 14:58:25.859: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Nov 17 14:58:25.859: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.862: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.696002ms
Nov 17 14:58:25.862: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:58:25.862: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.865: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.430995ms
Nov 17 14:58:25.865: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:58:25.865: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.586635ms
Nov 17 14:58:25.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:58:25.867: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.869: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.196179ms
Nov 17 14:58:25.869: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Nov 17 14:58:25.869: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.872: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.252269ms
Nov 17 14:58:25.872: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/17/23 14:58:25.872
Nov 17 14:58:25.877: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7982" to be "running"
Nov 17 14:58:25.890: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.263991ms
Nov 17 14:58:27.897: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019411141s
Nov 17 14:58:29.896: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018272503s
Nov 17 14:58:31.894: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016362152s
Nov 17 14:58:31.894: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:58:31.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7982" for this suite. 11/17/23 14:58:31.967
------------------------------
â€¢ [SLOW TEST] [68.367 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:57:23.608
    Nov 17 14:57:23.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-preemption 11/17/23 14:57:23.61
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:57:23.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:57:23.637
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Nov 17 14:57:23.656: INFO: Waiting up to 1m0s for all nodes to be ready
    Nov 17 14:58:23.721: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 11/17/23 14:58:23.724
    Nov 17 14:58:23.744: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Nov 17 14:58:23.756: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Nov 17 14:58:23.785: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Nov 17 14:58:23.804: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Nov 17 14:58:23.836: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Nov 17 14:58:23.844: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 11/17/23 14:58:23.844
    Nov 17 14:58:23.844: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:23.853: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619624ms
    Nov 17 14:58:25.859: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015096946s
    Nov 17 14:58:25.859: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Nov 17 14:58:25.859: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.862: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.696002ms
    Nov 17 14:58:25.862: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:58:25.862: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.865: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.430995ms
    Nov 17 14:58:25.865: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:58:25.865: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.586635ms
    Nov 17 14:58:25.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:58:25.867: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.869: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.196179ms
    Nov 17 14:58:25.869: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Nov 17 14:58:25.869: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.872: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.252269ms
    Nov 17 14:58:25.872: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 11/17/23 14:58:25.872
    Nov 17 14:58:25.877: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7982" to be "running"
    Nov 17 14:58:25.890: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.263991ms
    Nov 17 14:58:27.897: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019411141s
    Nov 17 14:58:29.896: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018272503s
    Nov 17 14:58:31.894: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.016362152s
    Nov 17 14:58:31.894: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:58:31.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7982" for this suite. 11/17/23 14:58:31.967
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:58:31.975
Nov 17 14:58:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 14:58:31.977
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:31.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:31.996
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 11/17/23 14:58:32
Nov 17 14:58:32.010: INFO: Waiting up to 5m0s for pod "pod-2fsw8" in namespace "pods-2789" to be "running"
Nov 17 14:58:32.012: INFO: Pod "pod-2fsw8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760657ms
Nov 17 14:58:34.018: INFO: Pod "pod-2fsw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.0087079s
Nov 17 14:58:34.018: INFO: Pod "pod-2fsw8" satisfied condition "running"
STEP: patching /status 11/17/23 14:58:34.018
Nov 17 14:58:34.027: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 14:58:34.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2789" for this suite. 11/17/23 14:58:34.032
------------------------------
â€¢ [2.063 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:58:31.975
    Nov 17 14:58:31.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 14:58:31.977
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:31.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:31.996
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 11/17/23 14:58:32
    Nov 17 14:58:32.010: INFO: Waiting up to 5m0s for pod "pod-2fsw8" in namespace "pods-2789" to be "running"
    Nov 17 14:58:32.012: INFO: Pod "pod-2fsw8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760657ms
    Nov 17 14:58:34.018: INFO: Pod "pod-2fsw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.0087079s
    Nov 17 14:58:34.018: INFO: Pod "pod-2fsw8" satisfied condition "running"
    STEP: patching /status 11/17/23 14:58:34.018
    Nov 17 14:58:34.027: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:58:34.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2789" for this suite. 11/17/23 14:58:34.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:58:34.04
Nov 17 14:58:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 14:58:34.041
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:34.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:34.061
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 14:58:34.078
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:58:34.347
STEP: Deploying the webhook pod 11/17/23 14:58:34.356
STEP: Wait for the deployment to be ready 11/17/23 14:58:34.374
Nov 17 14:58:34.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 14:58:36.409
STEP: Verifying the service has paired with the endpoint 11/17/23 14:58:36.425
Nov 17 14:58:37.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/17/23 14:58:37.433
STEP: create a pod that should be updated by the webhook 11/17/23 14:58:37.461
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 14:58:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2907" for this suite. 11/17/23 14:58:37.612
STEP: Destroying namespace "webhook-2907-markers" for this suite. 11/17/23 14:58:37.634
------------------------------
â€¢ [3.607 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:58:34.04
    Nov 17 14:58:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 14:58:34.041
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:34.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:34.061
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 14:58:34.078
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 14:58:34.347
    STEP: Deploying the webhook pod 11/17/23 14:58:34.356
    STEP: Wait for the deployment to be ready 11/17/23 14:58:34.374
    Nov 17 14:58:34.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 14:58:36.409
    STEP: Verifying the service has paired with the endpoint 11/17/23 14:58:36.425
    Nov 17 14:58:37.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 11/17/23 14:58:37.433
    STEP: create a pod that should be updated by the webhook 11/17/23 14:58:37.461
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:58:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2907" for this suite. 11/17/23 14:58:37.612
    STEP: Destroying namespace "webhook-2907-markers" for this suite. 11/17/23 14:58:37.634
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:58:37.648
Nov 17 14:58:37.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/17/23 14:58:37.65
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:37.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:37.69
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 11/17/23 14:58:37.698
STEP: Creating hostNetwork=false pod 11/17/23 14:58:37.698
Nov 17 14:58:37.718: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-409" to be "running and ready"
Nov 17 14:58:37.730: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022279ms
Nov 17 14:58:37.731: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:58:39.734: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01599497s
Nov 17 14:58:39.734: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:58:41.736: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017931722s
Nov 17 14:58:41.736: INFO: The phase of Pod test-pod is Running (Ready = true)
Nov 17 14:58:41.736: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 11/17/23 14:58:41.74
Nov 17 14:58:41.749: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-409" to be "running and ready"
Nov 17 14:58:41.753: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025314ms
Nov 17 14:58:41.753: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:58:43.758: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115422s
Nov 17 14:58:43.758: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Nov 17 14:58:43.758: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 11/17/23 14:58:43.761
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/17/23 14:58:43.762
Nov 17 14:58:43.762: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:43.763: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:43.763: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 17 14:58:43.870: INFO: Exec stderr: ""
Nov 17 14:58:43.871: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:43.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:43.872: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:43.872: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 17 14:58:43.975: INFO: Exec stderr: ""
Nov 17 14:58:43.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:43.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:43.977: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:43.977: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 17 14:58:44.082: INFO: Exec stderr: ""
Nov 17 14:58:44.082: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.083: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 17 14:58:44.200: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/17/23 14:58:44.2
Nov 17 14:58:44.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.201: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 17 14:58:44.305: INFO: Exec stderr: ""
Nov 17 14:58:44.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.306: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Nov 17 14:58:44.417: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/17/23 14:58:44.417
Nov 17 14:58:44.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.418: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 17 14:58:44.522: INFO: Exec stderr: ""
Nov 17 14:58:44.522: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.523: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.523: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Nov 17 14:58:44.637: INFO: Exec stderr: ""
Nov 17 14:58:44.637: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.638: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 17 14:58:44.740: INFO: Exec stderr: ""
Nov 17 14:58:44.741: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:44.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:44.742: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:44.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Nov 17 14:58:44.840: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Nov 17 14:58:44.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-409" for this suite. 11/17/23 14:58:44.845
------------------------------
â€¢ [SLOW TEST] [7.206 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:58:37.648
    Nov 17 14:58:37.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 11/17/23 14:58:37.65
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:37.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:37.69
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 11/17/23 14:58:37.698
    STEP: Creating hostNetwork=false pod 11/17/23 14:58:37.698
    Nov 17 14:58:37.718: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-409" to be "running and ready"
    Nov 17 14:58:37.730: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022279ms
    Nov 17 14:58:37.731: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:58:39.734: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01599497s
    Nov 17 14:58:39.734: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:58:41.736: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017931722s
    Nov 17 14:58:41.736: INFO: The phase of Pod test-pod is Running (Ready = true)
    Nov 17 14:58:41.736: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 11/17/23 14:58:41.74
    Nov 17 14:58:41.749: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-409" to be "running and ready"
    Nov 17 14:58:41.753: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025314ms
    Nov 17 14:58:41.753: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:58:43.758: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115422s
    Nov 17 14:58:43.758: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Nov 17 14:58:43.758: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 11/17/23 14:58:43.761
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 11/17/23 14:58:43.762
    Nov 17 14:58:43.762: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:43.763: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:43.763: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 17 14:58:43.870: INFO: Exec stderr: ""
    Nov 17 14:58:43.871: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:43.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:43.872: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:43.872: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 17 14:58:43.975: INFO: Exec stderr: ""
    Nov 17 14:58:43.975: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:43.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:43.977: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:43.977: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 17 14:58:44.082: INFO: Exec stderr: ""
    Nov 17 14:58:44.082: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.083: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 17 14:58:44.200: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 11/17/23 14:58:44.2
    Nov 17 14:58:44.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.201: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 17 14:58:44.305: INFO: Exec stderr: ""
    Nov 17 14:58:44.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.306: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Nov 17 14:58:44.417: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 11/17/23 14:58:44.417
    Nov 17 14:58:44.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.418: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 17 14:58:44.522: INFO: Exec stderr: ""
    Nov 17 14:58:44.522: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.523: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.523: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Nov 17 14:58:44.637: INFO: Exec stderr: ""
    Nov 17 14:58:44.637: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.638: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 17 14:58:44.740: INFO: Exec stderr: ""
    Nov 17 14:58:44.741: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-409 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:44.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:44.742: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:44.742: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-409/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Nov 17 14:58:44.840: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:58:44.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-409" for this suite. 11/17/23 14:58:44.845
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:58:44.855
Nov 17 14:58:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 14:58:44.856
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:44.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:44.882
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/17/23 14:58:44.886
Nov 17 14:58:44.895: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6366  0248ce5e-906b-40d0-8b65-c6bffe61037b 57818 0 2023-11-17 14:58:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-11-17 14:58:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mldst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mldst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 14:58:44.895: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6366" to be "running and ready"
Nov 17 14:58:44.900: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042115ms
Nov 17 14:58:44.900: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 17 14:58:46.905: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976935s
Nov 17 14:58:46.905: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Nov 17 14:58:46.905: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 11/17/23 14:58:46.905
Nov 17 14:58:46.906: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6366 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:46.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:46.907: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:46.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6366/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 11/17/23 14:58:47.031
Nov 17 14:58:47.031: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6366 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 17 14:58:47.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 14:58:47.032: INFO: ExecWithOptions: Clientset creation
Nov 17 14:58:47.032: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6366/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Nov 17 14:58:47.150: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 14:58:47.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6366" for this suite. 11/17/23 14:58:47.171
------------------------------
â€¢ [2.322 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:58:44.855
    Nov 17 14:58:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 14:58:44.856
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:44.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:44.882
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 11/17/23 14:58:44.886
    Nov 17 14:58:44.895: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6366  0248ce5e-906b-40d0-8b65-c6bffe61037b 57818 0 2023-11-17 14:58:44 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-11-17 14:58:44 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mldst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mldst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 14:58:44.895: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6366" to be "running and ready"
    Nov 17 14:58:44.900: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042115ms
    Nov 17 14:58:44.900: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 14:58:46.905: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976935s
    Nov 17 14:58:46.905: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Nov 17 14:58:46.905: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 11/17/23 14:58:46.905
    Nov 17 14:58:46.906: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6366 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:46.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:46.907: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:46.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6366/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 11/17/23 14:58:47.031
    Nov 17 14:58:47.031: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6366 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Nov 17 14:58:47.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 14:58:47.032: INFO: ExecWithOptions: Clientset creation
    Nov 17 14:58:47.032: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6366/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Nov 17 14:58:47.150: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:58:47.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6366" for this suite. 11/17/23 14:58:47.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:58:47.181
Nov 17 14:58:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 14:58:47.182
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:47.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:47.209
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 11/17/23 14:59:04.217
STEP: Creating a ResourceQuota 11/17/23 14:59:09.221
STEP: Ensuring resource quota status is calculated 11/17/23 14:59:09.228
STEP: Creating a ConfigMap 11/17/23 14:59:11.232
STEP: Ensuring resource quota status captures configMap creation 11/17/23 14:59:11.243
STEP: Deleting a ConfigMap 11/17/23 14:59:13.248
STEP: Ensuring resource quota status released usage 11/17/23 14:59:13.254
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 14:59:15.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6967" for this suite. 11/17/23 14:59:15.263
------------------------------
â€¢ [SLOW TEST] [28.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:58:47.181
    Nov 17 14:58:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 14:58:47.182
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:58:47.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:58:47.209
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 11/17/23 14:59:04.217
    STEP: Creating a ResourceQuota 11/17/23 14:59:09.221
    STEP: Ensuring resource quota status is calculated 11/17/23 14:59:09.228
    STEP: Creating a ConfigMap 11/17/23 14:59:11.232
    STEP: Ensuring resource quota status captures configMap creation 11/17/23 14:59:11.243
    STEP: Deleting a ConfigMap 11/17/23 14:59:13.248
    STEP: Ensuring resource quota status released usage 11/17/23 14:59:13.254
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:59:15.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6967" for this suite. 11/17/23 14:59:15.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:59:15.27
Nov 17 14:59:15.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 14:59:15.272
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:15.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:15.298
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 11/17/23 14:59:15.302
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 14:59:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-103" for this suite. 11/17/23 14:59:15.311
------------------------------
â€¢ [0.047 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:59:15.27
    Nov 17 14:59:15.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 14:59:15.272
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:15.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:15.298
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 11/17/23 14:59:15.302
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:59:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-103" for this suite. 11/17/23 14:59:15.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:59:15.32
Nov 17 14:59:15.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 14:59:15.322
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:15.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:15.34
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 11/17/23 14:59:15.343
Nov 17 14:59:15.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884" in namespace "projected-1726" to be "Succeeded or Failed"
Nov 17 14:59:15.355: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340857ms
Nov 17 14:59:17.359: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008424831s
Nov 17 14:59:19.359: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009042045s
STEP: Saw pod success 11/17/23 14:59:19.359
Nov 17 14:59:19.360: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884" satisfied condition "Succeeded or Failed"
Nov 17 14:59:19.363: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 container client-container: <nil>
STEP: delete the pod 11/17/23 14:59:19.382
Nov 17 14:59:19.396: INFO: Waiting for pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 to disappear
Nov 17 14:59:19.401: INFO: Pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 14:59:19.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1726" for this suite. 11/17/23 14:59:19.406
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:59:15.32
    Nov 17 14:59:15.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 14:59:15.322
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:15.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:15.34
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 11/17/23 14:59:15.343
    Nov 17 14:59:15.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884" in namespace "projected-1726" to be "Succeeded or Failed"
    Nov 17 14:59:15.355: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Pending", Reason="", readiness=false. Elapsed: 4.340857ms
    Nov 17 14:59:17.359: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008424831s
    Nov 17 14:59:19.359: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009042045s
    STEP: Saw pod success 11/17/23 14:59:19.359
    Nov 17 14:59:19.360: INFO: Pod "downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884" satisfied condition "Succeeded or Failed"
    Nov 17 14:59:19.363: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 container client-container: <nil>
    STEP: delete the pod 11/17/23 14:59:19.382
    Nov 17 14:59:19.396: INFO: Waiting for pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 to disappear
    Nov 17 14:59:19.401: INFO: Pod downwardapi-volume-1e51d82c-a1b6-49d1-bbe6-381ee8402884 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:59:19.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1726" for this suite. 11/17/23 14:59:19.406
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:59:19.415
Nov 17 14:59:19.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-runtime 11/17/23 14:59:19.416
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:19.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:19.438
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 11/17/23 14:59:19.442
STEP: wait for the container to reach Succeeded 11/17/23 14:59:19.45
STEP: get the container status 11/17/23 14:59:22.469
STEP: the container should be terminated 11/17/23 14:59:22.476
STEP: the termination message should be set 11/17/23 14:59:22.476
Nov 17 14:59:22.476: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 11/17/23 14:59:22.476
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 17 14:59:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3539" for this suite. 11/17/23 14:59:22.51
------------------------------
â€¢ [3.101 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:59:19.415
    Nov 17 14:59:19.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-runtime 11/17/23 14:59:19.416
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:19.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:19.438
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 11/17/23 14:59:19.442
    STEP: wait for the container to reach Succeeded 11/17/23 14:59:19.45
    STEP: get the container status 11/17/23 14:59:22.469
    STEP: the container should be terminated 11/17/23 14:59:22.476
    STEP: the termination message should be set 11/17/23 14:59:22.476
    Nov 17 14:59:22.476: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 11/17/23 14:59:22.476
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:59:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3539" for this suite. 11/17/23 14:59:22.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:59:22.517
Nov 17 14:59:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename endpointslice 11/17/23 14:59:22.518
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:22.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:22.544
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 11/17/23 14:59:27.665
STEP: referencing matching pods with named port 11/17/23 14:59:32.675
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/17/23 14:59:37.683
STEP: recreating EndpointSlices after they've been deleted 11/17/23 14:59:42.693
Nov 17 14:59:42.712: INFO: EndpointSlice for Service endpointslice-4700/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 17 14:59:52.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4700" for this suite. 11/17/23 14:59:52.728
------------------------------
â€¢ [SLOW TEST] [30.218 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:59:22.517
    Nov 17 14:59:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename endpointslice 11/17/23 14:59:22.518
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:22.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:22.544
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 11/17/23 14:59:27.665
    STEP: referencing matching pods with named port 11/17/23 14:59:32.675
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 11/17/23 14:59:37.683
    STEP: recreating EndpointSlices after they've been deleted 11/17/23 14:59:42.693
    Nov 17 14:59:42.712: INFO: EndpointSlice for Service endpointslice-4700/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 17 14:59:52.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4700" for this suite. 11/17/23 14:59:52.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 14:59:52.739
Nov 17 14:59:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 14:59:52.74
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:52.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:52.767
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5571 11/17/23 14:59:52.77
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 11/17/23 14:59:52.777
STEP: Creating pod with conflicting port in namespace statefulset-5571 11/17/23 14:59:52.789
STEP: Waiting until pod test-pod will start running in namespace statefulset-5571 11/17/23 14:59:52.804
Nov 17 14:59:52.804: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5571" to be "running"
Nov 17 14:59:52.809: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744922ms
Nov 17 14:59:54.815: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010810426s
Nov 17 14:59:54.815: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5571 11/17/23 14:59:54.815
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5571 11/17/23 14:59:54.823
Nov 17 14:59:54.842: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Pending. Waiting for statefulset controller to delete.
Nov 17 14:59:54.874: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Failed. Waiting for statefulset controller to delete.
Nov 17 14:59:54.888: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Failed. Waiting for statefulset controller to delete.
Nov 17 14:59:54.895: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5571
STEP: Removing pod with conflicting port in namespace statefulset-5571 11/17/23 14:59:54.895
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5571 and will be in running state 11/17/23 14:59:54.93
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 14:59:58.969: INFO: Deleting all statefulset in ns statefulset-5571
Nov 17 14:59:58.972: INFO: Scaling statefulset ss to 0
Nov 17 15:00:08.996: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 15:00:09.000: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5571" for this suite. 11/17/23 15:00:09.048
------------------------------
â€¢ [SLOW TEST] [16.330 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 14:59:52.739
    Nov 17 14:59:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 14:59:52.74
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 14:59:52.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 14:59:52.767
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5571 11/17/23 14:59:52.77
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 11/17/23 14:59:52.777
    STEP: Creating pod with conflicting port in namespace statefulset-5571 11/17/23 14:59:52.789
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5571 11/17/23 14:59:52.804
    Nov 17 14:59:52.804: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5571" to be "running"
    Nov 17 14:59:52.809: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744922ms
    Nov 17 14:59:54.815: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010810426s
    Nov 17 14:59:54.815: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5571 11/17/23 14:59:54.815
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5571 11/17/23 14:59:54.823
    Nov 17 14:59:54.842: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Pending. Waiting for statefulset controller to delete.
    Nov 17 14:59:54.874: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 17 14:59:54.888: INFO: Observed stateful pod in namespace: statefulset-5571, name: ss-0, uid: 7cca69c1-5905-46e8-bbec-783f4815049c, status phase: Failed. Waiting for statefulset controller to delete.
    Nov 17 14:59:54.895: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5571
    STEP: Removing pod with conflicting port in namespace statefulset-5571 11/17/23 14:59:54.895
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5571 and will be in running state 11/17/23 14:59:54.93
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 14:59:58.969: INFO: Deleting all statefulset in ns statefulset-5571
    Nov 17 14:59:58.972: INFO: Scaling statefulset ss to 0
    Nov 17 15:00:08.996: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 15:00:09.000: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5571" for this suite. 11/17/23 15:00:09.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:09.072
Nov 17 15:00:09.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:00:09.074
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:09.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:09.123
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 11/17/23 15:00:09.126
Nov 17 15:00:09.141: INFO: Waiting up to 5m0s for pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd" in namespace "emptydir-5128" to be "Succeeded or Failed"
Nov 17 15:00:09.150: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14752ms
Nov 17 15:00:11.154: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.013493647s
Nov 17 15:00:13.155: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014075582s
STEP: Saw pod success 11/17/23 15:00:13.155
Nov 17 15:00:13.155: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd" satisfied condition "Succeeded or Failed"
Nov 17 15:00:13.158: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd container test-container: <nil>
STEP: delete the pod 11/17/23 15:00:13.164
Nov 17 15:00:13.173: INFO: Waiting for pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd to disappear
Nov 17 15:00:13.177: INFO: Pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:13.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5128" for this suite. 11/17/23 15:00:13.183
------------------------------
â€¢ [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:09.072
    Nov 17 15:00:09.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:00:09.074
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:09.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:09.123
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 11/17/23 15:00:09.126
    Nov 17 15:00:09.141: INFO: Waiting up to 5m0s for pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd" in namespace "emptydir-5128" to be "Succeeded or Failed"
    Nov 17 15:00:09.150: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14752ms
    Nov 17 15:00:11.154: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Running", Reason="", readiness=false. Elapsed: 2.013493647s
    Nov 17 15:00:13.155: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014075582s
    STEP: Saw pod success 11/17/23 15:00:13.155
    Nov 17 15:00:13.155: INFO: Pod "pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd" satisfied condition "Succeeded or Failed"
    Nov 17 15:00:13.158: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd container test-container: <nil>
    STEP: delete the pod 11/17/23 15:00:13.164
    Nov 17 15:00:13.173: INFO: Waiting for pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd to disappear
    Nov 17 15:00:13.177: INFO: Pod pod-0a2a9425-19ee-49dc-a568-7ddc148a07cd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:13.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5128" for this suite. 11/17/23 15:00:13.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:13.193
Nov 17 15:00:13.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 15:00:13.194
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:13.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:13.214
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-624160aa-14b9-48be-92a5-f977f612d395 11/17/23 15:00:13.218
STEP: Creating a pod to test consume secrets 11/17/23 15:00:13.225
Nov 17 15:00:13.234: INFO: Waiting up to 5m0s for pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680" in namespace "secrets-7306" to be "Succeeded or Failed"
Nov 17 15:00:13.240: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Pending", Reason="", readiness=false. Elapsed: 5.562248ms
Nov 17 15:00:15.244: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010285553s
Nov 17 15:00:17.243: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009278299s
STEP: Saw pod success 11/17/23 15:00:17.244
Nov 17 15:00:17.244: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680" satisfied condition "Succeeded or Failed"
Nov 17 15:00:17.246: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 15:00:17.252
Nov 17 15:00:17.266: INFO: Waiting for pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 to disappear
Nov 17 15:00:17.269: INFO: Pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7306" for this suite. 11/17/23 15:00:17.275
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:13.193
    Nov 17 15:00:13.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 15:00:13.194
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:13.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:13.214
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-624160aa-14b9-48be-92a5-f977f612d395 11/17/23 15:00:13.218
    STEP: Creating a pod to test consume secrets 11/17/23 15:00:13.225
    Nov 17 15:00:13.234: INFO: Waiting up to 5m0s for pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680" in namespace "secrets-7306" to be "Succeeded or Failed"
    Nov 17 15:00:13.240: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Pending", Reason="", readiness=false. Elapsed: 5.562248ms
    Nov 17 15:00:15.244: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010285553s
    Nov 17 15:00:17.243: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009278299s
    STEP: Saw pod success 11/17/23 15:00:17.244
    Nov 17 15:00:17.244: INFO: Pod "pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680" satisfied condition "Succeeded or Failed"
    Nov 17 15:00:17.246: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 15:00:17.252
    Nov 17 15:00:17.266: INFO: Waiting for pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 to disappear
    Nov 17 15:00:17.269: INFO: Pod pod-secrets-6edc3ea3-ec2f-4650-aff1-4fd938951680 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7306" for this suite. 11/17/23 15:00:17.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:17.286
Nov 17 15:00:17.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:00:17.287
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:17.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:17.307
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 11/17/23 15:00:17.311
STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:00:17.316
STEP: Creating a ResourceQuota with not terminating scope 11/17/23 15:00:19.322
STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:00:19.328
STEP: Creating a long running pod 11/17/23 15:00:21.333
STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/17/23 15:00:21.347
STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/17/23 15:00:23.352
STEP: Deleting the pod 11/17/23 15:00:25.356
STEP: Ensuring resource quota status released the pod usage 11/17/23 15:00:25.374
STEP: Creating a terminating pod 11/17/23 15:00:27.38
STEP: Ensuring resource quota with terminating scope captures the pod usage 11/17/23 15:00:27.395
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/17/23 15:00:29.4
STEP: Deleting the pod 11/17/23 15:00:31.404
STEP: Ensuring resource quota status released the pod usage 11/17/23 15:00:31.425
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4484" for this suite. 11/17/23 15:00:33.434
------------------------------
â€¢ [SLOW TEST] [16.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:17.286
    Nov 17 15:00:17.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:00:17.287
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:17.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:17.307
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 11/17/23 15:00:17.311
    STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:00:17.316
    STEP: Creating a ResourceQuota with not terminating scope 11/17/23 15:00:19.322
    STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:00:19.328
    STEP: Creating a long running pod 11/17/23 15:00:21.333
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 11/17/23 15:00:21.347
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 11/17/23 15:00:23.352
    STEP: Deleting the pod 11/17/23 15:00:25.356
    STEP: Ensuring resource quota status released the pod usage 11/17/23 15:00:25.374
    STEP: Creating a terminating pod 11/17/23 15:00:27.38
    STEP: Ensuring resource quota with terminating scope captures the pod usage 11/17/23 15:00:27.395
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 11/17/23 15:00:29.4
    STEP: Deleting the pod 11/17/23 15:00:31.404
    STEP: Ensuring resource quota status released the pod usage 11/17/23 15:00:31.425
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4484" for this suite. 11/17/23 15:00:33.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:33.442
Nov 17 15:00:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename events 11/17/23 15:00:33.444
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.465
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 11/17/23 15:00:33.469
Nov 17 15:00:33.474: INFO: created test-event-1
Nov 17 15:00:33.478: INFO: created test-event-2
Nov 17 15:00:33.483: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 11/17/23 15:00:33.483
STEP: delete collection of events 11/17/23 15:00:33.486
Nov 17 15:00:33.486: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 11/17/23 15:00:33.501
Nov 17 15:00:33.501: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:33.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7654" for this suite. 11/17/23 15:00:33.51
------------------------------
â€¢ [0.072 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:33.442
    Nov 17 15:00:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename events 11/17/23 15:00:33.444
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.465
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 11/17/23 15:00:33.469
    Nov 17 15:00:33.474: INFO: created test-event-1
    Nov 17 15:00:33.478: INFO: created test-event-2
    Nov 17 15:00:33.483: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 11/17/23 15:00:33.483
    STEP: delete collection of events 11/17/23 15:00:33.486
    Nov 17 15:00:33.486: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 11/17/23 15:00:33.501
    Nov 17 15:00:33.501: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:33.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7654" for this suite. 11/17/23 15:00:33.51
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:33.517
Nov 17 15:00:33.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:00:33.519
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.538
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:33.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1914" for this suite. 11/17/23 15:00:33.555
------------------------------
â€¢ [0.045 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:33.517
    Nov 17 15:00:33.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:00:33.519
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.538
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:33.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1914" for this suite. 11/17/23 15:00:33.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:33.564
Nov 17 15:00:33.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename subpath 11/17/23 15:00:33.565
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.586
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/17/23 15:00:33.589
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-kh6j 11/17/23 15:00:33.598
STEP: Creating a pod to test atomic-volume-subpath 11/17/23 15:00:33.598
Nov 17 15:00:33.608: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-kh6j" in namespace "subpath-1492" to be "Succeeded or Failed"
Nov 17 15:00:33.613: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450784ms
Nov 17 15:00:35.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.010459038s
Nov 17 15:00:37.618: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 4.009983703s
Nov 17 15:00:39.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.010020764s
Nov 17 15:00:41.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 8.00842219s
Nov 17 15:00:43.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 10.008876784s
Nov 17 15:00:45.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 12.008528559s
Nov 17 15:00:47.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 14.010188414s
Nov 17 15:00:49.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 16.010376385s
Nov 17 15:00:51.618: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 18.009538168s
Nov 17 15:00:53.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 20.00885723s
Nov 17 15:00:55.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=false. Elapsed: 22.008140508s
Nov 17 15:00:57.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008298297s
STEP: Saw pod success 11/17/23 15:00:57.617
Nov 17 15:00:57.617: INFO: Pod "pod-subpath-test-secret-kh6j" satisfied condition "Succeeded or Failed"
Nov 17 15:00:57.621: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-secret-kh6j container test-container-subpath-secret-kh6j: <nil>
STEP: delete the pod 11/17/23 15:00:57.627
Nov 17 15:00:57.642: INFO: Waiting for pod pod-subpath-test-secret-kh6j to disappear
Nov 17 15:00:57.644: INFO: Pod pod-subpath-test-secret-kh6j no longer exists
STEP: Deleting pod pod-subpath-test-secret-kh6j 11/17/23 15:00:57.645
Nov 17 15:00:57.645: INFO: Deleting pod "pod-subpath-test-secret-kh6j" in namespace "subpath-1492"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 17 15:00:57.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1492" for this suite. 11/17/23 15:00:57.652
------------------------------
â€¢ [SLOW TEST] [24.096 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:33.564
    Nov 17 15:00:33.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename subpath 11/17/23 15:00:33.565
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:33.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:33.586
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/17/23 15:00:33.589
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-kh6j 11/17/23 15:00:33.598
    STEP: Creating a pod to test atomic-volume-subpath 11/17/23 15:00:33.598
    Nov 17 15:00:33.608: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-kh6j" in namespace "subpath-1492" to be "Succeeded or Failed"
    Nov 17 15:00:33.613: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450784ms
    Nov 17 15:00:35.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.010459038s
    Nov 17 15:00:37.618: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 4.009983703s
    Nov 17 15:00:39.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.010020764s
    Nov 17 15:00:41.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 8.00842219s
    Nov 17 15:00:43.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 10.008876784s
    Nov 17 15:00:45.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 12.008528559s
    Nov 17 15:00:47.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 14.010188414s
    Nov 17 15:00:49.619: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 16.010376385s
    Nov 17 15:00:51.618: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 18.009538168s
    Nov 17 15:00:53.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=true. Elapsed: 20.00885723s
    Nov 17 15:00:55.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Running", Reason="", readiness=false. Elapsed: 22.008140508s
    Nov 17 15:00:57.617: INFO: Pod "pod-subpath-test-secret-kh6j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008298297s
    STEP: Saw pod success 11/17/23 15:00:57.617
    Nov 17 15:00:57.617: INFO: Pod "pod-subpath-test-secret-kh6j" satisfied condition "Succeeded or Failed"
    Nov 17 15:00:57.621: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-secret-kh6j container test-container-subpath-secret-kh6j: <nil>
    STEP: delete the pod 11/17/23 15:00:57.627
    Nov 17 15:00:57.642: INFO: Waiting for pod pod-subpath-test-secret-kh6j to disappear
    Nov 17 15:00:57.644: INFO: Pod pod-subpath-test-secret-kh6j no longer exists
    STEP: Deleting pod pod-subpath-test-secret-kh6j 11/17/23 15:00:57.645
    Nov 17 15:00:57.645: INFO: Deleting pod "pod-subpath-test-secret-kh6j" in namespace "subpath-1492"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:00:57.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1492" for this suite. 11/17/23 15:00:57.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:00:57.665
Nov 17 15:00:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-runtime 11/17/23 15:00:57.666
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:57.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:57.691
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 11/17/23 15:00:57.694
STEP: wait for the container to reach Failed 11/17/23 15:00:57.702
STEP: get the container status 11/17/23 15:01:01.725
STEP: the container should be terminated 11/17/23 15:01:01.729
STEP: the termination message should be set 11/17/23 15:01:01.729
Nov 17 15:01:01.729: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 11/17/23 15:01:01.729
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Nov 17 15:01:01.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6083" for this suite. 11/17/23 15:01:01.752
------------------------------
â€¢ [4.093 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:00:57.665
    Nov 17 15:00:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-runtime 11/17/23 15:00:57.666
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:00:57.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:00:57.691
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 11/17/23 15:00:57.694
    STEP: wait for the container to reach Failed 11/17/23 15:00:57.702
    STEP: get the container status 11/17/23 15:01:01.725
    STEP: the container should be terminated 11/17/23 15:01:01.729
    STEP: the termination message should be set 11/17/23 15:01:01.729
    Nov 17 15:01:01.729: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 11/17/23 15:01:01.729
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:01:01.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6083" for this suite. 11/17/23 15:01:01.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:01:01.762
Nov 17 15:01:01.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 15:01:01.764
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:01:01.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:01:01.787
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/17/23 15:01:01.791
Nov 17 15:01:01.809: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 17 15:01:06.814: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 15:01:06.814
STEP: getting scale subresource 11/17/23 15:01:06.815
STEP: updating a scale subresource 11/17/23 15:01:06.818
STEP: verifying the replicaset Spec.Replicas was modified 11/17/23 15:01:06.825
STEP: Patch a scale subresource 11/17/23 15:01:06.827
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:01:06.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-404" for this suite. 11/17/23 15:01:06.858
------------------------------
â€¢ [SLOW TEST] [5.107 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:01:01.762
    Nov 17 15:01:01.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 15:01:01.764
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:01:01.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:01:01.787
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 11/17/23 15:01:01.791
    Nov 17 15:01:01.809: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 17 15:01:06.814: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 15:01:06.814
    STEP: getting scale subresource 11/17/23 15:01:06.815
    STEP: updating a scale subresource 11/17/23 15:01:06.818
    STEP: verifying the replicaset Spec.Replicas was modified 11/17/23 15:01:06.825
    STEP: Patch a scale subresource 11/17/23 15:01:06.827
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:01:06.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-404" for this suite. 11/17/23 15:01:06.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:01:06.874
Nov 17 15:01:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 15:01:06.875
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:01:06.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:01:06.896
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 11/17/23 15:01:06.9
Nov 17 15:01:06.911: INFO: Waiting up to 2m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615" to be "running"
Nov 17 15:01:06.916: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400577ms
Nov 17 15:01:08.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008020826s
Nov 17 15:01:10.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009098723s
Nov 17 15:01:12.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009442251s
Nov 17 15:01:14.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008226549s
Nov 17 15:01:16.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008480583s
Nov 17 15:01:18.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008641874s
Nov 17 15:01:20.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007692168s
Nov 17 15:01:22.923: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011796087s
Nov 17 15:01:24.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008657749s
Nov 17 15:01:26.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008234957s
Nov 17 15:01:28.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00920187s
Nov 17 15:01:30.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007840222s
Nov 17 15:01:32.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009210065s
Nov 17 15:01:34.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008177423s
Nov 17 15:01:36.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009047678s
Nov 17 15:01:38.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01051621s
Nov 17 15:01:40.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009401465s
Nov 17 15:01:42.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011052193s
Nov 17 15:01:44.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011021095s
Nov 17 15:01:46.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009486793s
Nov 17 15:01:48.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010171097s
Nov 17 15:01:50.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009029633s
Nov 17 15:01:52.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01042549s
Nov 17 15:01:54.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009371197s
Nov 17 15:01:56.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010134888s
Nov 17 15:01:58.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008111021s
Nov 17 15:02:00.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009592705s
Nov 17 15:02:02.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00977702s
Nov 17 15:02:04.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008294756s
Nov 17 15:02:06.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008943511s
Nov 17 15:02:08.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010672883s
Nov 17 15:02:10.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009115931s
Nov 17 15:02:12.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008675347s
Nov 17 15:02:14.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008339291s
Nov 17 15:02:16.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009351573s
Nov 17 15:02:18.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008235813s
Nov 17 15:02:20.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010073163s
Nov 17 15:02:22.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00841013s
Nov 17 15:02:24.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009874707s
Nov 17 15:02:26.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008975582s
Nov 17 15:02:28.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009537471s
Nov 17 15:02:30.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009494266s
Nov 17 15:02:32.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010744035s
Nov 17 15:02:34.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008183002s
Nov 17 15:02:36.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008612158s
Nov 17 15:02:38.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009033543s
Nov 17 15:02:40.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010244403s
Nov 17 15:02:42.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010729383s
Nov 17 15:02:44.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008297531s
Nov 17 15:02:46.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011152001s
Nov 17 15:02:48.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011165414s
Nov 17 15:02:50.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010228129s
Nov 17 15:02:52.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009982657s
Nov 17 15:02:54.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008536047s
Nov 17 15:02:56.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010531477s
Nov 17 15:02:58.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009122931s
Nov 17 15:03:00.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009243511s
Nov 17 15:03:02.923: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011255166s
Nov 17 15:03:04.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009282873s
Nov 17 15:03:06.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010025559s
Nov 17 15:03:06.925: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013273079s
STEP: updating the pod 11/17/23 15:03:06.925
Nov 17 15:03:07.440: INFO: Successfully updated pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31"
STEP: waiting for pod running 11/17/23 15:03:07.44
Nov 17 15:03:07.440: INFO: Waiting up to 2m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615" to be "running"
Nov 17 15:03:07.444: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513775ms
Nov 17 15:03:09.447: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Running", Reason="", readiness=true. Elapsed: 2.007105602s
Nov 17 15:03:09.448: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" satisfied condition "running"
STEP: deleting the pod gracefully 11/17/23 15:03:09.448
Nov 17 15:03:09.448: INFO: Deleting pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615"
Nov 17 15:03:09.456: INFO: Wait up to 5m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 15:03:41.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6615" for this suite. 11/17/23 15:03:41.468
------------------------------
â€¢ [SLOW TEST] [154.599 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:01:06.874
    Nov 17 15:01:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 15:01:06.875
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:01:06.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:01:06.896
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 11/17/23 15:01:06.9
    Nov 17 15:01:06.911: INFO: Waiting up to 2m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615" to be "running"
    Nov 17 15:01:06.916: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.400577ms
    Nov 17 15:01:08.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008020826s
    Nov 17 15:01:10.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009098723s
    Nov 17 15:01:12.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009442251s
    Nov 17 15:01:14.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008226549s
    Nov 17 15:01:16.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008480583s
    Nov 17 15:01:18.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008641874s
    Nov 17 15:01:20.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007692168s
    Nov 17 15:01:22.923: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011796087s
    Nov 17 15:01:24.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008657749s
    Nov 17 15:01:26.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008234957s
    Nov 17 15:01:28.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00920187s
    Nov 17 15:01:30.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007840222s
    Nov 17 15:01:32.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009210065s
    Nov 17 15:01:34.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008177423s
    Nov 17 15:01:36.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009047678s
    Nov 17 15:01:38.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 32.01051621s
    Nov 17 15:01:40.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009401465s
    Nov 17 15:01:42.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011052193s
    Nov 17 15:01:44.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011021095s
    Nov 17 15:01:46.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009486793s
    Nov 17 15:01:48.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010171097s
    Nov 17 15:01:50.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009029633s
    Nov 17 15:01:52.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01042549s
    Nov 17 15:01:54.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009371197s
    Nov 17 15:01:56.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010134888s
    Nov 17 15:01:58.919: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008111021s
    Nov 17 15:02:00.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009592705s
    Nov 17 15:02:02.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00977702s
    Nov 17 15:02:04.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008294756s
    Nov 17 15:02:06.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008943511s
    Nov 17 15:02:08.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010672883s
    Nov 17 15:02:10.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009115931s
    Nov 17 15:02:12.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008675347s
    Nov 17 15:02:14.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008339291s
    Nov 17 15:02:16.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009351573s
    Nov 17 15:02:18.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008235813s
    Nov 17 15:02:20.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010073163s
    Nov 17 15:02:22.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00841013s
    Nov 17 15:02:24.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009874707s
    Nov 17 15:02:26.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008975582s
    Nov 17 15:02:28.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009537471s
    Nov 17 15:02:30.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009494266s
    Nov 17 15:02:32.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010744035s
    Nov 17 15:02:34.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008183002s
    Nov 17 15:02:36.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008612158s
    Nov 17 15:02:38.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009033543s
    Nov 17 15:02:40.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010244403s
    Nov 17 15:02:42.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010729383s
    Nov 17 15:02:44.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008297531s
    Nov 17 15:02:46.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.011152001s
    Nov 17 15:02:48.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011165414s
    Nov 17 15:02:50.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010228129s
    Nov 17 15:02:52.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009982657s
    Nov 17 15:02:54.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008536047s
    Nov 17 15:02:56.922: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010531477s
    Nov 17 15:02:58.920: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009122931s
    Nov 17 15:03:00.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009243511s
    Nov 17 15:03:02.923: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011255166s
    Nov 17 15:03:04.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009282873s
    Nov 17 15:03:06.921: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010025559s
    Nov 17 15:03:06.925: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013273079s
    STEP: updating the pod 11/17/23 15:03:06.925
    Nov 17 15:03:07.440: INFO: Successfully updated pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31"
    STEP: waiting for pod running 11/17/23 15:03:07.44
    Nov 17 15:03:07.440: INFO: Waiting up to 2m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615" to be "running"
    Nov 17 15:03:07.444: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513775ms
    Nov 17 15:03:09.447: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31": Phase="Running", Reason="", readiness=true. Elapsed: 2.007105602s
    Nov 17 15:03:09.448: INFO: Pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" satisfied condition "running"
    STEP: deleting the pod gracefully 11/17/23 15:03:09.448
    Nov 17 15:03:09.448: INFO: Deleting pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" in namespace "var-expansion-6615"
    Nov 17 15:03:09.456: INFO: Wait up to 5m0s for pod "var-expansion-76d7e8d1-fc36-4b89-8d75-a13a42ac5d31" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:03:41.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6615" for this suite. 11/17/23 15:03:41.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:03:41.474
Nov 17 15:03:41.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-watch 11/17/23 15:03:41.476
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:03:41.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:03:41.494
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Nov 17 15:03:41.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Creating first CR  11/17/23 15:03:44.052
Nov 17 15:03:44.058: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:03:44Z]] name:name1 resourceVersion:60033 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 11/17/23 15:03:54.061
Nov 17 15:03:54.068: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:03:54Z]] name:name2 resourceVersion:60146 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 11/17/23 15:04:04.069
Nov 17 15:04:04.078: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:04Z]] name:name1 resourceVersion:60239 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 11/17/23 15:04:14.082
Nov 17 15:04:14.091: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:14Z]] name:name2 resourceVersion:60294 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 11/17/23 15:04:24.091
Nov 17 15:04:24.112: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:04Z]] name:name1 resourceVersion:60349 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 11/17/23 15:04:34.112
Nov 17 15:04:34.121: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:14Z]] name:name2 resourceVersion:60404 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:04:44.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-6" for this suite. 11/17/23 15:04:44.647
------------------------------
â€¢ [SLOW TEST] [63.190 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:03:41.474
    Nov 17 15:03:41.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-watch 11/17/23 15:03:41.476
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:03:41.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:03:41.494
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Nov 17 15:03:41.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Creating first CR  11/17/23 15:03:44.052
    Nov 17 15:03:44.058: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:03:44Z]] name:name1 resourceVersion:60033 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 11/17/23 15:03:54.061
    Nov 17 15:03:54.068: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:03:54Z]] name:name2 resourceVersion:60146 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 11/17/23 15:04:04.069
    Nov 17 15:04:04.078: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:04Z]] name:name1 resourceVersion:60239 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 11/17/23 15:04:14.082
    Nov 17 15:04:14.091: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:14Z]] name:name2 resourceVersion:60294 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 11/17/23 15:04:24.091
    Nov 17 15:04:24.112: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:04Z]] name:name1 resourceVersion:60349 uid:fcd6032a-c32f-44fc-8c9c-df4e146caa2a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 11/17/23 15:04:34.112
    Nov 17 15:04:34.121: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-11-17T15:03:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-11-17T15:04:14Z]] name:name2 resourceVersion:60404 uid:0da5e007-990d-41e2-a802-f2dd67c0f36a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:04:44.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-6" for this suite. 11/17/23 15:04:44.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:04:44.673
Nov 17 15:04:44.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 15:04:44.674
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:44.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:44.7
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9018 11/17/23 15:04:44.704
STEP: changing the ExternalName service to type=NodePort 11/17/23 15:04:44.719
STEP: creating replication controller externalname-service in namespace services-9018 11/17/23 15:04:44.75
I1117 15:04:44.771813      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9018, replica count: 2
I1117 15:04:47.823676      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 15:04:47.823: INFO: Creating new exec pod
Nov 17 15:04:47.830: INFO: Waiting up to 5m0s for pod "execpodwxx4j" in namespace "services-9018" to be "running"
Nov 17 15:04:47.834: INFO: Pod "execpodwxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.7765ms
Nov 17 15:04:49.838: INFO: Pod "execpodwxx4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503756s
Nov 17 15:04:49.838: INFO: Pod "execpodwxx4j" satisfied condition "running"
Nov 17 15:04:50.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Nov 17 15:04:51.067: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 17 15:04:51.067: INFO: stdout: ""
Nov 17 15:04:51.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 10.97.22.58 80'
Nov 17 15:04:51.298: INFO: stderr: "+ nc -v -z -w 2 10.97.22.58 80\nConnection to 10.97.22.58 80 port [tcp/http] succeeded!\n"
Nov 17 15:04:51.298: INFO: stdout: ""
Nov 17 15:04:51.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 30193'
Nov 17 15:04:51.505: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 30193\nConnection to 172.16.0.3 30193 port [tcp/*] succeeded!\n"
Nov 17 15:04:51.505: INFO: stdout: ""
Nov 17 15:04:51.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 30193'
Nov 17 15:04:51.698: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 30193\nConnection to 172.16.0.4 30193 port [tcp/*] succeeded!\n"
Nov 17 15:04:51.698: INFO: stdout: ""
Nov 17 15:04:51.698: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 15:04:51.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9018" for this suite. 11/17/23 15:04:51.774
------------------------------
â€¢ [SLOW TEST] [7.111 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:04:44.673
    Nov 17 15:04:44.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 15:04:44.674
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:44.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:44.7
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9018 11/17/23 15:04:44.704
    STEP: changing the ExternalName service to type=NodePort 11/17/23 15:04:44.719
    STEP: creating replication controller externalname-service in namespace services-9018 11/17/23 15:04:44.75
    I1117 15:04:44.771813      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9018, replica count: 2
    I1117 15:04:47.823676      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 15:04:47.823: INFO: Creating new exec pod
    Nov 17 15:04:47.830: INFO: Waiting up to 5m0s for pod "execpodwxx4j" in namespace "services-9018" to be "running"
    Nov 17 15:04:47.834: INFO: Pod "execpodwxx4j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.7765ms
    Nov 17 15:04:49.838: INFO: Pod "execpodwxx4j": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503756s
    Nov 17 15:04:49.838: INFO: Pod "execpodwxx4j" satisfied condition "running"
    Nov 17 15:04:50.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Nov 17 15:04:51.067: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Nov 17 15:04:51.067: INFO: stdout: ""
    Nov 17 15:04:51.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 10.97.22.58 80'
    Nov 17 15:04:51.298: INFO: stderr: "+ nc -v -z -w 2 10.97.22.58 80\nConnection to 10.97.22.58 80 port [tcp/http] succeeded!\n"
    Nov 17 15:04:51.298: INFO: stdout: ""
    Nov 17 15:04:51.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 172.16.0.3 30193'
    Nov 17 15:04:51.505: INFO: stderr: "+ nc -v -z -w 2 172.16.0.3 30193\nConnection to 172.16.0.3 30193 port [tcp/*] succeeded!\n"
    Nov 17 15:04:51.505: INFO: stdout: ""
    Nov 17 15:04:51.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-9018 exec execpodwxx4j -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 30193'
    Nov 17 15:04:51.698: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 30193\nConnection to 172.16.0.4 30193 port [tcp/*] succeeded!\n"
    Nov 17 15:04:51.698: INFO: stdout: ""
    Nov 17 15:04:51.698: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:04:51.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9018" for this suite. 11/17/23 15:04:51.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:04:51.785
Nov 17 15:04:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename endpointslice 11/17/23 15:04:51.787
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:51.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:51.83
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Nov 17 15:04:51.843: INFO: Endpoints addresses: [172.16.0.2] , ports: [6443]
Nov 17 15:04:51.843: INFO: EndpointSlices addresses: [172.16.0.2] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Nov 17 15:04:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8111" for this suite. 11/17/23 15:04:51.851
------------------------------
â€¢ [0.075 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:04:51.785
    Nov 17 15:04:51.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename endpointslice 11/17/23 15:04:51.787
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:51.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:51.83
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Nov 17 15:04:51.843: INFO: Endpoints addresses: [172.16.0.2] , ports: [6443]
    Nov 17 15:04:51.843: INFO: EndpointSlices addresses: [172.16.0.2] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:04:51.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8111" for this suite. 11/17/23 15:04:51.851
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:04:51.86
Nov 17 15:04:51.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 15:04:51.862
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:51.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:51.887
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 11/17/23 15:04:51.89
STEP: Ensure pods equal to parallelism count is attached to the job 11/17/23 15:04:51.897
STEP: patching /status 11/17/23 15:04:55.902
STEP: updating /status 11/17/23 15:04:55.912
STEP: get /status 11/17/23 15:04:55.92
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 15:04:55.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7021" for this suite. 11/17/23 15:04:55.929
------------------------------
â€¢ [4.077 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:04:51.86
    Nov 17 15:04:51.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 15:04:51.862
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:51.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:51.887
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 11/17/23 15:04:51.89
    STEP: Ensure pods equal to parallelism count is attached to the job 11/17/23 15:04:51.897
    STEP: patching /status 11/17/23 15:04:55.902
    STEP: updating /status 11/17/23 15:04:55.912
    STEP: get /status 11/17/23 15:04:55.92
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:04:55.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7021" for this suite. 11/17/23 15:04:55.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:04:55.938
Nov 17 15:04:55.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename podtemplate 11/17/23 15:04:55.939
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:55.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:55.966
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 11/17/23 15:04:55.969
STEP: Replace a pod template 11/17/23 15:04:55.977
Nov 17 15:04:55.988: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 17 15:04:55.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1277" for this suite. 11/17/23 15:04:55.992
------------------------------
â€¢ [0.061 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:04:55.938
    Nov 17 15:04:55.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename podtemplate 11/17/23 15:04:55.939
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:55.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:55.966
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 11/17/23 15:04:55.969
    STEP: Replace a pod template 11/17/23 15:04:55.977
    Nov 17 15:04:55.988: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:04:55.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1277" for this suite. 11/17/23 15:04:55.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:04:56
Nov 17 15:04:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:04:56.001
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:56.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:56.046
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 11/17/23 15:04:56.055
STEP: Creating a ResourceQuota 11/17/23 15:05:01.059
STEP: Ensuring resource quota status is calculated 11/17/23 15:05:01.067
STEP: Creating a ReplicaSet 11/17/23 15:05:03.072
STEP: Ensuring resource quota status captures replicaset creation 11/17/23 15:05:03.088
STEP: Deleting a ReplicaSet 11/17/23 15:05:05.344
STEP: Ensuring resource quota status released usage 11/17/23 15:05:05.479
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:07.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4683" for this suite. 11/17/23 15:05:07.489
------------------------------
â€¢ [SLOW TEST] [11.497 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:04:56
    Nov 17 15:04:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:04:56.001
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:04:56.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:04:56.046
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 11/17/23 15:04:56.055
    STEP: Creating a ResourceQuota 11/17/23 15:05:01.059
    STEP: Ensuring resource quota status is calculated 11/17/23 15:05:01.067
    STEP: Creating a ReplicaSet 11/17/23 15:05:03.072
    STEP: Ensuring resource quota status captures replicaset creation 11/17/23 15:05:03.088
    STEP: Deleting a ReplicaSet 11/17/23 15:05:05.344
    STEP: Ensuring resource quota status released usage 11/17/23 15:05:05.479
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:07.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4683" for this suite. 11/17/23 15:05:07.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:07.499
Nov 17 15:05:07.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename subpath 11/17/23 15:05:07.5
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:07.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:07.529
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 11/17/23 15:05:07.534
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-gclz 11/17/23 15:05:07.549
STEP: Creating a pod to test atomic-volume-subpath 11/17/23 15:05:07.549
Nov 17 15:05:07.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gclz" in namespace "subpath-3480" to be "Succeeded or Failed"
Nov 17 15:05:07.569: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737671ms
Nov 17 15:05:09.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011086892s
Nov 17 15:05:11.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 4.011332266s
Nov 17 15:05:13.575: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 6.012372423s
Nov 17 15:05:15.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 8.012032784s
Nov 17 15:05:17.576: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 10.014025273s
Nov 17 15:05:19.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 12.012196378s
Nov 17 15:05:21.572: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 14.010051166s
Nov 17 15:05:23.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 16.011229189s
Nov 17 15:05:25.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 18.010617822s
Nov 17 15:05:27.575: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 20.012370881s
Nov 17 15:05:29.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=false. Elapsed: 22.01029258s
Nov 17 15:05:31.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011404333s
STEP: Saw pod success 11/17/23 15:05:31.574
Nov 17 15:05:31.574: INFO: Pod "pod-subpath-test-downwardapi-gclz" satisfied condition "Succeeded or Failed"
Nov 17 15:05:31.579: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-downwardapi-gclz container test-container-subpath-downwardapi-gclz: <nil>
STEP: delete the pod 11/17/23 15:05:31.598
Nov 17 15:05:31.610: INFO: Waiting for pod pod-subpath-test-downwardapi-gclz to disappear
Nov 17 15:05:31.614: INFO: Pod pod-subpath-test-downwardapi-gclz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gclz 11/17/23 15:05:31.614
Nov 17 15:05:31.614: INFO: Deleting pod "pod-subpath-test-downwardapi-gclz" in namespace "subpath-3480"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:31.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3480" for this suite. 11/17/23 15:05:31.621
------------------------------
â€¢ [SLOW TEST] [24.128 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:07.499
    Nov 17 15:05:07.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename subpath 11/17/23 15:05:07.5
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:07.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:07.529
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 11/17/23 15:05:07.534
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-gclz 11/17/23 15:05:07.549
    STEP: Creating a pod to test atomic-volume-subpath 11/17/23 15:05:07.549
    Nov 17 15:05:07.562: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gclz" in namespace "subpath-3480" to be "Succeeded or Failed"
    Nov 17 15:05:07.569: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737671ms
    Nov 17 15:05:09.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011086892s
    Nov 17 15:05:11.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 4.011332266s
    Nov 17 15:05:13.575: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 6.012372423s
    Nov 17 15:05:15.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 8.012032784s
    Nov 17 15:05:17.576: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 10.014025273s
    Nov 17 15:05:19.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 12.012196378s
    Nov 17 15:05:21.572: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 14.010051166s
    Nov 17 15:05:23.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 16.011229189s
    Nov 17 15:05:25.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 18.010617822s
    Nov 17 15:05:27.575: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=true. Elapsed: 20.012370881s
    Nov 17 15:05:29.573: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Running", Reason="", readiness=false. Elapsed: 22.01029258s
    Nov 17 15:05:31.574: INFO: Pod "pod-subpath-test-downwardapi-gclz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011404333s
    STEP: Saw pod success 11/17/23 15:05:31.574
    Nov 17 15:05:31.574: INFO: Pod "pod-subpath-test-downwardapi-gclz" satisfied condition "Succeeded or Failed"
    Nov 17 15:05:31.579: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-subpath-test-downwardapi-gclz container test-container-subpath-downwardapi-gclz: <nil>
    STEP: delete the pod 11/17/23 15:05:31.598
    Nov 17 15:05:31.610: INFO: Waiting for pod pod-subpath-test-downwardapi-gclz to disappear
    Nov 17 15:05:31.614: INFO: Pod pod-subpath-test-downwardapi-gclz no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-gclz 11/17/23 15:05:31.614
    Nov 17 15:05:31.614: INFO: Deleting pod "pod-subpath-test-downwardapi-gclz" in namespace "subpath-3480"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:31.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3480" for this suite. 11/17/23 15:05:31.621
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:31.628
Nov 17 15:05:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:05:31.629
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:31.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:31.649
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Nov 17 15:05:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 15:05:37.742
Nov 17 15:05:37.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 create -f -'
Nov 17 15:05:39.494: INFO: stderr: ""
Nov 17 15:05:39.494: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 17 15:05:39.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
Nov 17 15:05:39.596: INFO: stderr: ""
Nov 17 15:05:39.597: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 17 15:05:39.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 apply -f -'
Nov 17 15:05:40.176: INFO: stderr: ""
Nov 17 15:05:40.176: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 17 15:05:40.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
Nov 17 15:05:40.275: INFO: stderr: ""
Nov 17 15:05:40.275: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 11/17/23 15:05:40.275
Nov 17 15:05:40.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 explain e2e-test-crd-publish-openapi-4111-crds'
Nov 17 15:05:40.858: INFO: stderr: ""
Nov 17 15:05:40.858: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4111-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:45.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-404" for this suite. 11/17/23 15:05:45.134
------------------------------
â€¢ [SLOW TEST] [13.512 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:31.628
    Nov 17 15:05:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:05:31.629
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:31.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:31.649
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Nov 17 15:05:31.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 11/17/23 15:05:37.742
    Nov 17 15:05:37.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 create -f -'
    Nov 17 15:05:39.494: INFO: stderr: ""
    Nov 17 15:05:39.494: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 17 15:05:39.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
    Nov 17 15:05:39.596: INFO: stderr: ""
    Nov 17 15:05:39.597: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Nov 17 15:05:39.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 apply -f -'
    Nov 17 15:05:40.176: INFO: stderr: ""
    Nov 17 15:05:40.176: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Nov 17 15:05:40.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-4111-crds test-cr'
    Nov 17 15:05:40.275: INFO: stderr: ""
    Nov 17 15:05:40.275: INFO: stdout: "e2e-test-crd-publish-openapi-4111-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 11/17/23 15:05:40.275
    Nov 17 15:05:40.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=crd-publish-openapi-404 explain e2e-test-crd-publish-openapi-4111-crds'
    Nov 17 15:05:40.858: INFO: stderr: ""
    Nov 17 15:05:40.858: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4111-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:45.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-404" for this suite. 11/17/23 15:05:45.134
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:45.14
Nov 17 15:05:45.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:05:45.142
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:45.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:45.163
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Nov 17 15:05:45.180: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5182 to be scheduled
Nov 17 15:05:45.183: INFO: 1 pods are not scheduled: [runtimeclass-5182/test-runtimeclass-runtimeclass-5182-preconfigured-handler-6qp54(ffe88bcc-56c3-407c-afe9-eacd134cb388)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:47.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5182" for this suite. 11/17/23 15:05:47.198
------------------------------
â€¢ [2.065 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:45.14
    Nov 17 15:05:45.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:05:45.142
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:45.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:45.163
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Nov 17 15:05:45.180: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5182 to be scheduled
    Nov 17 15:05:45.183: INFO: 1 pods are not scheduled: [runtimeclass-5182/test-runtimeclass-runtimeclass-5182-preconfigured-handler-6qp54(ffe88bcc-56c3-407c-afe9-eacd134cb388)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:47.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5182" for this suite. 11/17/23 15:05:47.198
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:47.206
Nov 17 15:05:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename namespaces 11/17/23 15:05:47.208
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:47.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:47.224
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 11/17/23 15:05:47.227
Nov 17 15:05:47.230: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 11/17/23 15:05:47.23
Nov 17 15:05:47.237: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 11/17/23 15:05:47.237
Nov 17 15:05:47.245: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:47.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4469" for this suite. 11/17/23 15:05:47.251
------------------------------
â€¢ [0.052 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:47.206
    Nov 17 15:05:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename namespaces 11/17/23 15:05:47.208
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:47.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:47.224
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 11/17/23 15:05:47.227
    Nov 17 15:05:47.230: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 11/17/23 15:05:47.23
    Nov 17 15:05:47.237: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 11/17/23 15:05:47.237
    Nov 17 15:05:47.245: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:47.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4469" for this suite. 11/17/23 15:05:47.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:47.26
Nov 17 15:05:47.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 15:05:47.261
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:47.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:47.281
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-48aa43a7-917b-4755-8e59-4ba9c767427e 11/17/23 15:05:47.307
STEP: Creating a pod to test consume secrets 11/17/23 15:05:47.313
Nov 17 15:05:47.324: INFO: Waiting up to 5m0s for pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d" in namespace "secrets-4394" to be "Succeeded or Failed"
Nov 17 15:05:47.333: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.123104ms
Nov 17 15:05:49.337: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013385573s
Nov 17 15:05:51.339: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015160755s
STEP: Saw pod success 11/17/23 15:05:51.339
Nov 17 15:05:51.339: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d" satisfied condition "Succeeded or Failed"
Nov 17 15:05:51.342: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d container secret-volume-test: <nil>
STEP: delete the pod 11/17/23 15:05:51.349
Nov 17 15:05:51.363: INFO: Waiting for pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d to disappear
Nov 17 15:05:51.367: INFO: Pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:51.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4394" for this suite. 11/17/23 15:05:51.372
STEP: Destroying namespace "secret-namespace-9198" for this suite. 11/17/23 15:05:51.38
------------------------------
â€¢ [4.129 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:47.26
    Nov 17 15:05:47.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 15:05:47.261
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:47.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:47.281
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-48aa43a7-917b-4755-8e59-4ba9c767427e 11/17/23 15:05:47.307
    STEP: Creating a pod to test consume secrets 11/17/23 15:05:47.313
    Nov 17 15:05:47.324: INFO: Waiting up to 5m0s for pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d" in namespace "secrets-4394" to be "Succeeded or Failed"
    Nov 17 15:05:47.333: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.123104ms
    Nov 17 15:05:49.337: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013385573s
    Nov 17 15:05:51.339: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015160755s
    STEP: Saw pod success 11/17/23 15:05:51.339
    Nov 17 15:05:51.339: INFO: Pod "pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d" satisfied condition "Succeeded or Failed"
    Nov 17 15:05:51.342: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d container secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 15:05:51.349
    Nov 17 15:05:51.363: INFO: Waiting for pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d to disappear
    Nov 17 15:05:51.367: INFO: Pod pod-secrets-8b80107f-22cf-4218-8ec5-ba75f2d9048d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:51.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4394" for this suite. 11/17/23 15:05:51.372
    STEP: Destroying namespace "secret-namespace-9198" for this suite. 11/17/23 15:05:51.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:51.393
Nov 17 15:05:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:05:51.394
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:51.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:51.413
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 11/17/23 15:05:51.416
STEP: Getting a ResourceQuota 11/17/23 15:05:51.421
STEP: Updating a ResourceQuota 11/17/23 15:05:51.424
STEP: Verifying a ResourceQuota was modified 11/17/23 15:05:51.432
STEP: Deleting a ResourceQuota 11/17/23 15:05:51.435
STEP: Verifying the deleted ResourceQuota 11/17/23 15:05:51.442
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:51.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3550" for this suite. 11/17/23 15:05:51.449
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:51.393
    Nov 17 15:05:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:05:51.394
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:51.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:51.413
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 11/17/23 15:05:51.416
    STEP: Getting a ResourceQuota 11/17/23 15:05:51.421
    STEP: Updating a ResourceQuota 11/17/23 15:05:51.424
    STEP: Verifying a ResourceQuota was modified 11/17/23 15:05:51.432
    STEP: Deleting a ResourceQuota 11/17/23 15:05:51.435
    STEP: Verifying the deleted ResourceQuota 11/17/23 15:05:51.442
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:51.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3550" for this suite. 11/17/23 15:05:51.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:51.456
Nov 17 15:05:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 15:05:51.457
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:51.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:51.474
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Nov 17 15:05:51.477: INFO: Creating deployment "webserver-deployment"
Nov 17 15:05:51.482: INFO: Waiting for observed generation 1
Nov 17 15:05:53.490: INFO: Waiting for all required pods to come up
Nov 17 15:05:53.494: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 11/17/23 15:05:53.495
Nov 17 15:05:53.495: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hwg5z" in namespace "deployment-8141" to be "running"
Nov 17 15:05:53.495: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-26hps" in namespace "deployment-8141" to be "running"
Nov 17 15:05:53.499: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554777ms
Nov 17 15:05:53.499: INFO: Pod "webserver-deployment-7f5969cbc7-26hps": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611407ms
Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-26hps": Phase="Running", Reason="", readiness=true. Elapsed: 2.05212576s
Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-26hps" satisfied condition "running"
Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.052269676s
Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z" satisfied condition "running"
Nov 17 15:05:55.547: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 17 15:05:55.563: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 17 15:05:55.617: INFO: Updating deployment webserver-deployment
Nov 17 15:05:55.617: INFO: Waiting for observed generation 2
Nov 17 15:05:57.643: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 17 15:05:57.654: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 17 15:05:57.663: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 17 15:05:57.709: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 17 15:05:57.709: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 17 15:05:57.734: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 17 15:05:57.754: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 17 15:05:57.754: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 17 15:05:57.790: INFO: Updating deployment webserver-deployment
Nov 17 15:05:57.790: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 17 15:05:57.837: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 17 15:05:57.882: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 15:05:57.942: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8141  6c7a32e1-3580-4293-ad1e-bbb729f25338 61389 3 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c5b7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-17 15:05:56 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-17 15:05:57 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 17 15:05:58.036: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8141  ee2272b1-2ee5-48a3-a720-9735478a17cc 61387 3 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6c7a32e1-3580-4293-ad1e-bbb729f25338 0xc00881eff7 0xc00881eff8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7a32e1-3580-4293-ad1e-bbb729f25338\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00881f098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 15:05:58.036: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 17 15:05:58.036: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8141  a2415636-e404-4029-8b21-69e7842a24dc 61384 3 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6c7a32e1-3580-4293-ad1e-bbb729f25338 0xc00881ef07 0xc00881ef08}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7a32e1-3580-4293-ad1e-bbb729f25338\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00881ef98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 17 15:05:58.150: INFO: Pod "webserver-deployment-7f5969cbc7-5qqbn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5qqbn webserver-deployment-7f5969cbc7- deployment-8141  a95a6318-c0b3-4cfb-bd3a-4461c0a63dd4 61256 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f597 0xc00881f598}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcd5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcd5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.235,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2503193cb4266f551b3225d5c04ed5623284fa8bd7860d9055920952e67fec2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.150: INFO: Pod "webserver-deployment-7f5969cbc7-688gs" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-688gs webserver-deployment-7f5969cbc7- deployment-8141  e6bf143b-9a8d-4076-998c-18f7ef3ca4b8 61401 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f770 0xc00881f771}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffrzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffrzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-84hzl" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-84hzl webserver-deployment-7f5969cbc7- deployment-8141  0dec0393-685d-42e7-b955-7358dbe80a27 61422 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f8c0 0xc00881f8c1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dt6tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dt6tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-9cb97" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9cb97 webserver-deployment-7f5969cbc7- deployment-8141  3a1ec041-f949-4f06-a016-2151defd75b2 61257 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fa10 0xc00881fa11}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnvbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnvbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.185,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e77972e897896cf505a1bdf9f37d375f01b7a248016d6c118dd1a35d219473e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-bpqcz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bpqcz webserver-deployment-7f5969cbc7- deployment-8141  77b83cd2-c7ee-4be2-bf04-cb84d85b3f8a 61423 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fbe0 0xc00881fbe1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b2dk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b2dk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.152: INFO: Pod "webserver-deployment-7f5969cbc7-bzttm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bzttm webserver-deployment-7f5969cbc7- deployment-8141  6f84dcd7-ead7-4eeb-8154-df420bbbb904 61250 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fd17 0xc00881fd18}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhnsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhnsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.229,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://387b285d48b7170a3df8420849ccaaed3549c1cce91384e888a3863749c1a450,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.152: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hwg5z webserver-deployment-7f5969cbc7- deployment-8141  be005a3e-f823-4bf5-bcfa-2751bcf2b6b9 61278 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fef0 0xc00881fef1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plv5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plv5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.35,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0898c0edee3561a55f5693085f678a2387b743674a0a614b3399bcd2a8a3ec2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.153: INFO: Pod "webserver-deployment-7f5969cbc7-jpbn5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jpbn5 webserver-deployment-7f5969cbc7- deployment-8141  9fa6186f-e7d0-4afd-b2c1-23c6801d92da 61242 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f020c0 0xc008f020c1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.162,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8412009ee2385732b773e0eef63470cce7145296539dbff9ce98f4b898ddf5d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.153: INFO: Pod "webserver-deployment-7f5969cbc7-l4xmn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l4xmn webserver-deployment-7f5969cbc7- deployment-8141  9e3e0102-cde4-4c41-90ba-b2a02ae8f4eb 61417 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02290 0xc008f02291}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6qdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6qdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-l842b" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l842b webserver-deployment-7f5969cbc7- deployment-8141  68bae120-bb87-447e-9650-c84b8f25c50b 61260 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f023e0 0xc008f023e1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f28f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f28f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.61,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff1fc24fd173ec0d6b4f80516f8b6cb1c2cfecf423cb93136c80cff7eca0364c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-ll8f8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ll8f8 webserver-deployment-7f5969cbc7- deployment-8141  cd1a4ce3-9047-4daa-859c-924675f2d676 61248 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f025b0 0xc008f025b1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5qkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5qkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.4,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://86c7031132aca215809d8dffe0a841774ce604ed3a212d5544f26a12747986a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-mjfjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mjfjz webserver-deployment-7f5969cbc7- deployment-8141  768a0737-5919-4eeb-a0a2-b0987dcdbafe 61405 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02780 0xc008f02781}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qb8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qb8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-rd8qg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rd8qg webserver-deployment-7f5969cbc7- deployment-8141  52e89a9c-7a54-46d5-b39a-6d5239870181 61424 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02930 0xc008f02931}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8888q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8888q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-t5hbt" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5hbt webserver-deployment-7f5969cbc7- deployment-8141  48a929f0-50aa-41ae-9df8-78ff61c2ca9a 61251 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02a80 0xc008f02a81}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sncwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sncwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.27,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://963d87c673edbf7678cf43df969e026fc2c86173b1d5c328e6bbc3b39c16a196,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-t5hs7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5hs7 webserver-deployment-7f5969cbc7- deployment-8141  ec9e25f2-51e4-4f3b-9ad2-2f1b466cb5fc 61425 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02c50 0xc008f02c51}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6qjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6qjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-tbflr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tbflr webserver-deployment-7f5969cbc7- deployment-8141  2293a0e0-b915-4e35-bb02-14be97d1ddb6 61419 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02d87 0xc008f02d88}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j54lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j54lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-tmj5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tmj5v webserver-deployment-7f5969cbc7- deployment-8141  0e9880c9-a5a9-41f2-a6bb-9e997c4f7509 61416 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02ee0 0xc008f02ee1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6z4vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6z4vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-vdf4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vdf4p webserver-deployment-7f5969cbc7- deployment-8141  cf9ea5f7-6bcc-45f0-a871-28bf7963df8d 61421 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03017 0xc008f03018}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5xgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5xgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-xfnws" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xfnws webserver-deployment-7f5969cbc7- deployment-8141  ce1dac4a-f23c-4a8a-927f-9b22d5982cf4 61411 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03157 0xc008f03158}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbvc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbvc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-7f5969cbc7-zc4ml" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zc4ml webserver-deployment-7f5969cbc7- deployment-8141  42fd9395-830e-4854-a2b3-8964659a986f 61397 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03297 0xc008f03298}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9vtz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9vtz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-d9f79cb5-dgns5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dgns5 webserver-deployment-d9f79cb5- deployment-8141  77f14f7f-1b55-4b28-b6d8-c0ce33ebd7ac 61426 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f033df 0xc008f033f0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lp7ws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lp7ws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-d9f79cb5-dwlzl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dwlzl webserver-deployment-d9f79cb5- deployment-8141  712437b2-3b08-4493-8b7c-d19ba4fbd8f0 61339 0 2023-11-17 15:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f0353f 0xc008f03550}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jh2q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jh2q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-11-17 15:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-hrhht" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hrhht webserver-deployment-d9f79cb5- deployment-8141  4e1c66ef-660b-448a-960e-2e47a1a7d769 61410 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f0370f 0xc008f03720}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6z9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6z9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-klj2b" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-klj2b webserver-deployment-d9f79cb5- deployment-8141  c2016ec6-6f28-41ab-9091-f8612628cb35 61418 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03867 0xc008f03868}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4c98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4c98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-mfpqf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mfpqf webserver-deployment-d9f79cb5- deployment-8141  5f3ea271-48e6-4363-81ca-86a8a51dab5a 61412 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f039bf 0xc008f039d0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz4hd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz4hd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-nz4dn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nz4dn webserver-deployment-d9f79cb5- deployment-8141  cae545d0-28ef-4ed7-ba46-b1cda6833223 61409 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03b17 0xc008f03b18}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6gmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6gmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:,StartTime:2023-11-17 15:05:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-qldxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qldxt webserver-deployment-d9f79cb5- deployment-8141  0d9b261a-0c9f-43ea-8e01-1e800f899a58 61335 0 2023-11-17 15:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03cdf 0xc008f03cf0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2r7gl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2r7gl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-rqxws" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rqxws webserver-deployment-d9f79cb5- deployment-8141  85359f6a-c5b6-4ce7-8c87-12abffe6dcd7 61308 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03eaf 0xc008f03ec0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgbrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgbrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.160: INFO: Pod "webserver-deployment-d9f79cb5-s7mfs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s7mfs webserver-deployment-d9f79cb5- deployment-8141  34923566-e39d-4bb1-a036-5099d4602a7a 61306 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c07f 0xc008f2c090}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbpxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbpxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.160: INFO: Pod "webserver-deployment-d9f79cb5-shf88" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-shf88 webserver-deployment-d9f79cb5- deployment-8141  28337b09-719a-42b2-8249-e593575fc98f 61415 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c24f 0xc008f2c260}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fphnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fphnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.161: INFO: Pod "webserver-deployment-d9f79cb5-v8f2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v8f2r webserver-deployment-d9f79cb5- deployment-8141  45ac7bdd-559b-4865-9247-2f8d2f8841ff 61414 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c3a7 0xc008f2c3a8}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qfxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qfxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 17 15:05:58.161: INFO: Pod "webserver-deployment-d9f79cb5-vswrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vswrj webserver-deployment-d9f79cb5- deployment-8141  183532e9-0b8f-440e-bb91-498e8cf80f3e 61368 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c4f7 0xc008f2c4f8}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lf4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lf4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.102,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 15:05:58.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8141" for this suite. 11/17/23 15:05:58.202
------------------------------
â€¢ [SLOW TEST] [6.867 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:51.456
    Nov 17 15:05:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 15:05:51.457
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:51.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:51.474
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Nov 17 15:05:51.477: INFO: Creating deployment "webserver-deployment"
    Nov 17 15:05:51.482: INFO: Waiting for observed generation 1
    Nov 17 15:05:53.490: INFO: Waiting for all required pods to come up
    Nov 17 15:05:53.494: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 11/17/23 15:05:53.495
    Nov 17 15:05:53.495: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hwg5z" in namespace "deployment-8141" to be "running"
    Nov 17 15:05:53.495: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-26hps" in namespace "deployment-8141" to be "running"
    Nov 17 15:05:53.499: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554777ms
    Nov 17 15:05:53.499: INFO: Pod "webserver-deployment-7f5969cbc7-26hps": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611407ms
    Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-26hps": Phase="Running", Reason="", readiness=true. Elapsed: 2.05212576s
    Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-26hps" satisfied condition "running"
    Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.052269676s
    Nov 17 15:05:55.547: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z" satisfied condition "running"
    Nov 17 15:05:55.547: INFO: Waiting for deployment "webserver-deployment" to complete
    Nov 17 15:05:55.563: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Nov 17 15:05:55.617: INFO: Updating deployment webserver-deployment
    Nov 17 15:05:55.617: INFO: Waiting for observed generation 2
    Nov 17 15:05:57.643: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Nov 17 15:05:57.654: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Nov 17 15:05:57.663: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 17 15:05:57.709: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Nov 17 15:05:57.709: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Nov 17 15:05:57.734: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Nov 17 15:05:57.754: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Nov 17 15:05:57.754: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Nov 17 15:05:57.790: INFO: Updating deployment webserver-deployment
    Nov 17 15:05:57.790: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Nov 17 15:05:57.837: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Nov 17 15:05:57.882: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 15:05:57.942: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8141  6c7a32e1-3580-4293-ad1e-bbb729f25338 61389 3 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c5b7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-11-17 15:05:56 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-17 15:05:57 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Nov 17 15:05:58.036: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8141  ee2272b1-2ee5-48a3-a720-9735478a17cc 61387 3 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6c7a32e1-3580-4293-ad1e-bbb729f25338 0xc00881eff7 0xc00881eff8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7a32e1-3580-4293-ad1e-bbb729f25338\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00881f098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 15:05:58.036: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Nov 17 15:05:58.036: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8141  a2415636-e404-4029-8b21-69e7842a24dc 61384 3 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6c7a32e1-3580-4293-ad1e-bbb729f25338 0xc00881ef07 0xc00881ef08}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7a32e1-3580-4293-ad1e-bbb729f25338\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00881ef98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 15:05:58.150: INFO: Pod "webserver-deployment-7f5969cbc7-5qqbn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5qqbn webserver-deployment-7f5969cbc7- deployment-8141  a95a6318-c0b3-4cfb-bd3a-4461c0a63dd4 61256 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f597 0xc00881f598}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcd5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcd5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.235,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2503193cb4266f551b3225d5c04ed5623284fa8bd7860d9055920952e67fec2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.150: INFO: Pod "webserver-deployment-7f5969cbc7-688gs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-688gs webserver-deployment-7f5969cbc7- deployment-8141  e6bf143b-9a8d-4076-998c-18f7ef3ca4b8 61401 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f770 0xc00881f771}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffrzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffrzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-84hzl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-84hzl webserver-deployment-7f5969cbc7- deployment-8141  0dec0393-685d-42e7-b955-7358dbe80a27 61422 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881f8c0 0xc00881f8c1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dt6tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dt6tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-9cb97" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9cb97 webserver-deployment-7f5969cbc7- deployment-8141  3a1ec041-f949-4f06-a016-2151defd75b2 61257 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fa10 0xc00881fa11}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnvbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnvbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.185,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e77972e897896cf505a1bdf9f37d375f01b7a248016d6c118dd1a35d219473e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.151: INFO: Pod "webserver-deployment-7f5969cbc7-bpqcz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bpqcz webserver-deployment-7f5969cbc7- deployment-8141  77b83cd2-c7ee-4be2-bf04-cb84d85b3f8a 61423 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fbe0 0xc00881fbe1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b2dk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b2dk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.152: INFO: Pod "webserver-deployment-7f5969cbc7-bzttm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bzttm webserver-deployment-7f5969cbc7- deployment-8141  6f84dcd7-ead7-4eeb-8154-df420bbbb904 61250 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fd17 0xc00881fd18}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhnsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhnsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.229,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://387b285d48b7170a3df8420849ccaaed3549c1cce91384e888a3863749c1a450,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.152: INFO: Pod "webserver-deployment-7f5969cbc7-hwg5z" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hwg5z webserver-deployment-7f5969cbc7- deployment-8141  be005a3e-f823-4bf5-bcfa-2751bcf2b6b9 61278 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc00881fef0 0xc00881fef1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plv5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plv5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.35,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0898c0edee3561a55f5693085f678a2387b743674a0a614b3399bcd2a8a3ec2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.153: INFO: Pod "webserver-deployment-7f5969cbc7-jpbn5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jpbn5 webserver-deployment-7f5969cbc7- deployment-8141  9fa6186f-e7d0-4afd-b2c1-23c6801d92da 61242 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f020c0 0xc008f020c1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:10.10.0.162,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8412009ee2385732b773e0eef63470cce7145296539dbff9ce98f4b898ddf5d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.153: INFO: Pod "webserver-deployment-7f5969cbc7-l4xmn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l4xmn webserver-deployment-7f5969cbc7- deployment-8141  9e3e0102-cde4-4c41-90ba-b2a02ae8f4eb 61417 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02290 0xc008f02291}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6qdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6qdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-l842b" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l842b webserver-deployment-7f5969cbc7- deployment-8141  68bae120-bb87-447e-9650-c84b8f25c50b 61260 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f023e0 0xc008f023e1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.2.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f28f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f28f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:10.10.2.61,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff1fc24fd173ec0d6b4f80516f8b6cb1c2cfecf423cb93136c80cff7eca0364c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.2.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-ll8f8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ll8f8 webserver-deployment-7f5969cbc7- deployment-8141  cd1a4ce3-9047-4daa-859c-924675f2d676 61248 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f025b0 0xc008f025b1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5qkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5qkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.4,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://86c7031132aca215809d8dffe0a841774ce604ed3a212d5544f26a12747986a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.154: INFO: Pod "webserver-deployment-7f5969cbc7-mjfjz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mjfjz webserver-deployment-7f5969cbc7- deployment-8141  768a0737-5919-4eeb-a0a2-b0987dcdbafe 61405 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02780 0xc008f02781}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qb8m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qb8m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-rd8qg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rd8qg webserver-deployment-7f5969cbc7- deployment-8141  52e89a9c-7a54-46d5-b39a-6d5239870181 61424 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02930 0xc008f02931}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8888q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8888q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-t5hbt" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5hbt webserver-deployment-7f5969cbc7- deployment-8141  48a929f0-50aa-41ae-9df8-78ff61c2ca9a 61251 0 2023-11-17 15:05:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02a80 0xc008f02a81}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sncwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sncwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.27,StartTime:2023-11-17 15:05:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-11-17 15:05:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://963d87c673edbf7678cf43df969e026fc2c86173b1d5c328e6bbc3b39c16a196,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.155: INFO: Pod "webserver-deployment-7f5969cbc7-t5hs7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t5hs7 webserver-deployment-7f5969cbc7- deployment-8141  ec9e25f2-51e4-4f3b-9ad2-2f1b466cb5fc 61425 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02c50 0xc008f02c51}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6qjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6qjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-tbflr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tbflr webserver-deployment-7f5969cbc7- deployment-8141  2293a0e0-b915-4e35-bb02-14be97d1ddb6 61419 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02d87 0xc008f02d88}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j54lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j54lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-tmj5v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tmj5v webserver-deployment-7f5969cbc7- deployment-8141  0e9880c9-a5a9-41f2-a6bb-9e997c4f7509 61416 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f02ee0 0xc008f02ee1}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6z4vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6z4vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-vdf4p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vdf4p webserver-deployment-7f5969cbc7- deployment-8141  cf9ea5f7-6bcc-45f0-a871-28bf7963df8d 61421 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03017 0xc008f03018}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5xgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5xgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.156: INFO: Pod "webserver-deployment-7f5969cbc7-xfnws" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xfnws webserver-deployment-7f5969cbc7- deployment-8141  ce1dac4a-f23c-4a8a-927f-9b22d5982cf4 61411 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03157 0xc008f03158}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbvc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbvc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-7f5969cbc7-zc4ml" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zc4ml webserver-deployment-7f5969cbc7- deployment-8141  42fd9395-830e-4854-a2b3-8964659a986f 61397 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 a2415636-e404-4029-8b21-69e7842a24dc 0xc008f03297 0xc008f03298}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2415636-e404-4029-8b21-69e7842a24dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9vtz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9vtz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-d9f79cb5-dgns5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dgns5 webserver-deployment-d9f79cb5- deployment-8141  77f14f7f-1b55-4b28-b6d8-c0ce33ebd7ac 61426 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f033df 0xc008f033f0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lp7ws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lp7ws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.157: INFO: Pod "webserver-deployment-d9f79cb5-dwlzl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dwlzl webserver-deployment-d9f79cb5- deployment-8141  712437b2-3b08-4493-8b7c-d19ba4fbd8f0 61339 0 2023-11-17 15:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f0353f 0xc008f03550}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jh2q2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jh2q2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:,StartTime:2023-11-17 15:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-hrhht" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hrhht webserver-deployment-d9f79cb5- deployment-8141  4e1c66ef-660b-448a-960e-2e47a1a7d769 61410 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f0370f 0xc008f03720}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6z9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6z9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-klj2b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-klj2b webserver-deployment-d9f79cb5- deployment-8141  c2016ec6-6f28-41ab-9091-f8612628cb35 61418 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03867 0xc008f03868}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4c98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4c98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.158: INFO: Pod "webserver-deployment-d9f79cb5-mfpqf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mfpqf webserver-deployment-d9f79cb5- deployment-8141  5f3ea271-48e6-4363-81ca-86a8a51dab5a 61412 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f039bf 0xc008f039d0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz4hd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz4hd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-nz4dn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nz4dn webserver-deployment-d9f79cb5- deployment-8141  cae545d0-28ef-4ed7-ba46-b1cda6833223 61409 0 2023-11-17 15:05:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03b17 0xc008f03b18}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6gmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6gmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:,StartTime:2023-11-17 15:05:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-qldxt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qldxt webserver-deployment-d9f79cb5- deployment-8141  0d9b261a-0c9f-43ea-8e01-1e800f899a58 61335 0 2023-11-17 15:05:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03cdf 0xc008f03cf0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2r7gl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2r7gl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.159: INFO: Pod "webserver-deployment-d9f79cb5-rqxws" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rqxws webserver-deployment-d9f79cb5- deployment-8141  85359f6a-c5b6-4ce7-8c87-12abffe6dcd7 61308 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f03eaf 0xc008f03ec0}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgbrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgbrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.5,PodIP:,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.160: INFO: Pod "webserver-deployment-d9f79cb5-s7mfs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s7mfs webserver-deployment-d9f79cb5- deployment-8141  34923566-e39d-4bb1-a036-5099d4602a7a 61306 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c07f 0xc008f2c090}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbpxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbpxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.160: INFO: Pod "webserver-deployment-d9f79cb5-shf88" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-shf88 webserver-deployment-d9f79cb5- deployment-8141  28337b09-719a-42b2-8249-e593575fc98f 61415 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c24f 0xc008f2c260}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fphnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fphnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.161: INFO: Pod "webserver-deployment-d9f79cb5-v8f2r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v8f2r webserver-deployment-d9f79cb5- deployment-8141  45ac7bdd-559b-4865-9247-2f8d2f8841ff 61414 0 2023-11-17 15:05:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c3a7 0xc008f2c3a8}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qfxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qfxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Nov 17 15:05:58.161: INFO: Pod "webserver-deployment-d9f79cb5-vswrj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vswrj webserver-deployment-d9f79cb5- deployment-8141  183532e9-0b8f-440e-bb91-498e8cf80f3e 61368 0 2023-11-17 15:05:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 ee2272b1-2ee5-48a3-a720-9735478a17cc 0xc008f2c4f7 0xc008f2c4f8}] [] [{kube-controller-manager Update v1 2023-11-17 15:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee2272b1-2ee5-48a3-a720-9735478a17cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:05:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lf4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lf4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.3,PodIP:10.10.1.102,StartTime:2023-11-17 15:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:05:58.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8141" for this suite. 11/17/23 15:05:58.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:05:58.331
Nov 17 15:05:58.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 15:05:58.332
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:58.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:58.533
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/17/23 15:05:58.567
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 11/17/23 15:05:58.568
STEP: creating a pod to probe DNS 11/17/23 15:05:58.568
STEP: submitting the pod to kubernetes 11/17/23 15:05:58.568
Nov 17 15:05:58.633: INFO: Waiting up to 15m0s for pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228" in namespace "dns-1124" to be "running"
Nov 17 15:05:58.648: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 15.405352ms
Nov 17 15:06:00.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020752896s
Nov 17 15:06:02.652: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019463736s
Nov 17 15:06:04.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020937525s
Nov 17 15:06:06.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Running", Reason="", readiness=true. Elapsed: 8.021156667s
Nov 17 15:06:06.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228" satisfied condition "running"
STEP: retrieving the pod 11/17/23 15:06:06.654
STEP: looking for the results for each expected name from probers 11/17/23 15:06:06.658
Nov 17 15:06:06.679: INFO: DNS probes using dns-1124/dns-test-016c4292-248d-4ebe-9df7-cbe957674228 succeeded

STEP: deleting the pod 11/17/23 15:06:06.679
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 15:06:06.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1124" for this suite. 11/17/23 15:06:06.707
------------------------------
â€¢ [SLOW TEST] [8.389 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:05:58.331
    Nov 17 15:05:58.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 15:05:58.332
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:05:58.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:05:58.533
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/17/23 15:05:58.567
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     11/17/23 15:05:58.568
    STEP: creating a pod to probe DNS 11/17/23 15:05:58.568
    STEP: submitting the pod to kubernetes 11/17/23 15:05:58.568
    Nov 17 15:05:58.633: INFO: Waiting up to 15m0s for pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228" in namespace "dns-1124" to be "running"
    Nov 17 15:05:58.648: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 15.405352ms
    Nov 17 15:06:00.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020752896s
    Nov 17 15:06:02.652: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019463736s
    Nov 17 15:06:04.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020937525s
    Nov 17 15:06:06.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228": Phase="Running", Reason="", readiness=true. Elapsed: 8.021156667s
    Nov 17 15:06:06.654: INFO: Pod "dns-test-016c4292-248d-4ebe-9df7-cbe957674228" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 15:06:06.654
    STEP: looking for the results for each expected name from probers 11/17/23 15:06:06.658
    Nov 17 15:06:06.679: INFO: DNS probes using dns-1124/dns-test-016c4292-248d-4ebe-9df7-cbe957674228 succeeded

    STEP: deleting the pod 11/17/23 15:06:06.679
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:06:06.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1124" for this suite. 11/17/23 15:06:06.707
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:06:06.721
Nov 17 15:06:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:06:06.723
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:06.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:06.776
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Nov 17 15:06:06.819: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9549 to be scheduled
Nov 17 15:06:06.826: INFO: 1 pods are not scheduled: [runtimeclass-9549/test-runtimeclass-runtimeclass-9549-preconfigured-handler-xlxrc(caf87eb3-847e-4f8c-88b3-84f5db42c775)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 17 15:06:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9549" for this suite. 11/17/23 15:06:08.843
------------------------------
â€¢ [2.131 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:06:06.721
    Nov 17 15:06:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:06:06.723
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:06.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:06.776
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Nov 17 15:06:06.819: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9549 to be scheduled
    Nov 17 15:06:06.826: INFO: 1 pods are not scheduled: [runtimeclass-9549/test-runtimeclass-runtimeclass-9549-preconfigured-handler-xlxrc(caf87eb3-847e-4f8c-88b3-84f5db42c775)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:06:08.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9549" for this suite. 11/17/23 15:06:08.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:06:08.854
Nov 17 15:06:08.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:06:08.855
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:08.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:08.881
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/17/23 15:06:08.885
Nov 17 15:06:08.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 15:06:13.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:06:29.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3891" for this suite. 11/17/23 15:06:29.834
------------------------------
â€¢ [SLOW TEST] [20.991 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:06:08.854
    Nov 17 15:06:08.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:06:08.855
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:08.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:08.881
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 11/17/23 15:06:08.885
    Nov 17 15:06:08.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 15:06:13.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:06:29.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3891" for this suite. 11/17/23 15:06:29.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:06:29.845
Nov 17 15:06:29.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sysctl 11/17/23 15:06:29.847
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:29.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:29.874
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 11/17/23 15:06:29.879
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:06:29.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3155" for this suite. 11/17/23 15:06:29.891
------------------------------
â€¢ [0.057 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:06:29.845
    Nov 17 15:06:29.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sysctl 11/17/23 15:06:29.847
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:29.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:29.874
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 11/17/23 15:06:29.879
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:06:29.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3155" for this suite. 11/17/23 15:06:29.891
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:06:29.904
Nov 17 15:06:29.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:06:29.905
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:29.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:29.934
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/17/23 15:06:29.938
Nov 17 15:06:29.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/17/23 15:06:44.684
Nov 17 15:06:44.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
Nov 17 15:06:48.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4403" for this suite. 11/17/23 15:07:04.134
------------------------------
â€¢ [SLOW TEST] [34.237 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:06:29.904
    Nov 17 15:06:29.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:06:29.905
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:06:29.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:06:29.934
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 11/17/23 15:06:29.938
    Nov 17 15:06:29.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 11/17/23 15:06:44.684
    Nov 17 15:06:44.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    Nov 17 15:06:48.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4403" for this suite. 11/17/23 15:07:04.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:04.144
Nov 17 15:07:04.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 15:07:04.145
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:04.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:04.175
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 11/17/23 15:07:04.179
Nov 17 15:07:04.180: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-2615 proxy --unix-socket=/tmp/kubectl-proxy-unix814691055/test'
STEP: retrieving proxy /api/ output 11/17/23 15:07:04.245
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:04.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2615" for this suite. 11/17/23 15:07:04.253
------------------------------
â€¢ [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:04.144
    Nov 17 15:07:04.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 15:07:04.145
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:04.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:04.175
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 11/17/23 15:07:04.179
    Nov 17 15:07:04.180: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-2615 proxy --unix-socket=/tmp/kubectl-proxy-unix814691055/test'
    STEP: retrieving proxy /api/ output 11/17/23 15:07:04.245
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:04.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2615" for this suite. 11/17/23 15:07:04.253
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:04.26
Nov 17 15:07:04.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename prestop 11/17/23 15:07:04.262
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:04.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:04.288
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2600 11/17/23 15:07:04.294
STEP: Waiting for pods to come up. 11/17/23 15:07:04.308
Nov 17 15:07:04.308: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2600" to be "running"
Nov 17 15:07:04.313: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962931ms
Nov 17 15:07:06.318: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.009948044s
Nov 17 15:07:06.318: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2600 11/17/23 15:07:06.322
Nov 17 15:07:06.329: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2600" to be "running"
Nov 17 15:07:06.334: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515655ms
Nov 17 15:07:08.338: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008424055s
Nov 17 15:07:08.338: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 11/17/23 15:07:08.338
Nov 17 15:07:13.351: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 11/17/23 15:07:13.351
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:13.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2600" for this suite. 11/17/23 15:07:13.421
------------------------------
â€¢ [SLOW TEST] [9.172 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:04.26
    Nov 17 15:07:04.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename prestop 11/17/23 15:07:04.262
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:04.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:04.288
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2600 11/17/23 15:07:04.294
    STEP: Waiting for pods to come up. 11/17/23 15:07:04.308
    Nov 17 15:07:04.308: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2600" to be "running"
    Nov 17 15:07:04.313: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.962931ms
    Nov 17 15:07:06.318: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.009948044s
    Nov 17 15:07:06.318: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2600 11/17/23 15:07:06.322
    Nov 17 15:07:06.329: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2600" to be "running"
    Nov 17 15:07:06.334: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515655ms
    Nov 17 15:07:08.338: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.008424055s
    Nov 17 15:07:08.338: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 11/17/23 15:07:08.338
    Nov 17 15:07:13.351: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 11/17/23 15:07:13.351
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:13.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2600" for this suite. 11/17/23 15:07:13.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:13.437
Nov 17 15:07:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 15:07:13.439
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:13.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:13.466
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 11/17/23 15:07:13.47
Nov 17 15:07:13.470: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6784 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 11/17/23 15:07:13.556
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6784" for this suite. 11/17/23 15:07:13.576
------------------------------
â€¢ [0.145 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:13.437
    Nov 17 15:07:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 15:07:13.439
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:13.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:13.466
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 11/17/23 15:07:13.47
    Nov 17 15:07:13.470: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6784 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 11/17/23 15:07:13.556
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6784" for this suite. 11/17/23 15:07:13.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:13.583
Nov 17 15:07:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replication-controller 11/17/23 15:07:13.585
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:13.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:13.609
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 11/17/23 15:07:13.618
STEP: waiting for RC to be added 11/17/23 15:07:13.628
STEP: waiting for available Replicas 11/17/23 15:07:13.628
STEP: patching ReplicationController 11/17/23 15:07:15.489
STEP: waiting for RC to be modified 11/17/23 15:07:15.504
STEP: patching ReplicationController status 11/17/23 15:07:15.504
STEP: waiting for RC to be modified 11/17/23 15:07:15.515
STEP: waiting for available Replicas 11/17/23 15:07:15.515
STEP: fetching ReplicationController status 11/17/23 15:07:15.527
STEP: patching ReplicationController scale 11/17/23 15:07:15.532
STEP: waiting for RC to be modified 11/17/23 15:07:15.54
STEP: waiting for ReplicationController's scale to be the max amount 11/17/23 15:07:15.54
STEP: fetching ReplicationController; ensuring that it's patched 11/17/23 15:07:16.547
STEP: updating ReplicationController status 11/17/23 15:07:16.571
STEP: waiting for RC to be modified 11/17/23 15:07:16.606
STEP: listing all ReplicationControllers 11/17/23 15:07:16.607
STEP: checking that ReplicationController has expected values 11/17/23 15:07:16.62
STEP: deleting ReplicationControllers by collection 11/17/23 15:07:16.62
STEP: waiting for ReplicationController to have a DELETED watchEvent 11/17/23 15:07:16.685
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:16.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-117" for this suite. 11/17/23 15:07:16.904
------------------------------
â€¢ [3.368 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:13.583
    Nov 17 15:07:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replication-controller 11/17/23 15:07:13.585
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:13.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:13.609
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 11/17/23 15:07:13.618
    STEP: waiting for RC to be added 11/17/23 15:07:13.628
    STEP: waiting for available Replicas 11/17/23 15:07:13.628
    STEP: patching ReplicationController 11/17/23 15:07:15.489
    STEP: waiting for RC to be modified 11/17/23 15:07:15.504
    STEP: patching ReplicationController status 11/17/23 15:07:15.504
    STEP: waiting for RC to be modified 11/17/23 15:07:15.515
    STEP: waiting for available Replicas 11/17/23 15:07:15.515
    STEP: fetching ReplicationController status 11/17/23 15:07:15.527
    STEP: patching ReplicationController scale 11/17/23 15:07:15.532
    STEP: waiting for RC to be modified 11/17/23 15:07:15.54
    STEP: waiting for ReplicationController's scale to be the max amount 11/17/23 15:07:15.54
    STEP: fetching ReplicationController; ensuring that it's patched 11/17/23 15:07:16.547
    STEP: updating ReplicationController status 11/17/23 15:07:16.571
    STEP: waiting for RC to be modified 11/17/23 15:07:16.606
    STEP: listing all ReplicationControllers 11/17/23 15:07:16.607
    STEP: checking that ReplicationController has expected values 11/17/23 15:07:16.62
    STEP: deleting ReplicationControllers by collection 11/17/23 15:07:16.62
    STEP: waiting for ReplicationController to have a DELETED watchEvent 11/17/23 15:07:16.685
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:16.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-117" for this suite. 11/17/23 15:07:16.904
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:16.952
Nov 17 15:07:16.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:07:16.954
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:17.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:17.066
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 11/17/23 15:07:17.092
Nov 17 15:07:17.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85" in namespace "projected-7531" to be "Succeeded or Failed"
Nov 17 15:07:17.184: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Pending", Reason="", readiness=false. Elapsed: 28.865145ms
Nov 17 15:07:19.191: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03584076s
Nov 17 15:07:21.189: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034602812s
STEP: Saw pod success 11/17/23 15:07:21.19
Nov 17 15:07:21.190: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85" satisfied condition "Succeeded or Failed"
Nov 17 15:07:21.193: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 container client-container: <nil>
STEP: delete the pod 11/17/23 15:07:21.201
Nov 17 15:07:21.213: INFO: Waiting for pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 to disappear
Nov 17 15:07:21.216: INFO: Pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:21.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7531" for this suite. 11/17/23 15:07:21.222
------------------------------
â€¢ [4.279 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:16.952
    Nov 17 15:07:16.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:07:16.954
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:17.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:17.066
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 11/17/23 15:07:17.092
    Nov 17 15:07:17.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85" in namespace "projected-7531" to be "Succeeded or Failed"
    Nov 17 15:07:17.184: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Pending", Reason="", readiness=false. Elapsed: 28.865145ms
    Nov 17 15:07:19.191: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03584076s
    Nov 17 15:07:21.189: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034602812s
    STEP: Saw pod success 11/17/23 15:07:21.19
    Nov 17 15:07:21.190: INFO: Pod "downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85" satisfied condition "Succeeded or Failed"
    Nov 17 15:07:21.193: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 container client-container: <nil>
    STEP: delete the pod 11/17/23 15:07:21.201
    Nov 17 15:07:21.213: INFO: Waiting for pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 to disappear
    Nov 17 15:07:21.216: INFO: Pod downwardapi-volume-7d29fd77-2428-45d5-96b7-dceeaf1e9f85 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:21.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7531" for this suite. 11/17/23 15:07:21.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:21.233
Nov 17 15:07:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-pred 11/17/23 15:07:21.234
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:21.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:21.258
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 17 15:07:21.262: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 17 15:07:21.270: INFO: Waiting for terminating namespaces to be deleted...
Nov 17 15:07:21.275: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Nov 17 15:07:21.291: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container cert-manager ready: true, restart count 0
Nov 17 15:07:21.291: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container cainjector ready: true, restart count 0
Nov 17 15:07:21.291: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container webhook ready: true, restart count 0
Nov 17 15:07:21.291: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container minio ready: true, restart count 0
Nov 17 15:07:21.291: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container velero ready: true, restart count 0
Nov 17 15:07:21.291: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container traefik ready: true, restart count 0
Nov 17 15:07:21.291: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.291: INFO: 	Container kube-green ready: true, restart count 0
Nov 17 15:07:21.291: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:07:21.292: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container coredns ready: true, restart count 0
Nov 17 15:07:21.292: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container coredns ready: true, restart count 0
Nov 17 15:07:21.292: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container hubble-relay ready: true, restart count 0
Nov 17 15:07:21.292: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container backend ready: true, restart count 0
Nov 17 15:07:21.292: INFO: 	Container frontend ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kyverno ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kyverno-background ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kyverno-reports ready: true, restart count 0
Nov 17 15:07:21.292: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container local-path-provisioner ready: true, restart count 0
Nov 17 15:07:21.292: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:07:21.292: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container logging-operator ready: true, restart count 0
Nov 17 15:07:21.292: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 17 15:07:21.292: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:07:21.292: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 17 15:07:21.292: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container rbac-manager ready: true, restart count 0
Nov 17 15:07:21.292: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:07:21.292: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 15:07:21.292: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Nov 17 15:07:21.304: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:07:21.304: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 15:07:21.304: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:07:21.304: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:07:21.304: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:07:21.304: INFO: tester from prestop-2600 started at 2023-11-17 15:07:06 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container tester ready: true, restart count 0
Nov 17 15:07:21.304: INFO: rc-test-qwrrd from replication-controller-117 started at 2023-11-17 15:07:13 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container rc-test ready: true, restart count 0
Nov 17 15:07:21.304: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 17 15:07:21.304: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container e2e ready: true, restart count 0
Nov 17 15:07:21.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:07:21.304: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:07:21.304: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 15:07:21.304: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
Nov 17 15:07:21.317: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:07:21.317: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 15:07:21.317: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:07:21.317: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:07:21.317: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:07:21.317: INFO: 	Container fluentd ready: true, restart count 7
Nov 17 15:07:21.317: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container alertmanager ready: true, restart count 0
Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:07:21.317: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:07:21.317: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:07:21.317: INFO: 	Container prometheus ready: true, restart count 0
Nov 17 15:07:21.317: INFO: rc-test-jk7wh from replication-controller-117 started at 2023-11-17 15:07:15 +0000 UTC (1 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container rc-test ready: true, restart count 0
Nov 17 15:07:21.317: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:07:21.317: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:07:21.317: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node k8s-worker-1.c.operations-lab.internal 11/17/23 15:07:21.35
STEP: verifying the node has the label node k8s-worker-2.c.operations-lab.internal 11/17/23 15:07:21.371
STEP: verifying the node has the label node k8s-worker-3.c.operations-lab.internal 11/17/23 15:07:21.396
Nov 17 15:07:21.431: INFO: Pod cert-manager-7689849c74-smgkc requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.431: INFO: Pod cert-manager-cainjector-cdfcc5d5b-nq7vv requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.431: INFO: Pod cert-manager-webhook-57bd576df4-wmz82 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.431: INFO: Pod minio-bc8b57858-5v8tm requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.431: INFO: Pod velero-57c7d7c6c4-vdtv8 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod traefik-7cb9797f6-qn767 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod kube-green-85cfb6cdbd-5skd2 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod cilium-5ddmt requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod cilium-65vkv requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod cilium-dhcs4 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod cilium-operator-86c964c849-rx7t2 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod cilium-operator-86c964c849-v2hw8 requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod coredns-787d4945fb-kzc5z requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.432: INFO: Pod coredns-787d4945fb-ppt87 requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod hubble-relay-5d6dbd4d98-kv8w5 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod hubble-ui-8c9fc5b67-pr96g requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kube-proxy-8mmvh requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kube-proxy-f98r5 requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kube-proxy-m5kfg requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kyverno-5c8fd7bc64-4lx4g requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kyverno-background-5f955bc7fb-2lm9h requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.433: INFO: Pod kyverno-cleanup-66c9dd798b-kbtj7 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.434: INFO: Pod kyverno-reports-74995bc6df-6srr2 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.434: INFO: Pod local-path-provisioner-7f8667b75c-szgfl requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-k8kqf requesting resource cpu=100m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-ncj7c requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-r7fzw requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.435: INFO: Pod logging-fluentd-0 requesting resource cpu=500m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.435: INFO: Pod logging-operator-5df74f78f5-rvtbs requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.435: INFO: Pod alertmanager-alertmanager-0 requesting resource cpu=10m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod kube-state-metrics-8447695667-c6vfl requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod node-exporter-kvvhw requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod node-exporter-s4hnf requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod node-exporter-wpdk5 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod prometheus-operator-75f79b8c5d-ftm94 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.436: INFO: Pod prometheus-prometheus-0 requesting resource cpu=10m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod tester requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod rbac-manager-84bd6887f-9rp2m requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod rc-test-jk7wh requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod rc-test-qwrrd requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.437: INFO: Pod sonobuoy-e2e-job-a1d20c9e74d84b3f requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
STEP: Starting Pods to consume most of the cluster CPU. 11/17/23 15:07:21.438
Nov 17 15:07:21.438: INFO: Creating a pod which consumes cpu=2730m on Node k8s-worker-2.c.operations-lab.internal
Nov 17 15:07:21.449: INFO: Creating a pod which consumes cpu=2366m on Node k8s-worker-3.c.operations-lab.internal
Nov 17 15:07:21.460: INFO: Creating a pod which consumes cpu=2590m on Node k8s-worker-1.c.operations-lab.internal
Nov 17 15:07:21.478: INFO: Waiting up to 5m0s for pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8" in namespace "sched-pred-4052" to be "running"
Nov 17 15:07:21.484: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432944ms
Nov 17 15:07:23.490: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011888076s
Nov 17 15:07:23.490: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8" satisfied condition "running"
Nov 17 15:07:23.490: INFO: Waiting up to 5m0s for pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10" in namespace "sched-pred-4052" to be "running"
Nov 17 15:07:23.494: INFO: Pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10": Phase="Running", Reason="", readiness=true. Elapsed: 3.564763ms
Nov 17 15:07:23.494: INFO: Pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10" satisfied condition "running"
Nov 17 15:07:23.494: INFO: Waiting up to 5m0s for pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a" in namespace "sched-pred-4052" to be "running"
Nov 17 15:07:23.497: INFO: Pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.908853ms
Nov 17 15:07:23.497: INFO: Pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 11/17/23 15:07:23.497
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d1b65f336], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-1326c778-0097-450f-badc-eee30b22be10 to k8s-worker-3.c.operations-lab.internal] 11/17/23 15:07:23.501
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d48eb6fa4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d4a3eb970], Reason = [Created], Message = [Created container filler-pod-1326c778-0097-450f-badc-eee30b22be10] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d514dee64], Reason = [Started], Message = [Started container filler-pod-1326c778-0097-450f-badc-eee30b22be10] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d1c9224d3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a to k8s-worker-1.c.operations-lab.internal] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d4ad4d6a2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d4c9d4603], Reason = [Created], Message = [Created container filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d543116f5], Reason = [Started], Message = [Started container filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d1b0acf5a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8 to k8s-worker-2.c.operations-lab.internal] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d47dd97d6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d4933d285], Reason = [Created], Message = [Created container filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d50055b51], Reason = [Started], Message = [Started container filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8] 11/17/23 15:07:23.502
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1798717d94d24ba5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 11/17/23 15:07:23.522
STEP: removing the label node off the node k8s-worker-1.c.operations-lab.internal 11/17/23 15:07:24.522
STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.541
STEP: removing the label node off the node k8s-worker-2.c.operations-lab.internal 11/17/23 15:07:24.546
STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.565
STEP: removing the label node off the node k8s-worker-3.c.operations-lab.internal 11/17/23 15:07:24.571
STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.589
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:24.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4052" for this suite. 11/17/23 15:07:24.606
------------------------------
â€¢ [3.383 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:21.233
    Nov 17 15:07:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-pred 11/17/23 15:07:21.234
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:21.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:21.258
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 17 15:07:21.262: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 17 15:07:21.270: INFO: Waiting for terminating namespaces to be deleted...
    Nov 17 15:07:21.275: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Nov 17 15:07:21.291: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container cert-manager ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container cainjector ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container webhook ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container minio ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container velero ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container traefik ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.291: INFO: 	Container kube-green ready: true, restart count 0
    Nov 17 15:07:21.291: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container hubble-relay ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container backend ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: 	Container frontend ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kyverno ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kyverno-background ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kyverno-reports ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container logging-operator ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container rbac-manager ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 15:07:21.292: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Nov 17 15:07:21.304: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: tester from prestop-2600 started at 2023-11-17 15:07:06 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container tester ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: rc-test-qwrrd from replication-controller-117 started at 2023-11-17 15:07:13 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container rc-test ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container e2e ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 15:07:21.304: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
    Nov 17 15:07:21.317: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: 	Container fluentd ready: true, restart count 7
    Nov 17 15:07:21.317: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container alertmanager ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: 	Container prometheus ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: rc-test-jk7wh from replication-controller-117 started at 2023-11-17 15:07:15 +0000 UTC (1 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container rc-test ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:07:21.317: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:07:21.317: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node k8s-worker-1.c.operations-lab.internal 11/17/23 15:07:21.35
    STEP: verifying the node has the label node k8s-worker-2.c.operations-lab.internal 11/17/23 15:07:21.371
    STEP: verifying the node has the label node k8s-worker-3.c.operations-lab.internal 11/17/23 15:07:21.396
    Nov 17 15:07:21.431: INFO: Pod cert-manager-7689849c74-smgkc requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.431: INFO: Pod cert-manager-cainjector-cdfcc5d5b-nq7vv requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.431: INFO: Pod cert-manager-webhook-57bd576df4-wmz82 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.431: INFO: Pod minio-bc8b57858-5v8tm requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.431: INFO: Pod velero-57c7d7c6c4-vdtv8 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod traefik-7cb9797f6-qn767 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod kube-green-85cfb6cdbd-5skd2 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod cilium-5ddmt requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod cilium-65vkv requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod cilium-dhcs4 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod cilium-operator-86c964c849-rx7t2 requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod cilium-operator-86c964c849-v2hw8 requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod coredns-787d4945fb-kzc5z requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.432: INFO: Pod coredns-787d4945fb-ppt87 requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod hubble-relay-5d6dbd4d98-kv8w5 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod hubble-ui-8c9fc5b67-pr96g requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kube-proxy-8mmvh requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kube-proxy-f98r5 requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kube-proxy-m5kfg requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kyverno-5c8fd7bc64-4lx4g requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kyverno-background-5f955bc7fb-2lm9h requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.433: INFO: Pod kyverno-cleanup-66c9dd798b-kbtj7 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.434: INFO: Pod kyverno-reports-74995bc6df-6srr2 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.434: INFO: Pod local-path-provisioner-7f8667b75c-szgfl requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-k8kqf requesting resource cpu=100m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-ncj7c requesting resource cpu=100m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.434: INFO: Pod fluentbit-fluentbit-r7fzw requesting resource cpu=100m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.435: INFO: Pod logging-fluentd-0 requesting resource cpu=500m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.435: INFO: Pod logging-operator-5df74f78f5-rvtbs requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.435: INFO: Pod alertmanager-alertmanager-0 requesting resource cpu=10m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod kube-state-metrics-8447695667-c6vfl requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod node-exporter-kvvhw requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod node-exporter-s4hnf requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod node-exporter-wpdk5 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod prometheus-operator-75f79b8c5d-ftm94 requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.436: INFO: Pod prometheus-prometheus-0 requesting resource cpu=10m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod tester requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod rbac-manager-84bd6887f-9rp2m requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod rc-test-jk7wh requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod rc-test-qwrrd requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.437: INFO: Pod sonobuoy-e2e-job-a1d20c9e74d84b3f requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv requesting resource cpu=0m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v requesting resource cpu=0m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.438: INFO: Pod sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb requesting resource cpu=0m on Node k8s-worker-1.c.operations-lab.internal
    STEP: Starting Pods to consume most of the cluster CPU. 11/17/23 15:07:21.438
    Nov 17 15:07:21.438: INFO: Creating a pod which consumes cpu=2730m on Node k8s-worker-2.c.operations-lab.internal
    Nov 17 15:07:21.449: INFO: Creating a pod which consumes cpu=2366m on Node k8s-worker-3.c.operations-lab.internal
    Nov 17 15:07:21.460: INFO: Creating a pod which consumes cpu=2590m on Node k8s-worker-1.c.operations-lab.internal
    Nov 17 15:07:21.478: INFO: Waiting up to 5m0s for pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8" in namespace "sched-pred-4052" to be "running"
    Nov 17 15:07:21.484: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432944ms
    Nov 17 15:07:23.490: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011888076s
    Nov 17 15:07:23.490: INFO: Pod "filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8" satisfied condition "running"
    Nov 17 15:07:23.490: INFO: Waiting up to 5m0s for pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10" in namespace "sched-pred-4052" to be "running"
    Nov 17 15:07:23.494: INFO: Pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10": Phase="Running", Reason="", readiness=true. Elapsed: 3.564763ms
    Nov 17 15:07:23.494: INFO: Pod "filler-pod-1326c778-0097-450f-badc-eee30b22be10" satisfied condition "running"
    Nov 17 15:07:23.494: INFO: Waiting up to 5m0s for pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a" in namespace "sched-pred-4052" to be "running"
    Nov 17 15:07:23.497: INFO: Pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a": Phase="Running", Reason="", readiness=true. Elapsed: 2.908853ms
    Nov 17 15:07:23.497: INFO: Pod "filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 11/17/23 15:07:23.497
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d1b65f336], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-1326c778-0097-450f-badc-eee30b22be10 to k8s-worker-3.c.operations-lab.internal] 11/17/23 15:07:23.501
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d48eb6fa4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d4a3eb970], Reason = [Created], Message = [Created container filler-pod-1326c778-0097-450f-badc-eee30b22be10] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1326c778-0097-450f-badc-eee30b22be10.1798717d514dee64], Reason = [Started], Message = [Started container filler-pod-1326c778-0097-450f-badc-eee30b22be10] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d1c9224d3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a to k8s-worker-1.c.operations-lab.internal] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d4ad4d6a2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d4c9d4603], Reason = [Created], Message = [Created container filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a.1798717d543116f5], Reason = [Started], Message = [Started container filler-pod-6601e196-3c59-4737-a2dd-766f4806db0a] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d1b0acf5a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4052/filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8 to k8s-worker-2.c.operations-lab.internal] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d47dd97d6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d4933d285], Reason = [Created], Message = [Created container filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8.1798717d50055b51], Reason = [Started], Message = [Started container filler-pod-bd29e368-aa2f-4331-948d-36c84f1f1dc8] 11/17/23 15:07:23.502
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1798717d94d24ba5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 11/17/23 15:07:23.522
    STEP: removing the label node off the node k8s-worker-1.c.operations-lab.internal 11/17/23 15:07:24.522
    STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.541
    STEP: removing the label node off the node k8s-worker-2.c.operations-lab.internal 11/17/23 15:07:24.546
    STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.565
    STEP: removing the label node off the node k8s-worker-3.c.operations-lab.internal 11/17/23 15:07:24.571
    STEP: verifying the node doesn't have the label node 11/17/23 15:07:24.589
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:24.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4052" for this suite. 11/17/23 15:07:24.606
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:24.622
Nov 17 15:07:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename dns 11/17/23 15:07:24.625
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:24.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:24.656
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 11/17/23 15:07:24.662
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local;sleep 1; done
 11/17/23 15:07:24.672
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local;sleep 1; done
 11/17/23 15:07:24.672
STEP: creating a pod to probe DNS 11/17/23 15:07:24.672
STEP: submitting the pod to kubernetes 11/17/23 15:07:24.672
Nov 17 15:07:24.695: INFO: Waiting up to 15m0s for pod "dns-test-19cdc688-2730-4a56-8a97-729232233022" in namespace "dns-4935" to be "running"
Nov 17 15:07:24.700: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331714ms
Nov 17 15:07:26.707: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445646s
Nov 17 15:07:26.707: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022" satisfied condition "running"
STEP: retrieving the pod 11/17/23 15:07:26.707
STEP: looking for the results for each expected name from probers 11/17/23 15:07:26.71
Nov 17 15:07:26.717: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.724: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.728: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.733: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.739: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.745: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.750: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.758: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:26.758: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:31.763: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:31.767: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:31.778: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:31.784: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:31.794: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:36.765: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:36.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:36.786: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:36.790: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:36.805: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:41.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:41.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:41.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:41.785: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:41.793: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:46.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:46.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:46.780: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:46.785: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:46.793: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:51.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:51.768: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:51.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:51.787: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
Nov 17 15:07:51.797: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

Nov 17 15:07:56.791: INFO: DNS probes using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 succeeded

STEP: deleting the pod 11/17/23 15:07:56.791
STEP: deleting the test headless service 11/17/23 15:07:56.812
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Nov 17 15:07:56.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4935" for this suite. 11/17/23 15:07:56.872
------------------------------
â€¢ [SLOW TEST] [32.261 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:24.622
    Nov 17 15:07:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename dns 11/17/23 15:07:24.625
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:24.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:24.656
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 11/17/23 15:07:24.662
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local;sleep 1; done
     11/17/23 15:07:24.672
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4935.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local;sleep 1; done
     11/17/23 15:07:24.672
    STEP: creating a pod to probe DNS 11/17/23 15:07:24.672
    STEP: submitting the pod to kubernetes 11/17/23 15:07:24.672
    Nov 17 15:07:24.695: INFO: Waiting up to 15m0s for pod "dns-test-19cdc688-2730-4a56-8a97-729232233022" in namespace "dns-4935" to be "running"
    Nov 17 15:07:24.700: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331714ms
    Nov 17 15:07:26.707: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445646s
    Nov 17 15:07:26.707: INFO: Pod "dns-test-19cdc688-2730-4a56-8a97-729232233022" satisfied condition "running"
    STEP: retrieving the pod 11/17/23 15:07:26.707
    STEP: looking for the results for each expected name from probers 11/17/23 15:07:26.71
    Nov 17 15:07:26.717: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.724: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.728: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.733: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.739: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.745: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.750: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.758: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:26.758: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:31.763: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:31.767: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:31.778: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:31.784: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:31.794: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:36.765: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:36.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:36.786: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:36.790: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:36.805: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:41.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:41.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:41.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:41.785: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:41.793: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:46.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:46.770: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:46.780: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:46.785: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:46.793: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:51.764: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:51.768: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:51.782: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:51.787: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local from pod dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022: the server could not find the requested resource (get pods dns-test-19cdc688-2730-4a56-8a97-729232233022)
    Nov 17 15:07:51.797: INFO: Lookups using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4935.svc.cluster.local]

    Nov 17 15:07:56.791: INFO: DNS probes using dns-4935/dns-test-19cdc688-2730-4a56-8a97-729232233022 succeeded

    STEP: deleting the pod 11/17/23 15:07:56.791
    STEP: deleting the test headless service 11/17/23 15:07:56.812
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:07:56.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4935" for this suite. 11/17/23 15:07:56.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:07:56.886
Nov 17 15:07:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 15:07:56.888
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:56.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:56.91
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Nov 17 15:07:56.935: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 11/17/23 15:07:56.942
Nov 17 15:07:56.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:07:56.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 11/17/23 15:07:56.948
Nov 17 15:07:56.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:07:56.975: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:07:57.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:07:57.981: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:07:58.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 17 15:07:58.980: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 11/17/23 15:07:58.983
Nov 17 15:07:59.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 17 15:07:59.004: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Nov 17 15:08:00.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:00.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/17/23 15:08:00.009
Nov 17 15:08:00.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:00.028: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:01.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:01.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:02.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:02.034: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:03.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:03.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:04.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:04.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:05.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:05.036: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:08:06.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Nov 17 15:08:06.032: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:08:06.038
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3199, will wait for the garbage collector to delete the pods 11/17/23 15:08:06.038
Nov 17 15:08:06.097: INFO: Deleting DaemonSet.extensions daemon-set took: 6.044874ms
Nov 17 15:08:06.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.077218ms
Nov 17 15:08:06.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:08:06.701: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 15:08:06.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62844"},"items":null}

Nov 17 15:08:06.707: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62844"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:06.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3199" for this suite. 11/17/23 15:08:06.743
------------------------------
â€¢ [SLOW TEST] [9.867 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:07:56.886
    Nov 17 15:07:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 15:07:56.888
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:07:56.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:07:56.91
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Nov 17 15:07:56.935: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 11/17/23 15:07:56.942
    Nov 17 15:07:56.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:07:56.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 11/17/23 15:07:56.948
    Nov 17 15:07:56.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:07:56.975: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:07:57.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:07:57.981: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:07:58.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 17 15:07:58.980: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 11/17/23 15:07:58.983
    Nov 17 15:07:59.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 17 15:07:59.004: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Nov 17 15:08:00.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:00.009: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 11/17/23 15:08:00.009
    Nov 17 15:08:00.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:00.028: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:01.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:01.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:02.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:02.034: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:03.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:03.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:04.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:04.032: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:05.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:05.036: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:08:06.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Nov 17 15:08:06.032: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:08:06.038
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3199, will wait for the garbage collector to delete the pods 11/17/23 15:08:06.038
    Nov 17 15:08:06.097: INFO: Deleting DaemonSet.extensions daemon-set took: 6.044874ms
    Nov 17 15:08:06.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.077218ms
    Nov 17 15:08:06.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:08:06.701: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 15:08:06.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62844"},"items":null}

    Nov 17 15:08:06.707: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62844"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:06.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3199" for this suite. 11/17/23 15:08:06.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:06.766
Nov 17 15:08:06.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 15:08:06.767
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:06.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:06.815
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 15:08:06.836
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 15:08:07.302
STEP: Deploying the webhook pod 11/17/23 15:08:07.311
STEP: Wait for the deployment to be ready 11/17/23 15:08:07.327
Nov 17 15:08:07.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 15:08:09.355
STEP: Verifying the service has paired with the endpoint 11/17/23 15:08:09.374
Nov 17 15:08:10.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Nov 17 15:08:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2979-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 15:08:10.892
STEP: Creating a custom resource while v1 is storage version 11/17/23 15:08:10.912
STEP: Patching Custom Resource Definition to set v2 as storage 11/17/23 15:08:13.007
STEP: Patching the custom resource while v2 is storage version 11/17/23 15:08:13.031
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:13.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6325" for this suite. 11/17/23 15:08:13.731
STEP: Destroying namespace "webhook-6325-markers" for this suite. 11/17/23 15:08:13.74
------------------------------
â€¢ [SLOW TEST] [6.991 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:06.766
    Nov 17 15:08:06.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 15:08:06.767
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:06.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:06.815
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 15:08:06.836
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 15:08:07.302
    STEP: Deploying the webhook pod 11/17/23 15:08:07.311
    STEP: Wait for the deployment to be ready 11/17/23 15:08:07.327
    Nov 17 15:08:07.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 15:08:09.355
    STEP: Verifying the service has paired with the endpoint 11/17/23 15:08:09.374
    Nov 17 15:08:10.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Nov 17 15:08:10.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2979-crds.webhook.example.com via the AdmissionRegistration API 11/17/23 15:08:10.892
    STEP: Creating a custom resource while v1 is storage version 11/17/23 15:08:10.912
    STEP: Patching Custom Resource Definition to set v2 as storage 11/17/23 15:08:13.007
    STEP: Patching the custom resource while v2 is storage version 11/17/23 15:08:13.031
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:13.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6325" for this suite. 11/17/23 15:08:13.731
    STEP: Destroying namespace "webhook-6325-markers" for this suite. 11/17/23 15:08:13.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:13.759
Nov 17 15:08:13.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:08:13.761
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:13.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:13.796
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 11/17/23 15:08:13.806
STEP: Creating a ResourceQuota 11/17/23 15:08:18.81
STEP: Ensuring resource quota status is calculated 11/17/23 15:08:18.818
STEP: Creating a ReplicationController 11/17/23 15:08:20.821
STEP: Ensuring resource quota status captures replication controller creation 11/17/23 15:08:20.836
STEP: Deleting a ReplicationController 11/17/23 15:08:22.84
STEP: Ensuring resource quota status released usage 11/17/23 15:08:22.846
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4371" for this suite. 11/17/23 15:08:24.857
------------------------------
â€¢ [SLOW TEST] [11.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:13.759
    Nov 17 15:08:13.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:08:13.761
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:13.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:13.796
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 11/17/23 15:08:13.806
    STEP: Creating a ResourceQuota 11/17/23 15:08:18.81
    STEP: Ensuring resource quota status is calculated 11/17/23 15:08:18.818
    STEP: Creating a ReplicationController 11/17/23 15:08:20.821
    STEP: Ensuring resource quota status captures replication controller creation 11/17/23 15:08:20.836
    STEP: Deleting a ReplicationController 11/17/23 15:08:22.84
    STEP: Ensuring resource quota status released usage 11/17/23 15:08:22.846
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4371" for this suite. 11/17/23 15:08:24.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:24.866
Nov 17 15:08:24.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 15:08:24.868
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:24.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:24.894
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-2lbcm"  11/17/23 15:08:24.898
Nov 17 15:08:24.907: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-2lbcm"  11/17/23 15:08:24.908
Nov 17 15:08:24.918: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9058" for this suite. 11/17/23 15:08:24.924
------------------------------
â€¢ [0.065 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:24.866
    Nov 17 15:08:24.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 15:08:24.868
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:24.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:24.894
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-2lbcm"  11/17/23 15:08:24.898
    Nov 17 15:08:24.907: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-2lbcm"  11/17/23 15:08:24.908
    Nov 17 15:08:24.918: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9058" for this suite. 11/17/23 15:08:24.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:24.936
Nov 17 15:08:24.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 15:08:24.937
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:24.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:24.962
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 11/17/23 15:08:24.966
Nov 17 15:08:24.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13" in namespace "downward-api-5023" to be "Succeeded or Failed"
Nov 17 15:08:24.980: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.981524ms
Nov 17 15:08:26.986: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010116205s
Nov 17 15:08:28.984: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008923688s
STEP: Saw pod success 11/17/23 15:08:28.985
Nov 17 15:08:28.985: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13" satisfied condition "Succeeded or Failed"
Nov 17 15:08:28.988: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 container client-container: <nil>
STEP: delete the pod 11/17/23 15:08:28.994
Nov 17 15:08:29.006: INFO: Waiting for pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 to disappear
Nov 17 15:08:29.009: INFO: Pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:29.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5023" for this suite. 11/17/23 15:08:29.013
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:24.936
    Nov 17 15:08:24.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 15:08:24.937
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:24.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:24.962
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 11/17/23 15:08:24.966
    Nov 17 15:08:24.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13" in namespace "downward-api-5023" to be "Succeeded or Failed"
    Nov 17 15:08:24.980: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.981524ms
    Nov 17 15:08:26.986: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010116205s
    Nov 17 15:08:28.984: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008923688s
    STEP: Saw pod success 11/17/23 15:08:28.985
    Nov 17 15:08:28.985: INFO: Pod "downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13" satisfied condition "Succeeded or Failed"
    Nov 17 15:08:28.988: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 container client-container: <nil>
    STEP: delete the pod 11/17/23 15:08:28.994
    Nov 17 15:08:29.006: INFO: Waiting for pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 to disappear
    Nov 17 15:08:29.009: INFO: Pod downwardapi-volume-7269aca0-28a2-4b6e-8671-1c50b156fb13 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:29.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5023" for this suite. 11/17/23 15:08:29.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:29.022
Nov 17 15:08:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename secrets 11/17/23 15:08:29.023
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:29.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:29.061
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-e6a1e6e5-f850-40a4-b9d9-15c40ff7aab9 11/17/23 15:08:29.068
STEP: Creating secret with name s-test-opt-upd-0bb521f4-b06d-44b7-8aed-eaa3370e7e86 11/17/23 15:08:29.073
STEP: Creating the pod 11/17/23 15:08:29.079
Nov 17 15:08:29.088: INFO: Waiting up to 5m0s for pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb" in namespace "secrets-914" to be "running and ready"
Nov 17 15:08:29.093: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69662ms
Nov 17 15:08:29.093: INFO: The phase of Pod pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:08:31.097: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009479281s
Nov 17 15:08:31.097: INFO: The phase of Pod pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb is Running (Ready = true)
Nov 17 15:08:31.097: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e6a1e6e5-f850-40a4-b9d9-15c40ff7aab9 11/17/23 15:08:31.121
STEP: Updating secret s-test-opt-upd-0bb521f4-b06d-44b7-8aed-eaa3370e7e86 11/17/23 15:08:31.128
STEP: Creating secret with name s-test-opt-create-f128de75-88ed-460c-8935-d2edaddc85ea 11/17/23 15:08:31.134
STEP: waiting to observe update in volume 11/17/23 15:08:31.14
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:33.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-914" for this suite. 11/17/23 15:08:33.187
------------------------------
â€¢ [4.173 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:29.022
    Nov 17 15:08:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename secrets 11/17/23 15:08:29.023
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:29.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:29.061
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-e6a1e6e5-f850-40a4-b9d9-15c40ff7aab9 11/17/23 15:08:29.068
    STEP: Creating secret with name s-test-opt-upd-0bb521f4-b06d-44b7-8aed-eaa3370e7e86 11/17/23 15:08:29.073
    STEP: Creating the pod 11/17/23 15:08:29.079
    Nov 17 15:08:29.088: INFO: Waiting up to 5m0s for pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb" in namespace "secrets-914" to be "running and ready"
    Nov 17 15:08:29.093: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69662ms
    Nov 17 15:08:29.093: INFO: The phase of Pod pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:08:31.097: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009479281s
    Nov 17 15:08:31.097: INFO: The phase of Pod pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb is Running (Ready = true)
    Nov 17 15:08:31.097: INFO: Pod "pod-secrets-6a938d71-83f6-4c8e-bec4-e7cf834020bb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e6a1e6e5-f850-40a4-b9d9-15c40ff7aab9 11/17/23 15:08:31.121
    STEP: Updating secret s-test-opt-upd-0bb521f4-b06d-44b7-8aed-eaa3370e7e86 11/17/23 15:08:31.128
    STEP: Creating secret with name s-test-opt-create-f128de75-88ed-460c-8935-d2edaddc85ea 11/17/23 15:08:31.134
    STEP: waiting to observe update in volume 11/17/23 15:08:31.14
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:33.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-914" for this suite. 11/17/23 15:08:33.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:33.196
Nov 17 15:08:33.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 15:08:33.198
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:33.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:33.223
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9436 11/17/23 15:08:33.226
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9436 11/17/23 15:08:33.234
Nov 17 15:08:33.252: INFO: Found 0 stateful pods, waiting for 1
Nov 17 15:08:43.258: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 11/17/23 15:08:43.265
STEP: updating a scale subresource 11/17/23 15:08:43.269
STEP: verifying the statefulset Spec.Replicas was modified 11/17/23 15:08:43.275
STEP: Patch a scale subresource 11/17/23 15:08:43.279
STEP: verifying the statefulset Spec.Replicas was modified 11/17/23 15:08:43.295
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 15:08:43.305: INFO: Deleting all statefulset in ns statefulset-9436
Nov 17 15:08:43.311: INFO: Scaling statefulset ss to 0
Nov 17 15:08:53.338: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 15:08:53.341: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:08:53.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9436" for this suite. 11/17/23 15:08:53.381
------------------------------
â€¢ [SLOW TEST] [20.194 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:33.196
    Nov 17 15:08:33.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 15:08:33.198
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:33.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:33.223
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9436 11/17/23 15:08:33.226
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9436 11/17/23 15:08:33.234
    Nov 17 15:08:33.252: INFO: Found 0 stateful pods, waiting for 1
    Nov 17 15:08:43.258: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 11/17/23 15:08:43.265
    STEP: updating a scale subresource 11/17/23 15:08:43.269
    STEP: verifying the statefulset Spec.Replicas was modified 11/17/23 15:08:43.275
    STEP: Patch a scale subresource 11/17/23 15:08:43.279
    STEP: verifying the statefulset Spec.Replicas was modified 11/17/23 15:08:43.295
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 15:08:43.305: INFO: Deleting all statefulset in ns statefulset-9436
    Nov 17 15:08:43.311: INFO: Scaling statefulset ss to 0
    Nov 17 15:08:53.338: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 15:08:53.341: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:08:53.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9436" for this suite. 11/17/23 15:08:53.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:08:53.391
Nov 17 15:08:53.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename container-probe 11/17/23 15:08:53.392
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:53.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:53.414
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c in namespace container-probe-9144 11/17/23 15:08:53.418
Nov 17 15:08:53.425: INFO: Waiting up to 5m0s for pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c" in namespace "container-probe-9144" to be "not pending"
Nov 17 15:08:53.430: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86524ms
Nov 17 15:08:55.437: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011060838s
Nov 17 15:08:55.437: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c" satisfied condition "not pending"
Nov 17 15:08:55.437: INFO: Started pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c in namespace container-probe-9144
STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 15:08:55.437
Nov 17 15:08:55.441: INFO: Initial restart count of pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is 0
Nov 17 15:09:15.492: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 1 (20.050675965s elapsed)
Nov 17 15:09:35.540: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 2 (40.098843751s elapsed)
Nov 17 15:09:55.588: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 3 (1m0.146579528s elapsed)
Nov 17 15:10:15.653: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 4 (1m20.212206763s elapsed)
Nov 17 15:11:15.794: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 5 (2m20.352887906s elapsed)
STEP: deleting the pod 11/17/23 15:11:15.794
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:15.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9144" for this suite. 11/17/23 15:11:15.81
------------------------------
â€¢ [SLOW TEST] [142.448 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:08:53.391
    Nov 17 15:08:53.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename container-probe 11/17/23 15:08:53.392
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:08:53.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:08:53.414
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c in namespace container-probe-9144 11/17/23 15:08:53.418
    Nov 17 15:08:53.425: INFO: Waiting up to 5m0s for pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c" in namespace "container-probe-9144" to be "not pending"
    Nov 17 15:08:53.430: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86524ms
    Nov 17 15:08:55.437: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011060838s
    Nov 17 15:08:55.437: INFO: Pod "liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c" satisfied condition "not pending"
    Nov 17 15:08:55.437: INFO: Started pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c in namespace container-probe-9144
    STEP: checking the pod's current state and verifying that restartCount is present 11/17/23 15:08:55.437
    Nov 17 15:08:55.441: INFO: Initial restart count of pod liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is 0
    Nov 17 15:09:15.492: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 1 (20.050675965s elapsed)
    Nov 17 15:09:35.540: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 2 (40.098843751s elapsed)
    Nov 17 15:09:55.588: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 3 (1m0.146579528s elapsed)
    Nov 17 15:10:15.653: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 4 (1m20.212206763s elapsed)
    Nov 17 15:11:15.794: INFO: Restart count of pod container-probe-9144/liveness-bc44f4af-090a-4d3c-8b5d-2b13eb41698c is now 5 (2m20.352887906s elapsed)
    STEP: deleting the pod 11/17/23 15:11:15.794
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:15.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9144" for this suite. 11/17/23 15:11:15.81
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:15.839
Nov 17 15:11:15.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 15:11:15.841
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:15.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:15.863
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Nov 17 15:11:15.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 create -f -'
Nov 17 15:11:19.963: INFO: stderr: ""
Nov 17 15:11:19.963: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 17 15:11:19.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 create -f -'
Nov 17 15:11:20.659: INFO: stderr: ""
Nov 17 15:11:20.659: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 11/17/23 15:11:20.659
Nov 17 15:11:21.664: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 15:11:21.664: INFO: Found 1 / 1
Nov 17 15:11:21.664: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 17 15:11:21.668: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 17 15:11:21.668: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 17 15:11:21.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe pod agnhost-primary-dn266'
Nov 17 15:11:21.765: INFO: stderr: ""
Nov 17 15:11:21.765: INFO: stdout: "Name:             agnhost-primary-dn266\nNamespace:        kubectl-3738\nPriority:         0\nService Account:  default\nNode:             k8s-worker-2.c.operations-lab.internal/172.16.0.4\nStart Time:       Fri, 17 Nov 2023 15:11:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.173\nIPs:\n  IP:           10.10.0.173\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://544a1a032ed8a57d83675eb5d04e04feebcbde8d55cd13c3e5664f81024e8024\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 17 Nov 2023 15:11:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dlc2x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dlc2x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3738/agnhost-primary-dn266 to k8s-worker-2.c.operations-lab.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Nov 17 15:11:21.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe rc agnhost-primary'
Nov 17 15:11:21.872: INFO: stderr: ""
Nov 17 15:11:21.872: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3738\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-dn266\n"
Nov 17 15:11:21.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe service agnhost-primary'
Nov 17 15:11:22.011: INFO: stderr: ""
Nov 17 15:11:22.011: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3738\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.201.82\nIPs:               10.105.201.82\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.173:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 17 15:11:22.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe node k8s-control-plane.c.operations-lab.internal'
Nov 17 15:11:22.148: INFO: stderr: ""
Nov 17 15:11:22.148: INFO: stdout: "Name:               k8s-control-plane.c.operations-lab.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-control-plane.c.operations-lab.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 17 Nov 2023 12:10:04 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-control-plane.c.operations-lab.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 17 Nov 2023 15:11:21 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 17 Nov 2023 13:33:23 +0000   Fri, 17 Nov 2023 13:33:23 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 13:33:23 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.2\n  Hostname:    k8s-control-plane.c.operations-lab.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8120756Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8018356Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 6f418680c39a9867e71db0905d183980\n  System UUID:                6f418680-c39a-9867-e71d-b0905d183980\n  Boot ID:                    c456d221-bfc8-41cc-924c-5e107bfab7ca\n  Kernel Version:             6.2.0-1018-gcp\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.24\n  Kubelet Version:            v1.26.11\n  Kube-Proxy Version:         v1.26.11\nPodCIDR:                      10.10.0.0/24\nPodCIDRs:                     10.10.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-vsqx4                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                 etcd-k8s-control-plane.c.operations-lab.internal                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         3h1m\n  kube-system                 kube-apiserver-k8s-control-plane.c.operations-lab.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-controller-manager-k8s-control-plane.c.operations-lab.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-proxy-2wkt4                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-scheduler-k8s-control-plane.c.operations-lab.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         3h1m\n  monitoring-system           node-exporter-vm8kp                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-af74b530fb954558-x2jdp                0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Nov 17 15:11:22.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe namespace kubectl-3738'
Nov 17 15:11:22.259: INFO: stderr: ""
Nov 17 15:11:22.259: INFO: stdout: "Name:         kubectl-3738\nLabels:       e2e-framework=kubectl\n              e2e-run=8bfdd49f-be19-486b-977c-05f3d6ba48c3\n              kubernetes.io/metadata.name=kubectl-3738\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3738" for this suite. 11/17/23 15:11:22.266
------------------------------
â€¢ [SLOW TEST] [6.437 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:15.839
    Nov 17 15:11:15.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 15:11:15.841
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:15.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:15.863
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Nov 17 15:11:15.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 create -f -'
    Nov 17 15:11:19.963: INFO: stderr: ""
    Nov 17 15:11:19.963: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Nov 17 15:11:19.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 create -f -'
    Nov 17 15:11:20.659: INFO: stderr: ""
    Nov 17 15:11:20.659: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 11/17/23 15:11:20.659
    Nov 17 15:11:21.664: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 15:11:21.664: INFO: Found 1 / 1
    Nov 17 15:11:21.664: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Nov 17 15:11:21.668: INFO: Selector matched 1 pods for map[app:agnhost]
    Nov 17 15:11:21.668: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Nov 17 15:11:21.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe pod agnhost-primary-dn266'
    Nov 17 15:11:21.765: INFO: stderr: ""
    Nov 17 15:11:21.765: INFO: stdout: "Name:             agnhost-primary-dn266\nNamespace:        kubectl-3738\nPriority:         0\nService Account:  default\nNode:             k8s-worker-2.c.operations-lab.internal/172.16.0.4\nStart Time:       Fri, 17 Nov 2023 15:11:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.173\nIPs:\n  IP:           10.10.0.173\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://544a1a032ed8a57d83675eb5d04e04feebcbde8d55cd13c3e5664f81024e8024\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 17 Nov 2023 15:11:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dlc2x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dlc2x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3738/agnhost-primary-dn266 to k8s-worker-2.c.operations-lab.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Nov 17 15:11:21.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe rc agnhost-primary'
    Nov 17 15:11:21.872: INFO: stderr: ""
    Nov 17 15:11:21.872: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3738\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-dn266\n"
    Nov 17 15:11:21.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe service agnhost-primary'
    Nov 17 15:11:22.011: INFO: stderr: ""
    Nov 17 15:11:22.011: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3738\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.201.82\nIPs:               10.105.201.82\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.173:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Nov 17 15:11:22.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe node k8s-control-plane.c.operations-lab.internal'
    Nov 17 15:11:22.148: INFO: stderr: ""
    Nov 17 15:11:22.148: INFO: stdout: "Name:               k8s-control-plane.c.operations-lab.internal\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-control-plane.c.operations-lab.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 17 Nov 2023 12:10:04 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-control-plane.c.operations-lab.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 17 Nov 2023 15:11:21 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 17 Nov 2023 13:33:23 +0000   Fri, 17 Nov 2023 13:33:23 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 12:10:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 17 Nov 2023 15:10:05 +0000   Fri, 17 Nov 2023 13:33:23 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.2\n  Hostname:    k8s-control-plane.c.operations-lab.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50620216Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8120756Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46651590989\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8018356Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 6f418680c39a9867e71db0905d183980\n  System UUID:                6f418680-c39a-9867-e71d-b0905d183980\n  Boot ID:                    c456d221-bfc8-41cc-924c-5e107bfab7ca\n  Kernel Version:             6.2.0-1018-gcp\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.24\n  Kubelet Version:            v1.26.11\n  Kube-Proxy Version:         v1.26.11\nPodCIDR:                      10.10.0.0/24\nPodCIDRs:                     10.10.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-vsqx4                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                 etcd-k8s-control-plane.c.operations-lab.internal                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         3h1m\n  kube-system                 kube-apiserver-k8s-control-plane.c.operations-lab.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-controller-manager-k8s-control-plane.c.operations-lab.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-proxy-2wkt4                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h1m\n  kube-system                 kube-scheduler-k8s-control-plane.c.operations-lab.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         3h1m\n  monitoring-system           node-exporter-vm8kp                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-af74b530fb954558-x2jdp                0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Nov 17 15:11:22.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-3738 describe namespace kubectl-3738'
    Nov 17 15:11:22.259: INFO: stderr: ""
    Nov 17 15:11:22.259: INFO: stdout: "Name:         kubectl-3738\nLabels:       e2e-framework=kubectl\n              e2e-run=8bfdd49f-be19-486b-977c-05f3d6ba48c3\n              kubernetes.io/metadata.name=kubectl-3738\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3738" for this suite. 11/17/23 15:11:22.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:22.277
Nov 17 15:11:22.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:11:22.279
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:22.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:22.306
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 11/17/23 15:11:22.31
Nov 17 15:11:22.321: INFO: Waiting up to 5m0s for pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca" in namespace "emptydir-8607" to be "Succeeded or Failed"
Nov 17 15:11:22.326: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042743ms
Nov 17 15:11:24.330: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009466652s
Nov 17 15:11:26.330: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009521615s
STEP: Saw pod success 11/17/23 15:11:26.33
Nov 17 15:11:26.331: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca" satisfied condition "Succeeded or Failed"
Nov 17 15:11:26.334: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca container test-container: <nil>
STEP: delete the pod 11/17/23 15:11:26.356
Nov 17 15:11:26.369: INFO: Waiting for pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca to disappear
Nov 17 15:11:26.372: INFO: Pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:26.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8607" for this suite. 11/17/23 15:11:26.378
------------------------------
â€¢ [4.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:22.277
    Nov 17 15:11:22.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:11:22.279
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:22.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:22.306
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 11/17/23 15:11:22.31
    Nov 17 15:11:22.321: INFO: Waiting up to 5m0s for pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca" in namespace "emptydir-8607" to be "Succeeded or Failed"
    Nov 17 15:11:22.326: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042743ms
    Nov 17 15:11:24.330: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009466652s
    Nov 17 15:11:26.330: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009521615s
    STEP: Saw pod success 11/17/23 15:11:26.33
    Nov 17 15:11:26.331: INFO: Pod "pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca" satisfied condition "Succeeded or Failed"
    Nov 17 15:11:26.334: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca container test-container: <nil>
    STEP: delete the pod 11/17/23 15:11:26.356
    Nov 17 15:11:26.369: INFO: Waiting for pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca to disappear
    Nov 17 15:11:26.372: INFO: Pod pod-dbd1f922-424d-4443-9f5e-56c63f38f5ca no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:26.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8607" for this suite. 11/17/23 15:11:26.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:26.392
Nov 17 15:11:26.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:11:26.394
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:26.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:26.418
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 11/17/23 15:11:26.421
Nov 17 15:11:26.431: INFO: Waiting up to 5m0s for pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c" in namespace "emptydir-6115" to be "Succeeded or Failed"
Nov 17 15:11:26.435: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18119ms
Nov 17 15:11:28.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Running", Reason="", readiness=false. Elapsed: 2.008872738s
Nov 17 15:11:30.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008625703s
STEP: Saw pod success 11/17/23 15:11:30.44
Nov 17 15:11:30.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c" satisfied condition "Succeeded or Failed"
Nov 17 15:11:30.443: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-0afc5d80-1932-4020-aed2-068fb37e798c container test-container: <nil>
STEP: delete the pod 11/17/23 15:11:30.449
Nov 17 15:11:30.463: INFO: Waiting for pod pod-0afc5d80-1932-4020-aed2-068fb37e798c to disappear
Nov 17 15:11:30.466: INFO: Pod pod-0afc5d80-1932-4020-aed2-068fb37e798c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6115" for this suite. 11/17/23 15:11:30.471
------------------------------
â€¢ [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:26.392
    Nov 17 15:11:26.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:11:26.394
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:26.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:26.418
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 11/17/23 15:11:26.421
    Nov 17 15:11:26.431: INFO: Waiting up to 5m0s for pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c" in namespace "emptydir-6115" to be "Succeeded or Failed"
    Nov 17 15:11:26.435: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18119ms
    Nov 17 15:11:28.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Running", Reason="", readiness=false. Elapsed: 2.008872738s
    Nov 17 15:11:30.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008625703s
    STEP: Saw pod success 11/17/23 15:11:30.44
    Nov 17 15:11:30.440: INFO: Pod "pod-0afc5d80-1932-4020-aed2-068fb37e798c" satisfied condition "Succeeded or Failed"
    Nov 17 15:11:30.443: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-0afc5d80-1932-4020-aed2-068fb37e798c container test-container: <nil>
    STEP: delete the pod 11/17/23 15:11:30.449
    Nov 17 15:11:30.463: INFO: Waiting for pod pod-0afc5d80-1932-4020-aed2-068fb37e798c to disappear
    Nov 17 15:11:30.466: INFO: Pod pod-0afc5d80-1932-4020-aed2-068fb37e798c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6115" for this suite. 11/17/23 15:11:30.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:30.479
Nov 17 15:11:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubectl 11/17/23 15:11:30.48
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:30.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:30.497
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 11/17/23 15:11:30.502
Nov 17 15:11:30.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 create -f -'
Nov 17 15:11:31.059: INFO: stderr: ""
Nov 17 15:11:31.059: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:31.059
Nov 17 15:11:31.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 15:11:31.156: INFO: stderr: ""
Nov 17 15:11:31.156: INFO: stdout: "update-demo-nautilus-dvrxg update-demo-nautilus-k8ng5 "
Nov 17 15:11:31.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:31.252: INFO: stderr: ""
Nov 17 15:11:31.252: INFO: stdout: ""
Nov 17 15:11:31.252: INFO: update-demo-nautilus-dvrxg is created but not running
Nov 17 15:11:36.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 15:11:36.356: INFO: stderr: ""
Nov 17 15:11:36.356: INFO: stdout: "update-demo-nautilus-dvrxg update-demo-nautilus-k8ng5 "
Nov 17 15:11:36.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:36.454: INFO: stderr: ""
Nov 17 15:11:36.454: INFO: stdout: "true"
Nov 17 15:11:36.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 15:11:36.550: INFO: stderr: ""
Nov 17 15:11:36.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 15:11:36.550: INFO: validating pod update-demo-nautilus-dvrxg
Nov 17 15:11:36.557: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 15:11:36.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 15:11:36.558: INFO: update-demo-nautilus-dvrxg is verified up and running
Nov 17 15:11:36.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-k8ng5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:36.649: INFO: stderr: ""
Nov 17 15:11:36.649: INFO: stdout: "true"
Nov 17 15:11:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-k8ng5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 15:11:36.743: INFO: stderr: ""
Nov 17 15:11:36.743: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 15:11:36.743: INFO: validating pod update-demo-nautilus-k8ng5
Nov 17 15:11:36.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 15:11:36.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 15:11:36.749: INFO: update-demo-nautilus-k8ng5 is verified up and running
STEP: scaling down the replication controller 11/17/23 15:11:36.749
Nov 17 15:11:36.752: INFO: scanned /root for discovery docs: <nil>
Nov 17 15:11:36.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 17 15:11:37.870: INFO: stderr: ""
Nov 17 15:11:37.870: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:37.87
Nov 17 15:11:37.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 15:11:37.957: INFO: stderr: ""
Nov 17 15:11:37.957: INFO: stdout: "update-demo-nautilus-dvrxg "
Nov 17 15:11:37.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:38.041: INFO: stderr: ""
Nov 17 15:11:38.041: INFO: stdout: "true"
Nov 17 15:11:38.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 15:11:38.129: INFO: stderr: ""
Nov 17 15:11:38.130: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 15:11:38.130: INFO: validating pod update-demo-nautilus-dvrxg
Nov 17 15:11:38.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 15:11:38.134: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 15:11:38.134: INFO: update-demo-nautilus-dvrxg is verified up and running
STEP: scaling up the replication controller 11/17/23 15:11:38.134
Nov 17 15:11:38.137: INFO: scanned /root for discovery docs: <nil>
Nov 17 15:11:38.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 17 15:11:39.268: INFO: stderr: ""
Nov 17 15:11:39.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:39.268
Nov 17 15:11:39.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 17 15:11:39.375: INFO: stderr: ""
Nov 17 15:11:39.375: INFO: stdout: "update-demo-nautilus-bq7rw update-demo-nautilus-dvrxg "
Nov 17 15:11:39.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-bq7rw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:39.475: INFO: stderr: ""
Nov 17 15:11:39.475: INFO: stdout: "true"
Nov 17 15:11:39.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-bq7rw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 15:11:39.572: INFO: stderr: ""
Nov 17 15:11:39.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 15:11:39.572: INFO: validating pod update-demo-nautilus-bq7rw
Nov 17 15:11:39.580: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 15:11:39.580: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 15:11:39.580: INFO: update-demo-nautilus-bq7rw is verified up and running
Nov 17 15:11:39.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 17 15:11:39.672: INFO: stderr: ""
Nov 17 15:11:39.672: INFO: stdout: "true"
Nov 17 15:11:39.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 17 15:11:39.769: INFO: stderr: ""
Nov 17 15:11:39.769: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Nov 17 15:11:39.769: INFO: validating pod update-demo-nautilus-dvrxg
Nov 17 15:11:39.773: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 17 15:11:39.774: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 17 15:11:39.774: INFO: update-demo-nautilus-dvrxg is verified up and running
STEP: using delete to clean up resources 11/17/23 15:11:39.774
Nov 17 15:11:39.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 delete --grace-period=0 --force -f -'
Nov 17 15:11:39.872: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 17 15:11:39.872: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 17 15:11:39.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get rc,svc -l name=update-demo --no-headers'
Nov 17 15:11:39.980: INFO: stderr: "No resources found in kubectl-6315 namespace.\n"
Nov 17 15:11:39.981: INFO: stdout: ""
Nov 17 15:11:39.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 17 15:11:40.078: INFO: stderr: ""
Nov 17 15:11:40.078: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:40.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6315" for this suite. 11/17/23 15:11:40.097
------------------------------
â€¢ [SLOW TEST] [9.628 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:30.479
    Nov 17 15:11:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubectl 11/17/23 15:11:30.48
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:30.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:30.497
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 11/17/23 15:11:30.502
    Nov 17 15:11:30.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 create -f -'
    Nov 17 15:11:31.059: INFO: stderr: ""
    Nov 17 15:11:31.059: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:31.059
    Nov 17 15:11:31.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 15:11:31.156: INFO: stderr: ""
    Nov 17 15:11:31.156: INFO: stdout: "update-demo-nautilus-dvrxg update-demo-nautilus-k8ng5 "
    Nov 17 15:11:31.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:31.252: INFO: stderr: ""
    Nov 17 15:11:31.252: INFO: stdout: ""
    Nov 17 15:11:31.252: INFO: update-demo-nautilus-dvrxg is created but not running
    Nov 17 15:11:36.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 15:11:36.356: INFO: stderr: ""
    Nov 17 15:11:36.356: INFO: stdout: "update-demo-nautilus-dvrxg update-demo-nautilus-k8ng5 "
    Nov 17 15:11:36.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:36.454: INFO: stderr: ""
    Nov 17 15:11:36.454: INFO: stdout: "true"
    Nov 17 15:11:36.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 15:11:36.550: INFO: stderr: ""
    Nov 17 15:11:36.550: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 15:11:36.550: INFO: validating pod update-demo-nautilus-dvrxg
    Nov 17 15:11:36.557: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 15:11:36.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 15:11:36.558: INFO: update-demo-nautilus-dvrxg is verified up and running
    Nov 17 15:11:36.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-k8ng5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:36.649: INFO: stderr: ""
    Nov 17 15:11:36.649: INFO: stdout: "true"
    Nov 17 15:11:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-k8ng5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 15:11:36.743: INFO: stderr: ""
    Nov 17 15:11:36.743: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 15:11:36.743: INFO: validating pod update-demo-nautilus-k8ng5
    Nov 17 15:11:36.749: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 15:11:36.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 15:11:36.749: INFO: update-demo-nautilus-k8ng5 is verified up and running
    STEP: scaling down the replication controller 11/17/23 15:11:36.749
    Nov 17 15:11:36.752: INFO: scanned /root for discovery docs: <nil>
    Nov 17 15:11:36.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Nov 17 15:11:37.870: INFO: stderr: ""
    Nov 17 15:11:37.870: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:37.87
    Nov 17 15:11:37.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 15:11:37.957: INFO: stderr: ""
    Nov 17 15:11:37.957: INFO: stdout: "update-demo-nautilus-dvrxg "
    Nov 17 15:11:37.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:38.041: INFO: stderr: ""
    Nov 17 15:11:38.041: INFO: stdout: "true"
    Nov 17 15:11:38.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 15:11:38.129: INFO: stderr: ""
    Nov 17 15:11:38.130: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 15:11:38.130: INFO: validating pod update-demo-nautilus-dvrxg
    Nov 17 15:11:38.133: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 15:11:38.134: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 15:11:38.134: INFO: update-demo-nautilus-dvrxg is verified up and running
    STEP: scaling up the replication controller 11/17/23 15:11:38.134
    Nov 17 15:11:38.137: INFO: scanned /root for discovery docs: <nil>
    Nov 17 15:11:38.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Nov 17 15:11:39.268: INFO: stderr: ""
    Nov 17 15:11:39.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 11/17/23 15:11:39.268
    Nov 17 15:11:39.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Nov 17 15:11:39.375: INFO: stderr: ""
    Nov 17 15:11:39.375: INFO: stdout: "update-demo-nautilus-bq7rw update-demo-nautilus-dvrxg "
    Nov 17 15:11:39.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-bq7rw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:39.475: INFO: stderr: ""
    Nov 17 15:11:39.475: INFO: stdout: "true"
    Nov 17 15:11:39.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-bq7rw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 15:11:39.572: INFO: stderr: ""
    Nov 17 15:11:39.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 15:11:39.572: INFO: validating pod update-demo-nautilus-bq7rw
    Nov 17 15:11:39.580: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 15:11:39.580: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 15:11:39.580: INFO: update-demo-nautilus-bq7rw is verified up and running
    Nov 17 15:11:39.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Nov 17 15:11:39.672: INFO: stderr: ""
    Nov 17 15:11:39.672: INFO: stdout: "true"
    Nov 17 15:11:39.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods update-demo-nautilus-dvrxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Nov 17 15:11:39.769: INFO: stderr: ""
    Nov 17 15:11:39.769: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Nov 17 15:11:39.769: INFO: validating pod update-demo-nautilus-dvrxg
    Nov 17 15:11:39.773: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Nov 17 15:11:39.774: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Nov 17 15:11:39.774: INFO: update-demo-nautilus-dvrxg is verified up and running
    STEP: using delete to clean up resources 11/17/23 15:11:39.774
    Nov 17 15:11:39.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 delete --grace-period=0 --force -f -'
    Nov 17 15:11:39.872: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Nov 17 15:11:39.872: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Nov 17 15:11:39.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get rc,svc -l name=update-demo --no-headers'
    Nov 17 15:11:39.980: INFO: stderr: "No resources found in kubectl-6315 namespace.\n"
    Nov 17 15:11:39.981: INFO: stdout: ""
    Nov 17 15:11:39.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=kubectl-6315 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Nov 17 15:11:40.078: INFO: stderr: ""
    Nov 17 15:11:40.078: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:40.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6315" for this suite. 11/17/23 15:11:40.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:40.108
Nov 17 15:11:40.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename csiinlinevolumes 11/17/23 15:11:40.109
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:40.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:40.144
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 11/17/23 15:11:40.149
STEP: getting 11/17/23 15:11:40.17
STEP: listing 11/17/23 15:11:40.18
STEP: deleting 11/17/23 15:11:40.183
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:40.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9671" for this suite. 11/17/23 15:11:40.207
------------------------------
â€¢ [0.108 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:40.108
    Nov 17 15:11:40.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename csiinlinevolumes 11/17/23 15:11:40.109
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:40.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:40.144
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 11/17/23 15:11:40.149
    STEP: getting 11/17/23 15:11:40.17
    STEP: listing 11/17/23 15:11:40.18
    STEP: deleting 11/17/23 15:11:40.183
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:40.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9671" for this suite. 11/17/23 15:11:40.207
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:40.216
Nov 17 15:11:40.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:11:40.218
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:40.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:40.242
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-46f36b3e-60a5-4e46-b7cb-8dc09764f217 11/17/23 15:11:40.245
STEP: Creating a pod to test consume configMaps 11/17/23 15:11:40.252
Nov 17 15:11:40.262: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a" in namespace "projected-6585" to be "Succeeded or Failed"
Nov 17 15:11:40.266: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.61505ms
Nov 17 15:11:42.272: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009081716s
Nov 17 15:11:44.271: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008921674s
STEP: Saw pod success 11/17/23 15:11:44.272
Nov 17 15:11:44.272: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a" satisfied condition "Succeeded or Failed"
Nov 17 15:11:44.276: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a container agnhost-container: <nil>
STEP: delete the pod 11/17/23 15:11:44.284
Nov 17 15:11:44.298: INFO: Waiting for pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a to disappear
Nov 17 15:11:44.302: INFO: Pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:44.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6585" for this suite. 11/17/23 15:11:44.308
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:40.216
    Nov 17 15:11:40.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:11:40.218
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:40.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:40.242
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-46f36b3e-60a5-4e46-b7cb-8dc09764f217 11/17/23 15:11:40.245
    STEP: Creating a pod to test consume configMaps 11/17/23 15:11:40.252
    Nov 17 15:11:40.262: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a" in namespace "projected-6585" to be "Succeeded or Failed"
    Nov 17 15:11:40.266: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.61505ms
    Nov 17 15:11:42.272: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009081716s
    Nov 17 15:11:44.271: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008921674s
    STEP: Saw pod success 11/17/23 15:11:44.272
    Nov 17 15:11:44.272: INFO: Pod "pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a" satisfied condition "Succeeded or Failed"
    Nov 17 15:11:44.276: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 15:11:44.284
    Nov 17 15:11:44.298: INFO: Waiting for pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a to disappear
    Nov 17 15:11:44.302: INFO: Pod pod-projected-configmaps-46dea76e-834d-43c4-a9f3-3d9537ed295a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:44.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6585" for this suite. 11/17/23 15:11:44.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:44.316
Nov 17 15:11:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename webhook 11/17/23 15:11:44.318
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:44.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:44.345
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 11/17/23 15:11:44.365
STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 15:11:44.857
STEP: Deploying the webhook pod 11/17/23 15:11:44.865
STEP: Wait for the deployment to be ready 11/17/23 15:11:44.884
Nov 17 15:11:44.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 15:11:46.913
STEP: Verifying the service has paired with the endpoint 11/17/23 15:11:46.93
Nov 17 15:11:47.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 11/17/23 15:11:47.934
STEP: create a pod that should be denied by the webhook 11/17/23 15:11:47.957
STEP: create a pod that causes the webhook to hang 11/17/23 15:11:47.972
STEP: create a configmap that should be denied by the webhook 11/17/23 15:11:57.982
STEP: create a configmap that should be admitted by the webhook 11/17/23 15:11:57.997
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/17/23 15:11:58.01
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/17/23 15:11:58.02
STEP: create a namespace that bypass the webhook 11/17/23 15:11:58.029
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/17/23 15:11:58.038
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:11:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7292" for this suite. 11/17/23 15:11:58.188
STEP: Destroying namespace "webhook-7292-markers" for this suite. 11/17/23 15:11:58.206
------------------------------
â€¢ [SLOW TEST] [13.913 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:44.316
    Nov 17 15:11:44.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename webhook 11/17/23 15:11:44.318
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:44.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:44.345
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 11/17/23 15:11:44.365
    STEP: Create role binding to let webhook read extension-apiserver-authentication 11/17/23 15:11:44.857
    STEP: Deploying the webhook pod 11/17/23 15:11:44.865
    STEP: Wait for the deployment to be ready 11/17/23 15:11:44.884
    Nov 17 15:11:44.901: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 15:11:46.913
    STEP: Verifying the service has paired with the endpoint 11/17/23 15:11:46.93
    Nov 17 15:11:47.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 11/17/23 15:11:47.934
    STEP: create a pod that should be denied by the webhook 11/17/23 15:11:47.957
    STEP: create a pod that causes the webhook to hang 11/17/23 15:11:47.972
    STEP: create a configmap that should be denied by the webhook 11/17/23 15:11:57.982
    STEP: create a configmap that should be admitted by the webhook 11/17/23 15:11:57.997
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 11/17/23 15:11:58.01
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 11/17/23 15:11:58.02
    STEP: create a namespace that bypass the webhook 11/17/23 15:11:58.029
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 11/17/23 15:11:58.038
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:11:58.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7292" for this suite. 11/17/23 15:11:58.188
    STEP: Destroying namespace "webhook-7292-markers" for this suite. 11/17/23 15:11:58.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:11:58.236
Nov 17 15:11:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 15:11:58.238
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:58.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:58.314
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 11/17/23 15:11:58.337
STEP: watching for Pod to be ready 11/17/23 15:11:58.354
Nov 17 15:11:58.357: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 17 15:11:58.363: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
Nov 17 15:11:58.393: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
Nov 17 15:11:59.446: INFO: Found Pod pod-test in namespace pods-6800 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 11/17/23 15:11:59.451
STEP: getting the Pod and ensuring that it's patched 11/17/23 15:11:59.469
STEP: replacing the Pod's status Ready condition to False 11/17/23 15:11:59.482
STEP: check the Pod again to ensure its Ready conditions are False 11/17/23 15:11:59.5
STEP: deleting the Pod via a Collection with a LabelSelector 11/17/23 15:11:59.501
STEP: watching for the Pod to be deleted 11/17/23 15:11:59.51
Nov 17 15:11:59.513: INFO: observed event type MODIFIED
Nov 17 15:12:01.452: INFO: observed event type MODIFIED
Nov 17 15:12:02.460: INFO: observed event type MODIFIED
Nov 17 15:12:02.470: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 15:12:02.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6800" for this suite. 11/17/23 15:12:02.486
------------------------------
â€¢ [4.262 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:11:58.236
    Nov 17 15:11:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 15:11:58.238
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:11:58.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:11:58.314
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 11/17/23 15:11:58.337
    STEP: watching for Pod to be ready 11/17/23 15:11:58.354
    Nov 17 15:11:58.357: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Nov 17 15:11:58.363: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
    Nov 17 15:11:58.393: INFO: observed Pod pod-test in namespace pods-6800 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
    Nov 17 15:11:59.446: INFO: Found Pod pod-test in namespace pods-6800 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-11-17 15:11:58 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 11/17/23 15:11:59.451
    STEP: getting the Pod and ensuring that it's patched 11/17/23 15:11:59.469
    STEP: replacing the Pod's status Ready condition to False 11/17/23 15:11:59.482
    STEP: check the Pod again to ensure its Ready conditions are False 11/17/23 15:11:59.5
    STEP: deleting the Pod via a Collection with a LabelSelector 11/17/23 15:11:59.501
    STEP: watching for the Pod to be deleted 11/17/23 15:11:59.51
    Nov 17 15:11:59.513: INFO: observed event type MODIFIED
    Nov 17 15:12:01.452: INFO: observed event type MODIFIED
    Nov 17 15:12:02.460: INFO: observed event type MODIFIED
    Nov 17 15:12:02.470: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:12:02.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6800" for this suite. 11/17/23 15:12:02.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:12:02.498
Nov 17 15:12:02.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename job 11/17/23 15:12:02.5
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:12:02.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:12:02.539
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 11/17/23 15:12:02.544
STEP: Ensuring active pods == parallelism 11/17/23 15:12:02.555
STEP: delete a job 11/17/23 15:12:04.56
STEP: deleting Job.batch foo in namespace job-3746, will wait for the garbage collector to delete the pods 11/17/23 15:12:04.56
Nov 17 15:12:04.620: INFO: Deleting Job.batch foo took: 6.505494ms
Nov 17 15:12:04.721: INFO: Terminating Job.batch foo pods took: 101.202712ms
STEP: Ensuring job was deleted 11/17/23 15:12:37.722
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Nov 17 15:12:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3746" for this suite. 11/17/23 15:12:37.735
------------------------------
â€¢ [SLOW TEST] [35.248 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:12:02.498
    Nov 17 15:12:02.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename job 11/17/23 15:12:02.5
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:12:02.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:12:02.539
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 11/17/23 15:12:02.544
    STEP: Ensuring active pods == parallelism 11/17/23 15:12:02.555
    STEP: delete a job 11/17/23 15:12:04.56
    STEP: deleting Job.batch foo in namespace job-3746, will wait for the garbage collector to delete the pods 11/17/23 15:12:04.56
    Nov 17 15:12:04.620: INFO: Deleting Job.batch foo took: 6.505494ms
    Nov 17 15:12:04.721: INFO: Terminating Job.batch foo pods took: 101.202712ms
    STEP: Ensuring job was deleted 11/17/23 15:12:37.722
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:12:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3746" for this suite. 11/17/23 15:12:37.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:12:37.749
Nov 17 15:12:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename svcaccounts 11/17/23 15:12:37.75
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:12:37.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:12:37.78
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Nov 17 15:12:37.805: INFO: created pod
Nov 17 15:12:37.805: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1841" to be "Succeeded or Failed"
Nov 17 15:12:37.809: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.249432ms
Nov 17 15:12:39.813: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007976136s
Nov 17 15:12:41.812: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006955082s
STEP: Saw pod success 11/17/23 15:12:41.812
Nov 17 15:12:41.813: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 17 15:13:11.813: INFO: polling logs
Nov 17 15:13:11.819: INFO: Pod logs: 
I1117 15:12:38.968353       1 log.go:198] OK: Got token
I1117 15:12:38.968416       1 log.go:198] validating with in-cluster discovery
I1117 15:12:38.968818       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I1117 15:12:38.968850       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1841:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700234558, NotBefore:1700233958, IssuedAt:1700233958, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1841", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3b3734a0-e26c-4695-9f32-f947ee5e2786"}}}
I1117 15:12:38.998523       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1117 15:12:39.007871       1 log.go:198] OK: Validated signature on JWT
I1117 15:12:39.008003       1 log.go:198] OK: Got valid claims from token!
I1117 15:12:39.008042       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1841:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700234558, NotBefore:1700233958, IssuedAt:1700233958, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1841", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3b3734a0-e26c-4695-9f32-f947ee5e2786"}}}

Nov 17 15:13:11.820: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:11.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1841" for this suite. 11/17/23 15:13:11.83
------------------------------
â€¢ [SLOW TEST] [34.086 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:12:37.749
    Nov 17 15:12:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename svcaccounts 11/17/23 15:12:37.75
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:12:37.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:12:37.78
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Nov 17 15:12:37.805: INFO: created pod
    Nov 17 15:12:37.805: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1841" to be "Succeeded or Failed"
    Nov 17 15:12:37.809: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.249432ms
    Nov 17 15:12:39.813: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007976136s
    Nov 17 15:12:41.812: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006955082s
    STEP: Saw pod success 11/17/23 15:12:41.812
    Nov 17 15:12:41.813: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Nov 17 15:13:11.813: INFO: polling logs
    Nov 17 15:13:11.819: INFO: Pod logs: 
    I1117 15:12:38.968353       1 log.go:198] OK: Got token
    I1117 15:12:38.968416       1 log.go:198] validating with in-cluster discovery
    I1117 15:12:38.968818       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I1117 15:12:38.968850       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1841:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700234558, NotBefore:1700233958, IssuedAt:1700233958, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1841", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3b3734a0-e26c-4695-9f32-f947ee5e2786"}}}
    I1117 15:12:38.998523       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I1117 15:12:39.007871       1 log.go:198] OK: Validated signature on JWT
    I1117 15:12:39.008003       1 log.go:198] OK: Got valid claims from token!
    I1117 15:12:39.008042       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1841:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1700234558, NotBefore:1700233958, IssuedAt:1700233958, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1841", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3b3734a0-e26c-4695-9f32-f947ee5e2786"}}}

    Nov 17 15:13:11.820: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:11.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1841" for this suite. 11/17/23 15:13:11.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:11.836
Nov 17 15:13:11.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:13:11.837
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:11.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:11.859
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 11/17/23 15:13:11.863
Nov 17 15:13:11.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: mark a version not serverd 11/17/23 15:13:19.74
STEP: check the unserved version gets removed 11/17/23 15:13:19.763
STEP: check the other version is not changed 11/17/23 15:13:23.933
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:31.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2581" for this suite. 11/17/23 15:13:31.757
------------------------------
â€¢ [SLOW TEST] [19.926 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:11.836
    Nov 17 15:13:11.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-publish-openapi 11/17/23 15:13:11.837
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:11.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:11.859
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 11/17/23 15:13:11.863
    Nov 17 15:13:11.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: mark a version not serverd 11/17/23 15:13:19.74
    STEP: check the unserved version gets removed 11/17/23 15:13:19.763
    STEP: check the other version is not changed 11/17/23 15:13:23.933
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:31.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2581" for this suite. 11/17/23 15:13:31.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:31.763
Nov 17 15:13:31.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 15:13:31.765
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:31.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:31.784
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Nov 17 15:13:31.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:32.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-954" for this suite. 11/17/23 15:13:32.824
------------------------------
â€¢ [1.080 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:31.763
    Nov 17 15:13:31.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename custom-resource-definition 11/17/23 15:13:31.765
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:31.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:31.784
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Nov 17 15:13:31.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:32.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-954" for this suite. 11/17/23 15:13:32.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:32.845
Nov 17 15:13:32.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:13:32.846
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:32.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:32.961
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 11/17/23 15:13:32.972
Nov 17 15:13:32.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b" in namespace "projected-1653" to be "Succeeded or Failed"
Nov 17 15:13:33.014: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.041874ms
Nov 17 15:13:35.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027047972s
Nov 17 15:13:37.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027438612s
STEP: Saw pod success 11/17/23 15:13:37.02
Nov 17 15:13:37.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b" satisfied condition "Succeeded or Failed"
Nov 17 15:13:37.025: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b container client-container: <nil>
STEP: delete the pod 11/17/23 15:13:37.033
Nov 17 15:13:37.055: INFO: Waiting for pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b to disappear
Nov 17 15:13:37.060: INFO: Pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1653" for this suite. 11/17/23 15:13:37.066
------------------------------
â€¢ [4.230 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:32.845
    Nov 17 15:13:32.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:13:32.846
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:32.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:32.961
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 11/17/23 15:13:32.972
    Nov 17 15:13:32.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b" in namespace "projected-1653" to be "Succeeded or Failed"
    Nov 17 15:13:33.014: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.041874ms
    Nov 17 15:13:35.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027047972s
    Nov 17 15:13:37.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027438612s
    STEP: Saw pod success 11/17/23 15:13:37.02
    Nov 17 15:13:37.020: INFO: Pod "downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b" satisfied condition "Succeeded or Failed"
    Nov 17 15:13:37.025: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b container client-container: <nil>
    STEP: delete the pod 11/17/23 15:13:37.033
    Nov 17 15:13:37.055: INFO: Waiting for pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b to disappear
    Nov 17 15:13:37.060: INFO: Pod downwardapi-volume-307919bd-c143-40fb-afe2-d8ac27e92c0b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1653" for this suite. 11/17/23 15:13:37.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:37.075
Nov 17 15:13:37.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:13:37.077
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:37.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:37.099
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-62520c3e-a367-40e1-a6f6-0f028ff0069c 11/17/23 15:13:37.111
STEP: Creating configMap with name cm-test-opt-upd-469b6390-524a-4c2e-87f5-3529488cf2dd 11/17/23 15:13:37.118
STEP: Creating the pod 11/17/23 15:13:37.124
Nov 17 15:13:37.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89" in namespace "projected-359" to be "running and ready"
Nov 17 15:13:37.149: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89": Phase="Pending", Reason="", readiness=false. Elapsed: 10.228152ms
Nov 17 15:13:37.149: INFO: The phase of Pod pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:13:39.155: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89": Phase="Running", Reason="", readiness=true. Elapsed: 2.015955432s
Nov 17 15:13:39.155: INFO: The phase of Pod pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89 is Running (Ready = true)
Nov 17 15:13:39.155: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-62520c3e-a367-40e1-a6f6-0f028ff0069c 11/17/23 15:13:39.18
STEP: Updating configmap cm-test-opt-upd-469b6390-524a-4c2e-87f5-3529488cf2dd 11/17/23 15:13:39.189
STEP: Creating configMap with name cm-test-opt-create-89e53af8-250e-4f80-a9f7-5cedaa81b30d 11/17/23 15:13:39.196
STEP: waiting to observe update in volume 11/17/23 15:13:39.202
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-359" for this suite. 11/17/23 15:13:41.239
------------------------------
â€¢ [4.173 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:37.075
    Nov 17 15:13:37.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:13:37.077
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:37.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:37.099
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-62520c3e-a367-40e1-a6f6-0f028ff0069c 11/17/23 15:13:37.111
    STEP: Creating configMap with name cm-test-opt-upd-469b6390-524a-4c2e-87f5-3529488cf2dd 11/17/23 15:13:37.118
    STEP: Creating the pod 11/17/23 15:13:37.124
    Nov 17 15:13:37.139: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89" in namespace "projected-359" to be "running and ready"
    Nov 17 15:13:37.149: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89": Phase="Pending", Reason="", readiness=false. Elapsed: 10.228152ms
    Nov 17 15:13:37.149: INFO: The phase of Pod pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:13:39.155: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89": Phase="Running", Reason="", readiness=true. Elapsed: 2.015955432s
    Nov 17 15:13:39.155: INFO: The phase of Pod pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89 is Running (Ready = true)
    Nov 17 15:13:39.155: INFO: Pod "pod-projected-configmaps-718f17fe-5fe9-4c2b-ada7-d23d18d69b89" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-62520c3e-a367-40e1-a6f6-0f028ff0069c 11/17/23 15:13:39.18
    STEP: Updating configmap cm-test-opt-upd-469b6390-524a-4c2e-87f5-3529488cf2dd 11/17/23 15:13:39.189
    STEP: Creating configMap with name cm-test-opt-create-89e53af8-250e-4f80-a9f7-5cedaa81b30d 11/17/23 15:13:39.196
    STEP: waiting to observe update in volume 11/17/23 15:13:39.202
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:41.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-359" for this suite. 11/17/23 15:13:41.239
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:41.252
Nov 17 15:13:41.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubelet-test 11/17/23 15:13:41.253
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:41.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:41.276
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Nov 17 15:13:41.290: INFO: Waiting up to 5m0s for pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde" in namespace "kubelet-test-2349" to be "running and ready"
Nov 17 15:13:41.293: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393526ms
Nov 17 15:13:41.293: INFO: The phase of Pod busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:13:43.297: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde": Phase="Running", Reason="", readiness=true. Elapsed: 2.007314122s
Nov 17 15:13:43.297: INFO: The phase of Pod busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde is Running (Ready = true)
Nov 17 15:13:43.297: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2349" for this suite. 11/17/23 15:13:43.311
------------------------------
â€¢ [2.066 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:41.252
    Nov 17 15:13:41.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubelet-test 11/17/23 15:13:41.253
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:41.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:41.276
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Nov 17 15:13:41.290: INFO: Waiting up to 5m0s for pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde" in namespace "kubelet-test-2349" to be "running and ready"
    Nov 17 15:13:41.293: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393526ms
    Nov 17 15:13:41.293: INFO: The phase of Pod busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:13:43.297: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde": Phase="Running", Reason="", readiness=true. Elapsed: 2.007314122s
    Nov 17 15:13:43.297: INFO: The phase of Pod busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde is Running (Ready = true)
    Nov 17 15:13:43.297: INFO: Pod "busybox-scheduling-8130c746-4fe5-4a81-862b-eafe6c139cde" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2349" for this suite. 11/17/23 15:13:43.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:43.32
Nov 17 15:13:43.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 15:13:43.321
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:43.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:43.342
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 11/17/23 15:13:43.345
Nov 17 15:13:43.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883" in namespace "downward-api-4422" to be "Succeeded or Failed"
Nov 17 15:13:43.358: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Pending", Reason="", readiness=false. Elapsed: 3.032013ms
Nov 17 15:13:45.364: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008502337s
Nov 17 15:13:47.363: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008322365s
STEP: Saw pod success 11/17/23 15:13:47.363
Nov 17 15:13:47.364: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883" satisfied condition "Succeeded or Failed"
Nov 17 15:13:47.367: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 container client-container: <nil>
STEP: delete the pod 11/17/23 15:13:47.374
Nov 17 15:13:47.390: INFO: Waiting for pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 to disappear
Nov 17 15:13:47.393: INFO: Pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4422" for this suite. 11/17/23 15:13:47.398
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:43.32
    Nov 17 15:13:43.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 15:13:43.321
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:43.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:43.342
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 11/17/23 15:13:43.345
    Nov 17 15:13:43.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883" in namespace "downward-api-4422" to be "Succeeded or Failed"
    Nov 17 15:13:43.358: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Pending", Reason="", readiness=false. Elapsed: 3.032013ms
    Nov 17 15:13:45.364: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008502337s
    Nov 17 15:13:47.363: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008322365s
    STEP: Saw pod success 11/17/23 15:13:47.363
    Nov 17 15:13:47.364: INFO: Pod "downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883" satisfied condition "Succeeded or Failed"
    Nov 17 15:13:47.367: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 container client-container: <nil>
    STEP: delete the pod 11/17/23 15:13:47.374
    Nov 17 15:13:47.390: INFO: Waiting for pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 to disappear
    Nov 17 15:13:47.393: INFO: Pod downwardapi-volume-25aea9e2-3b84-4bf4-b246-059675162883 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4422" for this suite. 11/17/23 15:13:47.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:47.408
Nov 17 15:13:47.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 15:13:47.409
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:47.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:47.433
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 11/17/23 15:13:47.436
Nov 17 15:13:47.446: INFO: Waiting up to 5m0s for pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c" in namespace "downward-api-7643" to be "Succeeded or Failed"
Nov 17 15:13:47.452: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181583ms
Nov 17 15:13:49.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010915228s
Nov 17 15:13:51.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011063134s
STEP: Saw pod success 11/17/23 15:13:51.457
Nov 17 15:13:51.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c" satisfied condition "Succeeded or Failed"
Nov 17 15:13:51.460: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c container dapi-container: <nil>
STEP: delete the pod 11/17/23 15:13:51.467
Nov 17 15:13:51.478: INFO: Waiting for pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c to disappear
Nov 17 15:13:51.481: INFO: Pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:51.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7643" for this suite. 11/17/23 15:13:51.486
------------------------------
â€¢ [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:47.408
    Nov 17 15:13:47.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 15:13:47.409
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:47.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:47.433
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 11/17/23 15:13:47.436
    Nov 17 15:13:47.446: INFO: Waiting up to 5m0s for pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c" in namespace "downward-api-7643" to be "Succeeded or Failed"
    Nov 17 15:13:47.452: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.181583ms
    Nov 17 15:13:49.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010915228s
    Nov 17 15:13:51.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011063134s
    STEP: Saw pod success 11/17/23 15:13:51.457
    Nov 17 15:13:51.457: INFO: Pod "downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c" satisfied condition "Succeeded or Failed"
    Nov 17 15:13:51.460: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c container dapi-container: <nil>
    STEP: delete the pod 11/17/23 15:13:51.467
    Nov 17 15:13:51.478: INFO: Waiting for pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c to disappear
    Nov 17 15:13:51.481: INFO: Pod downward-api-0fe5d16f-cb9b-450e-adf8-ec025f901c7c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:51.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7643" for this suite. 11/17/23 15:13:51.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:51.496
Nov 17 15:13:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 15:13:51.497
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:51.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:51.516
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 11/17/23 15:13:51.52
Nov 17 15:13:51.530: INFO: Waiting up to 5m0s for pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278" in namespace "pods-5171" to be "running and ready"
Nov 17 15:13:51.535: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138478ms
Nov 17 15:13:51.535: INFO: The phase of Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:13:53.539: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278": Phase="Running", Reason="", readiness=true. Elapsed: 2.009519909s
Nov 17 15:13:53.539: INFO: The phase of Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 is Running (Ready = true)
Nov 17 15:13:53.539: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278" satisfied condition "running and ready"
Nov 17 15:13:53.545: INFO: Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 has hostIP: 172.16.0.4
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:53.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5171" for this suite. 11/17/23 15:13:53.549
------------------------------
â€¢ [2.059 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:51.496
    Nov 17 15:13:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 15:13:51.497
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:51.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:51.516
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 11/17/23 15:13:51.52
    Nov 17 15:13:51.530: INFO: Waiting up to 5m0s for pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278" in namespace "pods-5171" to be "running and ready"
    Nov 17 15:13:51.535: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138478ms
    Nov 17 15:13:51.535: INFO: The phase of Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:13:53.539: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278": Phase="Running", Reason="", readiness=true. Elapsed: 2.009519909s
    Nov 17 15:13:53.539: INFO: The phase of Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 is Running (Ready = true)
    Nov 17 15:13:53.539: INFO: Pod "pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278" satisfied condition "running and ready"
    Nov 17 15:13:53.545: INFO: Pod pod-hostip-c48f5b53-057b-43ff-a23c-767e74f5e278 has hostIP: 172.16.0.4
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:53.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5171" for this suite. 11/17/23 15:13:53.549
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:53.556
Nov 17 15:13:53.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename pods 11/17/23 15:13:53.557
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:53.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:53.576
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 11/17/23 15:13:53.579
STEP: submitting the pod to kubernetes 11/17/23 15:13:53.579
Nov 17 15:13:53.586: INFO: Waiting up to 5m0s for pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" in namespace "pods-67" to be "running and ready"
Nov 17 15:13:53.593: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794233ms
Nov 17 15:13:53.593: INFO: The phase of Pod pod-update-7a9c2156-42c2-43bf-878c-785b6310860f is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:13:55.599: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740238s
Nov 17 15:13:55.599: INFO: The phase of Pod pod-update-7a9c2156-42c2-43bf-878c-785b6310860f is Running (Ready = true)
Nov 17 15:13:55.599: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 11/17/23 15:13:55.603
STEP: updating the pod 11/17/23 15:13:55.606
Nov 17 15:13:56.121: INFO: Successfully updated pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f"
Nov 17 15:13:56.121: INFO: Waiting up to 5m0s for pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" in namespace "pods-67" to be "running"
Nov 17 15:13:56.127: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Running", Reason="", readiness=true. Elapsed: 5.775727ms
Nov 17 15:13:56.127: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 11/17/23 15:13:56.127
Nov 17 15:13:56.142: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Nov 17 15:13:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-67" for this suite. 11/17/23 15:13:56.155
------------------------------
â€¢ [2.615 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:53.556
    Nov 17 15:13:53.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename pods 11/17/23 15:13:53.557
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:53.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:53.576
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 11/17/23 15:13:53.579
    STEP: submitting the pod to kubernetes 11/17/23 15:13:53.579
    Nov 17 15:13:53.586: INFO: Waiting up to 5m0s for pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" in namespace "pods-67" to be "running and ready"
    Nov 17 15:13:53.593: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794233ms
    Nov 17 15:13:53.593: INFO: The phase of Pod pod-update-7a9c2156-42c2-43bf-878c-785b6310860f is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:13:55.599: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012740238s
    Nov 17 15:13:55.599: INFO: The phase of Pod pod-update-7a9c2156-42c2-43bf-878c-785b6310860f is Running (Ready = true)
    Nov 17 15:13:55.599: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 11/17/23 15:13:55.603
    STEP: updating the pod 11/17/23 15:13:55.606
    Nov 17 15:13:56.121: INFO: Successfully updated pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f"
    Nov 17 15:13:56.121: INFO: Waiting up to 5m0s for pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" in namespace "pods-67" to be "running"
    Nov 17 15:13:56.127: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f": Phase="Running", Reason="", readiness=true. Elapsed: 5.775727ms
    Nov 17 15:13:56.127: INFO: Pod "pod-update-7a9c2156-42c2-43bf-878c-785b6310860f" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 11/17/23 15:13:56.127
    Nov 17 15:13:56.142: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:13:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-67" for this suite. 11/17/23 15:13:56.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:13:56.172
Nov 17 15:13:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename var-expansion 11/17/23 15:13:56.174
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:56.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:56.203
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Nov 17 15:13:56.218: INFO: Waiting up to 2m0s for pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" in namespace "var-expansion-2629" to be "container 0 failed with reason CreateContainerConfigError"
Nov 17 15:13:56.222: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041232ms
Nov 17 15:13:58.227: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008849329s
Nov 17 15:13:58.227: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Nov 17 15:13:58.227: INFO: Deleting pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" in namespace "var-expansion-2629"
Nov 17 15:13:58.237: INFO: Wait up to 5m0s for pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:00.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2629" for this suite. 11/17/23 15:14:00.249
------------------------------
â€¢ [4.083 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:13:56.172
    Nov 17 15:13:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename var-expansion 11/17/23 15:13:56.174
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:13:56.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:13:56.203
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Nov 17 15:13:56.218: INFO: Waiting up to 2m0s for pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" in namespace "var-expansion-2629" to be "container 0 failed with reason CreateContainerConfigError"
    Nov 17 15:13:56.222: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041232ms
    Nov 17 15:13:58.227: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008849329s
    Nov 17 15:13:58.227: INFO: Pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Nov 17 15:13:58.227: INFO: Deleting pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" in namespace "var-expansion-2629"
    Nov 17 15:13:58.237: INFO: Wait up to 5m0s for pod "var-expansion-f9b64b12-2b9b-4e42-93a8-910ad8b2b658" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:00.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2629" for this suite. 11/17/23 15:14:00.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:00.256
Nov 17 15:14:00.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename disruption 11/17/23 15:14:00.257
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:00.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:00.327
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 11/17/23 15:14:00.335
STEP: Waiting for all pods to be running 11/17/23 15:14:02.381
Nov 17 15:14:02.389: INFO: running pods: 0 < 3
Nov 17 15:14:04.394: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:06.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8053" for this suite. 11/17/23 15:14:06.405
------------------------------
â€¢ [SLOW TEST] [6.156 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:00.256
    Nov 17 15:14:00.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename disruption 11/17/23 15:14:00.257
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:00.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:00.327
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 11/17/23 15:14:00.335
    STEP: Waiting for all pods to be running 11/17/23 15:14:02.381
    Nov 17 15:14:02.389: INFO: running pods: 0 < 3
    Nov 17 15:14:04.394: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:06.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8053" for this suite. 11/17/23 15:14:06.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:06.416
Nov 17 15:14:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename kubelet-test 11/17/23 15:14:06.417
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:06.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:06.444
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-208" for this suite. 11/17/23 15:14:10.471
------------------------------
â€¢ [4.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:06.416
    Nov 17 15:14:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename kubelet-test 11/17/23 15:14:06.417
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:06.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:06.444
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-208" for this suite. 11/17/23 15:14:10.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:10.48
Nov 17 15:14:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:14:10.482
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:10.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:10.506
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 11/17/23 15:14:10.51
STEP: Counting existing ResourceQuota 11/17/23 15:14:15.513
STEP: Creating a ResourceQuota 11/17/23 15:14:20.517
STEP: Ensuring resource quota status is calculated 11/17/23 15:14:20.525
STEP: Creating a Secret 11/17/23 15:14:22.53
STEP: Ensuring resource quota status captures secret creation 11/17/23 15:14:22.546
STEP: Deleting a secret 11/17/23 15:14:24.55
STEP: Ensuring resource quota status released usage 11/17/23 15:14:24.557
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3785" for this suite. 11/17/23 15:14:26.57
------------------------------
â€¢ [SLOW TEST] [16.099 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:10.48
    Nov 17 15:14:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:14:10.482
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:10.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:10.506
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 11/17/23 15:14:10.51
    STEP: Counting existing ResourceQuota 11/17/23 15:14:15.513
    STEP: Creating a ResourceQuota 11/17/23 15:14:20.517
    STEP: Ensuring resource quota status is calculated 11/17/23 15:14:20.525
    STEP: Creating a Secret 11/17/23 15:14:22.53
    STEP: Ensuring resource quota status captures secret creation 11/17/23 15:14:22.546
    STEP: Deleting a secret 11/17/23 15:14:24.55
    STEP: Ensuring resource quota status released usage 11/17/23 15:14:24.557
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3785" for this suite. 11/17/23 15:14:26.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:26.58
Nov 17 15:14:26.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:14:26.581
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:26.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:26.606
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 11/17/23 15:14:26.609
Nov 17 15:14:26.618: INFO: Waiting up to 5m0s for pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3" in namespace "emptydir-5177" to be "Succeeded or Failed"
Nov 17 15:14:26.624: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992187ms
Nov 17 15:14:28.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010157164s
Nov 17 15:14:30.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01082616s
STEP: Saw pod success 11/17/23 15:14:30.629
Nov 17 15:14:30.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3" satisfied condition "Succeeded or Failed"
Nov 17 15:14:30.633: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 container test-container: <nil>
STEP: delete the pod 11/17/23 15:14:30.639
Nov 17 15:14:30.651: INFO: Waiting for pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 to disappear
Nov 17 15:14:30.655: INFO: Pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5177" for this suite. 11/17/23 15:14:30.66
------------------------------
â€¢ [4.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:26.58
    Nov 17 15:14:26.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:14:26.581
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:26.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:26.606
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 11/17/23 15:14:26.609
    Nov 17 15:14:26.618: INFO: Waiting up to 5m0s for pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3" in namespace "emptydir-5177" to be "Succeeded or Failed"
    Nov 17 15:14:26.624: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.992187ms
    Nov 17 15:14:28.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010157164s
    Nov 17 15:14:30.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01082616s
    STEP: Saw pod success 11/17/23 15:14:30.629
    Nov 17 15:14:30.629: INFO: Pod "pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3" satisfied condition "Succeeded or Failed"
    Nov 17 15:14:30.633: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 container test-container: <nil>
    STEP: delete the pod 11/17/23 15:14:30.639
    Nov 17 15:14:30.651: INFO: Waiting for pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 to disappear
    Nov 17 15:14:30.655: INFO: Pod pod-1794b3d2-1c32-4b6e-a1d2-0b408163afb3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5177" for this suite. 11/17/23 15:14:30.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:30.669
Nov 17 15:14:30.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename watch 11/17/23 15:14:30.67
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:30.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:30.692
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 11/17/23 15:14:30.695
STEP: creating a new configmap 11/17/23 15:14:30.697
STEP: modifying the configmap once 11/17/23 15:14:30.702
STEP: changing the label value of the configmap 11/17/23 15:14:30.71
STEP: Expecting to observe a delete notification for the watched object 11/17/23 15:14:30.718
Nov 17 15:14:30.719: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66129 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:14:30.719: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66130 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:14:30.719: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66131 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 11/17/23 15:14:30.719
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/17/23 15:14:30.729
STEP: changing the label value of the configmap back 11/17/23 15:14:40.73
STEP: modifying the configmap a third time 11/17/23 15:14:40.74
STEP: deleting the configmap 11/17/23 15:14:40.747
STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/17/23 15:14:40.753
Nov 17 15:14:40.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66201 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:14:40.753: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66202 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:14:40.754: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66203 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-383" for this suite. 11/17/23 15:14:40.759
------------------------------
â€¢ [SLOW TEST] [10.096 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:30.669
    Nov 17 15:14:30.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename watch 11/17/23 15:14:30.67
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:30.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:30.692
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 11/17/23 15:14:30.695
    STEP: creating a new configmap 11/17/23 15:14:30.697
    STEP: modifying the configmap once 11/17/23 15:14:30.702
    STEP: changing the label value of the configmap 11/17/23 15:14:30.71
    STEP: Expecting to observe a delete notification for the watched object 11/17/23 15:14:30.718
    Nov 17 15:14:30.719: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66129 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:14:30.719: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66130 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:14:30.719: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66131 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 11/17/23 15:14:30.719
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 11/17/23 15:14:30.729
    STEP: changing the label value of the configmap back 11/17/23 15:14:40.73
    STEP: modifying the configmap a third time 11/17/23 15:14:40.74
    STEP: deleting the configmap 11/17/23 15:14:40.747
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 11/17/23 15:14:40.753
    Nov 17 15:14:40.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66201 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:14:40.753: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66202 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:14:40.754: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-383  24efb070-1042-43f1-a2ed-0e90f709e164 66203 0 2023-11-17 15:14:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-11-17 15:14:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-383" for this suite. 11/17/23 15:14:40.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:40.767
Nov 17 15:14:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:14:40.769
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:40.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:40.794
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-66bf3b34-9fac-4335-835a-ee35bf379921 11/17/23 15:14:40.798
STEP: Creating a pod to test consume configMaps 11/17/23 15:14:40.805
Nov 17 15:14:40.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134" in namespace "projected-603" to be "Succeeded or Failed"
Nov 17 15:14:40.821: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419415ms
Nov 17 15:14:42.825: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008391948s
Nov 17 15:14:44.826: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209046s
STEP: Saw pod success 11/17/23 15:14:44.826
Nov 17 15:14:44.826: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134" satisfied condition "Succeeded or Failed"
Nov 17 15:14:44.830: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 container agnhost-container: <nil>
STEP: delete the pod 11/17/23 15:14:44.836
Nov 17 15:14:44.849: INFO: Waiting for pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 to disappear
Nov 17 15:14:44.852: INFO: Pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:44.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-603" for this suite. 11/17/23 15:14:44.859
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:40.767
    Nov 17 15:14:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:14:40.769
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:40.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:40.794
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-66bf3b34-9fac-4335-835a-ee35bf379921 11/17/23 15:14:40.798
    STEP: Creating a pod to test consume configMaps 11/17/23 15:14:40.805
    Nov 17 15:14:40.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134" in namespace "projected-603" to be "Succeeded or Failed"
    Nov 17 15:14:40.821: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.419415ms
    Nov 17 15:14:42.825: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008391948s
    Nov 17 15:14:44.826: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209046s
    STEP: Saw pod success 11/17/23 15:14:44.826
    Nov 17 15:14:44.826: INFO: Pod "pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134" satisfied condition "Succeeded or Failed"
    Nov 17 15:14:44.830: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 15:14:44.836
    Nov 17 15:14:44.849: INFO: Waiting for pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 to disappear
    Nov 17 15:14:44.852: INFO: Pod pod-projected-configmaps-3f9f1471-2f54-45f4-a864-b52a0adf5134 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:44.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-603" for this suite. 11/17/23 15:14:44.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:44.87
Nov 17 15:14:44.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:14:44.872
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:44.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:44.893
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 11/17/23 15:14:44.897
Nov 17 15:14:44.908: INFO: Waiting up to 5m0s for pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302" in namespace "projected-4763" to be "running and ready"
Nov 17 15:14:44.913: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63684ms
Nov 17 15:14:44.913: INFO: The phase of Pod labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302 is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:14:46.918: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302": Phase="Running", Reason="", readiness=true. Elapsed: 2.009987751s
Nov 17 15:14:46.919: INFO: The phase of Pod labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302 is Running (Ready = true)
Nov 17 15:14:46.919: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302" satisfied condition "running and ready"
Nov 17 15:14:47.444: INFO: Successfully updated pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Nov 17 15:14:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4763" for this suite. 11/17/23 15:14:51.482
------------------------------
â€¢ [SLOW TEST] [6.617 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:44.87
    Nov 17 15:14:44.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:14:44.872
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:44.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:44.893
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 11/17/23 15:14:44.897
    Nov 17 15:14:44.908: INFO: Waiting up to 5m0s for pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302" in namespace "projected-4763" to be "running and ready"
    Nov 17 15:14:44.913: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63684ms
    Nov 17 15:14:44.913: INFO: The phase of Pod labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302 is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:14:46.918: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302": Phase="Running", Reason="", readiness=true. Elapsed: 2.009987751s
    Nov 17 15:14:46.919: INFO: The phase of Pod labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302 is Running (Ready = true)
    Nov 17 15:14:46.919: INFO: Pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302" satisfied condition "running and ready"
    Nov 17 15:14:47.444: INFO: Successfully updated pod "labelsupdateb11aabe4-47dd-49ff-b734-3d730af51302"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:14:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4763" for this suite. 11/17/23 15:14:51.482
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:14:51.488
Nov 17 15:14:51.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:14:51.489
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:51.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:51.521
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 11/17/23 15:14:51.525
STEP: Creating a ResourceQuota 11/17/23 15:14:56.532
STEP: Ensuring resource quota status is calculated 11/17/23 15:14:56.537
STEP: Creating a Pod that fits quota 11/17/23 15:14:58.541
STEP: Ensuring ResourceQuota status captures the pod usage 11/17/23 15:14:58.559
STEP: Not allowing a pod to be created that exceeds remaining quota 11/17/23 15:15:00.564
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/17/23 15:15:00.567
STEP: Ensuring a pod cannot update its resource requirements 11/17/23 15:15:00.569
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/17/23 15:15:00.573
STEP: Deleting the pod 11/17/23 15:15:02.577
STEP: Ensuring resource quota status released the pod usage 11/17/23 15:15:02.594
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:04.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5763" for this suite. 11/17/23 15:15:05.039
------------------------------
â€¢ [SLOW TEST] [13.838 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:14:51.488
    Nov 17 15:14:51.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:14:51.489
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:14:51.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:14:51.521
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 11/17/23 15:14:51.525
    STEP: Creating a ResourceQuota 11/17/23 15:14:56.532
    STEP: Ensuring resource quota status is calculated 11/17/23 15:14:56.537
    STEP: Creating a Pod that fits quota 11/17/23 15:14:58.541
    STEP: Ensuring ResourceQuota status captures the pod usage 11/17/23 15:14:58.559
    STEP: Not allowing a pod to be created that exceeds remaining quota 11/17/23 15:15:00.564
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 11/17/23 15:15:00.567
    STEP: Ensuring a pod cannot update its resource requirements 11/17/23 15:15:00.569
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 11/17/23 15:15:00.573
    STEP: Deleting the pod 11/17/23 15:15:02.577
    STEP: Ensuring resource quota status released the pod usage 11/17/23 15:15:02.594
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:04.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5763" for this suite. 11/17/23 15:15:05.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:05.327
Nov 17 15:15:05.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename deployment 11/17/23 15:15:05.328
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:05.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:05.582
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Nov 17 15:15:05.605: INFO: Creating deployment "test-recreate-deployment"
Nov 17 15:15:05.644: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 17 15:15:05.705: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 17 15:15:07.711: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 17 15:15:07.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 17 15:15:09.718: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 17 15:15:09.729: INFO: Updating deployment test-recreate-deployment
Nov 17 15:15:09.729: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Nov 17 15:15:09.866: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4117  6b18336d-af18-4e76-9160-1cab2f968bea 66494 2 2023-11-17 15:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007312e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-17 15:15:09 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-17 15:15:09 +0000 UTC,LastTransitionTime:2023-11-17 15:15:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 17 15:15:09.870: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4117  e7d320b6-8630-4dcc-8d69-1f82bcdbaa20 66492 1 2023-11-17 15:15:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6b18336d-af18-4e76-9160-1cab2f968bea 0xc000731a20 0xc000731a21}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b18336d-af18-4e76-9160-1cab2f968bea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000731ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 15:15:09.870: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 17 15:15:09.870: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4117  df5509ad-00d1-437a-aadd-e480d8c8e9bd 66481 2 2023-11-17 15:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6b18336d-af18-4e76-9160-1cab2f968bea 0xc0007318d7 0xc0007318d8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b18336d-af18-4e76-9160-1cab2f968bea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000731988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 17 15:15:09.874: INFO: Pod "test-recreate-deployment-cff6dc657-vq28m" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-vq28m test-recreate-deployment-cff6dc657- deployment-4117  f090054f-c42f-4b9e-ab71-156e56dc8fd3 66493 0 2023-11-17 15:15:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e7d320b6-8630-4dcc-8d69-1f82bcdbaa20 0xc00b098130 0xc00b098131}] [] [{kube-controller-manager Update v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d320b6-8630-4dcc-8d69-1f82bcdbaa20\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9jxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9jxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:15:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:09.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4117" for this suite. 11/17/23 15:15:09.878
------------------------------
â€¢ [4.560 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:05.327
    Nov 17 15:15:05.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename deployment 11/17/23 15:15:05.328
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:05.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:05.582
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Nov 17 15:15:05.605: INFO: Creating deployment "test-recreate-deployment"
    Nov 17 15:15:05.644: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Nov 17 15:15:05.705: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Nov 17 15:15:07.711: INFO: Waiting deployment "test-recreate-deployment" to complete
    Nov 17 15:15:07.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.November, 17, 15, 15, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Nov 17 15:15:09.718: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Nov 17 15:15:09.729: INFO: Updating deployment test-recreate-deployment
    Nov 17 15:15:09.729: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Nov 17 15:15:09.866: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4117  6b18336d-af18-4e76-9160-1cab2f968bea 66494 2 2023-11-17 15:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007312e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-11-17 15:15:09 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-11-17 15:15:09 +0000 UTC,LastTransitionTime:2023-11-17 15:15:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Nov 17 15:15:09.870: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4117  e7d320b6-8630-4dcc-8d69-1f82bcdbaa20 66492 1 2023-11-17 15:15:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6b18336d-af18-4e76-9160-1cab2f968bea 0xc000731a20 0xc000731a21}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b18336d-af18-4e76-9160-1cab2f968bea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000731ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 15:15:09.870: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Nov 17 15:15:09.870: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4117  df5509ad-00d1-437a-aadd-e480d8c8e9bd 66481 2 2023-11-17 15:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6b18336d-af18-4e76-9160-1cab2f968bea 0xc0007318d7 0xc0007318d8}] [] [{kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b18336d-af18-4e76-9160-1cab2f968bea\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000731988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Nov 17 15:15:09.874: INFO: Pod "test-recreate-deployment-cff6dc657-vq28m" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-vq28m test-recreate-deployment-cff6dc657- deployment-4117  f090054f-c42f-4b9e-ab71-156e56dc8fd3 66493 0 2023-11-17 15:15:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 e7d320b6-8630-4dcc-8d69-1f82bcdbaa20 0xc00b098130 0xc00b098131}] [] [{kube-controller-manager Update v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d320b6-8630-4dcc-8d69-1f82bcdbaa20\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-11-17 15:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9jxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9jxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2.c.operations-lab.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-11-17 15:15:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.4,PodIP:,StartTime:2023-11-17 15:15:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:09.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4117" for this suite. 11/17/23 15:15:09.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:09.89
Nov 17 15:15:09.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context-test 11/17/23 15:15:09.891
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:09.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:09.911
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Nov 17 15:15:09.924: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b" in namespace "security-context-test-755" to be "Succeeded or Failed"
Nov 17 15:15:09.928: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.540152ms
Nov 17 15:15:11.934: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009683568s
Nov 17 15:15:13.933: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209367s
Nov 17 15:15:13.933: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b" satisfied condition "Succeeded or Failed"
Nov 17 15:15:13.940: INFO: Got logs for pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-755" for this suite. 11/17/23 15:15:13.944
------------------------------
â€¢ [4.060 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:09.89
    Nov 17 15:15:09.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context-test 11/17/23 15:15:09.891
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:09.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:09.911
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Nov 17 15:15:09.924: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b" in namespace "security-context-test-755" to be "Succeeded or Failed"
    Nov 17 15:15:09.928: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.540152ms
    Nov 17 15:15:11.934: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009683568s
    Nov 17 15:15:13.933: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009209367s
    Nov 17 15:15:13.933: INFO: Pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b" satisfied condition "Succeeded or Failed"
    Nov 17 15:15:13.940: INFO: Got logs for pod "busybox-privileged-false-6f6e1fa9-369c-40cd-9df3-28ad690bd42b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-755" for this suite. 11/17/23 15:15:13.944
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:13.951
Nov 17 15:15:13.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename statefulset 11/17/23 15:15:13.952
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:13.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:13.97
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5755 11/17/23 15:15:13.974
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5755 11/17/23 15:15:13.986
Nov 17 15:15:14.001: INFO: Found 0 stateful pods, waiting for 1
Nov 17 15:15:24.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 11/17/23 15:15:24.012
STEP: Getting /status 11/17/23 15:15:24.021
Nov 17 15:15:24.026: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 11/17/23 15:15:24.026
Nov 17 15:15:24.034: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 11/17/23 15:15:24.034
Nov 17 15:15:24.037: INFO: Observed &StatefulSet event: ADDED
Nov 17 15:15:24.037: INFO: Found Statefulset ss in namespace statefulset-5755 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 17 15:15:24.037: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 11/17/23 15:15:24.037
Nov 17 15:15:24.037: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 17 15:15:24.046: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 11/17/23 15:15:24.046
Nov 17 15:15:24.049: INFO: Observed &StatefulSet event: ADDED
Nov 17 15:15:24.049: INFO: Observed Statefulset ss in namespace statefulset-5755 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 17 15:15:24.049: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Nov 17 15:15:24.049: INFO: Deleting all statefulset in ns statefulset-5755
Nov 17 15:15:24.052: INFO: Scaling statefulset ss to 0
Nov 17 15:15:34.071: INFO: Waiting for statefulset status.replicas updated to 0
Nov 17 15:15:34.075: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5755" for this suite. 11/17/23 15:15:34.102
------------------------------
â€¢ [SLOW TEST] [20.161 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:13.951
    Nov 17 15:15:13.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename statefulset 11/17/23 15:15:13.952
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:13.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:13.97
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5755 11/17/23 15:15:13.974
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5755 11/17/23 15:15:13.986
    Nov 17 15:15:14.001: INFO: Found 0 stateful pods, waiting for 1
    Nov 17 15:15:24.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 11/17/23 15:15:24.012
    STEP: Getting /status 11/17/23 15:15:24.021
    Nov 17 15:15:24.026: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 11/17/23 15:15:24.026
    Nov 17 15:15:24.034: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 11/17/23 15:15:24.034
    Nov 17 15:15:24.037: INFO: Observed &StatefulSet event: ADDED
    Nov 17 15:15:24.037: INFO: Found Statefulset ss in namespace statefulset-5755 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 17 15:15:24.037: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 11/17/23 15:15:24.037
    Nov 17 15:15:24.037: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 17 15:15:24.046: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 11/17/23 15:15:24.046
    Nov 17 15:15:24.049: INFO: Observed &StatefulSet event: ADDED
    Nov 17 15:15:24.049: INFO: Observed Statefulset ss in namespace statefulset-5755 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 17 15:15:24.049: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Nov 17 15:15:24.049: INFO: Deleting all statefulset in ns statefulset-5755
    Nov 17 15:15:24.052: INFO: Scaling statefulset ss to 0
    Nov 17 15:15:34.071: INFO: Waiting for statefulset status.replicas updated to 0
    Nov 17 15:15:34.075: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5755" for this suite. 11/17/23 15:15:34.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:34.114
Nov 17 15:15:34.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:15:34.116
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:34.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:34.139
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3849b9af-727f-4642-9c02-84cb8d780041 11/17/23 15:15:34.148
STEP: Creating the pod 11/17/23 15:15:34.153
Nov 17 15:15:34.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab" in namespace "projected-3524" to be "running and ready"
Nov 17 15:15:34.172: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.339856ms
Nov 17 15:15:34.172: INFO: The phase of Pod pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab is Pending, waiting for it to be Running (with Ready = true)
Nov 17 15:15:36.176: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.010437285s
Nov 17 15:15:36.176: INFO: The phase of Pod pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab is Running (Ready = true)
Nov 17 15:15:36.176: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-3849b9af-727f-4642-9c02-84cb8d780041 11/17/23 15:15:36.183
STEP: waiting to observe update in volume 11/17/23 15:15:36.189
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3524" for this suite. 11/17/23 15:15:38.206
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:34.114
    Nov 17 15:15:34.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:15:34.116
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:34.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:34.139
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-3849b9af-727f-4642-9c02-84cb8d780041 11/17/23 15:15:34.148
    STEP: Creating the pod 11/17/23 15:15:34.153
    Nov 17 15:15:34.165: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab" in namespace "projected-3524" to be "running and ready"
    Nov 17 15:15:34.172: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.339856ms
    Nov 17 15:15:34.172: INFO: The phase of Pod pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab is Pending, waiting for it to be Running (with Ready = true)
    Nov 17 15:15:36.176: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.010437285s
    Nov 17 15:15:36.176: INFO: The phase of Pod pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab is Running (Ready = true)
    Nov 17 15:15:36.176: INFO: Pod "pod-projected-configmaps-5bc4e35e-7e10-440f-b53b-036c0fa558ab" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-3849b9af-727f-4642-9c02-84cb8d780041 11/17/23 15:15:36.183
    STEP: waiting to observe update in volume 11/17/23 15:15:36.189
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3524" for this suite. 11/17/23 15:15:38.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:38.215
Nov 17 15:15:38.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename watch 11/17/23 15:15:38.216
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:38.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:38.234
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 11/17/23 15:15:38.237
STEP: creating a new configmap 11/17/23 15:15:38.239
STEP: modifying the configmap once 11/17/23 15:15:38.243
STEP: closing the watch once it receives two notifications 11/17/23 15:15:38.252
Nov 17 15:15:38.252: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66775 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:15:38.252: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66776 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 11/17/23 15:15:38.252
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/17/23 15:15:38.262
STEP: deleting the configmap 11/17/23 15:15:38.264
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/17/23 15:15:38.269
Nov 17 15:15:38.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66777 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 17 15:15:38.269: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66778 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7824" for this suite. 11/17/23 15:15:38.273
------------------------------
â€¢ [0.064 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:38.215
    Nov 17 15:15:38.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename watch 11/17/23 15:15:38.216
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:38.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:38.234
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 11/17/23 15:15:38.237
    STEP: creating a new configmap 11/17/23 15:15:38.239
    STEP: modifying the configmap once 11/17/23 15:15:38.243
    STEP: closing the watch once it receives two notifications 11/17/23 15:15:38.252
    Nov 17 15:15:38.252: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66775 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:15:38.252: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66776 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 11/17/23 15:15:38.252
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 11/17/23 15:15:38.262
    STEP: deleting the configmap 11/17/23 15:15:38.264
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 11/17/23 15:15:38.269
    Nov 17 15:15:38.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66777 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Nov 17 15:15:38.269: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7824  c3760714-75cb-4a73-9131-597d537561e3 66778 0 2023-11-17 15:15:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-11-17 15:15:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7824" for this suite. 11/17/23 15:15:38.273
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:38.28
Nov 17 15:15:38.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 15:15:38.282
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:38.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:38.299
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6345 11/17/23 15:15:38.302
STEP: creating service affinity-nodeport-transition in namespace services-6345 11/17/23 15:15:38.302
STEP: creating replication controller affinity-nodeport-transition in namespace services-6345 11/17/23 15:15:38.319
I1117 15:15:38.331298      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6345, replica count: 3
I1117 15:15:41.382597      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 15:15:41.395: INFO: Creating new exec pod
Nov 17 15:15:41.403: INFO: Waiting up to 5m0s for pod "execpod-affinity2dp6h" in namespace "services-6345" to be "running"
Nov 17 15:15:41.408: INFO: Pod "execpod-affinity2dp6h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.419338ms
Nov 17 15:15:43.414: INFO: Pod "execpod-affinity2dp6h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010724781s
Nov 17 15:15:43.414: INFO: Pod "execpod-affinity2dp6h" satisfied condition "running"
Nov 17 15:15:44.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Nov 17 15:15:44.643: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 17 15:15:44.643: INFO: stdout: ""
Nov 17 15:15:44.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 10.108.6.119 80'
Nov 17 15:15:44.839: INFO: stderr: "+ nc -v -z -w 2 10.108.6.119 80\nConnection to 10.108.6.119 80 port [tcp/http] succeeded!\n"
Nov 17 15:15:44.839: INFO: stdout: ""
Nov 17 15:15:44.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 32484'
Nov 17 15:15:45.043: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 32484\nConnection to 172.16.0.5 32484 port [tcp/*] succeeded!\n"
Nov 17 15:15:45.043: INFO: stdout: ""
Nov 17 15:15:45.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32484'
Nov 17 15:15:45.256: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32484\nConnection to 172.16.0.4 32484 port [tcp/*] succeeded!\n"
Nov 17 15:15:45.256: INFO: stdout: ""
Nov 17 15:15:45.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32484/ ; done'
Nov 17 15:15:45.576: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n"
Nov 17 15:15:45.576: INFO: stdout: "\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98"
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
Nov 17 15:15:45.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32484/ ; done'
Nov 17 15:15:45.891: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n"
Nov 17 15:15:45.891: INFO: stdout: "\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv"
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
Nov 17 15:15:45.891: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6345, will wait for the garbage collector to delete the pods 11/17/23 15:15:45.905
Nov 17 15:15:45.967: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.022459ms
Nov 17 15:15:46.067: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.577759ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:48.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6345" for this suite. 11/17/23 15:15:48.402
------------------------------
â€¢ [SLOW TEST] [10.127 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:38.28
    Nov 17 15:15:38.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 15:15:38.282
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:38.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:38.299
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6345 11/17/23 15:15:38.302
    STEP: creating service affinity-nodeport-transition in namespace services-6345 11/17/23 15:15:38.302
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6345 11/17/23 15:15:38.319
    I1117 15:15:38.331298      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6345, replica count: 3
    I1117 15:15:41.382597      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 15:15:41.395: INFO: Creating new exec pod
    Nov 17 15:15:41.403: INFO: Waiting up to 5m0s for pod "execpod-affinity2dp6h" in namespace "services-6345" to be "running"
    Nov 17 15:15:41.408: INFO: Pod "execpod-affinity2dp6h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.419338ms
    Nov 17 15:15:43.414: INFO: Pod "execpod-affinity2dp6h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010724781s
    Nov 17 15:15:43.414: INFO: Pod "execpod-affinity2dp6h" satisfied condition "running"
    Nov 17 15:15:44.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Nov 17 15:15:44.643: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Nov 17 15:15:44.643: INFO: stdout: ""
    Nov 17 15:15:44.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 10.108.6.119 80'
    Nov 17 15:15:44.839: INFO: stderr: "+ nc -v -z -w 2 10.108.6.119 80\nConnection to 10.108.6.119 80 port [tcp/http] succeeded!\n"
    Nov 17 15:15:44.839: INFO: stdout: ""
    Nov 17 15:15:44.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 172.16.0.5 32484'
    Nov 17 15:15:45.043: INFO: stderr: "+ nc -v -z -w 2 172.16.0.5 32484\nConnection to 172.16.0.5 32484 port [tcp/*] succeeded!\n"
    Nov 17 15:15:45.043: INFO: stdout: ""
    Nov 17 15:15:45.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c nc -v -z -w 2 172.16.0.4 32484'
    Nov 17 15:15:45.256: INFO: stderr: "+ nc -v -z -w 2 172.16.0.4 32484\nConnection to 172.16.0.4 32484 port [tcp/*] succeeded!\n"
    Nov 17 15:15:45.256: INFO: stdout: ""
    Nov 17 15:15:45.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32484/ ; done'
    Nov 17 15:15:45.576: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n"
    Nov 17 15:15:45.576: INFO: stdout: "\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98\naffinity-nodeport-transition-2pxb7\naffinity-nodeport-transition-hhx98"
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-2pxb7
    Nov 17 15:15:45.576: INFO: Received response from host: affinity-nodeport-transition-hhx98
    Nov 17 15:15:45.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-6345 exec execpod-affinity2dp6h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.5:32484/ ; done'
    Nov 17 15:15:45.891: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.5:32484/\n"
    Nov 17 15:15:45.891: INFO: stdout: "\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv\naffinity-nodeport-transition-kbsvv"
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Received response from host: affinity-nodeport-transition-kbsvv
    Nov 17 15:15:45.891: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6345, will wait for the garbage collector to delete the pods 11/17/23 15:15:45.905
    Nov 17 15:15:45.967: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.022459ms
    Nov 17 15:15:46.067: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.577759ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:48.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6345" for this suite. 11/17/23 15:15:48.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:48.41
Nov 17 15:15:48.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename lease-test 11/17/23 15:15:48.411
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:48.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:48.434
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:48.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-5450" for this suite. 11/17/23 15:15:48.513
------------------------------
â€¢ [0.116 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:48.41
    Nov 17 15:15:48.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename lease-test 11/17/23 15:15:48.411
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:48.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:48.434
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:48.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-5450" for this suite. 11/17/23 15:15:48.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:48.526
Nov 17 15:15:48.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:15:48.528
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:48.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:48.566
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 11/17/23 15:15:48.569
Nov 17 15:15:48.580: INFO: Waiting up to 5m0s for pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd" in namespace "emptydir-6035" to be "Succeeded or Failed"
Nov 17 15:15:48.583: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265548ms
Nov 17 15:15:50.587: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007701447s
Nov 17 15:15:52.588: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008084237s
STEP: Saw pod success 11/17/23 15:15:52.588
Nov 17 15:15:52.588: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd" satisfied condition "Succeeded or Failed"
Nov 17 15:15:52.592: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd container test-container: <nil>
STEP: delete the pod 11/17/23 15:15:52.599
Nov 17 15:15:52.616: INFO: Waiting for pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd to disappear
Nov 17 15:15:52.620: INFO: Pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6035" for this suite. 11/17/23 15:15:52.625
------------------------------
â€¢ [4.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:48.526
    Nov 17 15:15:48.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:15:48.528
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:48.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:48.566
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 11/17/23 15:15:48.569
    Nov 17 15:15:48.580: INFO: Waiting up to 5m0s for pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd" in namespace "emptydir-6035" to be "Succeeded or Failed"
    Nov 17 15:15:48.583: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265548ms
    Nov 17 15:15:50.587: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007701447s
    Nov 17 15:15:52.588: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008084237s
    STEP: Saw pod success 11/17/23 15:15:52.588
    Nov 17 15:15:52.588: INFO: Pod "pod-edbc4257-4ba5-474a-baf8-200ab03572cd" satisfied condition "Succeeded or Failed"
    Nov 17 15:15:52.592: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd container test-container: <nil>
    STEP: delete the pod 11/17/23 15:15:52.599
    Nov 17 15:15:52.616: INFO: Waiting for pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd to disappear
    Nov 17 15:15:52.620: INFO: Pod pod-edbc4257-4ba5-474a-baf8-200ab03572cd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6035" for this suite. 11/17/23 15:15:52.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:52.634
Nov 17 15:15:52.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 15:15:52.635
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:52.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:52.659
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 11/17/23 15:15:52.686
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 15:15:52.694
Nov 17 15:15:52.699: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:15:52.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:15:52.702: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:15:53.708: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:15:53.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:15:53.711: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:15:54.707: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:15:54.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 15:15:54.710: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/17/23 15:15:54.712
Nov 17 15:15:54.741: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:15:54.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 15:15:54.748: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 11/17/23 15:15:54.748
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:15:55.768
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3475, will wait for the garbage collector to delete the pods 11/17/23 15:15:55.768
Nov 17 15:15:55.832: INFO: Deleting DaemonSet.extensions daemon-set took: 8.037784ms
Nov 17 15:15:55.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.039875ms
Nov 17 15:15:57.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:15:57.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 15:15:57.339: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"67148"},"items":null}

Nov 17 15:15:57.343: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"67148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:15:57.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3475" for this suite. 11/17/23 15:15:57.359
------------------------------
â€¢ [4.732 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:52.634
    Nov 17 15:15:52.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 15:15:52.635
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:52.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:52.659
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 11/17/23 15:15:52.686
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 15:15:52.694
    Nov 17 15:15:52.699: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:15:52.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:15:52.702: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:15:53.708: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:15:53.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:15:53.711: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:15:54.707: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:15:54.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 15:15:54.710: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 11/17/23 15:15:54.712
    Nov 17 15:15:54.741: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:15:54.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 15:15:54.748: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 11/17/23 15:15:54.748
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:15:55.768
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3475, will wait for the garbage collector to delete the pods 11/17/23 15:15:55.768
    Nov 17 15:15:55.832: INFO: Deleting DaemonSet.extensions daemon-set took: 8.037784ms
    Nov 17 15:15:55.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.039875ms
    Nov 17 15:15:57.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:15:57.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 15:15:57.339: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"67148"},"items":null}

    Nov 17 15:15:57.343: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"67148"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:15:57.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3475" for this suite. 11/17/23 15:15:57.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:15:57.367
Nov 17 15:15:57.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename emptydir 11/17/23 15:15:57.368
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:57.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:57.394
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 11/17/23 15:15:57.397
Nov 17 15:15:57.411: INFO: Waiting up to 5m0s for pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae" in namespace "emptydir-2666" to be "Succeeded or Failed"
Nov 17 15:15:57.414: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250649ms
Nov 17 15:15:59.419: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.007805545s
Nov 17 15:16:01.420: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175891s
STEP: Saw pod success 11/17/23 15:16:01.42
Nov 17 15:16:01.420: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae" satisfied condition "Succeeded or Failed"
Nov 17 15:16:01.424: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae container test-container: <nil>
STEP: delete the pod 11/17/23 15:16:01.43
Nov 17 15:16:01.447: INFO: Waiting for pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae to disappear
Nov 17 15:16:01.450: INFO: Pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:01.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2666" for this suite. 11/17/23 15:16:01.456
------------------------------
â€¢ [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:15:57.367
    Nov 17 15:15:57.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename emptydir 11/17/23 15:15:57.368
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:15:57.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:15:57.394
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 11/17/23 15:15:57.397
    Nov 17 15:15:57.411: INFO: Waiting up to 5m0s for pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae" in namespace "emptydir-2666" to be "Succeeded or Failed"
    Nov 17 15:15:57.414: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250649ms
    Nov 17 15:15:59.419: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.007805545s
    Nov 17 15:16:01.420: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009175891s
    STEP: Saw pod success 11/17/23 15:16:01.42
    Nov 17 15:16:01.420: INFO: Pod "pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae" satisfied condition "Succeeded or Failed"
    Nov 17 15:16:01.424: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae container test-container: <nil>
    STEP: delete the pod 11/17/23 15:16:01.43
    Nov 17 15:16:01.447: INFO: Waiting for pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae to disappear
    Nov 17 15:16:01.450: INFO: Pod pod-ee08ffa1-2d72-4dc9-9f08-e19273cb23ae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:01.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2666" for this suite. 11/17/23 15:16:01.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:01.466
Nov 17 15:16:01.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:16:01.468
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:01.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:01.49
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 11/17/23 15:16:01.493
STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:16:01.5
STEP: Creating a ResourceQuota with not best effort scope 11/17/23 15:16:03.505
STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:16:03.511
STEP: Creating a best-effort pod 11/17/23 15:16:05.518
STEP: Ensuring resource quota with best effort scope captures the pod usage 11/17/23 15:16:05.535
STEP: Ensuring resource quota with not best effort ignored the pod usage 11/17/23 15:16:07.539
STEP: Deleting the pod 11/17/23 15:16:09.544
STEP: Ensuring resource quota status released the pod usage 11/17/23 15:16:09.565
STEP: Creating a not best-effort pod 11/17/23 15:16:11.57
STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/17/23 15:16:11.59
STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/17/23 15:16:13.594
STEP: Deleting the pod 11/17/23 15:16:15.599
STEP: Ensuring resource quota status released the pod usage 11/17/23 15:16:15.612
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:17.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9603" for this suite. 11/17/23 15:16:17.621
------------------------------
â€¢ [SLOW TEST] [16.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:01.466
    Nov 17 15:16:01.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:16:01.468
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:01.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:01.49
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 11/17/23 15:16:01.493
    STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:16:01.5
    STEP: Creating a ResourceQuota with not best effort scope 11/17/23 15:16:03.505
    STEP: Ensuring ResourceQuota status is calculated 11/17/23 15:16:03.511
    STEP: Creating a best-effort pod 11/17/23 15:16:05.518
    STEP: Ensuring resource quota with best effort scope captures the pod usage 11/17/23 15:16:05.535
    STEP: Ensuring resource quota with not best effort ignored the pod usage 11/17/23 15:16:07.539
    STEP: Deleting the pod 11/17/23 15:16:09.544
    STEP: Ensuring resource quota status released the pod usage 11/17/23 15:16:09.565
    STEP: Creating a not best-effort pod 11/17/23 15:16:11.57
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 11/17/23 15:16:11.59
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 11/17/23 15:16:13.594
    STEP: Deleting the pod 11/17/23 15:16:15.599
    STEP: Ensuring resource quota status released the pod usage 11/17/23 15:16:15.612
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:17.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9603" for this suite. 11/17/23 15:16:17.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:17.634
Nov 17 15:16:17.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 15:16:17.635
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:17.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:17.658
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-8757/configmap-test-400b83fc-8c9f-4879-858b-4fc4652619ee 11/17/23 15:16:17.661
STEP: Creating a pod to test consume configMaps 11/17/23 15:16:17.665
Nov 17 15:16:17.674: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852" in namespace "configmap-8757" to be "Succeeded or Failed"
Nov 17 15:16:17.678: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844082ms
Nov 17 15:16:19.682: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008260141s
Nov 17 15:16:21.683: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008861255s
STEP: Saw pod success 11/17/23 15:16:21.683
Nov 17 15:16:21.683: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852" satisfied condition "Succeeded or Failed"
Nov 17 15:16:21.686: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 container env-test: <nil>
STEP: delete the pod 11/17/23 15:16:21.691
Nov 17 15:16:21.702: INFO: Waiting for pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 to disappear
Nov 17 15:16:21.705: INFO: Pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:21.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8757" for this suite. 11/17/23 15:16:21.709
------------------------------
â€¢ [4.081 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:17.634
    Nov 17 15:16:17.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 15:16:17.635
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:17.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:17.658
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-8757/configmap-test-400b83fc-8c9f-4879-858b-4fc4652619ee 11/17/23 15:16:17.661
    STEP: Creating a pod to test consume configMaps 11/17/23 15:16:17.665
    Nov 17 15:16:17.674: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852" in namespace "configmap-8757" to be "Succeeded or Failed"
    Nov 17 15:16:17.678: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844082ms
    Nov 17 15:16:19.682: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008260141s
    Nov 17 15:16:21.683: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008861255s
    STEP: Saw pod success 11/17/23 15:16:21.683
    Nov 17 15:16:21.683: INFO: Pod "pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852" satisfied condition "Succeeded or Failed"
    Nov 17 15:16:21.686: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 container env-test: <nil>
    STEP: delete the pod 11/17/23 15:16:21.691
    Nov 17 15:16:21.702: INFO: Waiting for pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 to disappear
    Nov 17 15:16:21.705: INFO: Pod pod-configmaps-0f940fc4-ce42-47e9-a7b9-53055312f852 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:21.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8757" for this suite. 11/17/23 15:16:21.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:21.717
Nov 17 15:16:21.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename replicaset 11/17/23 15:16:21.718
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:21.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:21.752
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 11/17/23 15:16:21.762
STEP: Verify that the required pods have come up. 11/17/23 15:16:21.769
Nov 17 15:16:21.771: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 17 15:16:26.779: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 11/17/23 15:16:26.779
STEP: Getting /status 11/17/23 15:16:26.779
Nov 17 15:16:26.794: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 11/17/23 15:16:26.794
Nov 17 15:16:26.843: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 11/17/23 15:16:26.843
Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: ADDED
Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.848: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.848: INFO: Found replicaset test-rs in namespace replicaset-1004 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 17 15:16:26.848: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 11/17/23 15:16:26.848
Nov 17 15:16:26.848: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 17 15:16:26.868: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 11/17/23 15:16:26.868
Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: ADDED
Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.880: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.880: INFO: Observed replicaset test-rs in namespace replicaset-1004 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 17 15:16:26.880: INFO: Observed &ReplicaSet event: MODIFIED
Nov 17 15:16:26.880: INFO: Found replicaset test-rs in namespace replicaset-1004 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 17 15:16:26.880: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1004" for this suite. 11/17/23 15:16:26.892
------------------------------
â€¢ [SLOW TEST] [5.218 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:21.717
    Nov 17 15:16:21.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename replicaset 11/17/23 15:16:21.718
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:21.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:21.752
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 11/17/23 15:16:21.762
    STEP: Verify that the required pods have come up. 11/17/23 15:16:21.769
    Nov 17 15:16:21.771: INFO: Pod name sample-pod: Found 0 pods out of 1
    Nov 17 15:16:26.779: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 11/17/23 15:16:26.779
    STEP: Getting /status 11/17/23 15:16:26.779
    Nov 17 15:16:26.794: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 11/17/23 15:16:26.794
    Nov 17 15:16:26.843: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 11/17/23 15:16:26.843
    Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: ADDED
    Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.847: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.848: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.848: INFO: Found replicaset test-rs in namespace replicaset-1004 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Nov 17 15:16:26.848: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 11/17/23 15:16:26.848
    Nov 17 15:16:26.848: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Nov 17 15:16:26.868: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 11/17/23 15:16:26.868
    Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: ADDED
    Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.879: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.880: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.880: INFO: Observed replicaset test-rs in namespace replicaset-1004 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Nov 17 15:16:26.880: INFO: Observed &ReplicaSet event: MODIFIED
    Nov 17 15:16:26.880: INFO: Found replicaset test-rs in namespace replicaset-1004 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Nov 17 15:16:26.880: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1004" for this suite. 11/17/23 15:16:26.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:26.936
Nov 17 15:16:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename sched-pred 11/17/23 15:16:26.938
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:27.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:27.02
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Nov 17 15:16:27.044: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 17 15:16:27.072: INFO: Waiting for terminating namespaces to be deleted...
Nov 17 15:16:27.077: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
Nov 17 15:16:27.100: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container cert-manager ready: true, restart count 0
Nov 17 15:16:27.100: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container cainjector ready: true, restart count 0
Nov 17 15:16:27.100: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container webhook ready: true, restart count 0
Nov 17 15:16:27.100: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container minio ready: true, restart count 0
Nov 17 15:16:27.100: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container velero ready: true, restart count 0
Nov 17 15:16:27.100: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container traefik ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kube-green ready: true, restart count 0
Nov 17 15:16:27.100: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:16:27.100: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container coredns ready: true, restart count 0
Nov 17 15:16:27.100: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container coredns ready: true, restart count 0
Nov 17 15:16:27.100: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container hubble-relay ready: true, restart count 0
Nov 17 15:16:27.100: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container backend ready: true, restart count 0
Nov 17 15:16:27.100: INFO: 	Container frontend ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kyverno ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kyverno-background ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kyverno-cleanup ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kyverno-reports ready: true, restart count 0
Nov 17 15:16:27.100: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container local-path-provisioner ready: true, restart count 0
Nov 17 15:16:27.100: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:16:27.100: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container logging-operator ready: true, restart count 0
Nov 17 15:16:27.100: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov 17 15:16:27.100: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:16:27.100: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container prometheus-operator ready: true, restart count 0
Nov 17 15:16:27.100: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container rbac-manager ready: true, restart count 0
Nov 17 15:16:27.100: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:16:27.100: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 15:16:27.100: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
Nov 17 15:16:27.122: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:16:27.123: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 15:16:27.123: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:16:27.123: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:16:27.123: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:16:27.123: INFO: test-rs-clggw from replicaset-1004 started at 2023-11-17 15:16:21 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container httpd ready: true, restart count 0
Nov 17 15:16:27.123: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 17 15:16:27.123: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container e2e ready: true, restart count 0
Nov 17 15:16:27.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:16:27.123: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:16:27.123: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 17 15:16:27.123: INFO: 
Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
Nov 17 15:16:27.149: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container cilium-agent ready: true, restart count 0
Nov 17 15:16:27.149: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container cilium-operator ready: true, restart count 0
Nov 17 15:16:27.149: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 17 15:16:27.149: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container fluent-bit ready: true, restart count 0
Nov 17 15:16:27.149: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:16:27.149: INFO: 	Container fluentd ready: true, restart count 7
Nov 17 15:16:27.149: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container alertmanager ready: true, restart count 0
Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:16:27.149: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container node-exporter ready: true, restart count 0
Nov 17 15:16:27.149: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
Nov 17 15:16:27.149: INFO: 	Container prometheus ready: true, restart count 0
Nov 17 15:16:27.149: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
Nov 17 15:16:27.149: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 17 15:16:27.149: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 11/17/23 15:16:27.149
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.179871fc2bdff53a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 11/17/23 15:16:27.232
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1675" for this suite. 11/17/23 15:16:28.232
------------------------------
â€¢ [1.312 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:26.936
    Nov 17 15:16:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename sched-pred 11/17/23 15:16:26.938
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:27.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:27.02
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Nov 17 15:16:27.044: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Nov 17 15:16:27.072: INFO: Waiting for terminating namespaces to be deleted...
    Nov 17 15:16:27.077: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-1.c.operations-lab.internal before test
    Nov 17 15:16:27.100: INFO: cert-manager-7689849c74-smgkc from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container cert-manager ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: cert-manager-cainjector-cdfcc5d5b-nq7vv from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container cainjector ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: cert-manager-webhook-57bd576df4-wmz82 from cert-manager-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container webhook ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: minio-bc8b57858-5v8tm from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container minio ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: velero-57c7d7c6c4-vdtv8 from dr-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container velero ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: traefik-7cb9797f6-qn767 from ingress-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container traefik ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kube-green-85cfb6cdbd-5skd2 from kube-green-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kube-green ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: cilium-dhcs4 from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: coredns-787d4945fb-kzc5z from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: coredns-787d4945fb-ppt87 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container coredns ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: hubble-relay-5d6dbd4d98-kv8w5 from kube-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container hubble-relay ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: hubble-ui-8c9fc5b67-pr96g from kube-system started at 2023-11-17 13:33:10 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container backend ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: 	Container frontend ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kube-proxy-m5kfg from kube-system started at 2023-11-17 12:10:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kyverno-5c8fd7bc64-4lx4g from kyverno-system started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kyverno ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kyverno-background-5f955bc7fb-2lm9h from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kyverno-background ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kyverno-cleanup-66c9dd798b-kbtj7 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kyverno-cleanup ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kyverno-reports-74995bc6df-6srr2 from kyverno-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kyverno-reports ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: local-path-provisioner-7f8667b75c-szgfl from local-path-storage started at 2023-11-17 13:33:10 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container local-path-provisioner ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: fluentbit-fluentbit-r7fzw from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: logging-operator-5df74f78f5-rvtbs from logging-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container logging-operator ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: kube-state-metrics-8447695667-c6vfl from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: node-exporter-wpdk5 from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: prometheus-operator-75f79b8c5d-ftm94 from monitoring-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container prometheus-operator ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: rbac-manager-84bd6887f-9rp2m from rbac-manager-system started at 2023-11-17 13:33:11 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container rbac-manager ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-zfbsb from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 15:16:27.100: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-2.c.operations-lab.internal before test
    Nov 17 15:16:27.122: INFO: cilium-65vkv from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: cilium-operator-86c964c849-rx7t2 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: kube-proxy-8mmvh from kube-system started at 2023-11-17 12:10:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: fluentbit-fluentbit-ncj7c from logging-system started at 2023-11-17 14:32:53 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: node-exporter-s4hnf from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: test-rs-clggw from replicaset-1004 started at 2023-11-17 15:16:21 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container httpd ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: sonobuoy from sonobuoy started at 2023-11-17 13:40:47 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: sonobuoy-e2e-job-a1d20c9e74d84b3f from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container e2e ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-z2g2v from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: 	Container systemd-logs ready: true, restart count 0
    Nov 17 15:16:27.123: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker-3.c.operations-lab.internal before test
    Nov 17 15:16:27.149: INFO: cilium-5ddmt from kube-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container cilium-agent ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: cilium-operator-86c964c849-v2hw8 from kube-system started at 2023-11-17 13:32:43 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container cilium-operator ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: kube-proxy-f98r5 from kube-system started at 2023-11-17 12:10:35 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container kube-proxy ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: fluentbit-fluentbit-k8kqf from logging-system started at 2023-11-17 13:34:07 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container fluent-bit ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: logging-fluentd-0 from logging-system started at 2023-11-17 13:34:21 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: 	Container fluentd ready: true, restart count 7
    Nov 17 15:16:27.149: INFO: alertmanager-alertmanager-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container alertmanager ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: node-exporter-kvvhw from monitoring-system started at 2023-11-17 13:32:41 +0000 UTC (1 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container node-exporter ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: prometheus-prometheus-0 from monitoring-system started at 2023-11-17 14:32:29 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container config-reloader ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: 	Container prometheus ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: sonobuoy-systemd-logs-daemon-set-af74b530fb954558-997lv from sonobuoy started at 2023-11-17 13:40:48 +0000 UTC (2 container statuses recorded)
    Nov 17 15:16:27.149: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Nov 17 15:16:27.149: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 11/17/23 15:16:27.149
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.179871fc2bdff53a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 11/17/23 15:16:27.232
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1675" for this suite. 11/17/23 15:16:28.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:28.25
Nov 17 15:16:28.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename crd-webhook 11/17/23 15:16:28.252
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:28.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:28.292
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 11/17/23 15:16:28.3
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/17/23 15:16:28.642
STEP: Deploying the custom resource conversion webhook pod 11/17/23 15:16:28.65
STEP: Wait for the deployment to be ready 11/17/23 15:16:28.663
Nov 17 15:16:28.684: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 11/17/23 15:16:30.695
STEP: Verifying the service has paired with the endpoint 11/17/23 15:16:30.711
Nov 17 15:16:31.711: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Nov 17 15:16:31.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Creating a v1 custom resource 11/17/23 15:16:34.383
STEP: v2 custom resource should be converted 11/17/23 15:16:34.402
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:34.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-2615" for this suite. 11/17/23 15:16:35.041
------------------------------
â€¢ [SLOW TEST] [6.814 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:28.25
    Nov 17 15:16:28.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename crd-webhook 11/17/23 15:16:28.252
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:28.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:28.292
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 11/17/23 15:16:28.3
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 11/17/23 15:16:28.642
    STEP: Deploying the custom resource conversion webhook pod 11/17/23 15:16:28.65
    STEP: Wait for the deployment to be ready 11/17/23 15:16:28.663
    Nov 17 15:16:28.684: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 11/17/23 15:16:30.695
    STEP: Verifying the service has paired with the endpoint 11/17/23 15:16:30.711
    Nov 17 15:16:31.711: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Nov 17 15:16:31.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Creating a v1 custom resource 11/17/23 15:16:34.383
    STEP: v2 custom resource should be converted 11/17/23 15:16:34.402
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:34.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-2615" for this suite. 11/17/23 15:16:35.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:35.065
Nov 17 15:16:35.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename podtemplate 11/17/23 15:16:35.067
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:35.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:35.094
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 11/17/23 15:16:35.1
Nov 17 15:16:35.108: INFO: created test-podtemplate-1
Nov 17 15:16:35.117: INFO: created test-podtemplate-2
Nov 17 15:16:35.125: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 11/17/23 15:16:35.126
STEP: delete collection of pod templates 11/17/23 15:16:35.13
Nov 17 15:16:35.131: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 11/17/23 15:16:35.148
Nov 17 15:16:35.148: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:35.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4613" for this suite. 11/17/23 15:16:35.158
------------------------------
â€¢ [0.101 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:35.065
    Nov 17 15:16:35.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename podtemplate 11/17/23 15:16:35.067
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:35.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:35.094
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 11/17/23 15:16:35.1
    Nov 17 15:16:35.108: INFO: created test-podtemplate-1
    Nov 17 15:16:35.117: INFO: created test-podtemplate-2
    Nov 17 15:16:35.125: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 11/17/23 15:16:35.126
    STEP: delete collection of pod templates 11/17/23 15:16:35.13
    Nov 17 15:16:35.131: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 11/17/23 15:16:35.148
    Nov 17 15:16:35.148: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:35.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4613" for this suite. 11/17/23 15:16:35.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:35.17
Nov 17 15:16:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename projected 11/17/23 15:16:35.172
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:35.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:35.196
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-238f9979-7f8c-497b-89c5-0bad41d9828f 11/17/23 15:16:35.201
STEP: Creating a pod to test consume secrets 11/17/23 15:16:35.208
Nov 17 15:16:35.221: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357" in namespace "projected-4971" to be "Succeeded or Failed"
Nov 17 15:16:35.225: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073566ms
Nov 17 15:16:37.230: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008883135s
Nov 17 15:16:39.229: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008377499s
STEP: Saw pod success 11/17/23 15:16:39.229
Nov 17 15:16:39.229: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357" satisfied condition "Succeeded or Failed"
Nov 17 15:16:39.232: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 container projected-secret-volume-test: <nil>
STEP: delete the pod 11/17/23 15:16:39.241
Nov 17 15:16:39.253: INFO: Waiting for pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 to disappear
Nov 17 15:16:39.256: INFO: Pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4971" for this suite. 11/17/23 15:16:39.26
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:35.17
    Nov 17 15:16:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename projected 11/17/23 15:16:35.172
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:35.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:35.196
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-238f9979-7f8c-497b-89c5-0bad41d9828f 11/17/23 15:16:35.201
    STEP: Creating a pod to test consume secrets 11/17/23 15:16:35.208
    Nov 17 15:16:35.221: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357" in namespace "projected-4971" to be "Succeeded or Failed"
    Nov 17 15:16:35.225: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073566ms
    Nov 17 15:16:37.230: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008883135s
    Nov 17 15:16:39.229: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008377499s
    STEP: Saw pod success 11/17/23 15:16:39.229
    Nov 17 15:16:39.229: INFO: Pod "pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357" satisfied condition "Succeeded or Failed"
    Nov 17 15:16:39.232: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 container projected-secret-volume-test: <nil>
    STEP: delete the pod 11/17/23 15:16:39.241
    Nov 17 15:16:39.253: INFO: Waiting for pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 to disappear
    Nov 17 15:16:39.256: INFO: Pod pod-projected-secrets-a0199af5-f790-4866-b69f-a114b78ad357 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4971" for this suite. 11/17/23 15:16:39.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:39.265
Nov 17 15:16:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename downward-api 11/17/23 15:16:39.267
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:39.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:39.285
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 11/17/23 15:16:39.288
Nov 17 15:16:39.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102" in namespace "downward-api-1828" to be "Succeeded or Failed"
Nov 17 15:16:39.307: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Pending", Reason="", readiness=false. Elapsed: 9.113034ms
Nov 17 15:16:41.312: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013983154s
Nov 17 15:16:43.313: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015038134s
STEP: Saw pod success 11/17/23 15:16:43.313
Nov 17 15:16:43.313: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102" satisfied condition "Succeeded or Failed"
Nov 17 15:16:43.316: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 container client-container: <nil>
STEP: delete the pod 11/17/23 15:16:43.322
Nov 17 15:16:43.336: INFO: Waiting for pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 to disappear
Nov 17 15:16:43.338: INFO: Pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1828" for this suite. 11/17/23 15:16:43.342
------------------------------
â€¢ [4.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:39.265
    Nov 17 15:16:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename downward-api 11/17/23 15:16:39.267
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:39.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:39.285
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 11/17/23 15:16:39.288
    Nov 17 15:16:39.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102" in namespace "downward-api-1828" to be "Succeeded or Failed"
    Nov 17 15:16:39.307: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Pending", Reason="", readiness=false. Elapsed: 9.113034ms
    Nov 17 15:16:41.312: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013983154s
    Nov 17 15:16:43.313: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015038134s
    STEP: Saw pod success 11/17/23 15:16:43.313
    Nov 17 15:16:43.313: INFO: Pod "downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102" satisfied condition "Succeeded or Failed"
    Nov 17 15:16:43.316: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 container client-container: <nil>
    STEP: delete the pod 11/17/23 15:16:43.322
    Nov 17 15:16:43.336: INFO: Waiting for pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 to disappear
    Nov 17 15:16:43.338: INFO: Pod downwardapi-volume-be5744b2-0894-43e2-89a0-75d898145102 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1828" for this suite. 11/17/23 15:16:43.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:43.348
Nov 17 15:16:43.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename security-context-test 11/17/23 15:16:43.349
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:43.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:43.369
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Nov 17 15:16:43.383: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985" in namespace "security-context-test-5692" to be "Succeeded or Failed"
Nov 17 15:16:43.391: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221315ms
Nov 17 15:16:45.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0133508s
Nov 17 15:16:47.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013728386s
Nov 17 15:16:47.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:47.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5692" for this suite. 11/17/23 15:16:47.402
------------------------------
â€¢ [4.062 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:43.348
    Nov 17 15:16:43.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename security-context-test 11/17/23 15:16:43.349
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:43.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:43.369
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Nov 17 15:16:43.383: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985" in namespace "security-context-test-5692" to be "Succeeded or Failed"
    Nov 17 15:16:43.391: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221315ms
    Nov 17 15:16:45.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0133508s
    Nov 17 15:16:47.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013728386s
    Nov 17 15:16:47.397: INFO: Pod "busybox-user-65534-4993516e-ece8-4376-85aa-3df18cb1d985" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:47.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5692" for this suite. 11/17/23 15:16:47.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:47.411
Nov 17 15:16:47.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 15:16:47.412
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:47.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:47.437
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-9d95ea1e-738e-4fd2-afa5-31627a6833f3 11/17/23 15:16:47.441
STEP: Creating a pod to test consume configMaps 11/17/23 15:16:47.449
Nov 17 15:16:47.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae" in namespace "configmap-282" to be "Succeeded or Failed"
Nov 17 15:16:47.469: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.63432ms
Nov 17 15:16:49.475: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011712749s
Nov 17 15:16:51.474: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011474749s
STEP: Saw pod success 11/17/23 15:16:51.474
Nov 17 15:16:51.474: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae" satisfied condition "Succeeded or Failed"
Nov 17 15:16:51.478: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae container agnhost-container: <nil>
STEP: delete the pod 11/17/23 15:16:51.485
Nov 17 15:16:51.502: INFO: Waiting for pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae to disappear
Nov 17 15:16:51.505: INFO: Pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-282" for this suite. 11/17/23 15:16:51.51
------------------------------
â€¢ [4.107 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:47.411
    Nov 17 15:16:47.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 15:16:47.412
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:47.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:47.437
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-9d95ea1e-738e-4fd2-afa5-31627a6833f3 11/17/23 15:16:47.441
    STEP: Creating a pod to test consume configMaps 11/17/23 15:16:47.449
    Nov 17 15:16:47.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae" in namespace "configmap-282" to be "Succeeded or Failed"
    Nov 17 15:16:47.469: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.63432ms
    Nov 17 15:16:49.475: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011712749s
    Nov 17 15:16:51.474: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011474749s
    STEP: Saw pod success 11/17/23 15:16:51.474
    Nov 17 15:16:51.474: INFO: Pod "pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae" satisfied condition "Succeeded or Failed"
    Nov 17 15:16:51.478: INFO: Trying to get logs from node k8s-worker-2.c.operations-lab.internal pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae container agnhost-container: <nil>
    STEP: delete the pod 11/17/23 15:16:51.485
    Nov 17 15:16:51.502: INFO: Waiting for pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae to disappear
    Nov 17 15:16:51.505: INFO: Pod pod-configmaps-37d00b3e-6a9d-491f-9e92-4a692d3389ae no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:51.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-282" for this suite. 11/17/23 15:16:51.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:51.518
Nov 17 15:16:51.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:16:51.52
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:51.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:51.542
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-6998-delete-me 11/17/23 15:16:51.55
STEP: Waiting for the RuntimeClass to disappear 11/17/23 15:16:51.558
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Nov 17 15:16:51.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6998" for this suite. 11/17/23 15:16:51.572
------------------------------
â€¢ [0.063 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:51.518
    Nov 17 15:16:51.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename runtimeclass 11/17/23 15:16:51.52
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:51.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:51.542
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-6998-delete-me 11/17/23 15:16:51.55
    STEP: Waiting for the RuntimeClass to disappear 11/17/23 15:16:51.558
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:16:51.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6998" for this suite. 11/17/23 15:16:51.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:16:51.584
Nov 17 15:16:51.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename services 11/17/23 15:16:51.585
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:51.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:51.605
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-769 11/17/23 15:16:51.61
STEP: creating service affinity-clusterip in namespace services-769 11/17/23 15:16:51.61
STEP: creating replication controller affinity-clusterip in namespace services-769 11/17/23 15:16:51.626
I1117 15:16:51.647195      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-769, replica count: 3
I1117 15:16:54.698742      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 17 15:16:54.704: INFO: Creating new exec pod
Nov 17 15:16:54.708: INFO: Waiting up to 5m0s for pod "execpod-affinityjr468" in namespace "services-769" to be "running"
Nov 17 15:16:54.712: INFO: Pod "execpod-affinityjr468": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259519ms
Nov 17 15:16:56.715: INFO: Pod "execpod-affinityjr468": Phase="Running", Reason="", readiness=true. Elapsed: 2.006878333s
Nov 17 15:16:56.715: INFO: Pod "execpod-affinityjr468" satisfied condition "running"
Nov 17 15:16:57.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Nov 17 15:16:57.938: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 17 15:16:57.938: INFO: stdout: ""
Nov 17 15:16:57.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c nc -v -z -w 2 10.109.133.110 80'
Nov 17 15:16:58.143: INFO: stderr: "+ nc -v -z -w 2 10.109.133.110 80\nConnection to 10.109.133.110 80 port [tcp/http] succeeded!\n"
Nov 17 15:16:58.143: INFO: stdout: ""
Nov 17 15:16:58.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.133.110:80/ ; done'
Nov 17 15:16:58.434: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n"
Nov 17 15:16:58.435: INFO: stdout: "\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq"
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
Nov 17 15:16:58.435: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-769, will wait for the garbage collector to delete the pods 11/17/23 15:16:58.448
Nov 17 15:16:58.509: INFO: Deleting ReplicationController affinity-clusterip took: 6.343382ms
Nov 17 15:16:58.609: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.205156ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Nov 17 15:17:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-769" for this suite. 11/17/23 15:17:00.653
------------------------------
â€¢ [SLOW TEST] [9.078 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:16:51.584
    Nov 17 15:16:51.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename services 11/17/23 15:16:51.585
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:16:51.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:16:51.605
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-769 11/17/23 15:16:51.61
    STEP: creating service affinity-clusterip in namespace services-769 11/17/23 15:16:51.61
    STEP: creating replication controller affinity-clusterip in namespace services-769 11/17/23 15:16:51.626
    I1117 15:16:51.647195      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-769, replica count: 3
    I1117 15:16:54.698742      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Nov 17 15:16:54.704: INFO: Creating new exec pod
    Nov 17 15:16:54.708: INFO: Waiting up to 5m0s for pod "execpod-affinityjr468" in namespace "services-769" to be "running"
    Nov 17 15:16:54.712: INFO: Pod "execpod-affinityjr468": Phase="Pending", Reason="", readiness=false. Elapsed: 3.259519ms
    Nov 17 15:16:56.715: INFO: Pod "execpod-affinityjr468": Phase="Running", Reason="", readiness=true. Elapsed: 2.006878333s
    Nov 17 15:16:56.715: INFO: Pod "execpod-affinityjr468" satisfied condition "running"
    Nov 17 15:16:57.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Nov 17 15:16:57.938: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Nov 17 15:16:57.938: INFO: stdout: ""
    Nov 17 15:16:57.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c nc -v -z -w 2 10.109.133.110 80'
    Nov 17 15:16:58.143: INFO: stderr: "+ nc -v -z -w 2 10.109.133.110 80\nConnection to 10.109.133.110 80 port [tcp/http] succeeded!\n"
    Nov 17 15:16:58.143: INFO: stdout: ""
    Nov 17 15:16:58.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3908374908 --namespace=services-769 exec execpod-affinityjr468 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.133.110:80/ ; done'
    Nov 17 15:16:58.434: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.133.110:80/\n"
    Nov 17 15:16:58.435: INFO: stdout: "\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq\naffinity-clusterip-jbpgq"
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Received response from host: affinity-clusterip-jbpgq
    Nov 17 15:16:58.435: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-769, will wait for the garbage collector to delete the pods 11/17/23 15:16:58.448
    Nov 17 15:16:58.509: INFO: Deleting ReplicationController affinity-clusterip took: 6.343382ms
    Nov 17 15:16:58.609: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.205156ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:17:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-769" for this suite. 11/17/23 15:17:00.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:17:00.662
Nov 17 15:17:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename cronjob 11/17/23 15:17:00.664
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:17:00.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:17:00.685
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 11/17/23 15:17:00.69
STEP: Ensuring more than one job is running at a time 11/17/23 15:17:00.7
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/17/23 15:19:00.705
STEP: Removing cronjob 11/17/23 15:19:00.709
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Nov 17 15:19:00.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8061" for this suite. 11/17/23 15:19:00.72
------------------------------
â€¢ [SLOW TEST] [120.094 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:17:00.662
    Nov 17 15:17:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename cronjob 11/17/23 15:17:00.664
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:17:00.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:17:00.685
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 11/17/23 15:17:00.69
    STEP: Ensuring more than one job is running at a time 11/17/23 15:17:00.7
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 11/17/23 15:19:00.705
    STEP: Removing cronjob 11/17/23 15:19:00.709
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:19:00.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8061" for this suite. 11/17/23 15:19:00.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:19:00.772
Nov 17 15:19:00.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename gc 11/17/23 15:19:00.773
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:00.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:00.819
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 11/17/23 15:19:00.832
STEP: delete the rc 11/17/23 15:19:05.848
STEP: wait for the rc to be deleted 11/17/23 15:19:05.869
Nov 17 15:19:06.931: INFO: 88 pods remaining
Nov 17 15:19:06.931: INFO: 80 pods has nil DeletionTimestamp
Nov 17 15:19:06.931: INFO: 
Nov 17 15:19:07.890: INFO: 86 pods remaining
Nov 17 15:19:07.890: INFO: 70 pods has nil DeletionTimestamp
Nov 17 15:19:07.890: INFO: 
Nov 17 15:19:08.890: INFO: 80 pods remaining
Nov 17 15:19:08.890: INFO: 60 pods has nil DeletionTimestamp
Nov 17 15:19:08.890: INFO: 
Nov 17 15:19:09.885: INFO: 73 pods remaining
Nov 17 15:19:09.885: INFO: 40 pods has nil DeletionTimestamp
Nov 17 15:19:09.885: INFO: 
Nov 17 15:19:10.887: INFO: 69 pods remaining
Nov 17 15:19:10.887: INFO: 30 pods has nil DeletionTimestamp
Nov 17 15:19:10.887: INFO: 
Nov 17 15:19:11.885: INFO: 68 pods remaining
Nov 17 15:19:11.886: INFO: 20 pods has nil DeletionTimestamp
Nov 17 15:19:11.886: INFO: 
Nov 17 15:19:12.887: INFO: 66 pods remaining
Nov 17 15:19:12.887: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:12.888: INFO: 
Nov 17 15:19:13.886: INFO: 63 pods remaining
Nov 17 15:19:13.886: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:13.886: INFO: 
Nov 17 15:19:14.883: INFO: 51 pods remaining
Nov 17 15:19:14.883: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:14.884: INFO: 
Nov 17 15:19:15.881: INFO: 42 pods remaining
Nov 17 15:19:15.881: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:15.881: INFO: 
Nov 17 15:19:16.879: INFO: 33 pods remaining
Nov 17 15:19:16.879: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:16.879: INFO: 
Nov 17 15:19:17.880: INFO: 24 pods remaining
Nov 17 15:19:17.880: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:17.880: INFO: 
Nov 17 15:19:18.880: INFO: 14 pods remaining
Nov 17 15:19:18.880: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:18.880: INFO: 
Nov 17 15:19:19.880: INFO: 3 pods remaining
Nov 17 15:19:19.880: INFO: 0 pods has nil DeletionTimestamp
Nov 17 15:19:19.880: INFO: 
STEP: Gathering metrics 11/17/23 15:19:20.876
Nov 17 15:19:20.907: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
Nov 17 15:19:20.910: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.159303ms
Nov 17 15:19:20.910: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
Nov 17 15:19:20.910: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
Nov 17 15:19:21.008: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Nov 17 15:19:21.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7555" for this suite. 11/17/23 15:19:21.013
------------------------------
â€¢ [SLOW TEST] [20.247 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:19:00.772
    Nov 17 15:19:00.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename gc 11/17/23 15:19:00.773
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:00.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:00.819
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 11/17/23 15:19:00.832
    STEP: delete the rc 11/17/23 15:19:05.848
    STEP: wait for the rc to be deleted 11/17/23 15:19:05.869
    Nov 17 15:19:06.931: INFO: 88 pods remaining
    Nov 17 15:19:06.931: INFO: 80 pods has nil DeletionTimestamp
    Nov 17 15:19:06.931: INFO: 
    Nov 17 15:19:07.890: INFO: 86 pods remaining
    Nov 17 15:19:07.890: INFO: 70 pods has nil DeletionTimestamp
    Nov 17 15:19:07.890: INFO: 
    Nov 17 15:19:08.890: INFO: 80 pods remaining
    Nov 17 15:19:08.890: INFO: 60 pods has nil DeletionTimestamp
    Nov 17 15:19:08.890: INFO: 
    Nov 17 15:19:09.885: INFO: 73 pods remaining
    Nov 17 15:19:09.885: INFO: 40 pods has nil DeletionTimestamp
    Nov 17 15:19:09.885: INFO: 
    Nov 17 15:19:10.887: INFO: 69 pods remaining
    Nov 17 15:19:10.887: INFO: 30 pods has nil DeletionTimestamp
    Nov 17 15:19:10.887: INFO: 
    Nov 17 15:19:11.885: INFO: 68 pods remaining
    Nov 17 15:19:11.886: INFO: 20 pods has nil DeletionTimestamp
    Nov 17 15:19:11.886: INFO: 
    Nov 17 15:19:12.887: INFO: 66 pods remaining
    Nov 17 15:19:12.887: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:12.888: INFO: 
    Nov 17 15:19:13.886: INFO: 63 pods remaining
    Nov 17 15:19:13.886: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:13.886: INFO: 
    Nov 17 15:19:14.883: INFO: 51 pods remaining
    Nov 17 15:19:14.883: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:14.884: INFO: 
    Nov 17 15:19:15.881: INFO: 42 pods remaining
    Nov 17 15:19:15.881: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:15.881: INFO: 
    Nov 17 15:19:16.879: INFO: 33 pods remaining
    Nov 17 15:19:16.879: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:16.879: INFO: 
    Nov 17 15:19:17.880: INFO: 24 pods remaining
    Nov 17 15:19:17.880: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:17.880: INFO: 
    Nov 17 15:19:18.880: INFO: 14 pods remaining
    Nov 17 15:19:18.880: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:18.880: INFO: 
    Nov 17 15:19:19.880: INFO: 3 pods remaining
    Nov 17 15:19:19.880: INFO: 0 pods has nil DeletionTimestamp
    Nov 17 15:19:19.880: INFO: 
    STEP: Gathering metrics 11/17/23 15:19:20.876
    Nov 17 15:19:20.907: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" in namespace "kube-system" to be "running and ready"
    Nov 17 15:19:20.910: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal": Phase="Running", Reason="", readiness=true. Elapsed: 3.159303ms
    Nov 17 15:19:20.910: INFO: The phase of Pod kube-controller-manager-k8s-control-plane.c.operations-lab.internal is Running (Ready = true)
    Nov 17 15:19:20.910: INFO: Pod "kube-controller-manager-k8s-control-plane.c.operations-lab.internal" satisfied condition "running and ready"
    Nov 17 15:19:21.008: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:19:21.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7555" for this suite. 11/17/23 15:19:21.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:19:21.025
Nov 17 15:19:21.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename daemonsets 11/17/23 15:19:21.027
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:21.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:21.049
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Nov 17 15:19:21.069: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 15:19:21.075
Nov 17 15:19:21.082: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:21.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:19:21.089: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:19:22.094: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:22.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:19:22.098: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:19:23.094: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:23.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 15:19:23.101: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:19:24.093: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:24.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 15:19:24.098: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 11/17/23 15:19:24.114
STEP: Check that daemon pods images are updated. 11/17/23 15:19:24.133
Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-dpl89. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:24.170: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:25.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:25.176: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:25.182: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:26.175: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:26.175: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:26.175: INFO: Pod daemon-set-rdsnw is not available
Nov 17 15:19:26.183: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:27.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:27.180: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:28.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Nov 17 15:19:28.176: INFO: Pod daemon-set-crch9 is not available
Nov 17 15:19:28.182: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:29.180: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:30.177: INFO: Pod daemon-set-hwdhm is not available
Nov 17 15:19:30.183: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 11/17/23 15:19:30.184
Nov 17 15:19:30.188: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:30.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Nov 17 15:19:30.193: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
Nov 17 15:19:31.200: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Nov 17 15:19:31.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Nov 17 15:19:31.204: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:19:31.222
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3336, will wait for the garbage collector to delete the pods 11/17/23 15:19:31.222
Nov 17 15:19:31.284: INFO: Deleting DaemonSet.extensions daemon-set took: 7.315447ms
Nov 17 15:19:31.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821229ms
Nov 17 15:19:33.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Nov 17 15:19:33.691: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Nov 17 15:19:33.694: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70715"},"items":null}

Nov 17 15:19:33.700: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70715"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Nov 17 15:19:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3336" for this suite. 11/17/23 15:19:33.723
------------------------------
â€¢ [SLOW TEST] [12.705 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:19:21.025
    Nov 17 15:19:21.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename daemonsets 11/17/23 15:19:21.027
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:21.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:21.049
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Nov 17 15:19:21.069: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 11/17/23 15:19:21.075
    Nov 17 15:19:21.082: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:21.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:19:21.089: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:19:22.094: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:22.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:19:22.098: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:19:23.094: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:23.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 15:19:23.101: INFO: Node k8s-worker-2.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:19:24.093: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:24.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 15:19:24.098: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 11/17/23 15:19:24.114
    STEP: Check that daemon pods images are updated. 11/17/23 15:19:24.133
    Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:24.140: INFO: Wrong image for pod: daemon-set-dpl89. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:24.170: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:25.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:25.176: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:25.182: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:26.175: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:26.175: INFO: Wrong image for pod: daemon-set-b47nd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:26.175: INFO: Pod daemon-set-rdsnw is not available
    Nov 17 15:19:26.183: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:27.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:27.180: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:28.176: INFO: Wrong image for pod: daemon-set-4msht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Nov 17 15:19:28.176: INFO: Pod daemon-set-crch9 is not available
    Nov 17 15:19:28.182: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:29.180: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:30.177: INFO: Pod daemon-set-hwdhm is not available
    Nov 17 15:19:30.183: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 11/17/23 15:19:30.184
    Nov 17 15:19:30.188: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:30.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Nov 17 15:19:30.193: INFO: Node k8s-worker-1.c.operations-lab.internal is running 0 daemon pod, expected 1
    Nov 17 15:19:31.200: INFO: DaemonSet pods can't tolerate node k8s-control-plane.c.operations-lab.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Nov 17 15:19:31.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Nov 17 15:19:31.204: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 11/17/23 15:19:31.222
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3336, will wait for the garbage collector to delete the pods 11/17/23 15:19:31.222
    Nov 17 15:19:31.284: INFO: Deleting DaemonSet.extensions daemon-set took: 7.315447ms
    Nov 17 15:19:31.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821229ms
    Nov 17 15:19:33.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Nov 17 15:19:33.691: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Nov 17 15:19:33.694: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70715"},"items":null}

    Nov 17 15:19:33.700: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70715"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:19:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3336" for this suite. 11/17/23 15:19:33.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:19:33.731
Nov 17 15:19:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename configmap 11/17/23 15:19:33.732
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:33.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:33.765
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Nov 17 15:19:33.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4606" for this suite. 11/17/23 15:19:33.828
------------------------------
â€¢ [0.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:19:33.731
    Nov 17 15:19:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename configmap 11/17/23 15:19:33.732
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:33.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:33.765
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:19:33.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4606" for this suite. 11/17/23 15:19:33.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 11/17/23 15:19:33.838
Nov 17 15:19:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
STEP: Building a namespace api object, basename resourcequota 11/17/23 15:19:33.839
STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:33.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:33.862
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 11/17/23 15:19:33.866
STEP: Getting a ResourceQuota 11/17/23 15:19:33.872
STEP: Listing all ResourceQuotas with LabelSelector 11/17/23 15:19:33.877
STEP: Patching the ResourceQuota 11/17/23 15:19:33.883
STEP: Deleting a Collection of ResourceQuotas 11/17/23 15:19:33.891
STEP: Verifying the deleted ResourceQuota 11/17/23 15:19:33.906
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Nov 17 15:19:33.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7" for this suite. 11/17/23 15:19:33.914
------------------------------
â€¢ [0.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 11/17/23 15:19:33.838
    Nov 17 15:19:33.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3908374908
    STEP: Building a namespace api object, basename resourcequota 11/17/23 15:19:33.839
    STEP: Waiting for a default service account to be provisioned in namespace 11/17/23 15:19:33.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 11/17/23 15:19:33.862
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 11/17/23 15:19:33.866
    STEP: Getting a ResourceQuota 11/17/23 15:19:33.872
    STEP: Listing all ResourceQuotas with LabelSelector 11/17/23 15:19:33.877
    STEP: Patching the ResourceQuota 11/17/23 15:19:33.883
    STEP: Deleting a Collection of ResourceQuotas 11/17/23 15:19:33.891
    STEP: Verifying the deleted ResourceQuota 11/17/23 15:19:33.906
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Nov 17 15:19:33.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7" for this suite. 11/17/23 15:19:33.914
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Nov 17 15:19:33.924: INFO: Running AfterSuite actions on node 1
Nov 17 15:19:33.924: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Nov 17 15:19:33.924: INFO: Running AfterSuite actions on node 1
    Nov 17 15:19:33.924: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.108 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5924.500 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h38m44.993712293s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

