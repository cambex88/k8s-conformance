I1207 17:20:06.405990      21 e2e.go:129] Starting e2e run "e1468650-785e-4353-9741-8df0d516748a" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1638897606 - Will randomize all specs
Will run 346 of 6433 specs

Dec  7 17:20:08.281: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
E1207 17:20:08.282579      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Dec  7 17:20:08.284: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  7 17:20:08.350: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  7 17:20:08.448: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  7 17:20:08.448: INFO: expected 15 pod replicas in namespace 'kube-system', 15 are Running and Ready.
Dec  7 17:20:08.448: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  7 17:20:08.471: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Dec  7 17:20:08.471: INFO: e2e test version: v1.22.4
Dec  7 17:20:08.476: INFO: kube-apiserver version: v1.22.4+IKS
Dec  7 17:20:08.476: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 17:20:08.492: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:20:08.492: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename server-version
W1207 17:20:08.625051      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Dec  7 17:20:08.625: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Dec  7 17:20:08.662: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
W1207 17:20:08.675442      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W1207 17:20:08.691634      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Dec  7 17:20:08.714: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-4210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Dec  7 17:20:08.871: INFO: Major version: 1
STEP: Confirm minor version
Dec  7 17:20:08.871: INFO: cleanMinorVersion: 22
Dec  7 17:20:08.871: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:20:08.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4210" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":1,"skipped":5,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:20:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
E1207 17:20:08.925479      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  7 17:20:19.353: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1207 17:20:19.353376      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:20:19.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1331" for this suite.

• [SLOW TEST:10.475 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":2,"skipped":35,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:20:19.400: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2820
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:20:19.663: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  7 17:20:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2820 --namespace=crd-publish-openapi-2820 create -f -'
Dec  7 17:20:25.906: INFO: stderr: ""
Dec  7 17:20:25.906: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  7 17:20:25.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2820 --namespace=crd-publish-openapi-2820 delete e2e-test-crd-publish-openapi-5614-crds test-cr'
Dec  7 17:20:26.082: INFO: stderr: ""
Dec  7 17:20:26.082: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec  7 17:20:26.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2820 --namespace=crd-publish-openapi-2820 apply -f -'
Dec  7 17:20:26.325: INFO: stderr: ""
Dec  7 17:20:26.325: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  7 17:20:26.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2820 --namespace=crd-publish-openapi-2820 delete e2e-test-crd-publish-openapi-5614-crds test-cr'
Dec  7 17:20:26.431: INFO: stderr: ""
Dec  7 17:20:26.431: INFO: stdout: "e2e-test-crd-publish-openapi-5614-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec  7 17:20:26.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2820 explain e2e-test-crd-publish-openapi-5614-crds'
Dec  7 17:20:27.674: INFO: stderr: ""
Dec  7 17:20:27.674: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5614-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:20:32.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2820" for this suite.

• [SLOW TEST:12.985 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":3,"skipped":51,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:20:32.385: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:20:32.650: INFO: The status of Pod busybox-readonly-fsd84a4a57-4122-46fd-a136-5dc976d97bb8 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:20:34.664: INFO: The status of Pod busybox-readonly-fsd84a4a57-4122-46fd-a136-5dc976d97bb8 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:20:36.664: INFO: The status of Pod busybox-readonly-fsd84a4a57-4122-46fd-a136-5dc976d97bb8 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:20:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4157" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":55,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:20:36.785: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3640
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3640
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3640
Dec  7 17:20:37.067: INFO: Found 0 stateful pods, waiting for 1
Dec  7 17:20:47.088: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  7 17:20:47.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 17:20:47.339: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 17:20:47.339: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 17:20:47.339: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 17:20:47.350: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  7 17:20:57.373: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 17:20:57.373: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 17:20:57.424: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998647s
Dec  7 17:20:58.438: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989134068s
Dec  7 17:20:59.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974908723s
Dec  7 17:21:00.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.960647365s
Dec  7 17:21:01.478: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.946818301s
Dec  7 17:21:02.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.934623858s
Dec  7 17:21:03.507: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.92214422s
Dec  7 17:21:04.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.906516769s
Dec  7 17:21:05.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.893569365s
Dec  7 17:21:06.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 879.908716ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3640
Dec  7 17:21:07.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 17:21:07.814: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 17:21:07.814: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 17:21:07.814: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 17:21:07.825: INFO: Found 1 stateful pods, waiting for 3
Dec  7 17:21:17.851: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 17:21:17.851: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 17:21:17.851: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  7 17:21:27.848: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 17:21:27.848: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 17:21:27.848: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  7 17:21:27.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 17:21:28.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 17:21:28.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 17:21:28.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 17:21:28.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 17:21:28.534: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 17:21:28.534: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 17:21:28.534: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 17:21:28.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 17:21:28.805: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 17:21:28.805: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 17:21:28.805: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 17:21:28.805: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 17:21:28.818: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  7 17:21:38.852: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 17:21:38.852: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 17:21:38.852: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 17:21:38.890: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998572s
Dec  7 17:21:39.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987111981s
Dec  7 17:21:40.920: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972257364s
Dec  7 17:21:41.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957456484s
Dec  7 17:21:42.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944384158s
Dec  7 17:21:43.961: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930019435s
Dec  7 17:21:44.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916498352s
Dec  7 17:21:46.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.883808963s
Dec  7 17:21:47.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.868172116s
Dec  7 17:21:48.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 854.205705ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3640
Dec  7 17:21:49.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 17:21:49.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 17:21:49.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 17:21:49.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 17:21:49.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 17:21:49.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 17:21:49.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 17:21:49.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 17:21:49.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-3640 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 17:21:49.910: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 17:21:49.910: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 17:21:49.910: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 17:21:49.910: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 17:21:59.994: INFO: Deleting all statefulset in ns statefulset-3640
Dec  7 17:22:00.004: INFO: Scaling statefulset ss to 0
Dec  7 17:22:00.037: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 17:22:00.046: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:00.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3640" for this suite.

• [SLOW TEST:83.338 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":5,"skipped":69,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:00.125: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  7 17:22:06.476: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1207 17:22:06.476918      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:06.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6896" for this suite.

• [SLOW TEST:6.395 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":6,"skipped":100,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:06.520: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:22:06.733: INFO: Creating deployment "webserver-deployment"
Dec  7 17:22:06.749: INFO: Waiting for observed generation 1
Dec  7 17:22:08.781: INFO: Waiting for all required pods to come up
Dec  7 17:22:08.799: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  7 17:22:10.834: INFO: Waiting for deployment "webserver-deployment" to complete
Dec  7 17:22:10.859: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec  7 17:22:10.888: INFO: Updating deployment webserver-deployment
Dec  7 17:22:10.888: INFO: Waiting for observed generation 2
Dec  7 17:22:12.921: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  7 17:22:12.936: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  7 17:22:12.947: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  7 17:22:12.989: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  7 17:22:12.989: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  7 17:22:13.004: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  7 17:22:13.030: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec  7 17:22:13.030: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec  7 17:22:13.060: INFO: Updating deployment webserver-deployment
Dec  7 17:22:13.060: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec  7 17:22:13.088: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  7 17:22:13.105: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 17:22:15.178: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3883  e149fae6-8e63-4f72-8d46-60488360b436 20113 3 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041332d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-12-07 17:22:13 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-12-07 17:22:13 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec  7 17:22:15.210: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-3883  5ccfad79-9688-4a92-b27b-6ec943d1abbd 20108 3 2021-12-07 17:22:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e149fae6-8e63-4f72-8d46-60488360b436 0xc0041336e7 0xc0041336e8}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e149fae6-8e63-4f72-8d46-60488360b436\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004133788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 17:22:15.210: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec  7 17:22:15.210: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-3883  b0747c86-4933-4fb1-a0ee-71cece063b10 20238 3 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e149fae6-8e63-4f72-8d46-60488360b436 0xc0041337e7 0xc0041337e8}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e149fae6-8e63-4f72-8d46-60488360b436\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004133878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Dec  7 17:22:15.259: INFO: Pod "webserver-deployment-795d758f88-2vzwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2vzwn webserver-deployment-795d758f88- deployment-3883  e59dab68-adee-4904-857a-026a4c3912fb 20012 0 2021-12-07 17:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:a9f5676badb43b1c71806f2350ea3ba20661dc300ab6a42e95c5e62a58280b00 cni.projectcalico.org/podIP:172.30.34.159/32 cni.projectcalico.org/podIPs:172.30.34.159/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8117 0xc0040c8118}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-psbtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-psbtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.259: INFO: Pod "webserver-deployment-795d758f88-5f7v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5f7v9 webserver-deployment-795d758f88- deployment-3883  26c1d01b-5bc4-4f00-be4b-ddde00b4f7d6 20236 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:5372ac13890818a7170f352250542eed55530e0f13472d92dcaea80ea5864c97 cni.projectcalico.org/podIP:172.30.30.125/32 cni.projectcalico.org/podIPs:172.30.30.125/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8407 0xc0040c8408}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gms9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gms9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.259: INFO: Pod "webserver-deployment-795d758f88-6wmsb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6wmsb webserver-deployment-795d758f88- deployment-3883  6706269b-f0a5-4151-97a3-0b9a901ec7cf 20208 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:0dfb38ed41ffd5eec363184b0afdb4463e2089aeacb426df3c9a27fef66f7db4 cni.projectcalico.org/podIP:172.30.34.163/32 cni.projectcalico.org/podIPs:172.30.34.163/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8727 0xc0040c8728}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2zpdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zpdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.260: INFO: Pod "webserver-deployment-795d758f88-8lr8v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8lr8v webserver-deployment-795d758f88- deployment-3883  29290b06-f312-4833-b62c-177119ca6c1e 20156 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8947 0xc0040c8948}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jr798,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jr798,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.260: INFO: Pod "webserver-deployment-795d758f88-9n42h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9n42h webserver-deployment-795d758f88- deployment-3883  790434cf-e6e2-43d4-8e94-ade02472bff4 20178 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:0d39b3a0bb9b4505113eab28b19c4c9589362ac6ce23959bdba3c76653b7bdb0 cni.projectcalico.org/podIP:172.30.11.10/32 cni.projectcalico.org/podIPs:172.30.11.10/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8b57 0xc0040c8b58}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6jhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6jhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.260: INFO: Pod "webserver-deployment-795d758f88-9w5ml" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9w5ml webserver-deployment-795d758f88- deployment-3883  27408abe-38e4-4c66-b34a-202319271faf 20134 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8d67 0xc0040c8d68}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2r5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2r5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.260: INFO: Pod "webserver-deployment-795d758f88-ckfb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ckfb6 webserver-deployment-795d758f88- deployment-3883  145575eb-dcdd-44cb-9c82-ae0cd603f2d2 19988 0 2021-12-07 17:22:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:28c8cfbb8ca8bed816a3aa973bfb90ea45f3ab9c4c1a2c3ab3121331c82d5a2d cni.projectcalico.org/podIP:172.30.34.158/32 cni.projectcalico.org/podIPs:172.30.34.158/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c8f77 0xc0040c8f78}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrtm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrtm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.260: INFO: Pod "webserver-deployment-795d758f88-h95z9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h95z9 webserver-deployment-795d758f88- deployment-3883  ef440c8f-73d5-40ea-b690-94dd8ca905f9 19993 0 2021-12-07 17:22:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:eb6ec460f2c95592e2466037490486c4e96df3c9d0fcddceb2ea5fd2610cc1bb cni.projectcalico.org/podIP:172.30.30.120/32 cni.projectcalico.org/podIPs:172.30.30.120/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c91a7 0xc0040c91a8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhfk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhfk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.261: INFO: Pod "webserver-deployment-795d758f88-m5v55" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m5v55 webserver-deployment-795d758f88- deployment-3883  7437d45a-d59a-49cc-a5fd-804f89b59f83 19982 0 2021-12-07 17:22:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:7a9e8fd3189569574388bf324c25d789067e82e094914f3dfada87cc9b507510 cni.projectcalico.org/podIP:172.30.11.7/32 cni.projectcalico.org/podIPs:172.30.11.7/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c93d7 0xc0040c93d8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwrxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwrxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.261: INFO: Pod "webserver-deployment-795d758f88-pkkv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pkkv6 webserver-deployment-795d758f88- deployment-3883  8626230c-297f-46da-aedf-e538549f1bda 20231 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:551838689dd2a7259485a99caead9106b093f20e22bcf953b77fb5e857f27b62 cni.projectcalico.org/podIP:172.30.11.13/32 cni.projectcalico.org/podIPs:172.30.11.13/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c9617 0xc0040c9618}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxxnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxxnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.261: INFO: Pod "webserver-deployment-795d758f88-q2d27" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-q2d27 webserver-deployment-795d758f88- deployment-3883  74abdac9-8de9-4e9d-bfb2-760f63452212 20200 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:ad9cf101dfd84023c94e3761923d165ab187491d7a0bff059f0a4c0976c6c5b1 cni.projectcalico.org/podIP:172.30.30.123/32 cni.projectcalico.org/podIPs:172.30.30.123/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c9847 0xc0040c9848}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tkf5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tkf5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.261: INFO: Pod "webserver-deployment-795d758f88-qd79x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qd79x webserver-deployment-795d758f88- deployment-3883  f43c7e21-b7ce-4e45-b380-e50c31d8c608 20217 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:3ea0e5f9461f3b84dafc9078498a97e1c36571f304c52ef4825a667fcd38d460 cni.projectcalico.org/podIP:172.30.11.16/32 cni.projectcalico.org/podIPs:172.30.11.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c9a77 0xc0040c9a78}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z928d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z928d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.262: INFO: Pod "webserver-deployment-795d758f88-swjw4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-swjw4 webserver-deployment-795d758f88- deployment-3883  8d3f6b3a-0d78-471d-8fb6-0b83174275e9 20010 0 2021-12-07 17:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:34c29450874ef2652152bcef5dc13383dc6da3077714c493f5853e18b49ba656 cni.projectcalico.org/podIP:172.30.30.121/32 cni.projectcalico.org/podIPs:172.30.30.121/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5ccfad79-9688-4a92-b27b-6ec943d1abbd 0xc0040c9ca7 0xc0040c9ca8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ccfad79-9688-4a92-b27b-6ec943d1abbd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljzwj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljzwj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.262: INFO: Pod "webserver-deployment-847dcfb7fb-5qwz5" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5qwz5 webserver-deployment-847dcfb7fb- deployment-3883  ced8852c-b6d6-4f03-ad6d-c0f012eec2e2 20242 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7250ee415058e0daa9abf2bd502efb6c40cec7c3a21174c1d97e18e95507742a cni.projectcalico.org/podIP:172.30.11.11/32 cni.projectcalico.org/podIPs:172.30.11.11/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0040c9ed7 0xc0040c9ed8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptslc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptslc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:172.30.11.11,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b3df61816cc8411d38de63b1d888f953510a58dcbd304ea5cdba203d8c7e31e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.11.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.262: INFO: Pod "webserver-deployment-847dcfb7fb-8ksv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8ksv2 webserver-deployment-847dcfb7fb- deployment-3883  f0797085-645f-46ad-ba67-4a3cd2973ecd 20141 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e40f7 0xc0015e40f8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfm44,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfm44,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.262: INFO: Pod "webserver-deployment-847dcfb7fb-8r4v2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8r4v2 webserver-deployment-847dcfb7fb- deployment-3883  0f849d26-fd3e-4ffb-ae48-1bb81bf4cd2a 20138 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e42c7 0xc0015e42c8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x6vr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x6vr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.262: INFO: Pod "webserver-deployment-847dcfb7fb-fnplx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fnplx webserver-deployment-847dcfb7fb- deployment-3883  3e6675ae-a127-44ba-aa11-41234ca401f3 19902 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:316377a1145c920b295600af754d84ecb696af58e936b8685589302a5dc180dd cni.projectcalico.org/podIP:172.30.34.156/32 cni.projectcalico.org/podIPs:172.30.34.156/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e44b7 0xc0015e44b8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2vgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2vgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.156,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://6914b89db3a9892ac7ae62056369b3b303170f811d236d978b0fc530aa35935a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.263: INFO: Pod "webserver-deployment-847dcfb7fb-fzd4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fzd4x webserver-deployment-847dcfb7fb- deployment-3883  a17d14e2-4e6b-4650-af1e-5769a9f47b08 20221 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:8be3342483c7d7362d42828ccc1a96c812087359fe9420c9f524b61af96d2c37 cni.projectcalico.org/podIP:172.30.30.124/32 cni.projectcalico.org/podIPs:172.30.30.124/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e46e7 0xc0015e46e8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4n6gx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4n6gx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.263: INFO: Pod "webserver-deployment-847dcfb7fb-jpvll" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jpvll webserver-deployment-847dcfb7fb- deployment-3883  b38555b4-ff25-49ac-9448-1a09cc471173 20241 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:1f813dd862d1d13e680d11d050dcba947e5477576453db765c01b576f9d62c6a cni.projectcalico.org/podIP:172.30.34.164/32 cni.projectcalico.org/podIPs:172.30.34.164/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e48f7 0xc0015e48f8}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4ddz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4ddz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.264: INFO: Pod "webserver-deployment-847dcfb7fb-kdz7p" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kdz7p webserver-deployment-847dcfb7fb- deployment-3883  b63285ea-9931-4cec-ac31-650c4b64461e 19888 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9bb848c4ce1c995a876db4ca07e8c93a6244669a62dd80197a4e75efccabdd12 cni.projectcalico.org/podIP:172.30.30.119/32 cni.projectcalico.org/podIPs:172.30.30.119/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e4b07 0xc0015e4b08}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rp8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rp8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.119,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://2acab2831d61e458db9093f3849ac1f7ca00eef30eb101300efcf19a6198b6ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.264: INFO: Pod "webserver-deployment-847dcfb7fb-l8d46" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-l8d46 webserver-deployment-847dcfb7fb- deployment-3883  4b88ed7c-d1ab-40f6-999e-d686767d4ebe 20237 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:e6cd4bfa3dde61865296ca14857fc029610ad102d390036c9597040fbbff3fc1 cni.projectcalico.org/podIP:172.30.30.122/32 cni.projectcalico.org/podIPs:172.30.30.122/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e4d37 0xc0015e4d38}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xt7gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xt7gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.122,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5ab13f78afb17167cb046997a91f2fac4d7285c355a12a42130a695c5049cc11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.265: INFO: Pod "webserver-deployment-847dcfb7fb-lprhk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lprhk webserver-deployment-847dcfb7fb- deployment-3883  f119d235-c5cf-461f-9259-912de88f59e7 20199 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:a449839576b9111266aef87bc7bfaf9ffba07d68a6f1e2655bbe0f42c237485b cni.projectcalico.org/podIP:172.30.34.162/32 cni.projectcalico.org/podIPs:172.30.34.162/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e4f67 0xc0015e4f68}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cffqd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cffqd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.265: INFO: Pod "webserver-deployment-847dcfb7fb-qd7cb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qd7cb webserver-deployment-847dcfb7fb- deployment-3883  bbbe4493-3191-4c50-9114-a6da293a4425 20225 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:c862f5532bba0b91185a0472029589e65e9520af3303f6c7322a410fa9ac9eaa cni.projectcalico.org/podIP:172.30.34.161/32 cni.projectcalico.org/podIPs:172.30.34.161/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e5177 0xc0015e5178}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmtpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmtpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.265: INFO: Pod "webserver-deployment-847dcfb7fb-qm9sm" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qm9sm webserver-deployment-847dcfb7fb- deployment-3883  4a01a166-0c0e-4cd1-b1ae-7c2c0b60feb1 20145 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e5367 0xc0015e5368}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2jpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2jpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.266: INFO: Pod "webserver-deployment-847dcfb7fb-r9vmt" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r9vmt webserver-deployment-847dcfb7fb- deployment-3883  f238a008-eaa6-433a-bc03-5736a901e4f8 19883 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:765e5d022b6da3e969dc3f76cfdf1be1faf239991f236a9e203ea11f31bd3678 cni.projectcalico.org/podIP:172.30.30.118/32 cni.projectcalico.org/podIPs:172.30.30.118/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e5557 0xc0015e5558}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwlmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwlmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.118,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b6577284c229fb44b39905ee91f99b145cb54e9f83d68663402ca57e943653c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.266: INFO: Pod "webserver-deployment-847dcfb7fb-rbhsn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rbhsn webserver-deployment-847dcfb7fb- deployment-3883  d908117c-f03e-45ff-a501-2ed77b18558b 19884 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:5bd3b7284494bae363c7a4480a481baa5411a2eab28fd2a43698b4396710a394 cni.projectcalico.org/podIP:172.30.11.6/32 cni.projectcalico.org/podIPs:172.30.11.6/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e5787 0xc0015e5788}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swzcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swzcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:172.30.11.6,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://c791ed9f773052a3c8304a4842e7ac95605c23be63704ab09723029c941212fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.11.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.266: INFO: Pod "webserver-deployment-847dcfb7fb-rxj2s" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rxj2s webserver-deployment-847dcfb7fb- deployment-3883  a97c81d1-201d-4cbe-81ed-987f96e8fd28 19894 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:e531a0605343b421c81c1aeb90e8bee50935209f5eb4f4d7cc4aba6499e855ee cni.projectcalico.org/podIP:172.30.11.2/32 cni.projectcalico.org/podIPs:172.30.11.2/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e59b0 0xc0015e59b1}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjn2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjn2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:172.30.11.2,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://cd0a3b7b882110382ab78470538081bb91368152d514cee23b53058866fff94e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.11.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.267: INFO: Pod "webserver-deployment-847dcfb7fb-s9h9g" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s9h9g webserver-deployment-847dcfb7fb- deployment-3883  ace7d9c8-7425-4f2c-b7ef-f03c23215c7f 20154 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0015e5bb0 0xc0015e5bb1}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dc468,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dc468,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.267: INFO: Pod "webserver-deployment-847dcfb7fb-tzmsq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tzmsq webserver-deployment-847dcfb7fb- deployment-3883  991ea1c5-4b87-44a7-bba3-2b68e2487c46 19887 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:192e3285d9bd6c9bd1f32c327abac3cbb4c9589116becf07c60ae6aa75b88f90 cni.projectcalico.org/podIP:172.30.11.1/32 cni.projectcalico.org/podIPs:172.30.11.1/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0017b8197 0xc0017b8198}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bg8j5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bg8j5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:172.30.11.1,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://10a07a2f5cb558a2827148baf22b2b4585f5455844b4054ac218c066b89f0e51,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.11.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.268: INFO: Pod "webserver-deployment-847dcfb7fb-w6pvc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-w6pvc webserver-deployment-847dcfb7fb- deployment-3883  2dbee00c-f2d5-4c90-a749-e4dfa2988994 19879 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7d1f884f22e289161628f8c4ba27a9036a022b7f3478f930ce1a4b659129662e cni.projectcalico.org/podIP:172.30.30.117/32 cni.projectcalico.org/podIPs:172.30.30.117/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0017b9210 0xc0017b9211}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9hw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9hw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.117,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://98d306f29412bea94790478e15aa87a2c8bf4142b4fd12bc7ebf55e7c1f561e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.268: INFO: Pod "webserver-deployment-847dcfb7fb-wvr5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wvr5h webserver-deployment-847dcfb7fb- deployment-3883  e68ac38a-b32e-4e7c-a4d1-8bd73a41ac03 20181 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:2b67a13dedd771d620247ac45f135c8190aa53d9f3e5347af359f27a31afaedf cni.projectcalico.org/podIP:172.30.34.160/32 cni.projectcalico.org/podIPs:172.30.34.160/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0017b9517 0xc0017b9518}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6vz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6vz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.268: INFO: Pod "webserver-deployment-847dcfb7fb-x8nwz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-x8nwz webserver-deployment-847dcfb7fb- deployment-3883  0e343772-cf87-479a-9b02-fbb85069deef 20204 0 2021-12-07 17:22:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:94f484f077fc724b1869abcab6a1ec27c02b22476c12e303ed8f5b5d61f0b994 cni.projectcalico.org/podIP:172.30.11.12/32 cni.projectcalico.org/podIPs:172.30.11.12/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc0017b9c37 0xc0017b9c38}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:22:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87g4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87g4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 17:22:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 17:22:15.268: INFO: Pod "webserver-deployment-847dcfb7fb-ztc7w" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ztc7w webserver-deployment-847dcfb7fb- deployment-3883  4e664eda-5b1e-4b6a-af78-c12720503bd5 19905 0 2021-12-07 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:230aeb7c2dcb8640a6988df160ac847ab43418d8494516e0b99efe15ad9d88bf cni.projectcalico.org/podIP:172.30.34.154/32 cni.projectcalico.org/podIPs:172.30.34.154/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb b0747c86-4933-4fb1-a0ee-71cece063b10 0xc003936157 0xc003936158}] []  [{kube-controller-manager Update v1 2021-12-07 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0747c86-4933-4fb1-a0ee-71cece063b10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd7t4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd7t4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.154,StartTime:2021-12-07 17:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://aaf3c7b64b03bb93916787fedb03e8fd9a85a39755078e5512a32218643f2c3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:15.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3883" for this suite.

• [SLOW TEST:8.804 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":7,"skipped":105,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:15.325: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6512
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6cd7b5a6-9f70-4284-be36-68718eae1848
STEP: Creating a pod to test consume configMaps
Dec  7 17:22:15.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab" in namespace "configmap-6512" to be "Succeeded or Failed"
Dec  7 17:22:15.604: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008073ms
Dec  7 17:22:17.620: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025902581s
Dec  7 17:22:19.639: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044332561s
Dec  7 17:22:21.652: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058179904s
Dec  7 17:22:23.666: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072264666s
Dec  7 17:22:25.717: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.122733557s
Dec  7 17:22:27.734: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.139782944s
STEP: Saw pod success
Dec  7 17:22:27.734: INFO: Pod "pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab" satisfied condition "Succeeded or Failed"
Dec  7 17:22:27.744: INFO: Trying to get logs from node 10.192.217.124 pod pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:22:27.861: INFO: Waiting for pod pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab to disappear
Dec  7 17:22:27.870: INFO: Pod pod-configmaps-4f33ab42-a20b-4502-bbc2-01f4e4a03fab no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:27.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6512" for this suite.

• [SLOW TEST:12.581 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":109,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:27.907: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1533
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec  7 17:22:28.132: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1533" for this suite.

• [SLOW TEST:24.390 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":9,"skipped":120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:52.298: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:22:52.621: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fa1c0660-3492-44c0-8f73-6ec8d18a668e", Controller:(*bool)(0xc006134236), BlockOwnerDeletion:(*bool)(0xc006134237)}}
Dec  7 17:22:52.635: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"36658bd6-fa0f-4bcd-a3ce-b70c5ec43e9e", Controller:(*bool)(0xc00607d4fe), BlockOwnerDeletion:(*bool)(0xc00607d4ff)}}
Dec  7 17:22:52.676: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2f38407a-417a-43be-a9e8-27718d752258", Controller:(*bool)(0xc0061344be), BlockOwnerDeletion:(*bool)(0xc0061344bf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:22:57.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3816" for this suite.

• [SLOW TEST:5.462 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":10,"skipped":160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:22:57.760: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Dec  7 17:22:58.028: INFO: Waiting up to 5m0s for pod "var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9" in namespace "var-expansion-7417" to be "Succeeded or Failed"
Dec  7 17:22:58.038: INFO: Pod "var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.354152ms
Dec  7 17:23:00.057: INFO: Pod "var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028453089s
Dec  7 17:23:02.068: INFO: Pod "var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039754077s
STEP: Saw pod success
Dec  7 17:23:02.068: INFO: Pod "var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9" satisfied condition "Succeeded or Failed"
Dec  7 17:23:02.079: INFO: Trying to get logs from node 10.192.217.92 pod var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9 container dapi-container: <nil>
STEP: delete the pod
Dec  7 17:23:02.222: INFO: Waiting for pod var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9 to disappear
Dec  7 17:23:02.233: INFO: Pod var-expansion-2f4f8086-eff6-4caa-a308-87537b28eba9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:02.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7417" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:23:02.268: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  7 17:23:02.510: INFO: Waiting up to 5m0s for pod "downward-api-60b02a63-a708-4296-9c77-3faf831bc316" in namespace "downward-api-6284" to be "Succeeded or Failed"
Dec  7 17:23:02.520: INFO: Pod "downward-api-60b02a63-a708-4296-9c77-3faf831bc316": Phase="Pending", Reason="", readiness=false. Elapsed: 10.629008ms
Dec  7 17:23:04.537: INFO: Pod "downward-api-60b02a63-a708-4296-9c77-3faf831bc316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027464783s
STEP: Saw pod success
Dec  7 17:23:04.537: INFO: Pod "downward-api-60b02a63-a708-4296-9c77-3faf831bc316" satisfied condition "Succeeded or Failed"
Dec  7 17:23:04.548: INFO: Trying to get logs from node 10.192.217.92 pod downward-api-60b02a63-a708-4296-9c77-3faf831bc316 container dapi-container: <nil>
STEP: delete the pod
Dec  7 17:23:04.611: INFO: Waiting for pod downward-api-60b02a63-a708-4296-9c77-3faf831bc316 to disappear
Dec  7 17:23:04.621: INFO: Pod downward-api-60b02a63-a708-4296-9c77-3faf831bc316 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:04.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6284" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":205,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:23:04.685: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec  7 17:23:04.996: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:05.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8172" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":13,"skipped":214,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:23:05.195: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1207 17:23:45.599251      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  7 17:23:45.599: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec  7 17:23:45.599: INFO: Deleting pod "simpletest.rc-25p5v" in namespace "gc-1373"
Dec  7 17:23:45.638: INFO: Deleting pod "simpletest.rc-5dpjp" in namespace "gc-1373"
Dec  7 17:23:45.670: INFO: Deleting pod "simpletest.rc-6rkp6" in namespace "gc-1373"
Dec  7 17:23:45.702: INFO: Deleting pod "simpletest.rc-7nlqx" in namespace "gc-1373"
Dec  7 17:23:45.751: INFO: Deleting pod "simpletest.rc-8t58v" in namespace "gc-1373"
Dec  7 17:23:45.789: INFO: Deleting pod "simpletest.rc-h2tbb" in namespace "gc-1373"
Dec  7 17:23:45.817: INFO: Deleting pod "simpletest.rc-qmb78" in namespace "gc-1373"
Dec  7 17:23:45.857: INFO: Deleting pod "simpletest.rc-wqsmb" in namespace "gc-1373"
Dec  7 17:23:45.893: INFO: Deleting pod "simpletest.rc-xl29v" in namespace "gc-1373"
Dec  7 17:23:45.932: INFO: Deleting pod "simpletest.rc-xqztk" in namespace "gc-1373"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:45.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1373" for this suite.

• [SLOW TEST:40.820 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":14,"skipped":224,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:23:46.017: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-2924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec  7 17:23:46.307: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec  7 17:23:46.340: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  7 17:23:46.340: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec  7 17:23:46.374: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec  7 17:23:46.374: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec  7 17:23:46.408: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec  7 17:23:46.408: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec  7 17:23:53.549: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:53.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2924" for this suite.

• [SLOW TEST:7.621 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":15,"skipped":225,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:23:53.638: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:23:54.363: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 17:23:56.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774494634, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774494634, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774494634, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774494634, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:23:59.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:23:59.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8862" for this suite.
STEP: Destroying namespace "webhook-8862-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.452 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":16,"skipped":237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:00.092: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:24:00.388: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  7 17:24:00.439: INFO: Number of nodes with available pods: 0
Dec  7 17:24:00.439: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:24:01.472: INFO: Number of nodes with available pods: 0
Dec  7 17:24:01.472: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:24:02.466: INFO: Number of nodes with available pods: 0
Dec  7 17:24:02.466: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:24:03.467: INFO: Number of nodes with available pods: 3
Dec  7 17:24:03.467: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  7 17:24:03.589: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:03.589: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:03.589: INFO: Wrong image for pod: daemon-set-vcvth. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:04.619: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:04.619: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:05.621: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:05.621: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:06.618: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:06.618: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:07.619: INFO: Pod daemon-set-8c6gf is not available
Dec  7 17:24:07.619: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:07.619: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:08.618: INFO: Pod daemon-set-8c6gf is not available
Dec  7 17:24:08.618: INFO: Wrong image for pod: daemon-set-kw29p. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:08.618: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:09.617: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:10.623: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:11.621: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:11.621: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:12.616: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:12.616: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:13.620: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:13.620: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:14.620: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:14.620: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:15.622: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:15.622: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:16.623: INFO: Wrong image for pod: daemon-set-lfv65. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec  7 17:24:16.623: INFO: Pod daemon-set-nz5l5 is not available
Dec  7 17:24:18.620: INFO: Pod daemon-set-7b8sf is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  7 17:24:18.665: INFO: Number of nodes with available pods: 2
Dec  7 17:24:18.665: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:24:19.693: INFO: Number of nodes with available pods: 2
Dec  7 17:24:19.694: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:24:20.691: INFO: Number of nodes with available pods: 2
Dec  7 17:24:20.691: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:24:21.695: INFO: Number of nodes with available pods: 2
Dec  7 17:24:21.695: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:24:22.692: INFO: Number of nodes with available pods: 2
Dec  7 17:24:22.692: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:24:23.694: INFO: Number of nodes with available pods: 3
Dec  7 17:24:23.694: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2096, will wait for the garbage collector to delete the pods
Dec  7 17:24:23.845: INFO: Deleting DaemonSet.extensions daemon-set took: 30.238926ms
Dec  7 17:24:23.945: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.621716ms
Dec  7 17:24:25.862: INFO: Number of nodes with available pods: 0
Dec  7 17:24:25.862: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 17:24:25.874: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21609"},"items":null}

Dec  7 17:24:25.885: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21609"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:24:25.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2096" for this suite.

• [SLOW TEST:25.884 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":17,"skipped":272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:25.979: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 17:24:26.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee" in namespace "projected-459" to be "Succeeded or Failed"
Dec  7 17:24:26.244: INFO: Pod "downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221215ms
Dec  7 17:24:28.259: INFO: Pod "downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025076242s
STEP: Saw pod success
Dec  7 17:24:28.259: INFO: Pod "downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee" satisfied condition "Succeeded or Failed"
Dec  7 17:24:28.270: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee container client-container: <nil>
STEP: delete the pod
Dec  7 17:24:28.329: INFO: Waiting for pod downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee to disappear
Dec  7 17:24:28.339: INFO: Pod downwardapi-volume-8b402a79-2ca4-481e-9cb0-f18981020fee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:24:28.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-459" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":307,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:24:28.653: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  7 17:24:28.678: INFO: Number of nodes with available pods: 0
Dec  7 17:24:28.678: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  7 17:24:28.755: INFO: Number of nodes with available pods: 0
Dec  7 17:24:28.755: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:29.768: INFO: Number of nodes with available pods: 0
Dec  7 17:24:29.768: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:30.769: INFO: Number of nodes with available pods: 0
Dec  7 17:24:30.769: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:31.771: INFO: Number of nodes with available pods: 1
Dec  7 17:24:31.771: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  7 17:24:31.829: INFO: Number of nodes with available pods: 1
Dec  7 17:24:31.829: INFO: Number of running nodes: 0, number of available pods: 1
Dec  7 17:24:32.841: INFO: Number of nodes with available pods: 0
Dec  7 17:24:32.841: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  7 17:24:32.868: INFO: Number of nodes with available pods: 0
Dec  7 17:24:32.868: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:33.885: INFO: Number of nodes with available pods: 0
Dec  7 17:24:33.885: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:34.882: INFO: Number of nodes with available pods: 0
Dec  7 17:24:34.882: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:35.882: INFO: Number of nodes with available pods: 0
Dec  7 17:24:35.882: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:36.883: INFO: Number of nodes with available pods: 0
Dec  7 17:24:36.883: INFO: Node 10.192.217.124 is running more than one daemon pod
Dec  7 17:24:37.883: INFO: Number of nodes with available pods: 1
Dec  7 17:24:37.883: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2382, will wait for the garbage collector to delete the pods
Dec  7 17:24:37.986: INFO: Deleting DaemonSet.extensions daemon-set took: 20.672956ms
Dec  7 17:24:38.087: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.887615ms
Dec  7 17:24:40.407: INFO: Number of nodes with available pods: 0
Dec  7 17:24:40.407: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 17:24:40.424: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21784"},"items":null}

Dec  7 17:24:40.434: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21784"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:24:40.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2382" for this suite.

• [SLOW TEST:12.188 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":19,"skipped":322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:40.562: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-22fb92dc-29e8-4fe6-a64d-a80820e86f66
STEP: Creating a pod to test consume secrets
Dec  7 17:24:40.823: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe" in namespace "projected-4565" to be "Succeeded or Failed"
Dec  7 17:24:40.833: INFO: Pod "pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.163739ms
Dec  7 17:24:42.845: INFO: Pod "pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022429303s
STEP: Saw pod success
Dec  7 17:24:42.845: INFO: Pod "pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe" satisfied condition "Succeeded or Failed"
Dec  7 17:24:42.855: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:24:42.919: INFO: Waiting for pod pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe to disappear
Dec  7 17:24:42.932: INFO: Pod pod-projected-secrets-0e4476c2-530e-4fe1-8cf6-30a965d672fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:24:42.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4565" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:42.964: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1955
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1955
STEP: creating replication controller externalsvc in namespace services-1955
I1207 17:24:43.298858      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1955, replica count: 2
I1207 17:24:46.349978      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec  7 17:24:46.502: INFO: Creating new exec pod
Dec  7 17:24:50.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-1955 exec execpodqbzfv -- /bin/sh -x -c nslookup nodeport-service.services-1955.svc.cluster.local'
Dec  7 17:24:50.827: INFO: stderr: "+ nslookup nodeport-service.services-1955.svc.cluster.local\n"
Dec  7 17:24:50.827: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1955.svc.cluster.local\tcanonical name = externalsvc.services-1955.svc.cluster.local.\nName:\texternalsvc.services-1955.svc.cluster.local\nAddress: 172.21.16.177\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1955, will wait for the garbage collector to delete the pods
Dec  7 17:24:50.938: INFO: Deleting ReplicationController externalsvc took: 47.813446ms
Dec  7 17:24:51.038: INFO: Terminating ReplicationController externalsvc pods took: 100.310997ms
Dec  7 17:24:53.642: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:24:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1955" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.764 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":21,"skipped":386,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:24:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8225
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8225
STEP: Waiting until pod test-pod will start running in namespace statefulset-8225
STEP: Creating statefulset with conflicting port in namespace statefulset-8225
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8225
Dec  7 17:24:58.146: INFO: Observed stateful pod in namespace: statefulset-8225, name: ss-0, uid: b8e248c8-3443-41c1-98b0-3f107b800f66, status phase: Pending. Waiting for statefulset controller to delete.
Dec  7 17:24:58.202: INFO: Observed stateful pod in namespace: statefulset-8225, name: ss-0, uid: b8e248c8-3443-41c1-98b0-3f107b800f66, status phase: Failed. Waiting for statefulset controller to delete.
Dec  7 17:24:58.224: INFO: Observed stateful pod in namespace: statefulset-8225, name: ss-0, uid: b8e248c8-3443-41c1-98b0-3f107b800f66, status phase: Failed. Waiting for statefulset controller to delete.
Dec  7 17:24:58.234: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8225
STEP: Removing pod with conflicting port in namespace statefulset-8225
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8225 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 17:25:00.298: INFO: Deleting all statefulset in ns statefulset-8225
Dec  7 17:25:00.308: INFO: Scaling statefulset ss to 0
Dec  7 17:25:10.370: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 17:25:10.380: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:25:10.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8225" for this suite.

• [SLOW TEST:16.762 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":22,"skipped":387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:25:10.490: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6151
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4a80db55-6bda-4257-a195-93a89d905201
STEP: Creating the pod
Dec  7 17:25:10.778: INFO: The status of Pod pod-projected-configmaps-ef81089a-a10a-488b-b557-17c012d41e0a is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:25:12.795: INFO: The status of Pod pod-projected-configmaps-ef81089a-a10a-488b-b557-17c012d41e0a is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:25:14.795: INFO: The status of Pod pod-projected-configmaps-ef81089a-a10a-488b-b557-17c012d41e0a is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-4a80db55-6bda-4257-a195-93a89d905201
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:26:22.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6151" for this suite.

• [SLOW TEST:71.755 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":410,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:26:22.245: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  7 17:26:22.508: INFO: Waiting up to 5m0s for pod "downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada" in namespace "downward-api-378" to be "Succeeded or Failed"
Dec  7 17:26:22.517: INFO: Pod "downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada": Phase="Pending", Reason="", readiness=false. Elapsed: 9.562119ms
Dec  7 17:26:24.532: INFO: Pod "downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023947473s
STEP: Saw pod success
Dec  7 17:26:24.532: INFO: Pod "downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada" satisfied condition "Succeeded or Failed"
Dec  7 17:26:24.542: INFO: Trying to get logs from node 10.192.217.92 pod downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada container dapi-container: <nil>
STEP: delete the pod
Dec  7 17:26:24.642: INFO: Waiting for pod downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada to disappear
Dec  7 17:26:24.652: INFO: Pod downward-api-f0e19eb2-820f-4593-a14b-6c2005b8bada no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:26:24.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-378" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":411,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:26:24.687: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2477, will wait for the garbage collector to delete the pods
Dec  7 17:26:29.037: INFO: Deleting Job.batch foo took: 20.392022ms
Dec  7 17:26:29.237: INFO: Terminating Job.batch foo pods took: 200.160229ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:27:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2477" for this suite.

• [SLOW TEST:35.909 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":25,"skipped":432,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:27:00.596: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9863
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-ad1807c4-5746-4b22-9236-5b916d0328da in namespace container-probe-9863
Dec  7 17:27:02.887: INFO: Started pod liveness-ad1807c4-5746-4b22-9236-5b916d0328da in namespace container-probe-9863
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 17:27:02.901: INFO: Initial restart count of pod liveness-ad1807c4-5746-4b22-9236-5b916d0328da is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:31:02.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9863" for this suite.

• [SLOW TEST:242.434 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":436,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:31:03.031: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-588
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  7 17:31:03.278: INFO: Waiting up to 5m0s for pod "pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1" in namespace "emptydir-588" to be "Succeeded or Failed"
Dec  7 17:31:03.288: INFO: Pod "pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.21885ms
Dec  7 17:31:05.303: INFO: Pod "pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02570186s
STEP: Saw pod success
Dec  7 17:31:05.303: INFO: Pod "pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1" satisfied condition "Succeeded or Failed"
Dec  7 17:31:05.314: INFO: Trying to get logs from node 10.192.217.92 pod pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1 container test-container: <nil>
STEP: delete the pod
Dec  7 17:31:05.423: INFO: Waiting for pod pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1 to disappear
Dec  7 17:31:05.432: INFO: Pod pod-81d7357d-bd5b-4c2a-a46d-5252c41ef4f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:31:05.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-588" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":453,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:31:05.467: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5543
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:31:05.725: INFO: created pod
Dec  7 17:31:05.725: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5543" to be "Succeeded or Failed"
Dec  7 17:31:05.735: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070304ms
Dec  7 17:31:07.751: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025284366s
Dec  7 17:31:09.767: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041949495s
STEP: Saw pod success
Dec  7 17:31:09.767: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec  7 17:31:39.770: INFO: polling logs
Dec  7 17:31:39.821: INFO: Pod logs: 
2021/12/07 17:31:07 OK: Got token
2021/12/07 17:31:07 validating with in-cluster discovery
2021/12/07 17:31:07 OK: got issuer https://kubernetes.default.svc
2021/12/07 17:31:07 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-5543:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1638898865, NotBefore:1638898265, IssuedAt:1638898265, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5543", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3f5e05aa-54ba-488a-ae8e-c75dedd8d6eb"}}}
2021/12/07 17:31:07 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2021/12/07 17:31:07 OK: Validated signature on JWT
2021/12/07 17:31:07 OK: Got valid claims from token!
2021/12/07 17:31:07 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-5543:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1638898865, NotBefore:1638898265, IssuedAt:1638898265, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5543", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3f5e05aa-54ba-488a-ae8e-c75dedd8d6eb"}}}

Dec  7 17:31:39.821: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:31:39.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5543" for this suite.

• [SLOW TEST:34.420 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":28,"skipped":458,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:31:39.887: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Dec  7 17:31:40.128: INFO: Waiting up to 5m0s for pod "client-containers-c377cab7-60d8-4dae-86b7-567a9e123402" in namespace "containers-1310" to be "Succeeded or Failed"
Dec  7 17:31:40.138: INFO: Pod "client-containers-c377cab7-60d8-4dae-86b7-567a9e123402": Phase="Pending", Reason="", readiness=false. Elapsed: 9.962594ms
Dec  7 17:31:42.155: INFO: Pod "client-containers-c377cab7-60d8-4dae-86b7-567a9e123402": Phase="Running", Reason="", readiness=true. Elapsed: 2.026236233s
Dec  7 17:31:44.169: INFO: Pod "client-containers-c377cab7-60d8-4dae-86b7-567a9e123402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040877979s
STEP: Saw pod success
Dec  7 17:31:44.169: INFO: Pod "client-containers-c377cab7-60d8-4dae-86b7-567a9e123402" satisfied condition "Succeeded or Failed"
Dec  7 17:31:44.203: INFO: Trying to get logs from node 10.192.217.124 pod client-containers-c377cab7-60d8-4dae-86b7-567a9e123402 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:31:44.327: INFO: Waiting for pod client-containers-c377cab7-60d8-4dae-86b7-567a9e123402 to disappear
Dec  7 17:31:44.337: INFO: Pod client-containers-c377cab7-60d8-4dae-86b7-567a9e123402 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:31:44.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1310" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":462,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:31:44.372: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:31:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-465" for this suite.

• [SLOW TEST:13.670 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":30,"skipped":471,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:31:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-00d45905-df55-4216-90ae-4f6bed841b41 in namespace container-probe-6388
Dec  7 17:32:02.332: INFO: Started pod busybox-00d45905-df55-4216-90ae-4f6bed841b41 in namespace container-probe-6388
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 17:32:02.342: INFO: Initial restart count of pod busybox-00d45905-df55-4216-90ae-4f6bed841b41 is 0
Dec  7 17:32:50.726: INFO: Restart count of pod container-probe-6388/busybox-00d45905-df55-4216-90ae-4f6bed841b41 is now 1 (48.383919623s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:32:50.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6388" for this suite.

• [SLOW TEST:52.760 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:32:50.803: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:32:51.019: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  7 17:32:53.118: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:32:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8794" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":32,"skipped":554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:32:54.187: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:32:54.440: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6fc9bd58-c861-4e76-818d-c93deb114b52" in namespace "security-context-test-7720" to be "Succeeded or Failed"
Dec  7 17:32:54.451: INFO: Pod "busybox-privileged-false-6fc9bd58-c861-4e76-818d-c93deb114b52": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588218ms
Dec  7 17:32:56.470: INFO: Pod "busybox-privileged-false-6fc9bd58-c861-4e76-818d-c93deb114b52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029288711s
Dec  7 17:32:56.470: INFO: Pod "busybox-privileged-false-6fc9bd58-c861-4e76-818d-c93deb114b52" satisfied condition "Succeeded or Failed"
Dec  7 17:32:56.499: INFO: Got logs for pod "busybox-privileged-false-6fc9bd58-c861-4e76-818d-c93deb114b52": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:32:56.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7720" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:32:56.549: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9a51bb9a-5315-4d5d-84ea-f58262b03e53
STEP: Creating a pod to test consume secrets
Dec  7 17:32:56.807: INFO: Waiting up to 5m0s for pod "pod-secrets-cef542f4-621b-4662-9359-1b7861500859" in namespace "secrets-2384" to be "Succeeded or Failed"
Dec  7 17:32:56.817: INFO: Pod "pod-secrets-cef542f4-621b-4662-9359-1b7861500859": Phase="Pending", Reason="", readiness=false. Elapsed: 10.208138ms
Dec  7 17:32:58.832: INFO: Pod "pod-secrets-cef542f4-621b-4662-9359-1b7861500859": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024895296s
Dec  7 17:33:00.847: INFO: Pod "pod-secrets-cef542f4-621b-4662-9359-1b7861500859": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04019119s
STEP: Saw pod success
Dec  7 17:33:00.847: INFO: Pod "pod-secrets-cef542f4-621b-4662-9359-1b7861500859" satisfied condition "Succeeded or Failed"
Dec  7 17:33:00.858: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-cef542f4-621b-4662-9359-1b7861500859 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:33:00.928: INFO: Waiting for pod pod-secrets-cef542f4-621b-4662-9359-1b7861500859 to disappear
Dec  7 17:33:00.939: INFO: Pod pod-secrets-cef542f4-621b-4662-9359-1b7861500859 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:00.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2384" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":635,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:00.976: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:01.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4964" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":35,"skipped":646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Dec  7 17:33:01.609: INFO: Creating simple deployment test-deployment-4rqhd
Dec  7 17:33:01.687: INFO: deployment "test-deployment-4rqhd" doesn't have the required revision set
Dec  7 17:33:03.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495181, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495181, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495181, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495181, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-4rqhd-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Dec  7 17:33:05.779: INFO: Deployment test-deployment-4rqhd has Conditions: [{Available True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4rqhd-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Dec  7 17:33:05.809: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495184, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495184, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495184, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495181, loc:(*time.Location)(0xa0a1d40)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4rqhd-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Dec  7 17:33:05.814: INFO: Observed &Deployment event: ADDED
Dec  7 17:33:05.814: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4rqhd-794dd694d8"}
Dec  7 17:33:05.815: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.815: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4rqhd-794dd694d8"}
Dec  7 17:33:05.815: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  7 17:33:05.815: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.815: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  7 17:33:05.815: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4rqhd-794dd694d8" is progressing.}
Dec  7 17:33:05.815: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.816: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  7 17:33:05.816: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4rqhd-794dd694d8" has successfully progressed.}
Dec  7 17:33:05.816: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.816: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  7 17:33:05.816: INFO: Observed Deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4rqhd-794dd694d8" has successfully progressed.}
Dec  7 17:33:05.816: INFO: Found Deployment test-deployment-4rqhd in namespace deployment-8753 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  7 17:33:05.816: INFO: Deployment test-deployment-4rqhd has an updated status
STEP: patching the Statefulset Status
Dec  7 17:33:05.816: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  7 17:33:05.839: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Dec  7 17:33:05.845: INFO: Observed &Deployment event: ADDED
Dec  7 17:33:05.845: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4rqhd-794dd694d8"}
Dec  7 17:33:05.845: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.845: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4rqhd-794dd694d8"}
Dec  7 17:33:05.845: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  7 17:33:05.845: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.845: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec  7 17:33:05.845: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:01 +0000 UTC 2021-12-07 17:33:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4rqhd-794dd694d8" is progressing.}
Dec  7 17:33:05.846: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.846: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  7 17:33:05.846: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4rqhd-794dd694d8" has successfully progressed.}
Dec  7 17:33:05.846: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.846: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec  7 17:33:05.846: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-12-07 17:33:04 +0000 UTC 2021-12-07 17:33:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4rqhd-794dd694d8" has successfully progressed.}
Dec  7 17:33:05.846: INFO: Observed deployment test-deployment-4rqhd in namespace deployment-8753 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  7 17:33:05.846: INFO: Observed &Deployment event: MODIFIED
Dec  7 17:33:05.846: INFO: Found deployment test-deployment-4rqhd in namespace deployment-8753 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec  7 17:33:05.846: INFO: Deployment test-deployment-4rqhd has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 17:33:05.872: INFO: Deployment "test-deployment-4rqhd":
&Deployment{ObjectMeta:{test-deployment-4rqhd  deployment-8753  9da7a1cf-c42d-4049-89ab-2bfeef93e764 23370 1 2021-12-07 17:33:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-12-07 17:33:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:33:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2021-12-07 17:33:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00772fd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  7 17:33:05.927: INFO: New ReplicaSet "test-deployment-4rqhd-794dd694d8" of Deployment "test-deployment-4rqhd":
&ReplicaSet{ObjectMeta:{test-deployment-4rqhd-794dd694d8  deployment-8753  e1d3de51-af87-45f7-9a3b-9e8cd663e5f9 23362 1 2021-12-07 17:33:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4rqhd 9da7a1cf-c42d-4049-89ab-2bfeef93e764 0xc0032a2130 0xc0032a2131}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:33:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9da7a1cf-c42d-4049-89ab-2bfeef93e764\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:33:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032a21d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  7 17:33:05.938: INFO: Pod "test-deployment-4rqhd-794dd694d8-l64gr" is available:
&Pod{ObjectMeta:{test-deployment-4rqhd-794dd694d8-l64gr test-deployment-4rqhd-794dd694d8- deployment-8753  36459bdc-ee23-4e65-8107-4df2123d9604 23361 0 2021-12-07 17:33:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:4b122fb70bdc8c85aea99b6a1b5e19e8b602a392c4c546a036b88ae252dd723a cni.projectcalico.org/podIP:172.30.30.86/32 cni.projectcalico.org/podIPs:172.30.30.86/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-4rqhd-794dd694d8 e1d3de51-af87-45f7-9a3b-9e8cd663e5f9 0xc0032d8290 0xc0032d8291}] []  [{kube-controller-manager Update v1 2021-12-07 17:33:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1d3de51-af87-45f7-9a3b-9e8cd663e5f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:33:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:33:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjz65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjz65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.86,StartTime:2021-12-07 17:33:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:33:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://62a812881a25ae060ecc8f70af27d182ef33b27a6027c8109e8d471bf4ec7296,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:05.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8753" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":36,"skipped":677,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:05.976: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:33:06.207: INFO: Got root ca configmap in namespace "svcaccounts-991"
Dec  7 17:33:06.230: INFO: Deleted root ca configmap in namespace "svcaccounts-991"
STEP: waiting for a new root ca configmap created
Dec  7 17:33:06.747: INFO: Recreated root ca configmap in namespace "svcaccounts-991"
Dec  7 17:33:06.764: INFO: Updated root ca configmap in namespace "svcaccounts-991"
STEP: waiting for the root ca configmap reconciled
Dec  7 17:33:07.282: INFO: Reconciled root ca configmap in namespace "svcaccounts-991"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:07.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-991" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":37,"skipped":682,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:07.318: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3696
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:33:07.594: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  7 17:33:12.615: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Dec  7 17:33:12.647: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Dec  7 17:33:12.695: INFO: observed ReplicaSet test-rs in namespace replicaset-3696 with ReadyReplicas 1, AvailableReplicas 1
Dec  7 17:33:12.695: INFO: observed ReplicaSet test-rs in namespace replicaset-3696 with ReadyReplicas 1, AvailableReplicas 1
Dec  7 17:33:12.751: INFO: observed ReplicaSet test-rs in namespace replicaset-3696 with ReadyReplicas 1, AvailableReplicas 1
Dec  7 17:33:12.751: INFO: observed ReplicaSet test-rs in namespace replicaset-3696 with ReadyReplicas 1, AvailableReplicas 1
Dec  7 17:33:15.494: INFO: observed ReplicaSet test-rs in namespace replicaset-3696 with ReadyReplicas 2, AvailableReplicas 2
Dec  7 17:33:15.996: INFO: observed Replicaset test-rs in namespace replicaset-3696 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:15.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3696" for this suite.

• [SLOW TEST:8.717 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":38,"skipped":684,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:16.036: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Dec  7 17:33:16.297: INFO: Waiting up to 5m0s for pod "client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98" in namespace "containers-1817" to be "Succeeded or Failed"
Dec  7 17:33:16.308: INFO: Pod "client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98": Phase="Pending", Reason="", readiness=false. Elapsed: 10.766921ms
Dec  7 17:33:18.322: INFO: Pod "client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024854896s
Dec  7 17:33:20.361: INFO: Pod "client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063221126s
STEP: Saw pod success
Dec  7 17:33:20.361: INFO: Pod "client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98" satisfied condition "Succeeded or Failed"
Dec  7 17:33:20.371: INFO: Trying to get logs from node 10.192.217.124 pod client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:33:20.438: INFO: Waiting for pod client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98 to disappear
Dec  7 17:33:20.449: INFO: Pod client-containers-cc6bf1d3-843f-48bd-9382-37661ba70a98 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:20.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1817" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":687,"failed":0}

------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:20.487: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec  7 17:33:20.758: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.758: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.777: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.777: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.818: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.818: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.865: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:20.865: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec  7 17:33:23.249: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  7 17:33:23.249: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec  7 17:33:23.597: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec  7 17:33:23.625: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 0
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.630: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.631: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.631: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.643: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.643: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.705: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.705: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:23.716: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:23.716: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:25.278: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:25.278: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:25.324: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
STEP: listing Deployments
Dec  7 17:33:25.347: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec  7 17:33:25.376: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec  7 17:33:25.395: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:25.402: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:25.428: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:25.497: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:25.512: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:27.304: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:27.324: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:27.410: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec  7 17:33:29.060: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec  7 17:33:29.147: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:29.147: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:29.147: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:29.147: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:29.147: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 1
Dec  7 17:33:29.149: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:29.149: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:29.149: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 2
Dec  7 17:33:29.150: INFO: observed Deployment test-deployment in namespace deployment-496 with ReadyReplicas 3
STEP: deleting the Deployment
Dec  7 17:33:29.215: INFO: observed event type MODIFIED
Dec  7 17:33:29.215: INFO: observed event type MODIFIED
Dec  7 17:33:29.215: INFO: observed event type MODIFIED
Dec  7 17:33:29.216: INFO: observed event type MODIFIED
Dec  7 17:33:29.216: INFO: observed event type MODIFIED
Dec  7 17:33:29.216: INFO: observed event type MODIFIED
Dec  7 17:33:29.216: INFO: observed event type MODIFIED
Dec  7 17:33:29.218: INFO: observed event type MODIFIED
Dec  7 17:33:29.218: INFO: observed event type MODIFIED
Dec  7 17:33:29.218: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 17:33:29.232: INFO: Log out all the ReplicaSets if there is no deployment created
Dec  7 17:33:29.248: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-496  b62807fb-a2f3-4e3e-8282-0e4a203f869f 23775 4 2021-12-07 17:33:23 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e3b34b61-158c-46d4-91f3-32bcfd3ee671 0xc0003771c7 0xc0003771c8}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:33:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b34b61-158c-46d4-91f3-32bcfd3ee671\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:33:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000377290 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec  7 17:33:29.267: INFO: pod: "test-deployment-56c98d85f9-nxz96":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-nxz96 test-deployment-56c98d85f9- deployment-496  ed08fdc2-18ce-4492-aa98-9726037d0d65 23771 0 2021-12-07 17:33:23 +0000 UTC 2021-12-07 17:33:30 +0000 UTC 0xc000377da8 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:595bf09983afb584f2e45c41298dfa15b8f5068894da809d57d5a3af643dbefb cni.projectcalico.org/podIP:172.30.34.140/32 cni.projectcalico.org/podIPs:172.30.34.140/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 b62807fb-a2f3-4e3e-8282-0e4a203f869f 0xc000377e67 0xc000377e68}] []  [{kube-controller-manager Update v1 2021-12-07 17:33:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b62807fb-a2f3-4e3e-8282-0e4a203f869f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:33:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgl7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgl7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.140,StartTime:2021-12-07 17:33:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:33:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:containerd://6df263e6bd6e8375096767b8723f3c6705c7368a426a6614219b383d7260607d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  7 17:33:29.267: INFO: pod: "test-deployment-56c98d85f9-srfvf":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-srfvf test-deployment-56c98d85f9- deployment-496  fca00d64-ebdb-4579-a60b-faa9b32d93dc 23735 0 2021-12-07 17:33:25 +0000 UTC 2021-12-07 17:33:28 +0000 UTC 0xc000056c30 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:d86516bf5c49d2019bf378512632150edce9cec07738d37c69a9c40d210efcc1 cni.projectcalico.org/podIP:172.30.30.90/32 cni.projectcalico.org/podIPs:172.30.30.90/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 b62807fb-a2f3-4e3e-8282-0e4a203f869f 0xc000056cf7 0xc000056cf8}] []  [{kube-controller-manager Update v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b62807fb-a2f3-4e3e-8282-0e4a203f869f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-12-07 17:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vt9df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt9df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:,StartTime:2021-12-07 17:33:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  7 17:33:29.267: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-496  cc9f64d9-6571-40a5-82f7-2680b584f826 23669 3 2021-12-07 17:33:20 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e3b34b61-158c-46d4-91f3-32bcfd3ee671 0xc000377307 0xc000377308}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:33:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b34b61-158c-46d4-91f3-32bcfd3ee671\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0003774a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec  7 17:33:29.278: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-496  4d5e046c-afa1-404b-ac66-ab8d96227bd8 23767 2 2021-12-07 17:33:25 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e3b34b61-158c-46d4-91f3-32bcfd3ee671 0xc000377557 0xc000377558}] []  [{kube-controller-manager Update apps/v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3b34b61-158c-46d4-91f3-32bcfd3ee671\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 17:33:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000377670 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec  7 17:33:29.290: INFO: pod: "test-deployment-d4dfddfbf-k7nb9":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-k7nb9 test-deployment-d4dfddfbf- deployment-496  414c0317-46a9-407a-b596-69cfca83ec3e 23766 0 2021-12-07 17:33:27 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:88aaa49a6d0e67a266788f184923af07b2555631df912dfaa6b8bd7e72d904cd cni.projectcalico.org/podIP:172.30.11.22/32 cni.projectcalico.org/podIPs:172.30.11.22/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4d5e046c-afa1-404b-ac66-ab8d96227bd8 0xc002e8f517 0xc002e8f518}] []  [{kube-controller-manager Update v1 2021-12-07 17:33:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5e046c-afa1-404b-ac66-ab8d96227bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:33:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:33:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pxkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pxkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:172.30.11.22,StartTime:2021-12-07 17:33:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:33:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://ee3eac3c0c9aac8cc9813f1ef387ac8dda55bcf5ae53aff0b2392aab9f4cc9cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.11.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec  7 17:33:29.291: INFO: pod: "test-deployment-d4dfddfbf-vst9j":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-vst9j test-deployment-d4dfddfbf- deployment-496  f4273131-2e6f-4e7e-8089-e02bcb69d19c 23731 0 2021-12-07 17:33:25 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:9b3b08cbfecaf879233c2216e575820defb10f3c260c620645e1b5cb2c672fd3 cni.projectcalico.org/podIP:172.30.34.139/32 cni.projectcalico.org/podIPs:172.30.34.139/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4d5e046c-afa1-404b-ac66-ab8d96227bd8 0xc002e8f767 0xc002e8f768}] []  [{kube-controller-manager Update v1 2021-12-07 17:33:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5e046c-afa1-404b-ac66-ab8d96227bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 17:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 17:33:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnwhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnwhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 17:33:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.139,StartTime:2021-12-07 17:33:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 17:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://10e85a61c631d6ee4818a7115dadfb66d06892ae78fd2773deefd9d97926f2b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:29.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-496" for this suite.

• [SLOW TEST:8.864 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":40,"skipped":687,"failed":0}
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:29.351: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec  7 17:33:29.642: INFO: observed Pod pod-test in namespace pods-4398 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec  7 17:33:29.651: INFO: observed Pod pod-test in namespace pods-4398 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  }]
Dec  7 17:33:29.686: INFO: observed Pod pod-test in namespace pods-4398 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  }]
Dec  7 17:33:30.712: INFO: observed Pod pod-test in namespace pods-4398 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  }]
Dec  7 17:33:31.621: INFO: Found Pod pod-test in namespace pods-4398 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 17:33:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Dec  7 17:33:31.659: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec  7 17:33:31.732: INFO: observed event type ADDED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.732: INFO: observed event type MODIFIED
Dec  7 17:33:31.734: INFO: observed event type MODIFIED
Dec  7 17:33:33.631: INFO: observed event type MODIFIED
Dec  7 17:33:34.227: INFO: observed event type MODIFIED
Dec  7 17:33:34.643: INFO: observed event type MODIFIED
Dec  7 17:33:34.946: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:34.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4398" for this suite.

• [SLOW TEST:5.653 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":41,"skipped":687,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:35.004: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1410
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-78a5eba4-49a9-4e24-bf03-e49c6a869955
STEP: Creating secret with name s-test-opt-upd-e1948637-68cf-4b55-8f31-41a0f8c6ba3b
STEP: Creating the pod
Dec  7 17:33:35.341: INFO: The status of Pod pod-projected-secrets-02d4d7f8-7a0d-4f05-9384-97470a899f03 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:33:37.353: INFO: The status of Pod pod-projected-secrets-02d4d7f8-7a0d-4f05-9384-97470a899f03 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:33:39.356: INFO: The status of Pod pod-projected-secrets-02d4d7f8-7a0d-4f05-9384-97470a899f03 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-78a5eba4-49a9-4e24-bf03-e49c6a869955
STEP: Updating secret s-test-opt-upd-e1948637-68cf-4b55-8f31-41a0f8c6ba3b
STEP: Creating secret with name s-test-opt-create-fe4438e9-0085-4eb2-8df3-a78092dca245
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:33:43.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1410" for this suite.

• [SLOW TEST:8.738 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":693,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:33:43.742: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  7 17:33:44.012: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 17:34:44.134: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Dec  7 17:34:44.207: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  7 17:34:44.246: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  7 17:34:44.295: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  7 17:34:44.311: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec  7 17:34:44.364: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec  7 17:34:44.381: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:34:58.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4524" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:75.116 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":43,"skipped":713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:34:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-598
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-67864d8b-fb4a-415d-a9f5-ab855bd15a59
STEP: Creating a pod to test consume secrets
Dec  7 17:34:59.234: INFO: Waiting up to 5m0s for pod "pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20" in namespace "secrets-598" to be "Succeeded or Failed"
Dec  7 17:34:59.258: INFO: Pod "pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20": Phase="Pending", Reason="", readiness=false. Elapsed: 23.80606ms
Dec  7 17:35:01.277: INFO: Pod "pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042938757s
Dec  7 17:35:03.293: INFO: Pod "pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058517072s
STEP: Saw pod success
Dec  7 17:35:03.293: INFO: Pod "pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20" satisfied condition "Succeeded or Failed"
Dec  7 17:35:03.303: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:35:03.400: INFO: Waiting for pod pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20 to disappear
Dec  7 17:35:03.410: INFO: Pod pod-secrets-d36ed64c-75cb-47c8-a975-c903d7fd3f20 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:03.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-598" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":760,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-74j9p in namespace proxy-861
I1207 17:35:03.721651      21 runners.go:190] Created replication controller with name: proxy-service-74j9p, namespace: proxy-861, replica count: 1
I1207 17:35:04.772473      21 runners.go:190] proxy-service-74j9p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1207 17:35:05.772972      21 runners.go:190] proxy-service-74j9p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1207 17:35:06.774106      21 runners.go:190] proxy-service-74j9p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:35:06.796: INFO: setup took 3.126213568s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  7 17:35:06.876: INFO: (0) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 79.532943ms)
Dec  7 17:35:06.896: INFO: (0) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 98.177606ms)
Dec  7 17:35:06.896: INFO: (0) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 98.984936ms)
Dec  7 17:35:06.900: INFO: (0) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 103.103283ms)
Dec  7 17:35:06.901: INFO: (0) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 104.517981ms)
Dec  7 17:35:06.901: INFO: (0) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 104.24759ms)
Dec  7 17:35:06.904: INFO: (0) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 106.255357ms)
Dec  7 17:35:06.904: INFO: (0) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 106.650948ms)
Dec  7 17:35:06.935: INFO: (0) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 137.74823ms)
Dec  7 17:35:06.935: INFO: (0) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 137.322014ms)
Dec  7 17:35:06.935: INFO: (0) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 138.02891ms)
Dec  7 17:35:06.944: INFO: (0) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 147.318149ms)
Dec  7 17:35:06.946: INFO: (0) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 149.089872ms)
Dec  7 17:35:06.947: INFO: (0) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 149.358311ms)
Dec  7 17:35:06.947: INFO: (0) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 149.897378ms)
Dec  7 17:35:06.948: INFO: (0) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 151.648609ms)
Dec  7 17:35:06.964: INFO: (1) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 15.168666ms)
Dec  7 17:35:06.967: INFO: (1) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 18.218385ms)
Dec  7 17:35:06.968: INFO: (1) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 18.825093ms)
Dec  7 17:35:06.971: INFO: (1) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 21.887321ms)
Dec  7 17:35:06.996: INFO: (1) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 46.690882ms)
Dec  7 17:35:06.999: INFO: (1) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 48.976848ms)
Dec  7 17:35:07.001: INFO: (1) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 52.439003ms)
Dec  7 17:35:07.001: INFO: (1) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 51.321556ms)
Dec  7 17:35:07.001: INFO: (1) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 51.483695ms)
Dec  7 17:35:07.002: INFO: (1) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 51.958745ms)
Dec  7 17:35:07.002: INFO: (1) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 52.260455ms)
Dec  7 17:35:07.009: INFO: (1) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 59.131108ms)
Dec  7 17:35:07.012: INFO: (1) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 62.82232ms)
Dec  7 17:35:07.027: INFO: (1) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 76.494451ms)
Dec  7 17:35:07.027: INFO: (1) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 76.448473ms)
Dec  7 17:35:07.034: INFO: (1) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 84.229116ms)
Dec  7 17:35:07.050: INFO: (2) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 16.382597ms)
Dec  7 17:35:07.057: INFO: (2) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.500192ms)
Dec  7 17:35:07.057: INFO: (2) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.637536ms)
Dec  7 17:35:07.057: INFO: (2) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.194096ms)
Dec  7 17:35:07.058: INFO: (2) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 24.190878ms)
Dec  7 17:35:07.058: INFO: (2) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 24.068652ms)
Dec  7 17:35:07.058: INFO: (2) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 24.009038ms)
Dec  7 17:35:07.059: INFO: (2) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 24.767251ms)
Dec  7 17:35:07.059: INFO: (2) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 25.341924ms)
Dec  7 17:35:07.059: INFO: (2) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 24.965237ms)
Dec  7 17:35:07.063: INFO: (2) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 29.109274ms)
Dec  7 17:35:07.071: INFO: (2) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 37.037805ms)
Dec  7 17:35:07.072: INFO: (2) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 38.159553ms)
Dec  7 17:35:07.073: INFO: (2) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 38.687277ms)
Dec  7 17:35:07.073: INFO: (2) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 39.054056ms)
Dec  7 17:35:07.073: INFO: (2) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 39.128629ms)
Dec  7 17:35:07.088: INFO: (3) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 14.728106ms)
Dec  7 17:35:07.096: INFO: (3) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.691239ms)
Dec  7 17:35:07.096: INFO: (3) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.728716ms)
Dec  7 17:35:07.097: INFO: (3) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 22.932896ms)
Dec  7 17:35:07.098: INFO: (3) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 23.95668ms)
Dec  7 17:35:07.098: INFO: (3) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 24.152292ms)
Dec  7 17:35:07.098: INFO: (3) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 24.177974ms)
Dec  7 17:35:07.099: INFO: (3) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 25.223382ms)
Dec  7 17:35:07.099: INFO: (3) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 25.352811ms)
Dec  7 17:35:07.099: INFO: (3) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 25.462538ms)
Dec  7 17:35:07.105: INFO: (3) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 30.971824ms)
Dec  7 17:35:07.109: INFO: (3) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 34.721746ms)
Dec  7 17:35:07.112: INFO: (3) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 38.306201ms)
Dec  7 17:35:07.113: INFO: (3) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 38.765543ms)
Dec  7 17:35:07.113: INFO: (3) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 39.085502ms)
Dec  7 17:35:07.113: INFO: (3) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 39.084624ms)
Dec  7 17:35:07.129: INFO: (4) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 15.737264ms)
Dec  7 17:35:07.136: INFO: (4) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.444345ms)
Dec  7 17:35:07.136: INFO: (4) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 22.664134ms)
Dec  7 17:35:07.136: INFO: (4) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.206244ms)
Dec  7 17:35:07.136: INFO: (4) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 23.21487ms)
Dec  7 17:35:07.136: INFO: (4) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 23.586178ms)
Dec  7 17:35:07.137: INFO: (4) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.843271ms)
Dec  7 17:35:07.137: INFO: (4) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 23.805458ms)
Dec  7 17:35:07.137: INFO: (4) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 23.824543ms)
Dec  7 17:35:07.138: INFO: (4) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 25.123234ms)
Dec  7 17:35:07.138: INFO: (4) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 24.876153ms)
Dec  7 17:35:07.141: INFO: (4) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 28.120979ms)
Dec  7 17:35:07.145: INFO: (4) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 31.77446ms)
Dec  7 17:35:07.145: INFO: (4) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 32.185714ms)
Dec  7 17:35:07.145: INFO: (4) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 32.273256ms)
Dec  7 17:35:07.146: INFO: (4) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 32.763836ms)
Dec  7 17:35:07.171: INFO: (5) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 25.060021ms)
Dec  7 17:35:07.171: INFO: (5) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 25.270439ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 25.405432ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 25.390487ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 25.266275ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 25.430845ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 25.296682ms)
Dec  7 17:35:07.172: INFO: (5) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 25.680463ms)
Dec  7 17:35:07.178: INFO: (5) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 30.993933ms)
Dec  7 17:35:07.178: INFO: (5) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 31.528503ms)
Dec  7 17:35:07.186: INFO: (5) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 39.975331ms)
Dec  7 17:35:07.187: INFO: (5) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 40.555203ms)
Dec  7 17:35:07.187: INFO: (5) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 40.020895ms)
Dec  7 17:35:07.187: INFO: (5) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 40.166742ms)
Dec  7 17:35:07.187: INFO: (5) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 41.276648ms)
Dec  7 17:35:07.188: INFO: (5) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 41.658797ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 47.260463ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 47.28583ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 47.184425ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 47.204497ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 47.26416ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 47.509546ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 47.254975ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 47.398329ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 47.526623ms)
Dec  7 17:35:07.235: INFO: (6) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 47.527327ms)
Dec  7 17:35:07.237: INFO: (6) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 49.265877ms)
Dec  7 17:35:07.238: INFO: (6) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 49.841072ms)
Dec  7 17:35:07.242: INFO: (6) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 53.900132ms)
Dec  7 17:35:07.242: INFO: (6) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 53.925877ms)
Dec  7 17:35:07.242: INFO: (6) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 54.155086ms)
Dec  7 17:35:07.242: INFO: (6) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 53.877843ms)
Dec  7 17:35:07.265: INFO: (7) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.238696ms)
Dec  7 17:35:07.269: INFO: (7) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 26.475206ms)
Dec  7 17:35:07.269: INFO: (7) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 26.38701ms)
Dec  7 17:35:07.269: INFO: (7) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 26.413901ms)
Dec  7 17:35:07.269: INFO: (7) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 26.613856ms)
Dec  7 17:35:07.269: INFO: (7) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 26.674198ms)
Dec  7 17:35:07.270: INFO: (7) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 28.039301ms)
Dec  7 17:35:07.271: INFO: (7) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 28.525787ms)
Dec  7 17:35:07.271: INFO: (7) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 28.66853ms)
Dec  7 17:35:07.271: INFO: (7) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 28.758235ms)
Dec  7 17:35:07.275: INFO: (7) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 32.40086ms)
Dec  7 17:35:07.276: INFO: (7) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 33.594408ms)
Dec  7 17:35:07.290: INFO: (7) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 47.023666ms)
Dec  7 17:35:07.290: INFO: (7) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 47.351744ms)
Dec  7 17:35:07.290: INFO: (7) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 47.557231ms)
Dec  7 17:35:07.304: INFO: (7) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 61.495085ms)
Dec  7 17:35:07.320: INFO: (8) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 15.641277ms)
Dec  7 17:35:07.324: INFO: (8) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 19.462219ms)
Dec  7 17:35:07.324: INFO: (8) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 19.842308ms)
Dec  7 17:35:07.324: INFO: (8) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 20.350181ms)
Dec  7 17:35:07.328: INFO: (8) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 23.633063ms)
Dec  7 17:35:07.328: INFO: (8) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.994241ms)
Dec  7 17:35:07.328: INFO: (8) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.784502ms)
Dec  7 17:35:07.328: INFO: (8) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 24.381277ms)
Dec  7 17:35:07.328: INFO: (8) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 24.278344ms)
Dec  7 17:35:07.329: INFO: (8) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 24.305824ms)
Dec  7 17:35:07.329: INFO: (8) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 24.727555ms)
Dec  7 17:35:07.335: INFO: (8) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 30.383037ms)
Dec  7 17:35:07.339: INFO: (8) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 34.681842ms)
Dec  7 17:35:07.339: INFO: (8) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 35.186692ms)
Dec  7 17:35:07.340: INFO: (8) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 35.262149ms)
Dec  7 17:35:07.340: INFO: (8) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 36.038377ms)
Dec  7 17:35:07.357: INFO: (9) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 15.995288ms)
Dec  7 17:35:07.360: INFO: (9) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 19.229159ms)
Dec  7 17:35:07.361: INFO: (9) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 19.814371ms)
Dec  7 17:35:07.361: INFO: (9) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 20.031327ms)
Dec  7 17:35:07.362: INFO: (9) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 21.058215ms)
Dec  7 17:35:07.362: INFO: (9) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 21.170459ms)
Dec  7 17:35:07.362: INFO: (9) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 20.908083ms)
Dec  7 17:35:07.363: INFO: (9) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.207431ms)
Dec  7 17:35:07.363: INFO: (9) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.094574ms)
Dec  7 17:35:07.363: INFO: (9) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 21.912577ms)
Dec  7 17:35:07.377: INFO: (9) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 35.536502ms)
Dec  7 17:35:07.377: INFO: (9) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 35.611988ms)
Dec  7 17:35:07.377: INFO: (9) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 35.757166ms)
Dec  7 17:35:07.377: INFO: (9) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 36.055349ms)
Dec  7 17:35:07.377: INFO: (9) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 36.200455ms)
Dec  7 17:35:07.392: INFO: (9) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 51.059926ms)
Dec  7 17:35:07.408: INFO: (10) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 15.905454ms)
Dec  7 17:35:07.412: INFO: (10) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 19.467372ms)
Dec  7 17:35:07.413: INFO: (10) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 20.654484ms)
Dec  7 17:35:07.414: INFO: (10) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 21.035918ms)
Dec  7 17:35:07.414: INFO: (10) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 21.704405ms)
Dec  7 17:35:07.414: INFO: (10) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 21.804512ms)
Dec  7 17:35:07.414: INFO: (10) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 22.035168ms)
Dec  7 17:35:07.415: INFO: (10) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 22.624566ms)
Dec  7 17:35:07.415: INFO: (10) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.932331ms)
Dec  7 17:35:07.415: INFO: (10) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 23.124757ms)
Dec  7 17:35:07.416: INFO: (10) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 23.501023ms)
Dec  7 17:35:07.421: INFO: (10) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 29.017153ms)
Dec  7 17:35:07.425: INFO: (10) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 32.633997ms)
Dec  7 17:35:07.425: INFO: (10) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 32.831054ms)
Dec  7 17:35:07.425: INFO: (10) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 32.975174ms)
Dec  7 17:35:07.426: INFO: (10) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 33.762285ms)
Dec  7 17:35:07.456: INFO: (11) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 29.48995ms)
Dec  7 17:35:07.458: INFO: (11) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 31.368642ms)
Dec  7 17:35:07.460: INFO: (11) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 32.685333ms)
Dec  7 17:35:07.460: INFO: (11) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 33.074821ms)
Dec  7 17:35:07.460: INFO: (11) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 33.861426ms)
Dec  7 17:35:07.461: INFO: (11) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 34.225915ms)
Dec  7 17:35:07.461: INFO: (11) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 34.176812ms)
Dec  7 17:35:07.462: INFO: (11) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 34.985331ms)
Dec  7 17:35:07.471: INFO: (11) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 43.569698ms)
Dec  7 17:35:07.473: INFO: (11) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 45.905545ms)
Dec  7 17:35:07.474: INFO: (11) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 46.813514ms)
Dec  7 17:35:07.474: INFO: (11) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 47.800842ms)
Dec  7 17:35:07.476: INFO: (11) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 49.207226ms)
Dec  7 17:35:07.478: INFO: (11) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 51.384549ms)
Dec  7 17:35:07.481: INFO: (11) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 54.226692ms)
Dec  7 17:35:07.481: INFO: (11) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 54.790088ms)
Dec  7 17:35:07.504: INFO: (12) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.518871ms)
Dec  7 17:35:07.505: INFO: (12) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 22.901499ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 25.009411ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 25.19349ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 25.187321ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 25.356917ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 25.395802ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 25.590996ms)
Dec  7 17:35:07.507: INFO: (12) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 25.96044ms)
Dec  7 17:35:07.508: INFO: (12) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 26.494178ms)
Dec  7 17:35:07.510: INFO: (12) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 28.586709ms)
Dec  7 17:35:07.512: INFO: (12) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 30.566148ms)
Dec  7 17:35:07.514: INFO: (12) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 32.164869ms)
Dec  7 17:35:07.515: INFO: (12) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 32.851373ms)
Dec  7 17:35:07.515: INFO: (12) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 33.410838ms)
Dec  7 17:35:07.515: INFO: (12) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 33.325533ms)
Dec  7 17:35:07.532: INFO: (13) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 16.70033ms)
Dec  7 17:35:07.537: INFO: (13) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 21.479553ms)
Dec  7 17:35:07.537: INFO: (13) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 21.670982ms)
Dec  7 17:35:07.537: INFO: (13) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 22.111769ms)
Dec  7 17:35:07.537: INFO: (13) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 21.270786ms)
Dec  7 17:35:07.538: INFO: (13) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 22.334614ms)
Dec  7 17:35:07.538: INFO: (13) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 22.272792ms)
Dec  7 17:35:07.538: INFO: (13) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.00282ms)
Dec  7 17:35:07.538: INFO: (13) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.583299ms)
Dec  7 17:35:07.538: INFO: (13) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 22.680741ms)
Dec  7 17:35:07.541: INFO: (13) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 25.812671ms)
Dec  7 17:35:07.548: INFO: (13) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 32.019326ms)
Dec  7 17:35:07.548: INFO: (13) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 32.580738ms)
Dec  7 17:35:07.548: INFO: (13) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 32.625871ms)
Dec  7 17:35:07.548: INFO: (13) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 33.021119ms)
Dec  7 17:35:07.566: INFO: (13) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 50.631306ms)
Dec  7 17:35:07.592: INFO: (14) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 25.43009ms)
Dec  7 17:35:07.592: INFO: (14) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 25.322672ms)
Dec  7 17:35:07.592: INFO: (14) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 25.826253ms)
Dec  7 17:35:07.593: INFO: (14) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 27.319353ms)
Dec  7 17:35:07.593: INFO: (14) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 27.356971ms)
Dec  7 17:35:07.594: INFO: (14) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 27.398262ms)
Dec  7 17:35:07.594: INFO: (14) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 27.934246ms)
Dec  7 17:35:07.595: INFO: (14) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 28.496007ms)
Dec  7 17:35:07.610: INFO: (14) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 43.153695ms)
Dec  7 17:35:07.610: INFO: (14) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 43.269064ms)
Dec  7 17:35:07.610: INFO: (14) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 43.304688ms)
Dec  7 17:35:07.610: INFO: (14) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 43.341113ms)
Dec  7 17:35:07.613: INFO: (14) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 47.031238ms)
Dec  7 17:35:07.614: INFO: (14) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 47.379523ms)
Dec  7 17:35:07.614: INFO: (14) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 47.338749ms)
Dec  7 17:35:07.614: INFO: (14) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 47.809546ms)
Dec  7 17:35:07.632: INFO: (15) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 17.745465ms)
Dec  7 17:35:07.635: INFO: (15) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 20.324791ms)
Dec  7 17:35:07.635: INFO: (15) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 21.125104ms)
Dec  7 17:35:07.636: INFO: (15) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 21.42196ms)
Dec  7 17:35:07.636: INFO: (15) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 21.957934ms)
Dec  7 17:35:07.636: INFO: (15) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 21.722242ms)
Dec  7 17:35:07.638: INFO: (15) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 23.653423ms)
Dec  7 17:35:07.639: INFO: (15) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 24.17365ms)
Dec  7 17:35:07.639: INFO: (15) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 24.281952ms)
Dec  7 17:35:07.639: INFO: (15) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 24.287958ms)
Dec  7 17:35:07.641: INFO: (15) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 26.146668ms)
Dec  7 17:35:07.644: INFO: (15) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 29.565365ms)
Dec  7 17:35:07.646: INFO: (15) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 31.969263ms)
Dec  7 17:35:07.647: INFO: (15) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 32.49309ms)
Dec  7 17:35:07.647: INFO: (15) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 32.896871ms)
Dec  7 17:35:07.648: INFO: (15) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 33.59952ms)
Dec  7 17:35:07.665: INFO: (16) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 16.812546ms)
Dec  7 17:35:07.668: INFO: (16) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 19.388333ms)
Dec  7 17:35:07.668: INFO: (16) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 20.119847ms)
Dec  7 17:35:07.670: INFO: (16) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 22.620611ms)
Dec  7 17:35:07.670: INFO: (16) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 22.369193ms)
Dec  7 17:35:07.671: INFO: (16) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.397624ms)
Dec  7 17:35:07.671: INFO: (16) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 22.470635ms)
Dec  7 17:35:07.671: INFO: (16) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 22.601119ms)
Dec  7 17:35:07.671: INFO: (16) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 22.743988ms)
Dec  7 17:35:07.671: INFO: (16) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 22.648353ms)
Dec  7 17:35:07.676: INFO: (16) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 27.913122ms)
Dec  7 17:35:07.677: INFO: (16) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 28.629275ms)
Dec  7 17:35:07.678: INFO: (16) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 29.865715ms)
Dec  7 17:35:07.679: INFO: (16) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 30.571448ms)
Dec  7 17:35:07.680: INFO: (16) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 31.743471ms)
Dec  7 17:35:07.680: INFO: (16) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 32.158123ms)
Dec  7 17:35:07.697: INFO: (17) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 16.068235ms)
Dec  7 17:35:07.700: INFO: (17) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 20.166956ms)
Dec  7 17:35:07.701: INFO: (17) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 20.42691ms)
Dec  7 17:35:07.701: INFO: (17) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 19.806964ms)
Dec  7 17:35:07.701: INFO: (17) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 19.976498ms)
Dec  7 17:35:07.702: INFO: (17) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 20.924963ms)
Dec  7 17:35:07.702: INFO: (17) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 20.793067ms)
Dec  7 17:35:07.702: INFO: (17) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 21.04234ms)
Dec  7 17:35:07.702: INFO: (17) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 21.083065ms)
Dec  7 17:35:07.702: INFO: (17) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 20.772558ms)
Dec  7 17:35:07.703: INFO: (17) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 22.148919ms)
Dec  7 17:35:07.707: INFO: (17) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 26.065182ms)
Dec  7 17:35:07.709: INFO: (17) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 28.041652ms)
Dec  7 17:35:07.709: INFO: (17) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 28.154475ms)
Dec  7 17:35:07.712: INFO: (17) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 30.5316ms)
Dec  7 17:35:07.712: INFO: (17) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 30.674144ms)
Dec  7 17:35:07.729: INFO: (18) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 16.795621ms)
Dec  7 17:35:07.730: INFO: (18) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 18.144425ms)
Dec  7 17:35:07.731: INFO: (18) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 18.549775ms)
Dec  7 17:35:07.731: INFO: (18) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 18.968209ms)
Dec  7 17:35:07.732: INFO: (18) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 19.322533ms)
Dec  7 17:35:07.732: INFO: (18) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 19.826539ms)
Dec  7 17:35:07.732: INFO: (18) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 20.220765ms)
Dec  7 17:35:07.742: INFO: (18) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 29.563833ms)
Dec  7 17:35:07.742: INFO: (18) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 29.608993ms)
Dec  7 17:35:07.742: INFO: (18) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 29.541916ms)
Dec  7 17:35:07.742: INFO: (18) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 29.871141ms)
Dec  7 17:35:07.762: INFO: (18) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 49.680803ms)
Dec  7 17:35:07.762: INFO: (18) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 49.903183ms)
Dec  7 17:35:07.762: INFO: (18) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 50.149531ms)
Dec  7 17:35:07.762: INFO: (18) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 50.323182ms)
Dec  7 17:35:07.762: INFO: (18) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 50.540302ms)
Dec  7 17:35:07.779: INFO: (19) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:160/proxy/: foo (200; 16.680243ms)
Dec  7 17:35:07.782: INFO: (19) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:162/proxy/: bar (200; 19.294146ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:162/proxy/: bar (200; 19.858917ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:1080/proxy/rewriteme">test</... (200; 20.242399ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/http:proxy-service-74j9p-lss5j:1080/proxy/rewriteme">t... (200; 20.419116ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:443/proxy/tlsrewriteme... (200; 20.428621ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:462/proxy/: tls qux (200; 20.436271ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/: <a href="/api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j/proxy/rewriteme">test</a> (200; 20.414341ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/proxy-service-74j9p-lss5j:160/proxy/: foo (200; 20.679971ms)
Dec  7 17:35:07.783: INFO: (19) /api/v1/namespaces/proxy-861/pods/https:proxy-service-74j9p-lss5j:460/proxy/: tls baz (200; 20.571898ms)
Dec  7 17:35:07.788: INFO: (19) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname2/proxy/: bar (200; 24.956172ms)
Dec  7 17:35:07.789: INFO: (19) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname2/proxy/: bar (200; 26.055284ms)
Dec  7 17:35:07.792: INFO: (19) /api/v1/namespaces/proxy-861/services/proxy-service-74j9p:portname1/proxy/: foo (200; 29.068879ms)
Dec  7 17:35:07.792: INFO: (19) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname2/proxy/: tls qux (200; 29.642957ms)
Dec  7 17:35:07.793: INFO: (19) /api/v1/namespaces/proxy-861/services/http:proxy-service-74j9p:portname1/proxy/: foo (200; 30.401282ms)
Dec  7 17:35:07.797: INFO: (19) /api/v1/namespaces/proxy-861/services/https:proxy-service-74j9p:tlsportname1/proxy/: tls baz (200; 33.929892ms)
STEP: deleting ReplicationController proxy-service-74j9p in namespace proxy-861, will wait for the garbage collector to delete the pods
Dec  7 17:35:07.876: INFO: Deleting ReplicationController proxy-service-74j9p took: 17.960973ms
Dec  7 17:35:07.977: INFO: Terminating ReplicationController proxy-service-74j9p pods took: 100.705168ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:10.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-861" for this suite.

• [SLOW TEST:7.347 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":45,"skipped":777,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:10.794: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 17:35:11.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0" in namespace "projected-9930" to be "Succeeded or Failed"
Dec  7 17:35:11.057: INFO: Pod "downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.719481ms
Dec  7 17:35:13.074: INFO: Pod "downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027463512s
Dec  7 17:35:15.089: INFO: Pod "downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042537233s
STEP: Saw pod success
Dec  7 17:35:15.090: INFO: Pod "downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0" satisfied condition "Succeeded or Failed"
Dec  7 17:35:15.100: INFO: Trying to get logs from node 10.192.217.124 pod downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0 container client-container: <nil>
STEP: delete the pod
Dec  7 17:35:15.242: INFO: Waiting for pod downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0 to disappear
Dec  7 17:35:15.262: INFO: Pod downwardapi-volume-333a698b-1752-4f7a-b746-9bfadb481fe0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:15.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9930" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":792,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:15.300: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  7 17:35:15.566: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:35:17.584: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:35:19.581: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  7 17:35:19.619: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:35:21.634: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec  7 17:35:21.665: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  7 17:35:21.677: INFO: Pod pod-with-prestop-http-hook still exists
Dec  7 17:35:23.678: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  7 17:35:23.691: INFO: Pod pod-with-prestop-http-hook still exists
Dec  7 17:35:25.678: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  7 17:35:25.693: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:25.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3135" for this suite.

• [SLOW TEST:10.462 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":794,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:25.766: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  7 17:35:26.028: INFO: Waiting up to 5m0s for pod "pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85" in namespace "emptydir-3273" to be "Succeeded or Failed"
Dec  7 17:35:26.039: INFO: Pod "pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85": Phase="Pending", Reason="", readiness=false. Elapsed: 10.49551ms
Dec  7 17:35:28.052: INFO: Pod "pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023693411s
STEP: Saw pod success
Dec  7 17:35:28.052: INFO: Pod "pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85" satisfied condition "Succeeded or Failed"
Dec  7 17:35:28.063: INFO: Trying to get logs from node 10.192.217.92 pod pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85 container test-container: <nil>
STEP: delete the pod
Dec  7 17:35:28.128: INFO: Waiting for pod pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85 to disappear
Dec  7 17:35:28.138: INFO: Pod pod-e19504df-ea51-4705-ad0b-ba9fdffb7c85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:28.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3273" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":799,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:28.175: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  7 17:35:28.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-9325 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Dec  7 17:35:28.607: INFO: stderr: ""
Dec  7 17:35:28.607: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Dec  7 17:35:28.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-9325 delete pods e2e-test-httpd-pod'
Dec  7 17:35:31.610: INFO: stderr: ""
Dec  7 17:35:31.610: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:31.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9325" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":49,"skipped":807,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:31.667: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-8cfdb03c-053a-478b-b9b4-ca394bf1cd01
STEP: Creating a pod to test consume secrets
Dec  7 17:35:31.937: INFO: Waiting up to 5m0s for pod "pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5" in namespace "secrets-3783" to be "Succeeded or Failed"
Dec  7 17:35:31.947: INFO: Pod "pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.820948ms
Dec  7 17:35:33.962: INFO: Pod "pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024546876s
STEP: Saw pod success
Dec  7 17:35:33.962: INFO: Pod "pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5" satisfied condition "Succeeded or Failed"
Dec  7 17:35:33.972: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:35:34.074: INFO: Waiting for pod pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5 to disappear
Dec  7 17:35:34.089: INFO: Pod pod-secrets-64e95fc4-58e0-4445-b93b-851dd15214e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:34.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3783" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":827,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:34.176: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6943
STEP: creating service affinity-clusterip in namespace services-6943
STEP: creating replication controller affinity-clusterip in namespace services-6943
I1207 17:35:34.456867      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6943, replica count: 3
I1207 17:35:37.508641      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:35:37.538: INFO: Creating new exec pod
Dec  7 17:35:42.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6943 exec execpod-affinityntpkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec  7 17:35:42.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec  7 17:35:42.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:35:42.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6943 exec execpod-affinityntpkb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.172.194 80'
Dec  7 17:35:43.205: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.172.194 80\nConnection to 172.21.172.194 80 port [tcp/http] succeeded!\n"
Dec  7 17:35:43.205: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:35:43.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6943 exec execpod-affinityntpkb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.172.194:80/ ; done'
Dec  7 17:35:43.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.172.194:80/\n"
Dec  7 17:35:43.546: INFO: stdout: "\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8\naffinity-clusterip-9xtc8"
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Received response from host: affinity-clusterip-9xtc8
Dec  7 17:35:43.546: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6943, will wait for the garbage collector to delete the pods
Dec  7 17:35:43.661: INFO: Deleting ReplicationController affinity-clusterip took: 18.05867ms
Dec  7 17:35:43.762: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.248751ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:46.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6943" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.423 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":51,"skipped":829,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:46.603: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  7 17:35:47.069: INFO: Number of nodes with available pods: 0
Dec  7 17:35:47.069: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:35:48.105: INFO: Number of nodes with available pods: 0
Dec  7 17:35:48.105: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:35:49.104: INFO: Number of nodes with available pods: 1
Dec  7 17:35:49.104: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:35:50.109: INFO: Number of nodes with available pods: 3
Dec  7 17:35:50.109: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Dec  7 17:35:50.131: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Dec  7 17:35:50.160: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Dec  7 17:35:50.166: INFO: Observed &DaemonSet event: ADDED
Dec  7 17:35:50.166: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.166: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.166: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.167: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.168: INFO: Found daemon set daemon-set in namespace daemonsets-8149 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  7 17:35:50.168: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Dec  7 17:35:50.190: INFO: Observed &DaemonSet event: ADDED
Dec  7 17:35:50.191: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.191: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.191: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.191: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.193: INFO: Observed daemon set daemon-set in namespace daemonsets-8149 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  7 17:35:50.193: INFO: Observed &DaemonSet event: MODIFIED
Dec  7 17:35:50.193: INFO: Found daemon set daemon-set in namespace daemonsets-8149 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec  7 17:35:50.193: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8149, will wait for the garbage collector to delete the pods
Dec  7 17:35:50.283: INFO: Deleting DaemonSet.extensions daemon-set took: 18.226617ms
Dec  7 17:35:50.384: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.588541ms
Dec  7 17:35:52.801: INFO: Number of nodes with available pods: 0
Dec  7 17:35:52.801: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 17:35:52.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24959"},"items":null}

Dec  7 17:35:52.821: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24959"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:35:52.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8149" for this suite.

• [SLOW TEST:6.319 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":52,"skipped":850,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:35:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec  7 17:35:57.735: INFO: Successfully updated pod "adopt-release--1-9wl7q"
STEP: Checking that the Job readopts the Pod
Dec  7 17:35:57.736: INFO: Waiting up to 15m0s for pod "adopt-release--1-9wl7q" in namespace "job-8944" to be "adopted"
Dec  7 17:35:57.746: INFO: Pod "adopt-release--1-9wl7q": Phase="Running", Reason="", readiness=true. Elapsed: 10.5467ms
Dec  7 17:35:59.785: INFO: Pod "adopt-release--1-9wl7q": Phase="Running", Reason="", readiness=true. Elapsed: 2.04912181s
Dec  7 17:35:59.785: INFO: Pod "adopt-release--1-9wl7q" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec  7 17:36:00.323: INFO: Successfully updated pod "adopt-release--1-9wl7q"
STEP: Checking that the Job releases the Pod
Dec  7 17:36:00.323: INFO: Waiting up to 15m0s for pod "adopt-release--1-9wl7q" in namespace "job-8944" to be "released"
Dec  7 17:36:00.333: INFO: Pod "adopt-release--1-9wl7q": Phase="Running", Reason="", readiness=true. Elapsed: 10.283279ms
Dec  7 17:36:02.348: INFO: Pod "adopt-release--1-9wl7q": Phase="Running", Reason="", readiness=true. Elapsed: 2.024918763s
Dec  7 17:36:02.348: INFO: Pod "adopt-release--1-9wl7q" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:36:02.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8944" for this suite.

• [SLOW TEST:9.478 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":53,"skipped":853,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:36:02.401: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  7 17:36:02.680: INFO: Waiting up to 5m0s for pod "pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21" in namespace "emptydir-3262" to be "Succeeded or Failed"
Dec  7 17:36:02.691: INFO: Pod "pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21": Phase="Pending", Reason="", readiness=false. Elapsed: 10.585713ms
Dec  7 17:36:04.712: INFO: Pod "pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031309936s
STEP: Saw pod success
Dec  7 17:36:04.712: INFO: Pod "pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21" satisfied condition "Succeeded or Failed"
Dec  7 17:36:04.722: INFO: Trying to get logs from node 10.192.217.92 pod pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21 container test-container: <nil>
STEP: delete the pod
Dec  7 17:36:04.790: INFO: Waiting for pod pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21 to disappear
Dec  7 17:36:04.799: INFO: Pod pod-cd81e3e3-3119-4593-ab73-5bc3da8e8b21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:36:04.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3262" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":860,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:36:04.835: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 in namespace container-probe-6735
Dec  7 17:36:07.100: INFO: Started pod liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 in namespace container-probe-6735
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 17:36:07.110: INFO: Initial restart count of pod liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is 0
Dec  7 17:36:27.288: INFO: Restart count of pod container-probe-6735/liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is now 1 (20.177018648s elapsed)
Dec  7 17:36:47.446: INFO: Restart count of pod container-probe-6735/liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is now 2 (40.335501569s elapsed)
Dec  7 17:37:07.653: INFO: Restart count of pod container-probe-6735/liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is now 3 (1m0.542149198s elapsed)
Dec  7 17:37:27.862: INFO: Restart count of pod container-probe-6735/liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is now 4 (1m20.751651352s elapsed)
Dec  7 17:38:40.498: INFO: Restart count of pod container-probe-6735/liveness-6028fea0-a0cf-43e6-a628-2d5cbe6226a1 is now 5 (2m33.386983636s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:38:40.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6735" for this suite.

• [SLOW TEST:155.789 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":869,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:38:40.624: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:07.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2575" for this suite.

• [SLOW TEST:27.016 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":872,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:07.641: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:39:08.269: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec  7 17:39:10.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495548, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495548, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495548, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495548, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:39:13.414: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:39:13.430: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:16.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3638" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.512 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":57,"skipped":880,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:17.153: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-b58ff4c9-8eed-41cd-8366-bc6fc9a952d8
STEP: Creating a pod to test consume configMaps
Dec  7 17:39:17.450: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73" in namespace "projected-266" to be "Succeeded or Failed"
Dec  7 17:39:17.461: INFO: Pod "pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.637872ms
Dec  7 17:39:19.476: INFO: Pod "pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026613881s
STEP: Saw pod success
Dec  7 17:39:19.476: INFO: Pod "pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73" satisfied condition "Succeeded or Failed"
Dec  7 17:39:19.486: INFO: Trying to get logs from node 10.192.217.124 pod pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:39:19.622: INFO: Waiting for pod pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73 to disappear
Dec  7 17:39:19.632: INFO: Pod pod-projected-configmaps-9e9cbca4-2631-4d94-881d-8d3bf34faa73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:19.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-266" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":888,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:19.677: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-885
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:39:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-885" for this suite.

• [SLOW TEST:7.377 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":59,"skipped":908,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:27.053: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-59
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ceb6461e-ecbd-482b-bd6e-ac396d239321
STEP: Creating a pod to test consume configMaps
Dec  7 17:39:27.319: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5" in namespace "projected-59" to be "Succeeded or Failed"
Dec  7 17:39:27.328: INFO: Pod "pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.437074ms
Dec  7 17:39:29.346: INFO: Pod "pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026912015s
Dec  7 17:39:31.362: INFO: Pod "pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042986888s
STEP: Saw pod success
Dec  7 17:39:31.362: INFO: Pod "pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5" satisfied condition "Succeeded or Failed"
Dec  7 17:39:31.372: INFO: Trying to get logs from node 10.192.217.124 pod pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:39:31.442: INFO: Waiting for pod pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5 to disappear
Dec  7 17:39:31.453: INFO: Pod pod-projected-configmaps-39958094-2dc3-47a8-af0c-b24a4b8c23f5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:31.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-59" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:31.491: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Dec  7 17:39:31.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3246 create -f -'
Dec  7 17:39:33.191: INFO: stderr: ""
Dec  7 17:39:33.191: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec  7 17:39:33.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3246 diff -f -'
Dec  7 17:39:33.438: INFO: rc: 1
Dec  7 17:39:33.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3246 delete -f -'
Dec  7 17:39:33.544: INFO: stderr: ""
Dec  7 17:39:33.544: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:33.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3246" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":61,"skipped":937,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:33.586: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-4943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:33.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4943" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":62,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:33.942: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-922/configmap-test-e76d053c-57f5-4125-bb7e-ac149f020658
STEP: Creating a pod to test consume configMaps
Dec  7 17:39:34.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538" in namespace "configmap-922" to be "Succeeded or Failed"
Dec  7 17:39:34.227: INFO: Pod "pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538": Phase="Pending", Reason="", readiness=false. Elapsed: 10.549771ms
Dec  7 17:39:36.243: INFO: Pod "pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026795865s
Dec  7 17:39:38.260: INFO: Pod "pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043164676s
STEP: Saw pod success
Dec  7 17:39:38.260: INFO: Pod "pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538" satisfied condition "Succeeded or Failed"
Dec  7 17:39:38.270: INFO: Trying to get logs from node 10.192.217.124 pod pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538 container env-test: <nil>
STEP: delete the pod
Dec  7 17:39:38.334: INFO: Waiting for pod pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538 to disappear
Dec  7 17:39:38.344: INFO: Pod pod-configmaps-65eaba16-c6fb-4769-be7b-c08ea2817538 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:39:38.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-922" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":984,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:39:38.383: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:40:38.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3124" for this suite.

• [SLOW TEST:60.338 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":993,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:40:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Dec  7 17:40:38.958: INFO: created test-podtemplate-1
Dec  7 17:40:38.976: INFO: created test-podtemplate-2
Dec  7 17:40:38.993: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec  7 17:40:39.013: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec  7 17:40:39.078: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:40:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7567" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":65,"skipped":997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:40:39.137: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Dec  7 17:40:41.451: INFO: running pods: 0 < 1
Dec  7 17:40:43.466: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:40:45.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3945" for this suite.

• [SLOW TEST:6.508 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":66,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:40:45.645: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:40:46.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 17:40:48.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495646, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495646, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774495646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:40:51.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:40:51.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8547" for this suite.
STEP: Destroying namespace "webhook-8547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.409 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":67,"skipped":1095,"failed":0}
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:40:52.054: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-557
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec  7 17:40:56.335: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-557 PodName:pod-sharedvolume-41366fcb-4b8e-4ee2-af86-5f16dfc94689 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 17:40:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 17:40:56.577: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:40:56.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-557" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":68,"skipped":1095,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:40:56.616: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9856
STEP: creating replication controller nodeport-test in namespace services-9856
I1207 17:40:56.913621      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9856, replica count: 2
Dec  7 17:40:59.964: INFO: Creating new exec pod
I1207 17:40:59.964557      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:41:03.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9856 exec execpodsw5k7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  7 17:41:03.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  7 17:41:03.403: INFO: stdout: ""
Dec  7 17:41:04.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9856 exec execpodsw5k7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec  7 17:41:04.660: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  7 17:41:04.660: INFO: stdout: "nodeport-test-ngnmd"
Dec  7 17:41:04.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9856 exec execpodsw5k7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.238.178 80'
Dec  7 17:41:04.954: INFO: stderr: "+ nc -v -t -w 2 172.21.238.178 80\n+ echo hostName\nConnection to 172.21.238.178 80 port [tcp/http] succeeded!\n"
Dec  7 17:41:04.954: INFO: stdout: "nodeport-test-rxzl9"
Dec  7 17:41:04.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9856 exec execpodsw5k7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.124 32457'
Dec  7 17:41:05.226: INFO: stderr: "+ nc -v -t -w 2 10.192.217.124 32457\n+ echo hostName\nConnection to 10.192.217.124 32457 port [tcp/*] succeeded!\n"
Dec  7 17:41:05.227: INFO: stdout: "nodeport-test-rxzl9"
Dec  7 17:41:05.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9856 exec execpodsw5k7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 32457'
Dec  7 17:41:05.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 32457\nConnection to 10.192.217.92 32457 port [tcp/*] succeeded!\n"
Dec  7 17:41:05.504: INFO: stdout: "nodeport-test-rxzl9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:05.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9856" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.934 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":69,"skipped":1104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:05.551: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-ce372264-b03f-4f89-b134-4579d6fb66cd
STEP: Creating a pod to test consume secrets
Dec  7 17:41:05.851: INFO: Waiting up to 5m0s for pod "pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f" in namespace "secrets-1745" to be "Succeeded or Failed"
Dec  7 17:41:05.860: INFO: Pod "pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.711697ms
Dec  7 17:41:07.875: INFO: Pod "pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023977259s
Dec  7 17:41:09.891: INFO: Pod "pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039902158s
STEP: Saw pod success
Dec  7 17:41:09.891: INFO: Pod "pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f" satisfied condition "Succeeded or Failed"
Dec  7 17:41:09.902: INFO: Trying to get logs from node 10.192.217.124 pod pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:41:10.020: INFO: Waiting for pod pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f to disappear
Dec  7 17:41:10.031: INFO: Pod pod-secrets-3a1c6a71-dc59-4b75-a4d3-cae4769b264f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1745" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1138,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:10.072: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3355
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec  7 17:41:10.308: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 17:41:15.200: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:32.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3355" for this suite.

• [SLOW TEST:22.816 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":71,"skipped":1144,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:32.888: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:33.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7309" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":72,"skipped":1150,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:33.321: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  7 17:41:33.580: INFO: Waiting up to 5m0s for pod "pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a" in namespace "emptydir-3934" to be "Succeeded or Failed"
Dec  7 17:41:33.590: INFO: Pod "pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.504621ms
Dec  7 17:41:35.633: INFO: Pod "pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053895322s
Dec  7 17:41:37.649: INFO: Pod "pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069683254s
STEP: Saw pod success
Dec  7 17:41:37.649: INFO: Pod "pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a" satisfied condition "Succeeded or Failed"
Dec  7 17:41:37.660: INFO: Trying to get logs from node 10.192.217.124 pod pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a container test-container: <nil>
STEP: delete the pod
Dec  7 17:41:37.719: INFO: Waiting for pod pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a to disappear
Dec  7 17:41:37.733: INFO: Pod pod-8d74c3fb-d499-41d9-bb42-3ca4e4dfe24a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:37.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3934" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:37.769: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  7 17:41:38.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1819  e9524814-632e-41ff-980a-3d74055de42b 26565 0 2021-12-07 17:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-12-07 17:41:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 17:41:38.117: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1819  e9524814-632e-41ff-980a-3d74055de42b 26566 0 2021-12-07 17:41:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-12-07 17:41:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:38.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1819" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":74,"skipped":1206,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:38.157: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-86c632e0-2b93-4542-abee-e26a919cd589
STEP: Creating a pod to test consume configMaps
Dec  7 17:41:38.427: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c" in namespace "projected-4331" to be "Succeeded or Failed"
Dec  7 17:41:38.438: INFO: Pod "pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.216137ms
Dec  7 17:41:40.451: INFO: Pod "pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023727818s
Dec  7 17:41:42.466: INFO: Pod "pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039230614s
STEP: Saw pod success
Dec  7 17:41:42.466: INFO: Pod "pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c" satisfied condition "Succeeded or Failed"
Dec  7 17:41:42.476: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:41:42.585: INFO: Waiting for pod pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c to disappear
Dec  7 17:41:42.595: INFO: Pod pod-projected-configmaps-3bcc9ce7-ae1f-49a3-bf58-5839fee0b02c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:42.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4331" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1207,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:42.626: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:54.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-674" for this suite.

• [SLOW TEST:11.425 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":76,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-8775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:58.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8775" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":77,"skipped":1253,"failed":0}
S
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:58.410: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  7 17:41:58.758: INFO: starting watch
STEP: patching
STEP: updating
Dec  7 17:41:58.794: INFO: waiting for watch events with expected annotations
Dec  7 17:41:58.794: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:41:58.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-256" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":78,"skipped":1254,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:41:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-8226
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8226 to expose endpoints map[]
Dec  7 17:41:59.298: INFO: successfully validated that service multi-endpoint-test in namespace services-8226 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8226
Dec  7 17:41:59.334: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:42:01.350: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:42:03.349: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8226 to expose endpoints map[pod1:[100]]
Dec  7 17:42:03.397: INFO: successfully validated that service multi-endpoint-test in namespace services-8226 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8226
Dec  7 17:42:03.424: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:42:05.441: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:42:07.440: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8226 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  7 17:42:07.528: INFO: successfully validated that service multi-endpoint-test in namespace services-8226 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Dec  7 17:42:07.528: INFO: Creating new exec pod
Dec  7 17:42:12.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8226 exec execpodlrwfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Dec  7 17:42:12.829: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec  7 17:42:12.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:42:12.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8226 exec execpodlrwfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.232.0 80'
Dec  7 17:42:13.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.232.0 80\nConnection to 172.21.232.0 80 port [tcp/http] succeeded!\n"
Dec  7 17:42:13.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:42:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8226 exec execpodlrwfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Dec  7 17:42:13.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec  7 17:42:13.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:42:13.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8226 exec execpodlrwfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.232.0 81'
Dec  7 17:42:13.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.232.0 81\nConnection to 172.21.232.0 81 port [tcp/*] succeeded!\n"
Dec  7 17:42:13.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8226
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8226 to expose endpoints map[pod2:[101]]
Dec  7 17:42:13.715: INFO: successfully validated that service multi-endpoint-test in namespace services-8226 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8226
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8226 to expose endpoints map[]
Dec  7 17:42:13.780: INFO: successfully validated that service multi-endpoint-test in namespace services-8226 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:42:13.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.931 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":79,"skipped":1260,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:42:13.889: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1887
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:42:14.108: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Creating first CR 
Dec  7 17:42:16.779: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:16Z]] name:name1 resourceVersion:26874 uid:efabc0ff-edd3-46c3-8134-0a62b27cecc9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec  7 17:42:26.812: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:26Z]] name:name2 resourceVersion:26916 uid:8e6b7830-e8c9-45b8-a493-c69845a4cf7f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec  7 17:42:36.845: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:36Z]] name:name1 resourceVersion:26930 uid:efabc0ff-edd3-46c3-8134-0a62b27cecc9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec  7 17:42:46.889: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:46Z]] name:name2 resourceVersion:26945 uid:8e6b7830-e8c9-45b8-a493-c69845a4cf7f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec  7 17:42:56.937: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:36Z]] name:name1 resourceVersion:26959 uid:efabc0ff-edd3-46c3-8134-0a62b27cecc9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec  7 17:43:06.975: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-07T17:42:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-07T17:42:46Z]] name:name2 resourceVersion:26973 uid:8e6b7830-e8c9-45b8-a493-c69845a4cf7f] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:43:17.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1887" for this suite.

• [SLOW TEST:63.736 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":80,"skipped":1275,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:43:17.625: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec  7 17:43:17.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 create -f -'
Dec  7 17:43:20.006: INFO: stderr: ""
Dec  7 17:43:20.006: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  7 17:43:20.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:20.084: INFO: stderr: ""
Dec  7 17:43:20.084: INFO: stdout: "update-demo-nautilus-7lvtr update-demo-nautilus-qqnhq "
Dec  7 17:43:20.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:20.163: INFO: stderr: ""
Dec  7 17:43:20.163: INFO: stdout: ""
Dec  7 17:43:20.163: INFO: update-demo-nautilus-7lvtr is created but not running
Dec  7 17:43:25.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:25.246: INFO: stderr: ""
Dec  7 17:43:25.246: INFO: stdout: "update-demo-nautilus-7lvtr update-demo-nautilus-qqnhq "
Dec  7 17:43:25.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:25.332: INFO: stderr: ""
Dec  7 17:43:25.332: INFO: stdout: ""
Dec  7 17:43:25.332: INFO: update-demo-nautilus-7lvtr is created but not running
Dec  7 17:43:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:30.424: INFO: stderr: ""
Dec  7 17:43:30.424: INFO: stdout: "update-demo-nautilus-7lvtr update-demo-nautilus-qqnhq "
Dec  7 17:43:30.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:30.511: INFO: stderr: ""
Dec  7 17:43:30.511: INFO: stdout: "true"
Dec  7 17:43:30.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:30.587: INFO: stderr: ""
Dec  7 17:43:30.587: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:30.587: INFO: validating pod update-demo-nautilus-7lvtr
Dec  7 17:43:30.637: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:30.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:30.637: INFO: update-demo-nautilus-7lvtr is verified up and running
Dec  7 17:43:30.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-qqnhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:30.706: INFO: stderr: ""
Dec  7 17:43:30.706: INFO: stdout: "true"
Dec  7 17:43:30.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-qqnhq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:30.787: INFO: stderr: ""
Dec  7 17:43:30.787: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:30.787: INFO: validating pod update-demo-nautilus-qqnhq
Dec  7 17:43:30.830: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:30.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:30.830: INFO: update-demo-nautilus-qqnhq is verified up and running
STEP: scaling down the replication controller
Dec  7 17:43:30.832: INFO: scanned /root for discovery docs: <nil>
Dec  7 17:43:30.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec  7 17:43:31.963: INFO: stderr: ""
Dec  7 17:43:31.963: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  7 17:43:31.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:32.043: INFO: stderr: ""
Dec  7 17:43:32.043: INFO: stdout: "update-demo-nautilus-7lvtr "
Dec  7 17:43:32.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:32.141: INFO: stderr: ""
Dec  7 17:43:32.141: INFO: stdout: "true"
Dec  7 17:43:32.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:32.233: INFO: stderr: ""
Dec  7 17:43:32.233: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:32.233: INFO: validating pod update-demo-nautilus-7lvtr
Dec  7 17:43:32.254: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:32.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:32.254: INFO: update-demo-nautilus-7lvtr is verified up and running
STEP: scaling up the replication controller
Dec  7 17:43:32.256: INFO: scanned /root for discovery docs: <nil>
Dec  7 17:43:32.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec  7 17:43:33.387: INFO: stderr: ""
Dec  7 17:43:33.387: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  7 17:43:33.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:33.471: INFO: stderr: ""
Dec  7 17:43:33.471: INFO: stdout: "update-demo-nautilus-7lvtr update-demo-nautilus-bpcvx "
Dec  7 17:43:33.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:33.548: INFO: stderr: ""
Dec  7 17:43:33.548: INFO: stdout: "true"
Dec  7 17:43:33.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:33.619: INFO: stderr: ""
Dec  7 17:43:33.619: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:33.619: INFO: validating pod update-demo-nautilus-7lvtr
Dec  7 17:43:33.636: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:33.636: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:33.636: INFO: update-demo-nautilus-7lvtr is verified up and running
Dec  7 17:43:33.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-bpcvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:33.709: INFO: stderr: ""
Dec  7 17:43:33.709: INFO: stdout: ""
Dec  7 17:43:33.709: INFO: update-demo-nautilus-bpcvx is created but not running
Dec  7 17:43:38.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:43:38.802: INFO: stderr: ""
Dec  7 17:43:38.802: INFO: stdout: "update-demo-nautilus-7lvtr update-demo-nautilus-bpcvx "
Dec  7 17:43:38.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:38.885: INFO: stderr: ""
Dec  7 17:43:38.885: INFO: stdout: "true"
Dec  7 17:43:38.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-7lvtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:38.973: INFO: stderr: ""
Dec  7 17:43:38.973: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:38.973: INFO: validating pod update-demo-nautilus-7lvtr
Dec  7 17:43:38.990: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:38.991: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:38.991: INFO: update-demo-nautilus-7lvtr is verified up and running
Dec  7 17:43:38.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-bpcvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:43:39.069: INFO: stderr: ""
Dec  7 17:43:39.069: INFO: stdout: "true"
Dec  7 17:43:39.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods update-demo-nautilus-bpcvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:43:39.155: INFO: stderr: ""
Dec  7 17:43:39.155: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:43:39.155: INFO: validating pod update-demo-nautilus-bpcvx
Dec  7 17:43:39.199: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:43:39.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:43:39.199: INFO: update-demo-nautilus-bpcvx is verified up and running
STEP: using delete to clean up resources
Dec  7 17:43:39.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 delete --grace-period=0 --force -f -'
Dec  7 17:43:39.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:43:39.303: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  7 17:43:39.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get rc,svc -l name=update-demo --no-headers'
Dec  7 17:43:39.433: INFO: stderr: "No resources found in kubectl-3331 namespace.\n"
Dec  7 17:43:39.433: INFO: stdout: ""
Dec  7 17:43:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-3331 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  7 17:43:39.517: INFO: stderr: ""
Dec  7 17:43:39.517: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:43:39.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3331" for this suite.

• [SLOW TEST:21.938 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":81,"skipped":1287,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:43:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7387
STEP: creating service affinity-nodeport in namespace services-7387
STEP: creating replication controller affinity-nodeport in namespace services-7387
I1207 17:43:39.901498      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7387, replica count: 3
I1207 17:43:42.952746      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:43:43.002: INFO: Creating new exec pod
Dec  7 17:43:46.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7387 exec execpod-affinityl6h7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec  7 17:43:46.466: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec  7 17:43:46.466: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:43:46.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7387 exec execpod-affinityl6h7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.63.199 80'
Dec  7 17:43:46.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.63.199 80\nConnection to 172.21.63.199 80 port [tcp/http] succeeded!\n"
Dec  7 17:43:46.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:43:46.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7387 exec execpod-affinityl6h7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 30185'
Dec  7 17:43:47.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 30185\nConnection to 10.192.217.92 30185 port [tcp/*] succeeded!\n"
Dec  7 17:43:47.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:43:47.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7387 exec execpod-affinityl6h7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.124 30185'
Dec  7 17:43:47.602: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.124 30185\nConnection to 10.192.217.124 30185 port [tcp/*] succeeded!\n"
Dec  7 17:43:47.602: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:43:47.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7387 exec execpod-affinityl6h7p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.192.217.108:30185/ ; done'
Dec  7 17:43:48.089: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30185/\n"
Dec  7 17:43:48.089: INFO: stdout: "\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn\naffinity-nodeport-gtznn"
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Received response from host: affinity-nodeport-gtznn
Dec  7 17:43:48.089: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7387, will wait for the garbage collector to delete the pods
Dec  7 17:43:48.208: INFO: Deleting ReplicationController affinity-nodeport took: 17.419562ms
Dec  7 17:43:48.308: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.286406ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:43:51.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7387" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.666 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":82,"skipped":1292,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:43:51.229: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 17:43:51.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b" in namespace "projected-2107" to be "Succeeded or Failed"
Dec  7 17:43:51.493: INFO: Pod "downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.432864ms
Dec  7 17:43:53.510: INFO: Pod "downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027555873s
Dec  7 17:43:55.526: INFO: Pod "downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0433606s
STEP: Saw pod success
Dec  7 17:43:55.526: INFO: Pod "downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b" satisfied condition "Succeeded or Failed"
Dec  7 17:43:55.536: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b container client-container: <nil>
STEP: delete the pod
Dec  7 17:43:55.635: INFO: Waiting for pod downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b to disappear
Dec  7 17:43:55.645: INFO: Pod downwardapi-volume-e5c383ad-cd1d-4390-bc36-ab87220dbf4b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:43:55.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2107" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1293,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:43:55.679: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:43:59.993: INFO: Deleting pod "var-expansion-89e37bd8-67f6-42d6-9be4-eb2565c89c5f" in namespace "var-expansion-1430"
Dec  7 17:44:00.013: INFO: Wait up to 5m0s for pod "var-expansion-89e37bd8-67f6-42d6-9be4-eb2565c89c5f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:02.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1430" for this suite.

• [SLOW TEST:6.409 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":84,"skipped":1294,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Dec  7 17:44:02.330: INFO: Waiting up to 5m0s for pod "client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab" in namespace "containers-9220" to be "Succeeded or Failed"
Dec  7 17:44:02.340: INFO: Pod "client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab": Phase="Pending", Reason="", readiness=false. Elapsed: 9.543758ms
Dec  7 17:44:04.355: INFO: Pod "client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02463556s
Dec  7 17:44:06.371: INFO: Pod "client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04063054s
STEP: Saw pod success
Dec  7 17:44:06.371: INFO: Pod "client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab" satisfied condition "Succeeded or Failed"
Dec  7 17:44:06.385: INFO: Trying to get logs from node 10.192.217.124 pod client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:44:06.491: INFO: Waiting for pod client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab to disappear
Dec  7 17:44:06.501: INFO: Pod client-containers-aeecf2da-9234-41bc-9570-fd8b4ce484ab no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:06.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9220" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1298,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:06.534: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  7 17:44:06.776: INFO: Waiting up to 5m0s for pod "pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e" in namespace "emptydir-3374" to be "Succeeded or Failed"
Dec  7 17:44:06.786: INFO: Pod "pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.501618ms
Dec  7 17:44:08.801: INFO: Pod "pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025117776s
Dec  7 17:44:10.816: INFO: Pod "pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040788266s
STEP: Saw pod success
Dec  7 17:44:10.817: INFO: Pod "pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e" satisfied condition "Succeeded or Failed"
Dec  7 17:44:10.857: INFO: Trying to get logs from node 10.192.217.92 pod pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e container test-container: <nil>
STEP: delete the pod
Dec  7 17:44:10.919: INFO: Waiting for pod pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e to disappear
Dec  7 17:44:10.932: INFO: Pod pod-afaf0dc6-aca1-45de-8ede-834c9adcba5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:10.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3374" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:10.965: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  7 17:44:11.181: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  7 17:44:11.229: INFO: Waiting for terminating namespaces to be deleted...
Dec  7 17:44:11.250: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.108 before test
Dec  7 17:44:11.280: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 17:44:11.280: INFO: calico-node-wl22v from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 17:44:11.280: INFO: calico-typha-7d788c697f-mvhs4 from kube-system started at 2021-12-07 15:54:49 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 17:44:11.280: INFO: coredns-b58d5f584-lngfv from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container coredns ready: true, restart count 0
Dec  7 17:44:11.280: INFO: ibm-keepalived-watcher-bjzrb from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 17:44:11.280: INFO: ibm-master-proxy-static-10.192.217.108 from kube-system started at 2021-12-07 15:54:14 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 17:44:11.280: INFO: 	Container pause ready: true, restart count 0
Dec  7 17:44:11.280: INFO: konnectivity-agent-4b249 from kube-system started at 2021-12-07 16:03:06 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 17:44:11.280: INFO: metrics-server-b9bc976b6-5wkrb from kube-system started at 2021-12-07 15:56:17 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container metrics-server ready: true, restart count 0
Dec  7 17:44:11.280: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Dec  7 17:44:11.280: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9 from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 17:44:11.280: INFO: sonobuoy-e2e-job-3b8635ba80e64550 from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container e2e ready: true, restart count 0
Dec  7 17:44:11.280: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 17:44:11.280: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.280: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 17:44:11.280: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 17:44:11.280: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.124 before test
Dec  7 17:44:11.348: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-12-07 15:57:45 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Dec  7 17:44:11.348: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-z4mpb from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 17:44:11.348: INFO: calico-node-d5dvk from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 17:44:11.348: INFO: calico-typha-7d788c697f-l2q8w from kube-system started at 2021-12-07 15:54:59 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 17:44:11.348: INFO: coredns-b58d5f584-ghv8w from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container coredns ready: true, restart count 0
Dec  7 17:44:11.348: INFO: ibm-keepalived-watcher-gjxv9 from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 17:44:11.348: INFO: ibm-master-proxy-static-10.192.217.124 from kube-system started at 2021-12-07 15:54:35 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 17:44:11.348: INFO: 	Container pause ready: true, restart count 0
Dec  7 17:44:11.348: INFO: konnectivity-agent-nz7z5 from kube-system started at 2021-12-07 16:03:10 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 17:44:11.348: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-9vnhs from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 17:44:11.348: INFO: sonobuoy from sonobuoy started at 2021-12-07 17:19:42 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  7 17:44:11.348: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-7vsjp from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 17:44:11.348: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 17:44:11.348: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.92 before test
Dec  7 17:44:11.415: INFO: catalog-operator-6c4b4d7c9-5q9kr from ibm-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container catalog-operator ready: true, restart count 0
Dec  7 17:44:11.415: INFO: olm-operator-785cdc5884-cb6c2 from ibm-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container olm-operator ready: true, restart count 0
Dec  7 17:44:11.415: INFO: calico-kube-controllers-749944fc4-s654d from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  7 17:44:11.415: INFO: calico-node-gnsxh from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 17:44:11.415: INFO: calico-typha-7d788c697f-f4drp from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 17:44:11.415: INFO: coredns-autoscaler-689fb74d49-sbpmc from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container autoscaler ready: true, restart count 0
Dec  7 17:44:11.415: INFO: coredns-b58d5f584-wdnnj from kube-system started at 2021-12-07 16:03:42 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container coredns ready: true, restart count 0
Dec  7 17:44:11.415: INFO: dashboard-metrics-scraper-6747f89c97-kjs9x from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  7 17:44:11.415: INFO: ibm-file-plugin-bbfc75b87-9t82f from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.415: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Dec  7 17:44:11.415: INFO: ibm-keepalived-watcher-8d77z from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 17:44:11.416: INFO: ibm-master-proxy-static-10.192.217.92 from kube-system started at 2021-12-07 15:54:06 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 17:44:11.416: INFO: 	Container pause ready: true, restart count 0
Dec  7 17:44:11.416: INFO: ibm-storage-watcher-65b9c4d74f-mmw65 from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Dec  7 17:44:11.416: INFO: konnectivity-agent-pcnkz from kube-system started at 2021-12-07 16:03:13 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 17:44:11.416: INFO: kubernetes-dashboard-54c47dd995-2hgcj from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  7 17:44:11.416: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-vgqvj from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 17:44:11.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 17:44:11.416: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e06ec5ba-654d-4849-92b8-190aa776cf50 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e06ec5ba-654d-4849-92b8-190aa776cf50 off the node 10.192.217.92
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e06ec5ba-654d-4849-92b8-190aa776cf50
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:15.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5743" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":87,"skipped":1323,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:15.733: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  7 17:44:15.979: INFO: Waiting up to 5m0s for pod "pod-5120546c-8862-409b-8d18-7c77e2222dd6" in namespace "emptydir-3489" to be "Succeeded or Failed"
Dec  7 17:44:15.989: INFO: Pod "pod-5120546c-8862-409b-8d18-7c77e2222dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.79635ms
Dec  7 17:44:18.004: INFO: Pod "pod-5120546c-8862-409b-8d18-7c77e2222dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025054835s
Dec  7 17:44:20.040: INFO: Pod "pod-5120546c-8862-409b-8d18-7c77e2222dd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06071768s
STEP: Saw pod success
Dec  7 17:44:20.040: INFO: Pod "pod-5120546c-8862-409b-8d18-7c77e2222dd6" satisfied condition "Succeeded or Failed"
Dec  7 17:44:20.080: INFO: Trying to get logs from node 10.192.217.124 pod pod-5120546c-8862-409b-8d18-7c77e2222dd6 container test-container: <nil>
STEP: delete the pod
Dec  7 17:44:20.185: INFO: Waiting for pod pod-5120546c-8862-409b-8d18-7c77e2222dd6 to disappear
Dec  7 17:44:20.195: INFO: Pod pod-5120546c-8862-409b-8d18-7c77e2222dd6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:20.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3489" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1327,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:20.251: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  7 17:44:20.626: INFO: Number of nodes with available pods: 0
Dec  7 17:44:20.626: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:44:21.669: INFO: Number of nodes with available pods: 0
Dec  7 17:44:21.669: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:44:22.657: INFO: Number of nodes with available pods: 0
Dec  7 17:44:22.657: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 17:44:23.673: INFO: Number of nodes with available pods: 3
Dec  7 17:44:23.673: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  7 17:44:23.739: INFO: Number of nodes with available pods: 2
Dec  7 17:44:23.739: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:44:24.773: INFO: Number of nodes with available pods: 2
Dec  7 17:44:24.773: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:44:25.774: INFO: Number of nodes with available pods: 2
Dec  7 17:44:25.774: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:44:26.770: INFO: Number of nodes with available pods: 2
Dec  7 17:44:26.770: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:44:27.784: INFO: Number of nodes with available pods: 2
Dec  7 17:44:27.784: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 17:44:28.771: INFO: Number of nodes with available pods: 3
Dec  7 17:44:28.771: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7186, will wait for the garbage collector to delete the pods
Dec  7 17:44:28.868: INFO: Deleting DaemonSet.extensions daemon-set took: 25.774161ms
Dec  7 17:44:29.069: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.080317ms
Dec  7 17:44:31.287: INFO: Number of nodes with available pods: 0
Dec  7 17:44:31.287: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 17:44:31.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27737"},"items":null}

Dec  7 17:44:31.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27738"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:44:31.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7186" for this suite.

• [SLOW TEST:11.167 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":89,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:44:31.418: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7062" for this suite.

• [SLOW TEST:330.350 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":90,"skipped":1382,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:01.769: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:50:02.693: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 17:50:04.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496202, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496202, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496202, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496202, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:50:07.806: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:08.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2773" for this suite.
STEP: Destroying namespace "webhook-2773-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.896 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":91,"skipped":1396,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:08.666: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1224
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:50:08.907: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1224" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":92,"skipped":1416,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:10.023: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  7 17:50:10.292: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:50:12.308: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:50:14.308: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  7 17:50:14.345: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:50:16.364: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  7 17:50:16.509: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  7 17:50:16.520: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  7 17:50:18.521: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  7 17:50:18.537: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  7 17:50:20.521: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  7 17:50:20.535: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:20.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4299" for this suite.

• [SLOW TEST:10.550 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:20.574: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec  7 17:50:20.936: INFO: running pods: 0 < 3
Dec  7 17:50:22.951: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:24.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4330" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":94,"skipped":1463,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:25.025: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Dec  7 17:50:25.291: INFO: Waiting up to 5m0s for pod "test-pod-9794de89-a772-4d29-9084-b7fd478b6177" in namespace "svcaccounts-6404" to be "Succeeded or Failed"
Dec  7 17:50:25.314: INFO: Pod "test-pod-9794de89-a772-4d29-9084-b7fd478b6177": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576898ms
Dec  7 17:50:27.355: INFO: Pod "test-pod-9794de89-a772-4d29-9084-b7fd478b6177": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063502016s
Dec  7 17:50:29.371: INFO: Pod "test-pod-9794de89-a772-4d29-9084-b7fd478b6177": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079582693s
STEP: Saw pod success
Dec  7 17:50:29.371: INFO: Pod "test-pod-9794de89-a772-4d29-9084-b7fd478b6177" satisfied condition "Succeeded or Failed"
Dec  7 17:50:29.381: INFO: Trying to get logs from node 10.192.217.108 pod test-pod-9794de89-a772-4d29-9084-b7fd478b6177 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:50:29.533: INFO: Waiting for pod test-pod-9794de89-a772-4d29-9084-b7fd478b6177 to disappear
Dec  7 17:50:29.543: INFO: Pod test-pod-9794de89-a772-4d29-9084-b7fd478b6177 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:29.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6404" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":95,"skipped":1475,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:29.578: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2192
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:50:29.795: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:30.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2192" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":96,"skipped":1477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:30.531: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:50:30.785: INFO: The status of Pod busybox-host-aliases7ffba4c4-8f08-4935-8324-cad59469d507 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:50:32.800: INFO: The status of Pod busybox-host-aliases7ffba4c4-8f08-4935-8324-cad59469d507 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:50:34.820: INFO: The status of Pod busybox-host-aliases7ffba4c4-8f08-4935-8324-cad59469d507 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:34.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8942" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1500,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:34.904: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:50:37.207: INFO: Deleting pod "var-expansion-6e3b46d7-32b0-470f-b1a6-3403be543256" in namespace "var-expansion-8527"
Dec  7 17:50:37.228: INFO: Wait up to 5m0s for pod "var-expansion-6e3b46d7-32b0-470f-b1a6-3403be543256" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:41.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8527" for this suite.

• [SLOW TEST:6.392 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":98,"skipped":1503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-6130/secret-test-634eb433-34ed-48e2-adae-e0157cb099e6
STEP: Creating a pod to test consume secrets
Dec  7 17:50:41.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2" in namespace "secrets-6130" to be "Succeeded or Failed"
Dec  7 17:50:41.572: INFO: Pod "pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.703718ms
Dec  7 17:50:43.586: INFO: Pod "pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024631224s
STEP: Saw pod success
Dec  7 17:50:43.586: INFO: Pod "pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2" satisfied condition "Succeeded or Failed"
Dec  7 17:50:43.596: INFO: Trying to get logs from node 10.192.217.92 pod pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2 container env-test: <nil>
STEP: delete the pod
Dec  7 17:50:43.759: INFO: Waiting for pod pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2 to disappear
Dec  7 17:50:43.770: INFO: Pod pod-configmaps-55e21de7-dd20-4a97-9469-d365a10a42c2 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6130" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:43.817: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:44.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3483" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":100,"skipped":1579,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:44.106: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec  7 17:50:44.349: INFO: namespace kubectl-8998
Dec  7 17:50:44.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-8998 create -f -'
Dec  7 17:50:46.222: INFO: stderr: ""
Dec  7 17:50:46.222: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  7 17:50:47.240: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:50:47.240: INFO: Found 0 / 1
Dec  7 17:50:48.236: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:50:48.236: INFO: Found 1 / 1
Dec  7 17:50:48.236: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  7 17:50:48.247: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:50:48.247: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  7 17:50:48.247: INFO: wait on agnhost-primary startup in kubectl-8998 
Dec  7 17:50:48.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-8998 logs agnhost-primary-ff7rx agnhost-primary'
Dec  7 17:50:48.400: INFO: stderr: ""
Dec  7 17:50:48.400: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec  7 17:50:48.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-8998 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec  7 17:50:48.513: INFO: stderr: ""
Dec  7 17:50:48.513: INFO: stdout: "service/rm2 exposed\n"
Dec  7 17:50:48.526: INFO: Service rm2 in namespace kubectl-8998 found.
STEP: exposing service
Dec  7 17:50:50.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-8998 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec  7 17:50:50.672: INFO: stderr: ""
Dec  7 17:50:50.672: INFO: stdout: "service/rm3 exposed\n"
Dec  7 17:50:50.687: INFO: Service rm3 in namespace kubectl-8998 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:50:52.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8998" for this suite.

• [SLOW TEST:8.654 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":101,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:50:52.762: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:50:53.291: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:50:56.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:50:56.386: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Dec  7 17:51:02.023: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:51:03.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3988" for this suite.
STEP: Destroying namespace "webhook-3988-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.442 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":102,"skipped":1711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:51:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  7 17:51:03.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-308 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  7 17:51:03.565: INFO: stderr: ""
Dec  7 17:51:03.565: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec  7 17:51:03.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-308 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Dec  7 17:51:04.905: INFO: stderr: ""
Dec  7 17:51:04.905: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  7 17:51:04.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-308 delete pods e2e-test-httpd-pod'
Dec  7 17:51:07.235: INFO: stderr: ""
Dec  7 17:51:07.235: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:51:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-308" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":103,"skipped":1736,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:51:07.281: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5971
STEP: creating service affinity-clusterip-transition in namespace services-5971
STEP: creating replication controller affinity-clusterip-transition in namespace services-5971
I1207 17:51:07.556096      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5971, replica count: 3
I1207 17:51:10.607620      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:51:10.648: INFO: Creating new exec pod
Dec  7 17:51:13.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-5971 exec execpod-affinitylxf2l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec  7 17:51:13.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec  7 17:51:13.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:51:13.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-5971 exec execpod-affinitylxf2l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.123.57 80'
Dec  7 17:51:14.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.123.57 80\nConnection to 172.21.123.57 80 port [tcp/http] succeeded!\n"
Dec  7 17:51:14.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:51:14.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-5971 exec execpod-affinitylxf2l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.123.57:80/ ; done'
Dec  7 17:51:14.601: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n"
Dec  7 17:51:14.601: INFO: stdout: "\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72"
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:14.601: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-5971 exec execpod-affinitylxf2l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.123.57:80/ ; done'
Dec  7 17:51:44.970: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n"
Dec  7 17:51:44.970: INFO: stdout: "\naffinity-clusterip-transition-nfndf\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-nfndf\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72\naffinity-clusterip-transition-57t72"
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-nfndf
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-nfndf
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:44.970: INFO: Received response from host: affinity-clusterip-transition-57t72
Dec  7 17:51:45.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-5971 exec execpod-affinitylxf2l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.123.57:80/ ; done'
Dec  7 17:51:45.361: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.123.57:80/\n"
Dec  7 17:51:45.361: INFO: stdout: "\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js\naffinity-clusterip-transition-b65js"
Dec  7 17:51:45.361: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.361: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.361: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.361: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Received response from host: affinity-clusterip-transition-b65js
Dec  7 17:51:45.362: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5971, will wait for the garbage collector to delete the pods
Dec  7 17:51:45.490: INFO: Deleting ReplicationController affinity-clusterip-transition took: 32.589663ms
Dec  7 17:51:45.791: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 301.099242ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:51:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:41.232 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":104,"skipped":1754,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:51:48.513: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  7 17:51:48.774: INFO: Waiting up to 5m0s for pod "pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1" in namespace "emptydir-1882" to be "Succeeded or Failed"
Dec  7 17:51:48.786: INFO: Pod "pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.903597ms
Dec  7 17:51:50.801: INFO: Pod "pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026387024s
STEP: Saw pod success
Dec  7 17:51:50.801: INFO: Pod "pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1" satisfied condition "Succeeded or Failed"
Dec  7 17:51:50.812: INFO: Trying to get logs from node 10.192.217.92 pod pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1 container test-container: <nil>
STEP: delete the pod
Dec  7 17:51:50.872: INFO: Waiting for pod pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1 to disappear
Dec  7 17:51:50.882: INFO: Pod pod-ba0ba1a7-d3b0-4adb-a832-253e1640b2f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:51:50.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1882" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:51:50.929: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a
Dec  7 17:51:51.176: INFO: Pod name my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a: Found 0 pods out of 1
Dec  7 17:51:56.191: INFO: Pod name my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a: Found 1 pods out of 1
Dec  7 17:51:56.191: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a" are running
Dec  7 17:51:56.202: INFO: Pod "my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a-4kx47" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 17:51:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 17:51:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 17:51:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 17:51:51 +0000 UTC Reason: Message:}])
Dec  7 17:51:56.202: INFO: Trying to dial the pod
Dec  7 17:52:01.281: INFO: Controller my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a: Got expected result from replica 1 [my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a-4kx47]: "my-hostname-basic-889e4702-5892-445b-a275-9d0e7cfc1e1a-4kx47", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:52:01.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8487" for this suite.

• [SLOW TEST:10.392 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":106,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:52:01.321: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:52:01.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-703" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":107,"skipped":1828,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:52:01.786: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  7 17:52:04.627: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4115 pod-service-account-1688098a-2293-4958-b326-8397532e0613 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  7 17:52:04.867: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4115 pod-service-account-1688098a-2293-4958-b326-8397532e0613 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  7 17:52:05.131: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4115 pod-service-account-1688098a-2293-4958-b326-8397532e0613 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:52:05.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4115" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":108,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:52:05.433: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-2efc40e2-e2a0-49da-adad-418abafe869b
STEP: Creating a pod to test consume configMaps
Dec  7 17:52:05.705: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7" in namespace "configmap-7284" to be "Succeeded or Failed"
Dec  7 17:52:05.716: INFO: Pod "pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.306793ms
Dec  7 17:52:07.733: INFO: Pod "pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027165687s
Dec  7 17:52:09.747: INFO: Pod "pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041434426s
STEP: Saw pod success
Dec  7 17:52:09.747: INFO: Pod "pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7" satisfied condition "Succeeded or Failed"
Dec  7 17:52:09.757: INFO: Trying to get logs from node 10.192.217.92 pod pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:52:09.823: INFO: Waiting for pod pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7 to disappear
Dec  7 17:52:09.833: INFO: Pod pod-configmaps-9bb081a1-4b1a-4fe6-ba04-f991a6dff8e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:52:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7284" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":1853,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:52:09.874: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec  7 17:52:10.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 create -f -'
Dec  7 17:52:10.315: INFO: stderr: ""
Dec  7 17:52:10.315: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  7 17:52:10.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:52:10.394: INFO: stderr: ""
Dec  7 17:52:10.394: INFO: stdout: "update-demo-nautilus-9ps4t update-demo-nautilus-nxd6j "
Dec  7 17:52:10.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods update-demo-nautilus-9ps4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:52:10.464: INFO: stderr: ""
Dec  7 17:52:10.464: INFO: stdout: ""
Dec  7 17:52:10.464: INFO: update-demo-nautilus-9ps4t is created but not running
Dec  7 17:52:15.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec  7 17:52:15.560: INFO: stderr: ""
Dec  7 17:52:15.560: INFO: stdout: "update-demo-nautilus-9ps4t update-demo-nautilus-nxd6j "
Dec  7 17:52:15.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods update-demo-nautilus-9ps4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:52:15.630: INFO: stderr: ""
Dec  7 17:52:15.630: INFO: stdout: "true"
Dec  7 17:52:15.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods update-demo-nautilus-9ps4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:52:15.705: INFO: stderr: ""
Dec  7 17:52:15.705: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:52:15.705: INFO: validating pod update-demo-nautilus-9ps4t
Dec  7 17:52:15.755: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:52:15.755: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:52:15.755: INFO: update-demo-nautilus-9ps4t is verified up and running
Dec  7 17:52:15.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods update-demo-nautilus-nxd6j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec  7 17:52:15.837: INFO: stderr: ""
Dec  7 17:52:15.837: INFO: stdout: "true"
Dec  7 17:52:15.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods update-demo-nautilus-nxd6j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec  7 17:52:15.917: INFO: stderr: ""
Dec  7 17:52:15.917: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec  7 17:52:15.917: INFO: validating pod update-demo-nautilus-nxd6j
Dec  7 17:52:15.980: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  7 17:52:15.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  7 17:52:15.980: INFO: update-demo-nautilus-nxd6j is verified up and running
STEP: using delete to clean up resources
Dec  7 17:52:15.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 delete --grace-period=0 --force -f -'
Dec  7 17:52:16.084: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:52:16.084: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  7 17:52:16.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get rc,svc -l name=update-demo --no-headers'
Dec  7 17:52:16.184: INFO: stderr: "No resources found in kubectl-4431 namespace.\n"
Dec  7 17:52:16.184: INFO: stdout: ""
Dec  7 17:52:16.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4431 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  7 17:52:16.266: INFO: stderr: ""
Dec  7 17:52:16.266: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:52:16.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4431" for this suite.

• [SLOW TEST:6.431 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":110,"skipped":1854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:52:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:54:00.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7225" for this suite.

• [SLOW TEST:104.323 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":111,"skipped":1876,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:54:00.629: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec  7 17:54:02.930: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3373 PodName:var-expansion-fe99e73a-c40b-4993-8fed-184c9c704285 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 17:54:02.931: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: test for file in mounted path
Dec  7 17:54:03.151: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3373 PodName:var-expansion-fe99e73a-c40b-4993-8fed-184c9c704285 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 17:54:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: updating the annotation value
Dec  7 17:54:03.874: INFO: Successfully updated pod "var-expansion-fe99e73a-c40b-4993-8fed-184c9c704285"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec  7 17:54:03.885: INFO: Deleting pod "var-expansion-fe99e73a-c40b-4993-8fed-184c9c704285" in namespace "var-expansion-3373"
Dec  7 17:54:03.905: INFO: Wait up to 5m0s for pod "var-expansion-fe99e73a-c40b-4993-8fed-184c9c704285" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:54:37.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3373" for this suite.

• [SLOW TEST:37.344 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":112,"skipped":1883,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:54:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:06.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7306" for this suite.

• [SLOW TEST:28.428 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":113,"skipped":1887,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:06.402: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Dec  7 17:55:06.636: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec  7 17:55:06.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:06.877: INFO: stderr: ""
Dec  7 17:55:06.877: INFO: stdout: "service/agnhost-replica created\n"
Dec  7 17:55:06.877: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec  7 17:55:06.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:07.198: INFO: stderr: ""
Dec  7 17:55:07.198: INFO: stdout: "service/agnhost-primary created\n"
Dec  7 17:55:07.198: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  7 17:55:07.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:07.421: INFO: stderr: ""
Dec  7 17:55:07.421: INFO: stdout: "service/frontend created\n"
Dec  7 17:55:07.421: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec  7 17:55:07.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:07.633: INFO: stderr: ""
Dec  7 17:55:07.633: INFO: stdout: "deployment.apps/frontend created\n"
Dec  7 17:55:07.633: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  7 17:55:07.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:07.860: INFO: stderr: ""
Dec  7 17:55:07.860: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec  7 17:55:07.860: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  7 17:55:07.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 create -f -'
Dec  7 17:55:08.074: INFO: stderr: ""
Dec  7 17:55:08.074: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec  7 17:55:08.074: INFO: Waiting for all frontend pods to be Running.
Dec  7 17:55:13.125: INFO: Waiting for frontend to serve content.
Dec  7 17:55:13.185: INFO: Trying to add a new entry to the guestbook.
Dec  7 17:55:13.217: INFO: Verifying that added entry can be retrieved.
Dec  7 17:55:13.265: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Dec  7 17:55:18.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:18.474: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:18.474: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec  7 17:55:18.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:18.626: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:18.626: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec  7 17:55:18.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:18.780: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:18.780: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  7 17:55:18.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:18.878: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:18.878: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  7 17:55:18.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:18.979: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:18.979: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec  7 17:55:18.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6999 delete --grace-period=0 --force -f -'
Dec  7 17:55:19.095: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 17:55:19.095: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6999" for this suite.

• [SLOW TEST:12.730 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":114,"skipped":1901,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:19.132: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:24.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4081" for this suite.

• [SLOW TEST:5.572 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":115,"skipped":1909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:24.705: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5604
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-f110dfc7-11cb-4a62-aae1-692d156f27e0
STEP: Creating the pod
Dec  7 17:55:25.003: INFO: The status of Pod pod-configmaps-b6895eac-513c-46a6-b3fa-9d7beac2c8b2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:55:27.018: INFO: The status of Pod pod-configmaps-b6895eac-513c-46a6-b3fa-9d7beac2c8b2 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-f110dfc7-11cb-4a62-aae1-692d156f27e0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:29.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5604" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":1932,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:29.241: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-c88f2917-4d90-40b9-a188-083386082de9
STEP: Creating a pod to test consume configMaps
Dec  7 17:55:29.495: INFO: Waiting up to 5m0s for pod "pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d" in namespace "configmap-7368" to be "Succeeded or Failed"
Dec  7 17:55:29.506: INFO: Pod "pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.133466ms
Dec  7 17:55:31.520: INFO: Pod "pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025501744s
STEP: Saw pod success
Dec  7 17:55:31.520: INFO: Pod "pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d" satisfied condition "Succeeded or Failed"
Dec  7 17:55:31.530: INFO: Trying to get logs from node 10.192.217.124 pod pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:55:31.670: INFO: Waiting for pod pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d to disappear
Dec  7 17:55:31.679: INFO: Pod pod-configmaps-e84d5259-37c2-44ce-ba1c-918595f99b1d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:31.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7368" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":1937,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:31.734: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-c140cfee-9af1-4ebf-851f-cdbbcbc74e92
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:31.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-890" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":118,"skipped":1940,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:32.020: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:55:32.282: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b7a8188a-7b18-4762-ad24-73e5b8037400" in namespace "security-context-test-2702" to be "Succeeded or Failed"
Dec  7 17:55:32.292: INFO: Pod "busybox-user-65534-b7a8188a-7b18-4762-ad24-73e5b8037400": Phase="Pending", Reason="", readiness=false. Elapsed: 9.837345ms
Dec  7 17:55:34.307: INFO: Pod "busybox-user-65534-b7a8188a-7b18-4762-ad24-73e5b8037400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024994734s
Dec  7 17:55:36.320: INFO: Pod "busybox-user-65534-b7a8188a-7b18-4762-ad24-73e5b8037400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037347129s
Dec  7 17:55:36.320: INFO: Pod "busybox-user-65534-b7a8188a-7b18-4762-ad24-73e5b8037400" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:36.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2702" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":1958,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:36.362: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:55:36.648: INFO: The status of Pod pod-secrets-8b947567-f855-4870-ae09-5f4a2ba8714d is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:55:38.664: INFO: The status of Pod pod-secrets-8b947567-f855-4870-ae09-5f4a2ba8714d is Pending, waiting for it to be Running (with Ready = true)
Dec  7 17:55:40.667: INFO: The status of Pod pod-secrets-8b947567-f855-4870-ae09-5f4a2ba8714d is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:40.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7834" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":120,"skipped":1978,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:40.798: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:55:41.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 create -f -'
Dec  7 17:55:41.214: INFO: stderr: ""
Dec  7 17:55:41.214: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec  7 17:55:41.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 create -f -'
Dec  7 17:55:41.464: INFO: stderr: ""
Dec  7 17:55:41.465: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  7 17:55:42.483: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:55:42.483: INFO: Found 0 / 1
Dec  7 17:55:43.478: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:55:43.478: INFO: Found 1 / 1
Dec  7 17:55:43.478: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  7 17:55:43.489: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:55:43.489: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  7 17:55:43.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 describe pod agnhost-primary-kxvsz'
Dec  7 17:55:43.623: INFO: stderr: ""
Dec  7 17:55:43.623: INFO: stdout: "Name:         agnhost-primary-kxvsz\nNamespace:    kubectl-1390\nPriority:     0\nNode:         10.192.217.92/10.192.217.92\nStart Time:   Tue, 07 Dec 2021 17:55:41 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 6185e2c0bee737971a98e4901dec38d8fc2e3faddbaf2b43d4475eb203e13759\n              cni.projectcalico.org/podIP: 172.30.34.144/32\n              cni.projectcalico.org/podIPs: 172.30.34.144/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.34.144\nIPs:\n  IP:           172.30.34.144\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6be19dca27cf6894bf4831367935322f281c024533a1812bd8562d31afe2d3ed\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 07 Dec 2021 17:55:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vn6sw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vn6sw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1390/agnhost-primary-kxvsz to 10.192.217.92\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec  7 17:55:43.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 describe rc agnhost-primary'
Dec  7 17:55:43.743: INFO: stderr: ""
Dec  7 17:55:43.743: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1390\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-kxvsz\n"
Dec  7 17:55:43.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 describe service agnhost-primary'
Dec  7 17:55:43.863: INFO: stderr: ""
Dec  7 17:55:43.863: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1390\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.237.135\nIPs:               172.21.237.135\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.34.144:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  7 17:55:43.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 describe node 10.192.217.108'
Dec  7 17:55:44.122: INFO: stderr: ""
Dec  7 17:55:44.122: INFO: stdout: "Name:               10.192.217.108\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-tok\n                    failure-domain.beta.kubernetes.io/zone=tok04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=128.168.75.55\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.192.217.108\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=jp-tok\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c6nnvugt0l0nrclc8f80-kubee2epvgb-default-00000285\n                    ibm-cloud.kubernetes.io/worker-pool-id=c6nnvugt0l0nrclc8f80-a94e7ff\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.22.4_1532\n                    ibm-cloud.kubernetes.io/zone=tok04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.192.217.108\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2723058\n                    publicVLAN=2723052\n                    topology.kubernetes.io/region=jp-tok\n                    topology.kubernetes.io/zone=tok04\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.192.217.108/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.11.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 07 Dec 2021 15:54:26 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.192.217.108\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 07 Dec 2021 17:55:36 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 07 Dec 2021 15:55:11 +0000   Tue, 07 Dec 2021 15:55:11 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 07 Dec 2021 17:54:52 +0000   Tue, 07 Dec 2021 15:54:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 07 Dec 2021 17:54:52 +0000   Tue, 07 Dec 2021 15:54:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 07 Dec 2021 17:54:52 +0000   Tue, 07 Dec 2021 15:54:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 07 Dec 2021 17:54:52 +0000   Tue, 07 Dec 2021 15:54:46 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.192.217.108\n  ExternalIP:  128.168.75.55\n  Hostname:    10.192.217.108\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      102685624Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16410268Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      93986994917\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13618844Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 46e2656f9d9e46a196c7b12971b0f7d7\n  System UUID:                7C253D92-AA45-D301-C9F5-B120CD2CC3FB\n  Boot ID:                    dc771bed-f903-4369-8d24-53279ad4af91\n  Kernel Version:             4.15.0-163-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.8\n  Kubelet Version:            v1.22.4+IKS\n  Kube-Proxy Version:         v1.22.4+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///c6nnvugt0l0nrclc8f80/kube-c6nnvugt0l0nrclc8f80-kubee2epvgb-default-00000285\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         116m\n  kube-system                 calico-node-wl22v                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         121m\n  kube-system                 calico-typha-7d788c697f-mvhs4                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         127m\n  kube-system                 coredns-b58d5f584-lngfv                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     112m\n  kube-system                 ibm-keepalived-watcher-bjzrb                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         121m\n  kube-system                 ibm-master-proxy-static-10.192.217.108                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      120m\n  kube-system                 konnectivity-agent-4b249                                   10m (0%)      0 (0%)      10Mi (0%)        50Mi (0%)      112m\n  kube-system                 metrics-server-b9bc976b6-5wkrb                             121m (3%)     216m (5%)   186Mi (1%)       436Mi (3%)     119m\n  kube-system                 public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9         10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         117m\n  sonobuoy                    sonobuoy-e2e-job-3b8635ba80e64550                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         35m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         35m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests       Limits\n  --------               --------       ------\n  cpu                    776m (19%)     516m (13%)\n  memory                 590354Ki (4%)  1407264Ki (10%)\n  ephemeral-storage      0 (0%)         0 (0%)\n  hugepages-1Gi          0 (0%)         0 (0%)\n  hugepages-2Mi          0 (0%)         0 (0%)\n  scheduling.k8s.io/foo  0              0\nEvents:                  <none>\n"
Dec  7 17:55:44.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1390 describe namespace kubectl-1390'
Dec  7 17:55:44.241: INFO: stderr: ""
Dec  7 17:55:44.241: INFO: stdout: "Name:         kubectl-1390\nLabels:       e2e-framework=kubectl\n              e2e-run=e1468650-785e-4353-9741-8df0d516748a\n              kubernetes.io/metadata.name=kubectl-1390\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:44.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1390" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":121,"skipped":1989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:44.281: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-e8a7b78b-1775-4c13-8ab6-76c4b87dea45-2297
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:55:44.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8271" for this suite.
STEP: Destroying namespace "nspatchtest-e8a7b78b-1775-4c13-8ab6-76c4b87dea45-2297" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":122,"skipped":2046,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:55:44.819: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-631
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec  7 17:55:45.036: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:56:05.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-631" for this suite.

• [SLOW TEST:20.571 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":123,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:56:05.391: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4646
STEP: creating service affinity-nodeport-transition in namespace services-4646
STEP: creating replication controller affinity-nodeport-transition in namespace services-4646
I1207 17:56:05.722396      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4646, replica count: 3
I1207 17:56:08.773125      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 17:56:08.821: INFO: Creating new exec pod
Dec  7 17:56:13.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec  7 17:56:14.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec  7 17:56:14.248: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:56:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.99.0 80'
Dec  7 17:56:14.503: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.99.0 80\nConnection to 172.21.99.0 80 port [tcp/http] succeeded!\n"
Dec  7 17:56:14.503: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:56:14.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 30307'
Dec  7 17:56:14.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 30307\nConnection to 10.192.217.92 30307 port [tcp/*] succeeded!\n"
Dec  7 17:56:14.758: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:56:14.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.124 30307'
Dec  7 17:56:15.052: INFO: stderr: "+ nc -v -t -w 2 10.192.217.124 30307\n+ echo hostName\nConnection to 10.192.217.124 30307 port [tcp/*] succeeded!\n"
Dec  7 17:56:15.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 17:56:15.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.192.217.108:30307/ ; done'
Dec  7 17:56:15.407: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n"
Dec  7 17:56:15.407: INFO: stdout: "\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9"
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:15.408: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.192.217.108:30307/ ; done'
Dec  7 17:56:45.764: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n"
Dec  7 17:56:45.764: INFO: stdout: "\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-7scnq\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-7scnq\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-7scnq\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-sfvp9\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-7scnq"
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-7scnq
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-7scnq
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-7scnq
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-sfvp9
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:45.764: INFO: Received response from host: affinity-nodeport-transition-7scnq
Dec  7 17:56:45.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4646 exec execpod-affinity2sw4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.192.217.108:30307/ ; done'
Dec  7 17:56:46.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:30307/\n"
Dec  7 17:56:46.202: INFO: stdout: "\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw\naffinity-nodeport-transition-4tlpw"
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Received response from host: affinity-nodeport-transition-4tlpw
Dec  7 17:56:46.202: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4646, will wait for the garbage collector to delete the pods
Dec  7 17:56:46.308: INFO: Deleting ReplicationController affinity-nodeport-transition took: 17.430451ms
Dec  7 17:56:46.509: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.935199ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:56:49.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4646" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:43.966 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":124,"skipped":2075,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:56:49.358: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-cfd979fc-7a41-41c9-abfe-0995c2e90c19
STEP: Creating a pod to test consume secrets
Dec  7 17:56:49.620: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9" in namespace "projected-8605" to be "Succeeded or Failed"
Dec  7 17:56:49.630: INFO: Pod "pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.480878ms
Dec  7 17:56:51.649: INFO: Pod "pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028861965s
Dec  7 17:56:53.665: INFO: Pod "pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045330401s
STEP: Saw pod success
Dec  7 17:56:53.665: INFO: Pod "pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9" satisfied condition "Succeeded or Failed"
Dec  7 17:56:53.676: INFO: Trying to get logs from node 10.192.217.124 pod pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  7 17:56:53.738: INFO: Waiting for pod pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9 to disappear
Dec  7 17:56:53.748: INFO: Pod pod-projected-secrets-6e249c40-16ae-43dd-ba2a-b26888ad06f9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:56:53.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8605" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2120,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:56:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Dec  7 17:56:54.045: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec  7 17:56:59.062: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Dec  7 17:56:59.077: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:56:59.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6784" for this suite.

• [SLOW TEST:5.408 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":126,"skipped":2126,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:56:59.190: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:56:59.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8507" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":127,"skipped":2133,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:56:59.631: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 17:56:59.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8" in namespace "downward-api-8794" to be "Succeeded or Failed"
Dec  7 17:56:59.895: INFO: Pod "downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03401ms
Dec  7 17:57:01.914: INFO: Pod "downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028377273s
Dec  7 17:57:03.928: INFO: Pod "downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04288514s
STEP: Saw pod success
Dec  7 17:57:03.928: INFO: Pod "downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8" satisfied condition "Succeeded or Failed"
Dec  7 17:57:03.939: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8 container client-container: <nil>
STEP: delete the pod
Dec  7 17:57:04.079: INFO: Waiting for pod downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8 to disappear
Dec  7 17:57:04.090: INFO: Pod downwardapi-volume-c0430fe8-fd04-4167-995a-da5932b35cb8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:57:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8794" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2143,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:57:04.124: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:57:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2036" for this suite.

• [SLOW TEST:17.448 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":129,"skipped":2153,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:57:21.573: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:57:21.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-130" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":130,"skipped":2153,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:57:21.855: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  7 17:57:22.106: INFO: Waiting up to 5m0s for pod "pod-981424f6-6e76-44a5-9cae-82b655407e9e" in namespace "emptydir-2437" to be "Succeeded or Failed"
Dec  7 17:57:22.116: INFO: Pod "pod-981424f6-6e76-44a5-9cae-82b655407e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.946048ms
Dec  7 17:57:24.133: INFO: Pod "pod-981424f6-6e76-44a5-9cae-82b655407e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027108673s
Dec  7 17:57:26.152: INFO: Pod "pod-981424f6-6e76-44a5-9cae-82b655407e9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045171368s
STEP: Saw pod success
Dec  7 17:57:26.152: INFO: Pod "pod-981424f6-6e76-44a5-9cae-82b655407e9e" satisfied condition "Succeeded or Failed"
Dec  7 17:57:26.162: INFO: Trying to get logs from node 10.192.217.92 pod pod-981424f6-6e76-44a5-9cae-82b655407e9e container test-container: <nil>
STEP: delete the pod
Dec  7 17:57:26.221: INFO: Waiting for pod pod-981424f6-6e76-44a5-9cae-82b655407e9e to disappear
Dec  7 17:57:26.233: INFO: Pod pod-981424f6-6e76-44a5-9cae-82b655407e9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:57:26.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2437" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:57:26.266: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  7 17:57:26.514: INFO: PodSpec: initContainers in spec.initContainers
Dec  7 17:58:10.393: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d6489b68-09b9-486e-9a3c-1e53f482f625", GenerateName:"", Namespace:"init-container-6426", SelfLink:"", UID:"74988de4-fe83-4cd1-a88f-001df26db1aa", ResourceVersion:"31289", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63774496646, loc:(*time.Location)(0xa0a1d40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"514510429"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"553dc20304e8d6fe894d6ae24bdbbf91bed38b242c35f56e80e378f2c1d137d9", "cni.projectcalico.org/podIP":"172.30.34.149/32", "cni.projectcalico.org/podIPs":"172.30.34.149/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00178a078), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00178a090), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00178a0a8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00178a0c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00178a0d8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00178a0f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wlwzp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003cd8100), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wlwzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wlwzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wlwzp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00809e118), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.192.217.92", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003a48000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00809e1a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00809e1c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00809e1c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00809e1cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0040d6050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496646, loc:(*time.Location)(0xa0a1d40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.192.217.92", PodIP:"172.30.34.149", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.34.149"}}, StartTime:(*v1.Time)(0xc00178a120), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a480e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a48150)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://41326a72176f41c49e73313a028f4e81376be62a25fb414333c8c58ed442353c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003cd82e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003cd8280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc00809e244)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:10.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6426" for this suite.

• [SLOW TEST:44.177 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":132,"skipped":2211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:10.445: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  7 17:58:10.720: INFO: Waiting up to 5m0s for pod "pod-6bb97145-1413-4ea7-92e5-ad215c5f3397" in namespace "emptydir-2410" to be "Succeeded or Failed"
Dec  7 17:58:10.754: INFO: Pod "pod-6bb97145-1413-4ea7-92e5-ad215c5f3397": Phase="Pending", Reason="", readiness=false. Elapsed: 34.510549ms
Dec  7 17:58:12.769: INFO: Pod "pod-6bb97145-1413-4ea7-92e5-ad215c5f3397": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049048689s
Dec  7 17:58:14.783: INFO: Pod "pod-6bb97145-1413-4ea7-92e5-ad215c5f3397": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063304246s
STEP: Saw pod success
Dec  7 17:58:14.783: INFO: Pod "pod-6bb97145-1413-4ea7-92e5-ad215c5f3397" satisfied condition "Succeeded or Failed"
Dec  7 17:58:14.794: INFO: Trying to get logs from node 10.192.217.124 pod pod-6bb97145-1413-4ea7-92e5-ad215c5f3397 container test-container: <nil>
STEP: delete the pod
Dec  7 17:58:14.854: INFO: Waiting for pod pod-6bb97145-1413-4ea7-92e5-ad215c5f3397 to disappear
Dec  7 17:58:14.864: INFO: Pod pod-6bb97145-1413-4ea7-92e5-ad215c5f3397 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:14.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2410" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 17:58:15.675: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 17:58:17.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496695, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496695, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496695, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774496695, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 17:58:20.787: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 17:58:20.805: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1676-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:24.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1264" for this suite.
STEP: Destroying namespace "webhook-1264-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":134,"skipped":2269,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:24.488: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:32.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1740" for this suite.

• [SLOW TEST:8.307 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":135,"skipped":2281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:32.796: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Dec  7 17:58:33.049: INFO: Waiting up to 5m0s for pod "var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab" in namespace "var-expansion-8593" to be "Succeeded or Failed"
Dec  7 17:58:33.059: INFO: Pod "var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.151315ms
Dec  7 17:58:35.075: INFO: Pod "var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.026173343s
Dec  7 17:58:37.115: INFO: Pod "var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06619599s
STEP: Saw pod success
Dec  7 17:58:37.116: INFO: Pod "var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab" satisfied condition "Succeeded or Failed"
Dec  7 17:58:37.149: INFO: Trying to get logs from node 10.192.217.92 pod var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab container dapi-container: <nil>
STEP: delete the pod
Dec  7 17:58:37.254: INFO: Waiting for pod var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab to disappear
Dec  7 17:58:37.264: INFO: Pod var-expansion-01cc9236-6ff3-44b0-9373-6e6d4345c1ab no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:37.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8593" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":136,"skipped":2391,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Dec  7 17:58:37.562: INFO: Waiting up to 5m0s for pod "var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693" in namespace "var-expansion-8585" to be "Succeeded or Failed"
Dec  7 17:58:37.572: INFO: Pod "var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693": Phase="Pending", Reason="", readiness=false. Elapsed: 9.876002ms
Dec  7 17:58:39.592: INFO: Pod "var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029956673s
STEP: Saw pod success
Dec  7 17:58:39.592: INFO: Pod "var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693" satisfied condition "Succeeded or Failed"
Dec  7 17:58:39.603: INFO: Trying to get logs from node 10.192.217.124 pod var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693 container dapi-container: <nil>
STEP: delete the pod
Dec  7 17:58:39.672: INFO: Waiting for pod var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693 to disappear
Dec  7 17:58:39.683: INFO: Pod var-expansion-18d528e3-4e09-4f64-9c7a-fbf413732693 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:39.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8585" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2391,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:39.811: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec  7 17:58:40.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1012 create -f -'
Dec  7 17:58:40.286: INFO: stderr: ""
Dec  7 17:58:40.287: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec  7 17:58:41.300: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:58:41.300: INFO: Found 0 / 1
Dec  7 17:58:42.304: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:58:42.304: INFO: Found 0 / 1
Dec  7 17:58:43.312: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:58:43.312: INFO: Found 1 / 1
Dec  7 17:58:43.312: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  7 17:58:43.324: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:58:43.324: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  7 17:58:43.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-1012 patch pod agnhost-primary-j2xbv -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  7 17:58:43.416: INFO: stderr: ""
Dec  7 17:58:43.416: INFO: stdout: "pod/agnhost-primary-j2xbv patched\n"
STEP: checking annotations
Dec  7 17:58:43.428: INFO: Selector matched 1 pods for map[app:agnhost]
Dec  7 17:58:43.428: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:43.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1012" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":138,"skipped":2399,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-1316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  7 17:58:43.837: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  7 17:58:43.856: INFO: starting watch
STEP: patching
STEP: updating
Dec  7 17:58:43.964: INFO: waiting for watch events with expected annotations
Dec  7 17:58:43.964: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:44.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1316" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":139,"skipped":2403,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-7ba8ed1b-2f55-41ad-92a0-3ba3c2c1e143
STEP: Creating a pod to test consume configMaps
Dec  7 17:58:44.510: INFO: Waiting up to 5m0s for pod "pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8" in namespace "configmap-5398" to be "Succeeded or Failed"
Dec  7 17:58:44.521: INFO: Pod "pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.91476ms
Dec  7 17:58:46.543: INFO: Pod "pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032163786s
Dec  7 17:58:48.556: INFO: Pod "pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045129705s
STEP: Saw pod success
Dec  7 17:58:48.556: INFO: Pod "pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8" satisfied condition "Succeeded or Failed"
Dec  7 17:58:48.567: INFO: Trying to get logs from node 10.192.217.124 pod pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 17:58:48.631: INFO: Waiting for pod pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8 to disappear
Dec  7 17:58:48.640: INFO: Pod pod-configmaps-ab43ca05-6919-4842-aafd-1500482f6cd8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 17:58:48.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5398" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2414,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 17:58:48.690: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-0152aacd-6803-43fc-afc8-9d5a3b58f2ba in namespace container-probe-6661
Dec  7 17:58:53.011: INFO: Started pod busybox-0152aacd-6803-43fc-afc8-9d5a3b58f2ba in namespace container-probe-6661
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 17:58:53.022: INFO: Initial restart count of pod busybox-0152aacd-6803-43fc-afc8-9d5a3b58f2ba is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:02:53.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6661" for this suite.

• [SLOW TEST:244.513 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2426,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:02:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7242
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-7242
Dec  7 18:02:53.460: INFO: Found 0 stateful pods, waiting for 1
Dec  7 18:03:03.482: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:03:03.626: INFO: Deleting all statefulset in ns statefulset-7242
Dec  7 18:03:03.636: INFO: Scaling statefulset ss to 0
Dec  7 18:03:13.766: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:03:13.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:13.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7242" for this suite.

• [SLOW TEST:20.662 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":142,"skipped":2428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:13.869: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  7 18:03:17.212: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:17.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7011" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2451,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:17.310: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Dec  7 18:03:17.585: INFO: Found Service test-service-6rsng in namespace services-4827 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec  7 18:03:17.586: INFO: Service test-service-6rsng created
STEP: Getting /status
Dec  7 18:03:17.600: INFO: Service test-service-6rsng has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Dec  7 18:03:17.628: INFO: observed Service test-service-6rsng in namespace services-4827 with annotations: map[] & LoadBalancer: {[]}
Dec  7 18:03:17.628: INFO: Found Service test-service-6rsng in namespace services-4827 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec  7 18:03:17.628: INFO: Service test-service-6rsng has service status patched
STEP: updating the ServiceStatus
Dec  7 18:03:17.658: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Dec  7 18:03:17.663: INFO: Observed Service test-service-6rsng in namespace services-4827 with annotations: map[] & Conditions: {[]}
Dec  7 18:03:17.663: INFO: Observed event: &Service{ObjectMeta:{test-service-6rsng  services-4827  f3325761-94b1-4069-9e1c-930c68843e24 32395 0 2021-12-07 18:03:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-12-07 18:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-12-07 18:03:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.243.77,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:nil,ClusterIPs:[172.21.243.77],IPFamilies:[],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec  7 18:03:17.664: INFO: Found Service test-service-6rsng in namespace services-4827 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  7 18:03:17.664: INFO: Service test-service-6rsng has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Dec  7 18:03:17.709: INFO: observed Service test-service-6rsng in namespace services-4827 with labels: map[test-service-static:true]
Dec  7 18:03:17.709: INFO: observed Service test-service-6rsng in namespace services-4827 with labels: map[test-service-static:true]
Dec  7 18:03:17.709: INFO: observed Service test-service-6rsng in namespace services-4827 with labels: map[test-service-static:true]
Dec  7 18:03:17.710: INFO: Found Service test-service-6rsng in namespace services-4827 with labels: map[test-service:patched test-service-static:true]
Dec  7 18:03:17.710: INFO: Service test-service-6rsng patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Dec  7 18:03:17.768: INFO: Observed event: ADDED
Dec  7 18:03:17.768: INFO: Observed event: MODIFIED
Dec  7 18:03:17.769: INFO: Observed event: MODIFIED
Dec  7 18:03:17.769: INFO: Observed event: MODIFIED
Dec  7 18:03:17.769: INFO: Found Service test-service-6rsng in namespace services-4827 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec  7 18:03:17.769: INFO: Service test-service-6rsng deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:17.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4827" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":144,"skipped":2462,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:17.814: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4197
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:03:18.042: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  7 18:03:20.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-4197 --namespace=crd-publish-openapi-4197 create -f -'
Dec  7 18:03:21.740: INFO: stderr: ""
Dec  7 18:03:21.740: INFO: stdout: "e2e-test-crd-publish-openapi-9835-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  7 18:03:21.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-4197 --namespace=crd-publish-openapi-4197 delete e2e-test-crd-publish-openapi-9835-crds test-cr'
Dec  7 18:03:21.841: INFO: stderr: ""
Dec  7 18:03:21.841: INFO: stdout: "e2e-test-crd-publish-openapi-9835-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec  7 18:03:21.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-4197 --namespace=crd-publish-openapi-4197 apply -f -'
Dec  7 18:03:23.770: INFO: stderr: ""
Dec  7 18:03:23.770: INFO: stdout: "e2e-test-crd-publish-openapi-9835-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  7 18:03:23.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-4197 --namespace=crd-publish-openapi-4197 delete e2e-test-crd-publish-openapi-9835-crds test-cr'
Dec  7 18:03:23.880: INFO: stderr: ""
Dec  7 18:03:23.880: INFO: stdout: "e2e-test-crd-publish-openapi-9835-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  7 18:03:23.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-4197 explain e2e-test-crd-publish-openapi-9835-crds'
Dec  7 18:03:25.183: INFO: stderr: ""
Dec  7 18:03:25.183: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9835-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:29.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4197" for this suite.

• [SLOW TEST:11.621 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":145,"skipped":2472,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:29.436: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  7 18:03:32.749: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:32.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9234" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2484,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:32.833: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  7 18:03:33.074: INFO: Waiting up to 5m0s for pod "downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552" in namespace "downward-api-1859" to be "Succeeded or Failed"
Dec  7 18:03:33.085: INFO: Pod "downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552": Phase="Pending", Reason="", readiness=false. Elapsed: 11.487125ms
Dec  7 18:03:35.101: INFO: Pod "downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027000934s
Dec  7 18:03:37.115: INFO: Pod "downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041214244s
STEP: Saw pod success
Dec  7 18:03:37.115: INFO: Pod "downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552" satisfied condition "Succeeded or Failed"
Dec  7 18:03:37.125: INFO: Trying to get logs from node 10.192.217.92 pod downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552 container dapi-container: <nil>
STEP: delete the pod
Dec  7 18:03:37.241: INFO: Waiting for pod downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552 to disappear
Dec  7 18:03:37.252: INFO: Pod downward-api-ba8b3f0e-246d-4031-8601-e07f92cb7552 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:03:37.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1859" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2487,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:03:37.289: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3099
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec  7 18:03:37.510: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec  7 18:03:52.287: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:03:56.475: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:12.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3099" for this suite.

• [SLOW TEST:34.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":148,"skipped":2491,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:12.225: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Dec  7 18:04:12.543: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-6163 proxy --unix-socket=/tmp/kubectl-proxy-unix492510007/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:12.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6163" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":149,"skipped":2493,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:12.617: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7493" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":150,"skipped":2496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:13.074: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:13.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1314" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":151,"skipped":2549,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:13.379: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  7 18:04:15.673: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:15.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-782" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":152,"skipped":2557,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:15.753: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3784
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  7 18:04:16.010: INFO: Waiting up to 5m0s for pod "pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a" in namespace "emptydir-3784" to be "Succeeded or Failed"
Dec  7 18:04:16.022: INFO: Pod "pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.055674ms
Dec  7 18:04:18.039: INFO: Pod "pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028646096s
Dec  7 18:04:20.055: INFO: Pod "pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045323248s
STEP: Saw pod success
Dec  7 18:04:20.055: INFO: Pod "pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a" satisfied condition "Succeeded or Failed"
Dec  7 18:04:20.066: INFO: Trying to get logs from node 10.192.217.124 pod pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a container test-container: <nil>
STEP: delete the pod
Dec  7 18:04:20.232: INFO: Waiting for pod pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a to disappear
Dec  7 18:04:20.273: INFO: Pod pod-b9b19a1e-09a3-4f90-8202-a7e8265d1f1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3784" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2561,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:20.339: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-jnpp
STEP: Creating a pod to test atomic-volume-subpath
Dec  7 18:04:20.626: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jnpp" in namespace "subpath-8541" to be "Succeeded or Failed"
Dec  7 18:04:20.637: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.720219ms
Dec  7 18:04:22.652: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.025855181s
Dec  7 18:04:24.666: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 4.039792317s
Dec  7 18:04:26.680: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 6.054029227s
Dec  7 18:04:28.706: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 8.08025721s
Dec  7 18:04:30.724: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 10.09817315s
Dec  7 18:04:32.739: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 12.113194234s
Dec  7 18:04:34.757: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 14.131178253s
Dec  7 18:04:36.773: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 16.146683444s
Dec  7 18:04:38.790: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 18.164524588s
Dec  7 18:04:40.806: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 20.180531502s
Dec  7 18:04:42.823: INFO: Pod "pod-subpath-test-configmap-jnpp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.197422459s
STEP: Saw pod success
Dec  7 18:04:42.823: INFO: Pod "pod-subpath-test-configmap-jnpp" satisfied condition "Succeeded or Failed"
Dec  7 18:04:42.834: INFO: Trying to get logs from node 10.192.217.92 pod pod-subpath-test-configmap-jnpp container test-container-subpath-configmap-jnpp: <nil>
STEP: delete the pod
Dec  7 18:04:42.902: INFO: Waiting for pod pod-subpath-test-configmap-jnpp to disappear
Dec  7 18:04:42.912: INFO: Pod pod-subpath-test-configmap-jnpp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jnpp
Dec  7 18:04:42.912: INFO: Deleting pod "pod-subpath-test-configmap-jnpp" in namespace "subpath-8541"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8541" for this suite.

• [SLOW TEST:22.627 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":154,"skipped":2570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:42.966: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:43.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2904" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":155,"skipped":2592,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:04:43.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed" in namespace "downward-api-6148" to be "Succeeded or Failed"
Dec  7 18:04:43.509: INFO: Pod "downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.059688ms
Dec  7 18:04:45.524: INFO: Pod "downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.024814491s
Dec  7 18:04:47.540: INFO: Pod "downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040804208s
STEP: Saw pod success
Dec  7 18:04:47.540: INFO: Pod "downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed" satisfied condition "Succeeded or Failed"
Dec  7 18:04:47.550: INFO: Trying to get logs from node 10.192.217.124 pod downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed container client-container: <nil>
STEP: delete the pod
Dec  7 18:04:47.610: INFO: Waiting for pod downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed to disappear
Dec  7 18:04:47.620: INFO: Pod downwardapi-volume-80afede2-da5e-45e4-a505-955882a5c6ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:04:47.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6148" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2604,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:04:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  7 18:04:48.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32918 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:04:48.031: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32918 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  7 18:04:58.086: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32970 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:04:58.086: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32970 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  7 18:05:08.137: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32984 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:05:08.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32984 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  7 18:05:18.180: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32999 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:05:18.180: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7319  fa944856-b88d-4f9a-8a5e-48b66ad1abfb 32999 0 2021-12-07 18:04:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-07 18:04:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  7 18:05:28.225: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7319  efcfba68-c18e-4687-8996-0c12685f3de6 33013 0 2021-12-07 18:05:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-07 18:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:05:28.226: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7319  efcfba68-c18e-4687-8996-0c12685f3de6 33013 0 2021-12-07 18:05:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-07 18:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  7 18:05:38.272: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7319  efcfba68-c18e-4687-8996-0c12685f3de6 33027 0 2021-12-07 18:05:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-07 18:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:05:38.272: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7319  efcfba68-c18e-4687-8996-0c12685f3de6 33027 0 2021-12-07 18:05:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-07 18:05:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:05:48.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7319" for this suite.

• [SLOW TEST:60.672 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":157,"skipped":2617,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:05:48.332: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8118
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-8118
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8118
Dec  7 18:05:48.688: INFO: Found 0 stateful pods, waiting for 1
Dec  7 18:05:58.717: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  7 18:05:58.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:05:59.064: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:05:59.064: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:05:59.064: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 18:05:59.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  7 18:06:09.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 18:06:09.102: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:06:09.150: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec  7 18:06:09.150: INFO: ss-0  10.192.217.92  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  }]
Dec  7 18:06:09.150: INFO: 
Dec  7 18:06:09.150: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  7 18:06:10.166: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988946364s
Dec  7 18:06:11.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973598877s
Dec  7 18:06:12.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960009231s
Dec  7 18:06:13.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945324021s
Dec  7 18:06:14.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929651028s
Dec  7 18:06:15.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.915089207s
Dec  7 18:06:16.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900389684s
Dec  7 18:06:17.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.885890483s
Dec  7 18:06:18.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 871.422743ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8118
Dec  7 18:06:19.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 18:06:19.557: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 18:06:19.557: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 18:06:19.557: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 18:06:19.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 18:06:19.864: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  7 18:06:19.864: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 18:06:19.864: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 18:06:19.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 18:06:20.167: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  7 18:06:20.167: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 18:06:20.167: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 18:06:20.181: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:06:20.181: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:06:20.181: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  7 18:06:20.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:06:20.530: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:06:20.530: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:06:20.530: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 18:06:20.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:06:20.897: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:06:20.897: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:06:20.897: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 18:06:20.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-8118 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:06:21.152: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:06:21.152: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:06:21.152: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 18:06:21.152: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:06:21.162: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  7 18:06:31.190: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 18:06:31.190: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 18:06:31.190: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  7 18:06:31.226: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec  7 18:06:31.226: INFO: ss-0  10.192.217.92   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  }]
Dec  7 18:06:31.226: INFO: ss-1  10.192.217.124  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  }]
Dec  7 18:06:31.226: INFO: ss-2  10.192.217.108  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  }]
Dec  7 18:06:31.226: INFO: 
Dec  7 18:06:31.226: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  7 18:06:32.249: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec  7 18:06:32.249: INFO: ss-0  10.192.217.92   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:05:48 +0000 UTC  }]
Dec  7 18:06:32.249: INFO: ss-1  10.192.217.124  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  }]
Dec  7 18:06:32.249: INFO: ss-2  10.192.217.108  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-07 18:06:09 +0000 UTC  }]
Dec  7 18:06:32.249: INFO: 
Dec  7 18:06:32.249: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  7 18:06:33.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.966374173s
Dec  7 18:06:34.282: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.952429674s
Dec  7 18:06:35.296: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.932963199s
Dec  7 18:06:36.310: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.91883885s
Dec  7 18:06:37.325: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.904435143s
Dec  7 18:06:38.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.889075999s
Dec  7 18:06:39.358: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.871152234s
Dec  7 18:06:40.372: INFO: Verifying statefulset ss doesn't scale past 0 for another 856.588204ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8118
Dec  7 18:06:41.385: INFO: Scaling statefulset ss to 0
Dec  7 18:06:41.437: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:06:41.448: INFO: Deleting all statefulset in ns statefulset-8118
Dec  7 18:06:41.458: INFO: Scaling statefulset ss to 0
Dec  7 18:06:41.493: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:06:41.503: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:41.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8118" for this suite.

• [SLOW TEST:53.256 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":158,"skipped":2624,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9985" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":159,"skipped":2627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:42.038: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7617
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  7 18:06:42.307: INFO: Waiting up to 5m0s for pod "pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651" in namespace "emptydir-7617" to be "Succeeded or Failed"
Dec  7 18:06:42.318: INFO: Pod "pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651": Phase="Pending", Reason="", readiness=false. Elapsed: 10.837058ms
Dec  7 18:06:44.334: INFO: Pod "pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027511197s
Dec  7 18:06:46.348: INFO: Pod "pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041515887s
STEP: Saw pod success
Dec  7 18:06:46.348: INFO: Pod "pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651" satisfied condition "Succeeded or Failed"
Dec  7 18:06:46.359: INFO: Trying to get logs from node 10.192.217.124 pod pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651 container test-container: <nil>
STEP: delete the pod
Dec  7 18:06:46.476: INFO: Waiting for pod pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651 to disappear
Dec  7 18:06:46.486: INFO: Pod pod-6f27185b-c4a9-4cd7-9a1a-85cb48bac651 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:46.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7617" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2669,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:46.553: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:49.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2395" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":161,"skipped":2681,"failed":0}
S
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:49.039: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  7 18:06:53.363: INFO: &Pod{ObjectMeta:{send-events-1c5dca21-684c-404b-92f3-5aed8dab9438  events-4380  4c17e28a-a1b5-4428-a4d5-38c368cbaef8 33468 0 2021-12-07 18:06:49 +0000 UTC <nil> <nil> map[name:foo time:283441836] map[cni.projectcalico.org/containerID:8b60ad3ab86273b844890b37296a0a22688e25862402cf2a8464d68c2818a1c1 cni.projectcalico.org/podIP:172.30.30.83/32 cni.projectcalico.org/podIPs:172.30.30.83/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-12-07 18:06:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 18:06:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 18:06:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxhdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxhdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:06:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:06:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:06:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:06:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.83,StartTime:2021-12-07 18:06:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 18:06:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://47227e4bed41894023c8ab529f9e4f0f9767971724d17545317e248f689ff2ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec  7 18:06:55.385: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  7 18:06:57.405: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:57.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4380" for this suite.

• [SLOW TEST:8.453 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":162,"skipped":2682,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:57.493: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  7 18:06:57.743: INFO: Waiting up to 5m0s for pod "downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a" in namespace "downward-api-9827" to be "Succeeded or Failed"
Dec  7 18:06:57.754: INFO: Pod "downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.755609ms
Dec  7 18:06:59.770: INFO: Pod "downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027680403s
STEP: Saw pod success
Dec  7 18:06:59.771: INFO: Pod "downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a" satisfied condition "Succeeded or Failed"
Dec  7 18:06:59.782: INFO: Trying to get logs from node 10.192.217.92 pod downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a container dapi-container: <nil>
STEP: delete the pod
Dec  7 18:06:59.890: INFO: Waiting for pod downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a to disappear
Dec  7 18:06:59.901: INFO: Pod downward-api-cdccbe8e-4616-4b8d-b55d-a28b35169a1a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:06:59.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9827" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":2686,"failed":0}
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:06:59.973: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:07:00.250: INFO: The status of Pod busybox-scheduling-4fb37e7a-9f62-4d1a-a7e2-aad35b95e210 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:07:02.274: INFO: The status of Pod busybox-scheduling-4fb37e7a-9f62-4d1a-a7e2-aad35b95e210 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:07:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5286" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":2692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:07:02.542: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:07:02.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d" in namespace "downward-api-6223" to be "Succeeded or Failed"
Dec  7 18:07:02.840: INFO: Pod "downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.724519ms
Dec  7 18:07:04.856: INFO: Pod "downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028257453s
Dec  7 18:07:06.882: INFO: Pod "downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053400183s
STEP: Saw pod success
Dec  7 18:07:06.882: INFO: Pod "downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d" satisfied condition "Succeeded or Failed"
Dec  7 18:07:06.897: INFO: Trying to get logs from node 10.192.217.124 pod downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d container client-container: <nil>
STEP: delete the pod
Dec  7 18:07:07.017: INFO: Waiting for pod downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d to disappear
Dec  7 18:07:07.030: INFO: Pod downwardapi-volume-24a87693-e384-41c3-8c5a-195c59abed4d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:07:07.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6223" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":2725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:07:07.073: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-xdz8
STEP: Creating a pod to test atomic-volume-subpath
Dec  7 18:07:07.456: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xdz8" in namespace "subpath-5380" to be "Succeeded or Failed"
Dec  7 18:07:07.472: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.270204ms
Dec  7 18:07:09.517: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061671071s
Dec  7 18:07:11.566: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 4.109860886s
Dec  7 18:07:13.591: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 6.135368598s
Dec  7 18:07:15.608: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 8.151818138s
Dec  7 18:07:17.635: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 10.179167845s
Dec  7 18:07:19.658: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 12.202279579s
Dec  7 18:07:21.680: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 14.223799625s
Dec  7 18:07:23.700: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 16.244497668s
Dec  7 18:07:25.718: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 18.262493636s
Dec  7 18:07:27.739: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 20.283717938s
Dec  7 18:07:29.763: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Running", Reason="", readiness=true. Elapsed: 22.307119791s
Dec  7 18:07:31.785: INFO: Pod "pod-subpath-test-configmap-xdz8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.329224515s
STEP: Saw pod success
Dec  7 18:07:31.785: INFO: Pod "pod-subpath-test-configmap-xdz8" satisfied condition "Succeeded or Failed"
Dec  7 18:07:31.797: INFO: Trying to get logs from node 10.192.217.124 pod pod-subpath-test-configmap-xdz8 container test-container-subpath-configmap-xdz8: <nil>
STEP: delete the pod
Dec  7 18:07:31.878: INFO: Waiting for pod pod-subpath-test-configmap-xdz8 to disappear
Dec  7 18:07:31.891: INFO: Pod pod-subpath-test-configmap-xdz8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xdz8
Dec  7 18:07:31.891: INFO: Deleting pod "pod-subpath-test-configmap-xdz8" in namespace "subpath-5380"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:07:31.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5380" for this suite.

• [SLOW TEST:24.875 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":166,"skipped":2747,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:07:31.948: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:07:33.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:07:35.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497253, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497253, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497253, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497253, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:07:38.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec  7 18:07:38.325: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:07:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6148" for this suite.
STEP: Destroying namespace "webhook-6148-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.682 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":167,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:07:38.633: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2364
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:07:38.880: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec  7 18:07:43.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 create -f -'
Dec  7 18:07:45.359: INFO: stderr: ""
Dec  7 18:07:45.359: INFO: stdout: "e2e-test-crd-publish-openapi-2726-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  7 18:07:45.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 delete e2e-test-crd-publish-openapi-2726-crds test-foo'
Dec  7 18:07:45.469: INFO: stderr: ""
Dec  7 18:07:45.469: INFO: stdout: "e2e-test-crd-publish-openapi-2726-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec  7 18:07:45.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 apply -f -'
Dec  7 18:07:46.753: INFO: stderr: ""
Dec  7 18:07:46.753: INFO: stdout: "e2e-test-crd-publish-openapi-2726-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  7 18:07:46.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 delete e2e-test-crd-publish-openapi-2726-crds test-foo'
Dec  7 18:07:46.931: INFO: stderr: ""
Dec  7 18:07:46.932: INFO: stdout: "e2e-test-crd-publish-openapi-2726-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec  7 18:07:46.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 create -f -'
Dec  7 18:07:47.114: INFO: rc: 1
Dec  7 18:07:47.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 apply -f -'
Dec  7 18:07:48.084: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec  7 18:07:48.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 create -f -'
Dec  7 18:07:48.250: INFO: rc: 1
Dec  7 18:07:48.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 --namespace=crd-publish-openapi-2364 apply -f -'
Dec  7 18:07:48.434: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec  7 18:07:48.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 explain e2e-test-crd-publish-openapi-2726-crds'
Dec  7 18:07:48.615: INFO: stderr: ""
Dec  7 18:07:48.615: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2726-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec  7 18:07:48.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 explain e2e-test-crd-publish-openapi-2726-crds.metadata'
Dec  7 18:07:48.801: INFO: stderr: ""
Dec  7 18:07:48.801: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2726-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec  7 18:07:48.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 explain e2e-test-crd-publish-openapi-2726-crds.spec'
Dec  7 18:07:48.995: INFO: stderr: ""
Dec  7 18:07:48.995: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2726-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec  7 18:07:48.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 explain e2e-test-crd-publish-openapi-2726-crds.spec.bars'
Dec  7 18:07:49.192: INFO: stderr: ""
Dec  7 18:07:49.192: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2726-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec  7 18:07:49.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2364 explain e2e-test-crd-publish-openapi-2726-crds.spec.bars2'
Dec  7 18:07:49.370: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:07:54.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2364" for this suite.

• [SLOW TEST:15.428 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":168,"skipped":2775,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:07:54.062: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:07:54.297: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  7 18:07:54.341: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  7 18:07:59.367: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  7 18:07:59.367: INFO: Creating deployment "test-rolling-update-deployment"
Dec  7 18:07:59.388: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  7 18:07:59.423: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  7 18:08:01.475: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  7 18:08:01.488: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 18:08:01.529: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1373  586ae121-df1b-489e-83df-2ccc518567c2 33906 1 2021-12-07 18:07:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-12-07 18:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00487a948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-12-07 18:07:59 +0000 UTC,LastTransitionTime:2021-12-07 18:07:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-12-07 18:08:01 +0000 UTC,LastTransitionTime:2021-12-07 18:07:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  7 18:08:01.544: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-1373  39b52c65-65a9-4ca7-bea0-59f506f0a3e3 33896 1 2021-12-07 18:07:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 586ae121-df1b-489e-83df-2ccc518567c2 0xc00481f587 0xc00481f588}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586ae121-df1b-489e-83df-2ccc518567c2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:08:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00481f648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:08:01.544: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  7 18:08:01.544: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1373  faf26403-b945-4f89-b021-305747b68ac5 33905 2 2021-12-07 18:07:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 586ae121-df1b-489e-83df-2ccc518567c2 0xc00481f447 0xc00481f448}] []  [{e2e.test Update apps/v1 2021-12-07 18:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"586ae121-df1b-489e-83df-2ccc518567c2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:08:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00481f518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:08:01.561: INFO: Pod "test-rolling-update-deployment-585b757574-vzjhq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-vzjhq test-rolling-update-deployment-585b757574- deployment-1373  f1112430-0183-416f-af3e-5abb04c697dd 33895 0 2021-12-07 18:07:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:438e79af4231fbeaf958e7adb29ca9278592223f851b1f8360366ab0d00d6dbc cni.projectcalico.org/podIP:172.30.34.165/32 cni.projectcalico.org/podIPs:172.30.34.165/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 39b52c65-65a9-4ca7-bea0-59f506f0a3e3 0xc00481fc37 0xc00481fc38}] []  [{kube-controller-manager Update v1 2021-12-07 18:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39b52c65-65a9-4ca7-bea0-59f506f0a3e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 18:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 18:08:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsqjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsqjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:08:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:08:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.165,StartTime:2021-12-07 18:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 18:08:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://618cdaa5fba9b0f5c1e173dac5c68798ae4ac8dee2f65cf07417ad5c78d11a8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:01.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1373" for this suite.

• [SLOW TEST:7.547 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":169,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:01.609: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:08:01.880: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2430
I1207 18:08:01.903364      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2430, replica count: 1
I1207 18:08:02.954785      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1207 18:08:03.955547      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1207 18:08:04.955723      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 18:08:05.097: INFO: Created: latency-svc-2bghf
Dec  7 18:08:05.117: INFO: Got endpoints: latency-svc-2bghf [60.852452ms]
Dec  7 18:08:05.158: INFO: Created: latency-svc-tjhwz
Dec  7 18:08:05.174: INFO: Got endpoints: latency-svc-tjhwz [57.003914ms]
Dec  7 18:08:05.178: INFO: Created: latency-svc-vlztf
Dec  7 18:08:05.190: INFO: Created: latency-svc-44zxg
Dec  7 18:08:05.196: INFO: Got endpoints: latency-svc-vlztf [78.428472ms]
Dec  7 18:08:05.202: INFO: Got endpoints: latency-svc-44zxg [84.622376ms]
Dec  7 18:08:05.208: INFO: Created: latency-svc-jx6l4
Dec  7 18:08:05.218: INFO: Created: latency-svc-lsxjn
Dec  7 18:08:05.222: INFO: Got endpoints: latency-svc-jx6l4 [104.371466ms]
Dec  7 18:08:05.227: INFO: Got endpoints: latency-svc-lsxjn [109.979496ms]
Dec  7 18:08:05.245: INFO: Created: latency-svc-d26jj
Dec  7 18:08:05.259: INFO: Created: latency-svc-8cb2x
Dec  7 18:08:05.274: INFO: Got endpoints: latency-svc-d26jj [72.438098ms]
Dec  7 18:08:05.275: INFO: Created: latency-svc-sd2gz
Dec  7 18:08:05.277: INFO: Got endpoints: latency-svc-8cb2x [158.977413ms]
Dec  7 18:08:05.288: INFO: Got endpoints: latency-svc-sd2gz [170.003581ms]
Dec  7 18:08:05.299: INFO: Created: latency-svc-hgl4h
Dec  7 18:08:05.308: INFO: Created: latency-svc-c7vg2
Dec  7 18:08:05.312: INFO: Got endpoints: latency-svc-hgl4h [193.727484ms]
Dec  7 18:08:05.323: INFO: Created: latency-svc-8kjs8
Dec  7 18:08:05.323: INFO: Got endpoints: latency-svc-c7vg2 [205.077467ms]
Dec  7 18:08:05.329: INFO: Got endpoints: latency-svc-8kjs8 [210.857967ms]
Dec  7 18:08:05.331: INFO: Created: latency-svc-n4dtw
Dec  7 18:08:05.341: INFO: Got endpoints: latency-svc-n4dtw [223.711391ms]
Dec  7 18:08:05.346: INFO: Created: latency-svc-qhvff
Dec  7 18:08:05.357: INFO: Got endpoints: latency-svc-qhvff [239.920665ms]
Dec  7 18:08:05.369: INFO: Created: latency-svc-8vhp7
Dec  7 18:08:05.380: INFO: Created: latency-svc-rv9g4
Dec  7 18:08:05.384: INFO: Got endpoints: latency-svc-8vhp7 [266.095383ms]
Dec  7 18:08:05.393: INFO: Got endpoints: latency-svc-rv9g4 [275.20781ms]
Dec  7 18:08:05.417: INFO: Created: latency-svc-r7vcg
Dec  7 18:08:05.424: INFO: Created: latency-svc-bzgjh
Dec  7 18:08:05.425: INFO: Created: latency-svc-r4pgh
Dec  7 18:08:05.429: INFO: Got endpoints: latency-svc-r7vcg [311.497009ms]
Dec  7 18:08:05.437: INFO: Created: latency-svc-b59fx
Dec  7 18:08:05.443: INFO: Got endpoints: latency-svc-bzgjh [269.180561ms]
Dec  7 18:08:05.445: INFO: Got endpoints: latency-svc-r4pgh [249.245235ms]
Dec  7 18:08:05.476: INFO: Got endpoints: latency-svc-b59fx [254.104288ms]
Dec  7 18:08:05.483: INFO: Created: latency-svc-qdsqp
Dec  7 18:08:05.483: INFO: Created: latency-svc-f8qkc
Dec  7 18:08:05.484: INFO: Created: latency-svc-cdb8k
Dec  7 18:08:05.490: INFO: Created: latency-svc-qbh6s
Dec  7 18:08:05.496: INFO: Got endpoints: latency-svc-cdb8k [268.426346ms]
Dec  7 18:08:05.500: INFO: Got endpoints: latency-svc-f8qkc [223.429028ms]
Dec  7 18:08:05.500: INFO: Got endpoints: latency-svc-qdsqp [226.044014ms]
Dec  7 18:08:05.502: INFO: Got endpoints: latency-svc-qbh6s [214.00935ms]
Dec  7 18:08:05.510: INFO: Created: latency-svc-9sbfm
Dec  7 18:08:05.527: INFO: Got endpoints: latency-svc-9sbfm [215.13014ms]
Dec  7 18:08:05.529: INFO: Created: latency-svc-hr59s
Dec  7 18:08:05.540: INFO: Created: latency-svc-2xvbh
Dec  7 18:08:05.546: INFO: Got endpoints: latency-svc-hr59s [222.550709ms]
Dec  7 18:08:05.551: INFO: Created: latency-svc-wtq7l
Dec  7 18:08:05.551: INFO: Got endpoints: latency-svc-2xvbh [221.824038ms]
Dec  7 18:08:05.563: INFO: Got endpoints: latency-svc-wtq7l [221.388614ms]
Dec  7 18:08:05.568: INFO: Created: latency-svc-qqgc2
Dec  7 18:08:05.579: INFO: Got endpoints: latency-svc-qqgc2 [221.612542ms]
Dec  7 18:08:05.658: INFO: Created: latency-svc-4jn2r
Dec  7 18:08:05.661: INFO: Created: latency-svc-f92bf
Dec  7 18:08:05.661: INFO: Created: latency-svc-dg7p2
Dec  7 18:08:05.661: INFO: Created: latency-svc-7lgch
Dec  7 18:08:05.673: INFO: Created: latency-svc-rwcqh
Dec  7 18:08:05.674: INFO: Created: latency-svc-8zjz4
Dec  7 18:08:05.677: INFO: Got endpoints: latency-svc-dg7p2 [283.951551ms]
Dec  7 18:08:05.678: INFO: Got endpoints: latency-svc-4jn2r [294.27325ms]
Dec  7 18:08:05.681: INFO: Got endpoints: latency-svc-7lgch [252.23132ms]
Dec  7 18:08:05.687: INFO: Got endpoints: latency-svc-f92bf [243.960851ms]
Dec  7 18:08:05.689: INFO: Created: latency-svc-85dbg
Dec  7 18:08:05.689: INFO: Got endpoints: latency-svc-8zjz4 [244.265664ms]
Dec  7 18:08:05.694: INFO: Got endpoints: latency-svc-rwcqh [217.465616ms]
Dec  7 18:08:05.695: INFO: Got endpoints: latency-svc-85dbg [198.588012ms]
Dec  7 18:08:05.701: INFO: Created: latency-svc-x5c4g
Dec  7 18:08:05.711: INFO: Created: latency-svc-7ltgt
Dec  7 18:08:05.714: INFO: Got endpoints: latency-svc-x5c4g [213.989848ms]
Dec  7 18:08:05.731: INFO: Got endpoints: latency-svc-7ltgt [230.290294ms]
Dec  7 18:08:05.731: INFO: Created: latency-svc-nbwrm
Dec  7 18:08:05.749: INFO: Got endpoints: latency-svc-nbwrm [247.111383ms]
Dec  7 18:08:05.811: INFO: Created: latency-svc-bd659
Dec  7 18:08:05.814: INFO: Got endpoints: latency-svc-bd659 [287.206449ms]
Dec  7 18:08:05.835: INFO: Created: latency-svc-94rfp
Dec  7 18:08:05.850: INFO: Created: latency-svc-k5kfs
Dec  7 18:08:05.869: INFO: Got endpoints: latency-svc-94rfp [322.799385ms]
Dec  7 18:08:05.890: INFO: Got endpoints: latency-svc-k5kfs [339.3079ms]
Dec  7 18:08:05.899: INFO: Created: latency-svc-s58hs
Dec  7 18:08:05.911: INFO: Got endpoints: latency-svc-s58hs [347.487465ms]
Dec  7 18:08:05.919: INFO: Created: latency-svc-wtnlm
Dec  7 18:08:05.931: INFO: Created: latency-svc-rrb67
Dec  7 18:08:05.931: INFO: Got endpoints: latency-svc-wtnlm [351.782372ms]
Dec  7 18:08:05.941: INFO: Got endpoints: latency-svc-rrb67 [264.04922ms]
Dec  7 18:08:05.950: INFO: Created: latency-svc-5g9x8
Dec  7 18:08:05.961: INFO: Got endpoints: latency-svc-5g9x8 [283.081761ms]
Dec  7 18:08:05.971: INFO: Created: latency-svc-c625l
Dec  7 18:08:05.982: INFO: Got endpoints: latency-svc-c625l [300.132273ms]
Dec  7 18:08:05.993: INFO: Created: latency-svc-65k9g
Dec  7 18:08:06.002: INFO: Got endpoints: latency-svc-65k9g [314.461812ms]
Dec  7 18:08:06.003: INFO: Created: latency-svc-t4rcj
Dec  7 18:08:06.026: INFO: Got endpoints: latency-svc-t4rcj [336.060745ms]
Dec  7 18:08:06.030: INFO: Created: latency-svc-s4b8t
Dec  7 18:08:06.035: INFO: Created: latency-svc-m69cn
Dec  7 18:08:06.045: INFO: Got endpoints: latency-svc-s4b8t [351.530583ms]
Dec  7 18:08:06.054: INFO: Got endpoints: latency-svc-m69cn [359.292257ms]
Dec  7 18:08:06.060: INFO: Created: latency-svc-fshsz
Dec  7 18:08:06.072: INFO: Got endpoints: latency-svc-fshsz [357.219715ms]
Dec  7 18:08:06.082: INFO: Created: latency-svc-b9kp4
Dec  7 18:08:06.094: INFO: Got endpoints: latency-svc-b9kp4 [363.093303ms]
Dec  7 18:08:06.102: INFO: Created: latency-svc-5m5nf
Dec  7 18:08:06.116: INFO: Got endpoints: latency-svc-5m5nf [366.7119ms]
Dec  7 18:08:06.117: INFO: Created: latency-svc-dcf5w
Dec  7 18:08:06.129: INFO: Got endpoints: latency-svc-dcf5w [314.662068ms]
Dec  7 18:08:06.132: INFO: Created: latency-svc-fzshl
Dec  7 18:08:06.150: INFO: Got endpoints: latency-svc-fzshl [281.491144ms]
Dec  7 18:08:06.156: INFO: Created: latency-svc-wn6bz
Dec  7 18:08:06.166: INFO: Got endpoints: latency-svc-wn6bz [275.537698ms]
Dec  7 18:08:06.169: INFO: Created: latency-svc-w627c
Dec  7 18:08:06.174: INFO: Created: latency-svc-hppmw
Dec  7 18:08:06.175: INFO: Got endpoints: latency-svc-w627c [264.295234ms]
Dec  7 18:08:06.189: INFO: Got endpoints: latency-svc-hppmw [257.842642ms]
Dec  7 18:08:06.192: INFO: Created: latency-svc-mdskt
Dec  7 18:08:06.203: INFO: Got endpoints: latency-svc-mdskt [261.395983ms]
Dec  7 18:08:06.212: INFO: Created: latency-svc-tfxc4
Dec  7 18:08:06.224: INFO: Created: latency-svc-h6lq6
Dec  7 18:08:06.227: INFO: Got endpoints: latency-svc-tfxc4 [265.388413ms]
Dec  7 18:08:06.235: INFO: Got endpoints: latency-svc-h6lq6 [253.704302ms]
Dec  7 18:08:06.250: INFO: Created: latency-svc-pr95j
Dec  7 18:08:06.253: INFO: Got endpoints: latency-svc-pr95j [250.8715ms]
Dec  7 18:08:06.258: INFO: Created: latency-svc-qtmx5
Dec  7 18:08:06.269: INFO: Got endpoints: latency-svc-qtmx5 [243.865351ms]
Dec  7 18:08:06.282: INFO: Created: latency-svc-vkmqq
Dec  7 18:08:06.297: INFO: Got endpoints: latency-svc-vkmqq [251.289375ms]
Dec  7 18:08:06.305: INFO: Created: latency-svc-jfk6m
Dec  7 18:08:06.318: INFO: Got endpoints: latency-svc-jfk6m [264.302225ms]
Dec  7 18:08:06.319: INFO: Created: latency-svc-mwkkl
Dec  7 18:08:06.334: INFO: Got endpoints: latency-svc-mwkkl [262.44943ms]
Dec  7 18:08:06.338: INFO: Created: latency-svc-29ptg
Dec  7 18:08:06.350: INFO: Got endpoints: latency-svc-29ptg [255.943048ms]
Dec  7 18:08:06.353: INFO: Created: latency-svc-2zt47
Dec  7 18:08:06.364: INFO: Got endpoints: latency-svc-2zt47 [247.835702ms]
Dec  7 18:08:06.370: INFO: Created: latency-svc-r5l47
Dec  7 18:08:06.381: INFO: Got endpoints: latency-svc-r5l47 [252.151709ms]
Dec  7 18:08:06.384: INFO: Created: latency-svc-bfqwv
Dec  7 18:08:06.395: INFO: Got endpoints: latency-svc-bfqwv [244.224431ms]
Dec  7 18:08:06.399: INFO: Created: latency-svc-p9cwd
Dec  7 18:08:06.415: INFO: Got endpoints: latency-svc-p9cwd [249.434369ms]
Dec  7 18:08:06.420: INFO: Created: latency-svc-l86d8
Dec  7 18:08:06.433: INFO: Got endpoints: latency-svc-l86d8 [257.560427ms]
Dec  7 18:08:06.440: INFO: Created: latency-svc-kp4rc
Dec  7 18:08:06.452: INFO: Created: latency-svc-bdg5m
Dec  7 18:08:06.453: INFO: Got endpoints: latency-svc-kp4rc [264.424764ms]
Dec  7 18:08:06.463: INFO: Got endpoints: latency-svc-bdg5m [260.857508ms]
Dec  7 18:08:06.467: INFO: Created: latency-svc-bbmh5
Dec  7 18:08:06.477: INFO: Got endpoints: latency-svc-bbmh5 [250.513661ms]
Dec  7 18:08:06.482: INFO: Created: latency-svc-smjms
Dec  7 18:08:06.494: INFO: Got endpoints: latency-svc-smjms [258.690626ms]
Dec  7 18:08:06.498: INFO: Created: latency-svc-nltjs
Dec  7 18:08:06.509: INFO: Got endpoints: latency-svc-nltjs [256.482079ms]
Dec  7 18:08:06.515: INFO: Created: latency-svc-2smfk
Dec  7 18:08:06.528: INFO: Got endpoints: latency-svc-2smfk [258.197332ms]
Dec  7 18:08:06.529: INFO: Created: latency-svc-rvn6f
Dec  7 18:08:06.543: INFO: Got endpoints: latency-svc-rvn6f [245.229077ms]
Dec  7 18:08:06.546: INFO: Created: latency-svc-pzz4k
Dec  7 18:08:06.562: INFO: Got endpoints: latency-svc-pzz4k [243.408692ms]
Dec  7 18:08:06.564: INFO: Created: latency-svc-9pjtd
Dec  7 18:08:06.578: INFO: Got endpoints: latency-svc-9pjtd [243.498354ms]
Dec  7 18:08:06.581: INFO: Created: latency-svc-4hhc7
Dec  7 18:08:06.595: INFO: Got endpoints: latency-svc-4hhc7 [245.059583ms]
Dec  7 18:08:06.601: INFO: Created: latency-svc-r9dcc
Dec  7 18:08:06.611: INFO: Created: latency-svc-w2d8s
Dec  7 18:08:06.615: INFO: Got endpoints: latency-svc-r9dcc [251.03645ms]
Dec  7 18:08:06.625: INFO: Got endpoints: latency-svc-w2d8s [243.312165ms]
Dec  7 18:08:06.629: INFO: Created: latency-svc-8r7hx
Dec  7 18:08:06.642: INFO: Got endpoints: latency-svc-8r7hx [246.932597ms]
Dec  7 18:08:06.648: INFO: Created: latency-svc-2jz59
Dec  7 18:08:06.662: INFO: Created: latency-svc-f8f2x
Dec  7 18:08:06.662: INFO: Got endpoints: latency-svc-2jz59 [246.866512ms]
Dec  7 18:08:06.672: INFO: Got endpoints: latency-svc-f8f2x [239.629244ms]
Dec  7 18:08:06.676: INFO: Created: latency-svc-6rgbq
Dec  7 18:08:06.689: INFO: Got endpoints: latency-svc-6rgbq [235.678914ms]
Dec  7 18:08:06.699: INFO: Created: latency-svc-fqvzf
Dec  7 18:08:06.710: INFO: Got endpoints: latency-svc-fqvzf [246.676511ms]
Dec  7 18:08:06.712: INFO: Created: latency-svc-9r4b5
Dec  7 18:08:06.723: INFO: Got endpoints: latency-svc-9r4b5 [245.039167ms]
Dec  7 18:08:06.728: INFO: Created: latency-svc-kc7qn
Dec  7 18:08:06.743: INFO: Got endpoints: latency-svc-kc7qn [249.187959ms]
Dec  7 18:08:06.747: INFO: Created: latency-svc-rll2s
Dec  7 18:08:06.759: INFO: Got endpoints: latency-svc-rll2s [249.430911ms]
Dec  7 18:08:06.766: INFO: Created: latency-svc-vc466
Dec  7 18:08:06.777: INFO: Got endpoints: latency-svc-vc466 [249.538636ms]
Dec  7 18:08:06.784: INFO: Created: latency-svc-t2k6q
Dec  7 18:08:06.795: INFO: Created: latency-svc-k77lk
Dec  7 18:08:06.795: INFO: Got endpoints: latency-svc-t2k6q [252.564816ms]
Dec  7 18:08:06.809: INFO: Got endpoints: latency-svc-k77lk [246.938291ms]
Dec  7 18:08:06.814: INFO: Created: latency-svc-dhmp7
Dec  7 18:08:06.825: INFO: Got endpoints: latency-svc-dhmp7 [247.619427ms]
Dec  7 18:08:06.829: INFO: Created: latency-svc-vzq6p
Dec  7 18:08:06.842: INFO: Got endpoints: latency-svc-vzq6p [246.878759ms]
Dec  7 18:08:06.845: INFO: Created: latency-svc-qgqg6
Dec  7 18:08:06.860: INFO: Got endpoints: latency-svc-qgqg6 [244.908313ms]
Dec  7 18:08:06.863: INFO: Created: latency-svc-qqvr7
Dec  7 18:08:06.874: INFO: Got endpoints: latency-svc-qqvr7 [249.124379ms]
Dec  7 18:08:06.880: INFO: Created: latency-svc-kqwgt
Dec  7 18:08:06.891: INFO: Got endpoints: latency-svc-kqwgt [249.612557ms]
Dec  7 18:08:06.893: INFO: Created: latency-svc-jjxbx
Dec  7 18:08:06.905: INFO: Got endpoints: latency-svc-jjxbx [242.926159ms]
Dec  7 18:08:06.922: INFO: Created: latency-svc-88pn5
Dec  7 18:08:06.924: INFO: Created: latency-svc-jcbmt
Dec  7 18:08:06.930: INFO: Got endpoints: latency-svc-88pn5 [257.643073ms]
Dec  7 18:08:06.936: INFO: Got endpoints: latency-svc-jcbmt [246.87533ms]
Dec  7 18:08:06.940: INFO: Created: latency-svc-9pztb
Dec  7 18:08:06.959: INFO: Got endpoints: latency-svc-9pztb [248.697918ms]
Dec  7 18:08:06.959: INFO: Created: latency-svc-b7lrl
Dec  7 18:08:06.970: INFO: Got endpoints: latency-svc-b7lrl [246.773197ms]
Dec  7 18:08:06.972: INFO: Created: latency-svc-rct4v
Dec  7 18:08:06.982: INFO: Got endpoints: latency-svc-rct4v [238.198433ms]
Dec  7 18:08:06.986: INFO: Created: latency-svc-6kpch
Dec  7 18:08:06.999: INFO: Got endpoints: latency-svc-6kpch [240.132053ms]
Dec  7 18:08:06.999: INFO: Created: latency-svc-hwcp4
Dec  7 18:08:07.012: INFO: Got endpoints: latency-svc-hwcp4 [234.423837ms]
Dec  7 18:08:07.028: INFO: Created: latency-svc-k55pm
Dec  7 18:08:07.031: INFO: Created: latency-svc-z97q4
Dec  7 18:08:07.040: INFO: Got endpoints: latency-svc-k55pm [244.330035ms]
Dec  7 18:08:07.041: INFO: Got endpoints: latency-svc-z97q4 [232.391116ms]
Dec  7 18:08:07.047: INFO: Created: latency-svc-k56ql
Dec  7 18:08:07.057: INFO: Got endpoints: latency-svc-k56ql [232.040215ms]
Dec  7 18:08:07.062: INFO: Created: latency-svc-vphr7
Dec  7 18:08:07.073: INFO: Got endpoints: latency-svc-vphr7 [231.312559ms]
Dec  7 18:08:07.078: INFO: Created: latency-svc-ntd9d
Dec  7 18:08:07.090: INFO: Got endpoints: latency-svc-ntd9d [230.281411ms]
Dec  7 18:08:07.097: INFO: Created: latency-svc-qqclr
Dec  7 18:08:07.108: INFO: Got endpoints: latency-svc-qqclr [233.968914ms]
Dec  7 18:08:07.111: INFO: Created: latency-svc-2hwj6
Dec  7 18:08:07.124: INFO: Got endpoints: latency-svc-2hwj6 [226.386227ms]
Dec  7 18:08:07.125: INFO: Created: latency-svc-8z6rb
Dec  7 18:08:07.135: INFO: Got endpoints: latency-svc-8z6rb [229.953576ms]
Dec  7 18:08:07.141: INFO: Created: latency-svc-p99c4
Dec  7 18:08:07.152: INFO: Got endpoints: latency-svc-p99c4 [221.741861ms]
Dec  7 18:08:07.155: INFO: Created: latency-svc-fxsvl
Dec  7 18:08:07.165: INFO: Got endpoints: latency-svc-fxsvl [228.703057ms]
Dec  7 18:08:07.171: INFO: Created: latency-svc-6vlqp
Dec  7 18:08:07.183: INFO: Got endpoints: latency-svc-6vlqp [223.997401ms]
Dec  7 18:08:07.187: INFO: Created: latency-svc-vcx6g
Dec  7 18:08:07.198: INFO: Got endpoints: latency-svc-vcx6g [228.406548ms]
Dec  7 18:08:07.201: INFO: Created: latency-svc-jswc7
Dec  7 18:08:07.218: INFO: Got endpoints: latency-svc-jswc7 [235.846968ms]
Dec  7 18:08:07.221: INFO: Created: latency-svc-m2wwt
Dec  7 18:08:07.232: INFO: Got endpoints: latency-svc-m2wwt [233.462608ms]
Dec  7 18:08:07.236: INFO: Created: latency-svc-kmvjk
Dec  7 18:08:07.246: INFO: Got endpoints: latency-svc-kmvjk [234.27737ms]
Dec  7 18:08:07.251: INFO: Created: latency-svc-j6k6w
Dec  7 18:08:07.258: INFO: Created: latency-svc-ptmv2
Dec  7 18:08:07.262: INFO: Got endpoints: latency-svc-j6k6w [222.54618ms]
Dec  7 18:08:07.270: INFO: Got endpoints: latency-svc-ptmv2 [228.075651ms]
Dec  7 18:08:07.272: INFO: Created: latency-svc-vxzwv
Dec  7 18:08:07.297: INFO: Got endpoints: latency-svc-vxzwv [239.552481ms]
Dec  7 18:08:07.303: INFO: Created: latency-svc-5qz7s
Dec  7 18:08:07.307: INFO: Created: latency-svc-glt5k
Dec  7 18:08:07.312: INFO: Got endpoints: latency-svc-5qz7s [238.538014ms]
Dec  7 18:08:07.318: INFO: Got endpoints: latency-svc-glt5k [227.582268ms]
Dec  7 18:08:07.321: INFO: Created: latency-svc-tzb9b
Dec  7 18:08:07.332: INFO: Got endpoints: latency-svc-tzb9b [224.131372ms]
Dec  7 18:08:07.339: INFO: Created: latency-svc-zbxmx
Dec  7 18:08:07.351: INFO: Got endpoints: latency-svc-zbxmx [227.456511ms]
Dec  7 18:08:07.361: INFO: Created: latency-svc-gxlg9
Dec  7 18:08:07.374: INFO: Got endpoints: latency-svc-gxlg9 [238.919867ms]
Dec  7 18:08:07.375: INFO: Created: latency-svc-9v7wp
Dec  7 18:08:07.384: INFO: Got endpoints: latency-svc-9v7wp [231.735125ms]
Dec  7 18:08:07.391: INFO: Created: latency-svc-ljsps
Dec  7 18:08:07.403: INFO: Got endpoints: latency-svc-ljsps [238.255107ms]
Dec  7 18:08:07.410: INFO: Created: latency-svc-45546
Dec  7 18:08:07.421: INFO: Got endpoints: latency-svc-45546 [238.058519ms]
Dec  7 18:08:07.427: INFO: Created: latency-svc-hfplh
Dec  7 18:08:07.439: INFO: Got endpoints: latency-svc-hfplh [240.833615ms]
Dec  7 18:08:07.442: INFO: Created: latency-svc-df6sg
Dec  7 18:08:07.452: INFO: Created: latency-svc-2x65h
Dec  7 18:08:07.457: INFO: Got endpoints: latency-svc-df6sg [239.48594ms]
Dec  7 18:08:07.463: INFO: Got endpoints: latency-svc-2x65h [230.31569ms]
Dec  7 18:08:07.468: INFO: Created: latency-svc-q7ws7
Dec  7 18:08:07.477: INFO: Got endpoints: latency-svc-q7ws7 [231.263756ms]
Dec  7 18:08:07.485: INFO: Created: latency-svc-mn5s6
Dec  7 18:08:07.501: INFO: Created: latency-svc-fd6v7
Dec  7 18:08:07.501: INFO: Got endpoints: latency-svc-mn5s6 [238.956082ms]
Dec  7 18:08:07.513: INFO: Got endpoints: latency-svc-fd6v7 [242.776082ms]
Dec  7 18:08:07.521: INFO: Created: latency-svc-lnknl
Dec  7 18:08:07.535: INFO: Created: latency-svc-hflz5
Dec  7 18:08:07.536: INFO: Got endpoints: latency-svc-lnknl [238.603028ms]
Dec  7 18:08:07.549: INFO: Got endpoints: latency-svc-hflz5 [236.807229ms]
Dec  7 18:08:07.555: INFO: Created: latency-svc-hs8vm
Dec  7 18:08:07.566: INFO: Got endpoints: latency-svc-hs8vm [247.552821ms]
Dec  7 18:08:07.568: INFO: Created: latency-svc-qzp4j
Dec  7 18:08:07.581: INFO: Got endpoints: latency-svc-qzp4j [248.897372ms]
Dec  7 18:08:07.588: INFO: Created: latency-svc-8gpbs
Dec  7 18:08:07.598: INFO: Got endpoints: latency-svc-8gpbs [246.477086ms]
Dec  7 18:08:07.604: INFO: Created: latency-svc-q75ll
Dec  7 18:08:07.615: INFO: Got endpoints: latency-svc-q75ll [240.203689ms]
Dec  7 18:08:07.619: INFO: Created: latency-svc-ngphz
Dec  7 18:08:07.631: INFO: Got endpoints: latency-svc-ngphz [246.964724ms]
Dec  7 18:08:07.635: INFO: Created: latency-svc-wf5gk
Dec  7 18:08:07.649: INFO: Got endpoints: latency-svc-wf5gk [245.560555ms]
Dec  7 18:08:07.655: INFO: Created: latency-svc-hjpvn
Dec  7 18:08:07.668: INFO: Got endpoints: latency-svc-hjpvn [246.447029ms]
Dec  7 18:08:07.672: INFO: Created: latency-svc-w89j7
Dec  7 18:08:07.686: INFO: Got endpoints: latency-svc-w89j7 [246.983998ms]
Dec  7 18:08:07.699: INFO: Created: latency-svc-nsss2
Dec  7 18:08:07.711: INFO: Got endpoints: latency-svc-nsss2 [253.658168ms]
Dec  7 18:08:07.715: INFO: Created: latency-svc-vt7pq
Dec  7 18:08:07.729: INFO: Got endpoints: latency-svc-vt7pq [265.722927ms]
Dec  7 18:08:07.731: INFO: Created: latency-svc-ndhlx
Dec  7 18:08:07.744: INFO: Got endpoints: latency-svc-ndhlx [265.848064ms]
Dec  7 18:08:07.746: INFO: Created: latency-svc-cf89l
Dec  7 18:08:07.758: INFO: Got endpoints: latency-svc-cf89l [257.153088ms]
Dec  7 18:08:07.765: INFO: Created: latency-svc-dqvhl
Dec  7 18:08:07.784: INFO: Got endpoints: latency-svc-dqvhl [271.862433ms]
Dec  7 18:08:07.793: INFO: Created: latency-svc-x25ln
Dec  7 18:08:07.806: INFO: Got endpoints: latency-svc-x25ln [269.822863ms]
Dec  7 18:08:07.806: INFO: Created: latency-svc-77mgm
Dec  7 18:08:07.824: INFO: Created: latency-svc-8x4bp
Dec  7 18:08:07.824: INFO: Got endpoints: latency-svc-77mgm [275.113845ms]
Dec  7 18:08:07.837: INFO: Created: latency-svc-qxx7d
Dec  7 18:08:07.843: INFO: Got endpoints: latency-svc-8x4bp [277.109436ms]
Dec  7 18:08:07.849: INFO: Got endpoints: latency-svc-qxx7d [268.138988ms]
Dec  7 18:08:07.859: INFO: Created: latency-svc-snnr8
Dec  7 18:08:07.864: INFO: Got endpoints: latency-svc-snnr8 [265.690049ms]
Dec  7 18:08:07.868: INFO: Created: latency-svc-lmgjb
Dec  7 18:08:07.878: INFO: Got endpoints: latency-svc-lmgjb [263.046683ms]
Dec  7 18:08:07.884: INFO: Created: latency-svc-c2lk5
Dec  7 18:08:07.894: INFO: Got endpoints: latency-svc-c2lk5 [263.334922ms]
Dec  7 18:08:07.904: INFO: Created: latency-svc-jtsmk
Dec  7 18:08:07.914: INFO: Created: latency-svc-7xtrd
Dec  7 18:08:07.924: INFO: Got endpoints: latency-svc-jtsmk [274.403483ms]
Dec  7 18:08:07.935: INFO: Got endpoints: latency-svc-7xtrd [266.919066ms]
Dec  7 18:08:07.937: INFO: Created: latency-svc-5jwld
Dec  7 18:08:07.947: INFO: Got endpoints: latency-svc-5jwld [260.78487ms]
Dec  7 18:08:07.948: INFO: Created: latency-svc-7v85t
Dec  7 18:08:07.959: INFO: Got endpoints: latency-svc-7v85t [248.364324ms]
Dec  7 18:08:07.966: INFO: Created: latency-svc-cwg55
Dec  7 18:08:07.983: INFO: Got endpoints: latency-svc-cwg55 [254.692533ms]
Dec  7 18:08:07.984: INFO: Created: latency-svc-rvlzh
Dec  7 18:08:07.994: INFO: Created: latency-svc-4wrz7
Dec  7 18:08:08.001: INFO: Got endpoints: latency-svc-rvlzh [257.428516ms]
Dec  7 18:08:08.010: INFO: Got endpoints: latency-svc-4wrz7 [251.05389ms]
Dec  7 18:08:08.011: INFO: Created: latency-svc-ftsl5
Dec  7 18:08:08.020: INFO: Got endpoints: latency-svc-ftsl5 [235.654555ms]
Dec  7 18:08:08.025: INFO: Created: latency-svc-j69jv
Dec  7 18:08:08.034: INFO: Got endpoints: latency-svc-j69jv [228.616562ms]
Dec  7 18:08:08.048: INFO: Created: latency-svc-dh769
Dec  7 18:08:08.055: INFO: Created: latency-svc-jjrk9
Dec  7 18:08:08.064: INFO: Got endpoints: latency-svc-dh769 [240.300856ms]
Dec  7 18:08:08.070: INFO: Got endpoints: latency-svc-jjrk9 [226.794102ms]
Dec  7 18:08:08.074: INFO: Created: latency-svc-r9k8c
Dec  7 18:08:08.082: INFO: Got endpoints: latency-svc-r9k8c [232.80929ms]
Dec  7 18:08:08.093: INFO: Created: latency-svc-pzz4d
Dec  7 18:08:08.103: INFO: Got endpoints: latency-svc-pzz4d [239.791539ms]
Dec  7 18:08:08.105: INFO: Created: latency-svc-dlvfp
Dec  7 18:08:08.117: INFO: Got endpoints: latency-svc-dlvfp [239.431304ms]
Dec  7 18:08:08.120: INFO: Created: latency-svc-mcqf9
Dec  7 18:08:08.135: INFO: Got endpoints: latency-svc-mcqf9 [240.803231ms]
Dec  7 18:08:08.139: INFO: Created: latency-svc-rcckt
Dec  7 18:08:08.153: INFO: Got endpoints: latency-svc-rcckt [229.074407ms]
Dec  7 18:08:08.156: INFO: Created: latency-svc-7xqnp
Dec  7 18:08:08.168: INFO: Got endpoints: latency-svc-7xqnp [232.741226ms]
Dec  7 18:08:08.173: INFO: Created: latency-svc-rmzn4
Dec  7 18:08:08.183: INFO: Got endpoints: latency-svc-rmzn4 [235.501764ms]
Dec  7 18:08:08.195: INFO: Created: latency-svc-c7wkr
Dec  7 18:08:08.214: INFO: Got endpoints: latency-svc-c7wkr [254.555429ms]
Dec  7 18:08:08.217: INFO: Created: latency-svc-pqxfh
Dec  7 18:08:08.226: INFO: Created: latency-svc-2cv4l
Dec  7 18:08:08.231: INFO: Got endpoints: latency-svc-pqxfh [247.909328ms]
Dec  7 18:08:08.240: INFO: Got endpoints: latency-svc-2cv4l [238.380276ms]
Dec  7 18:08:08.241: INFO: Created: latency-svc-4zt45
Dec  7 18:08:08.257: INFO: Got endpoints: latency-svc-4zt45 [247.432181ms]
Dec  7 18:08:08.283: INFO: Created: latency-svc-qkgnh
Dec  7 18:08:08.286: INFO: Created: latency-svc-n9hqw
Dec  7 18:08:08.309: INFO: Got endpoints: latency-svc-qkgnh [288.705743ms]
Dec  7 18:08:08.311: INFO: Created: latency-svc-ltk7f
Dec  7 18:08:08.312: INFO: Got endpoints: latency-svc-n9hqw [277.323564ms]
Dec  7 18:08:08.313: INFO: Created: latency-svc-b8hbl
Dec  7 18:08:08.326: INFO: Got endpoints: latency-svc-ltk7f [261.468956ms]
Dec  7 18:08:08.332: INFO: Got endpoints: latency-svc-b8hbl [262.428441ms]
Dec  7 18:08:08.367: INFO: Created: latency-svc-dqskh
Dec  7 18:08:08.373: INFO: Created: latency-svc-w45df
Dec  7 18:08:08.373: INFO: Created: latency-svc-h8mkx
Dec  7 18:08:08.376: INFO: Got endpoints: latency-svc-dqskh [294.05295ms]
Dec  7 18:08:08.386: INFO: Created: latency-svc-bqfcg
Dec  7 18:08:08.387: INFO: Got endpoints: latency-svc-w45df [283.848376ms]
Dec  7 18:08:08.387: INFO: Got endpoints: latency-svc-h8mkx [269.795701ms]
Dec  7 18:08:08.400: INFO: Got endpoints: latency-svc-bqfcg [264.737538ms]
Dec  7 18:08:08.400: INFO: Created: latency-svc-kfhbt
Dec  7 18:08:08.410: INFO: Got endpoints: latency-svc-kfhbt [257.294017ms]
Dec  7 18:08:08.419: INFO: Created: latency-svc-rwwfl
Dec  7 18:08:08.431: INFO: Got endpoints: latency-svc-rwwfl [263.243028ms]
Dec  7 18:08:08.436: INFO: Created: latency-svc-7xw7k
Dec  7 18:08:08.445: INFO: Created: latency-svc-sg64b
Dec  7 18:08:08.449: INFO: Got endpoints: latency-svc-7xw7k [266.223629ms]
Dec  7 18:08:08.456: INFO: Got endpoints: latency-svc-sg64b [242.287558ms]
Dec  7 18:08:08.464: INFO: Created: latency-svc-2sxjh
Dec  7 18:08:08.474: INFO: Got endpoints: latency-svc-2sxjh [242.384121ms]
Dec  7 18:08:08.478: INFO: Created: latency-svc-6lgdd
Dec  7 18:08:08.491: INFO: Got endpoints: latency-svc-6lgdd [251.319844ms]
Dec  7 18:08:08.497: INFO: Created: latency-svc-fxf76
Dec  7 18:08:08.523: INFO: Got endpoints: latency-svc-fxf76 [265.8606ms]
Dec  7 18:08:08.538: INFO: Created: latency-svc-m25qv
Dec  7 18:08:08.548: INFO: Got endpoints: latency-svc-m25qv [238.922887ms]
Dec  7 18:08:08.548: INFO: Latencies: [57.003914ms 72.438098ms 78.428472ms 84.622376ms 104.371466ms 109.979496ms 158.977413ms 170.003581ms 193.727484ms 198.588012ms 205.077467ms 210.857967ms 213.989848ms 214.00935ms 215.13014ms 217.465616ms 221.388614ms 221.612542ms 221.741861ms 221.824038ms 222.54618ms 222.550709ms 223.429028ms 223.711391ms 223.997401ms 224.131372ms 226.044014ms 226.386227ms 226.794102ms 227.456511ms 227.582268ms 228.075651ms 228.406548ms 228.616562ms 228.703057ms 229.074407ms 229.953576ms 230.281411ms 230.290294ms 230.31569ms 231.263756ms 231.312559ms 231.735125ms 232.040215ms 232.391116ms 232.741226ms 232.80929ms 233.462608ms 233.968914ms 234.27737ms 234.423837ms 235.501764ms 235.654555ms 235.678914ms 235.846968ms 236.807229ms 238.058519ms 238.198433ms 238.255107ms 238.380276ms 238.538014ms 238.603028ms 238.919867ms 238.922887ms 238.956082ms 239.431304ms 239.48594ms 239.552481ms 239.629244ms 239.791539ms 239.920665ms 240.132053ms 240.203689ms 240.300856ms 240.803231ms 240.833615ms 242.287558ms 242.384121ms 242.776082ms 242.926159ms 243.312165ms 243.408692ms 243.498354ms 243.865351ms 243.960851ms 244.224431ms 244.265664ms 244.330035ms 244.908313ms 245.039167ms 245.059583ms 245.229077ms 245.560555ms 246.447029ms 246.477086ms 246.676511ms 246.773197ms 246.866512ms 246.87533ms 246.878759ms 246.932597ms 246.938291ms 246.964724ms 246.983998ms 247.111383ms 247.432181ms 247.552821ms 247.619427ms 247.835702ms 247.909328ms 248.364324ms 248.697918ms 248.897372ms 249.124379ms 249.187959ms 249.245235ms 249.430911ms 249.434369ms 249.538636ms 249.612557ms 250.513661ms 250.8715ms 251.03645ms 251.05389ms 251.289375ms 251.319844ms 252.151709ms 252.23132ms 252.564816ms 253.658168ms 253.704302ms 254.104288ms 254.555429ms 254.692533ms 255.943048ms 256.482079ms 257.153088ms 257.294017ms 257.428516ms 257.560427ms 257.643073ms 257.842642ms 258.197332ms 258.690626ms 260.78487ms 260.857508ms 261.395983ms 261.468956ms 262.428441ms 262.44943ms 263.046683ms 263.243028ms 263.334922ms 264.04922ms 264.295234ms 264.302225ms 264.424764ms 264.737538ms 265.388413ms 265.690049ms 265.722927ms 265.848064ms 265.8606ms 266.095383ms 266.223629ms 266.919066ms 268.138988ms 268.426346ms 269.180561ms 269.795701ms 269.822863ms 271.862433ms 274.403483ms 275.113845ms 275.20781ms 275.537698ms 277.109436ms 277.323564ms 281.491144ms 283.081761ms 283.848376ms 283.951551ms 287.206449ms 288.705743ms 294.05295ms 294.27325ms 300.132273ms 311.497009ms 314.461812ms 314.662068ms 322.799385ms 336.060745ms 339.3079ms 347.487465ms 351.530583ms 351.782372ms 357.219715ms 359.292257ms 363.093303ms 366.7119ms]
Dec  7 18:08:08.549: INFO: 50 %ile: 246.932597ms
Dec  7 18:08:08.549: INFO: 90 %ile: 283.848376ms
Dec  7 18:08:08.549: INFO: 99 %ile: 363.093303ms
Dec  7 18:08:08.549: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:08.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2430" for this suite.

• [SLOW TEST:6.983 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":170,"skipped":2813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:08.594: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:20.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1983" for this suite.

• [SLOW TEST:11.723 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":171,"skipped":2860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:20.319: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9087.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9087.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9087.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9087.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9087.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9087.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 18:08:30.827: INFO: DNS probes using dns-9087/dns-test-2063edae-6deb-4a2f-9e40-6868ff335287 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:30.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9087" for this suite.

• [SLOW TEST:10.606 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":172,"skipped":2892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:30.927: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:08:31.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e" in namespace "projected-3964" to be "Succeeded or Failed"
Dec  7 18:08:31.215: INFO: Pod "downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.480693ms
Dec  7 18:08:33.236: INFO: Pod "downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036558081s
STEP: Saw pod success
Dec  7 18:08:33.236: INFO: Pod "downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e" satisfied condition "Succeeded or Failed"
Dec  7 18:08:33.249: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e container client-container: <nil>
STEP: delete the pod
Dec  7 18:08:33.366: INFO: Waiting for pod downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e to disappear
Dec  7 18:08:33.379: INFO: Pod downwardapi-volume-dee742f3-90d0-44fb-89e6-cb904ce86e0e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:33.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3964" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":2936,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:33.418: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  7 18:08:33.650: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:38.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9113" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":174,"skipped":2942,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:38.196: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  7 18:08:48.751: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1207 18:08:48.751305      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  7 18:08:48.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tdps" in namespace "gc-6254"
Dec  7 18:08:48.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cc6h" in namespace "gc-6254"
Dec  7 18:08:48.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-6knpf" in namespace "gc-6254"
Dec  7 18:08:48.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsw24" in namespace "gc-6254"
Dec  7 18:08:48.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2fm4" in namespace "gc-6254"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:08:49.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6254" for this suite.

• [SLOW TEST:10.866 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":175,"skipped":2942,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:08:49.063: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:00.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9500" for this suite.

• [SLOW TEST:11.463 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":176,"skipped":2947,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:00.527: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  7 18:09:00.845: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:09:02.866: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:09:04.868: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  7 18:09:04.932: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:09:06.954: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  7 18:09:07.038: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  7 18:09:07.053: INFO: Pod pod-with-poststart-http-hook still exists
Dec  7 18:09:09.054: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  7 18:09:09.077: INFO: Pod pod-with-poststart-http-hook still exists
Dec  7 18:09:11.053: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  7 18:09:11.083: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:11.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2393" for this suite.

• [SLOW TEST:10.623 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":2959,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  7 18:09:14.571: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2660" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":2963,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:14.707: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-2b442c3f-1a16-4bf0-8f06-ac461a927da4
STEP: Creating a pod to test consume configMaps
Dec  7 18:09:15.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2" in namespace "configmap-2046" to be "Succeeded or Failed"
Dec  7 18:09:15.029: INFO: Pod "pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.638808ms
Dec  7 18:09:17.050: INFO: Pod "pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2": Phase="Running", Reason="", readiness=true. Elapsed: 2.033573626s
Dec  7 18:09:19.073: INFO: Pod "pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056099446s
STEP: Saw pod success
Dec  7 18:09:19.073: INFO: Pod "pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2" satisfied condition "Succeeded or Failed"
Dec  7 18:09:19.086: INFO: Trying to get logs from node 10.192.217.124 pod pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  7 18:09:19.206: INFO: Waiting for pod pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2 to disappear
Dec  7 18:09:19.220: INFO: Pod pod-configmaps-1ec68058-988e-4db6-a80f-69d28b478ea2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:19.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2046" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":2963,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:19.270: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec  7 18:09:19.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4550 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec  7 18:09:19.627: INFO: stderr: ""
Dec  7 18:09:19.627: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec  7 18:09:24.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4550 get pod e2e-test-httpd-pod -o json'
Dec  7 18:09:24.767: INFO: stderr: ""
Dec  7 18:09:24.767: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"818cc147ab629e6678b07635be3cbb81db24c0c0e90a08823c98c957b184d4b5\",\n            \"cni.projectcalico.org/podIP\": \"172.30.30.104/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.30.104/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-12-07T18:09:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4550\",\n        \"resourceVersion\": \"37039\",\n        \"uid\": \"40a4e2a7-7a1d-4a0d-9412-998a998c8ebc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-v4dkw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.192.217.124\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-v4dkw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-07T18:09:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-07T18:09:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-07T18:09:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-07T18:09:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://01e08c8f817066b14ec9a432cb02971ea92c64a21a31268412a0d4c5d6b61ffd\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-12-07T18:09:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.192.217.124\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.30.104\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.30.104\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-12-07T18:09:19Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  7 18:09:24.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4550 replace -f -'
Dec  7 18:09:25.869: INFO: stderr: ""
Dec  7 18:09:25.869: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec  7 18:09:25.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4550 delete pods e2e-test-httpd-pod'
Dec  7 18:09:28.075: INFO: stderr: ""
Dec  7 18:09:28.075: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:28.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4550" for this suite.

• [SLOW TEST:8.854 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":180,"skipped":3001,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:28.124: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7813
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7813
STEP: creating replication controller externalsvc in namespace services-7813
I1207 18:09:28.476475      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7813, replica count: 2
I1207 18:09:31.527993      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec  7 18:09:31.614: INFO: Creating new exec pod
Dec  7 18:09:35.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-7813 exec execpodwfkt5 -- /bin/sh -x -c nslookup clusterip-service.services-7813.svc.cluster.local'
Dec  7 18:09:36.026: INFO: stderr: "+ nslookup clusterip-service.services-7813.svc.cluster.local\n"
Dec  7 18:09:36.026: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7813.svc.cluster.local\tcanonical name = externalsvc.services-7813.svc.cluster.local.\nName:\texternalsvc.services-7813.svc.cluster.local\nAddress: 172.21.144.24\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7813, will wait for the garbage collector to delete the pods
Dec  7 18:09:36.114: INFO: Deleting ReplicationController externalsvc took: 25.054282ms
Dec  7 18:09:36.214: INFO: Terminating ReplicationController externalsvc pods took: 100.562199ms
Dec  7 18:09:39.277: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7813" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.238 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":181,"skipped":3003,"failed":0}
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:39.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-246" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3003,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:39.710: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Dec  7 18:09:39.991: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  7 18:09:45.025: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Dec  7 18:09:45.040: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Dec  7 18:09:45.077: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Dec  7 18:09:45.083: INFO: Observed &ReplicaSet event: ADDED
Dec  7 18:09:45.083: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.083: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.084: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.084: INFO: Found replicaset test-rs in namespace replicaset-7066 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec  7 18:09:45.084: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Dec  7 18:09:45.084: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  7 18:09:45.110: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Dec  7 18:09:45.117: INFO: Observed &ReplicaSet event: ADDED
Dec  7 18:09:45.117: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.117: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.117: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.117: INFO: Observed replicaset test-rs in namespace replicaset-7066 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  7 18:09:45.118: INFO: Observed &ReplicaSet event: MODIFIED
Dec  7 18:09:45.118: INFO: Found replicaset test-rs in namespace replicaset-7066 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec  7 18:09:45.118: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:45.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7066" for this suite.

• [SLOW TEST:5.454 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":183,"skipped":3014,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-4373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  7 18:09:46.326: INFO: starting watch
STEP: patching
STEP: updating
Dec  7 18:09:46.387: INFO: waiting for watch events with expected annotations
Dec  7 18:09:46.387: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:46.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4373" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":184,"skipped":3017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:46.686: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:47.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1538" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":185,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:47.153: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:09:47.388: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  7 18:09:47.438: INFO: The status of Pod pod-exec-websocket-f99a5822-f15d-45ae-845a-398a71fb2c7c is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:09:49.459: INFO: The status of Pod pod-exec-websocket-f99a5822-f15d-45ae-845a-398a71fb2c7c is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:09:51.460: INFO: The status of Pod pod-exec-websocket-f99a5822-f15d-45ae-845a-398a71fb2c7c is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:51.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3771" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:51.778: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  7 18:09:52.061: INFO: Waiting up to 5m0s for pod "pod-dfe1957d-304b-483d-91b6-aeacec834e2f" in namespace "emptydir-3190" to be "Succeeded or Failed"
Dec  7 18:09:52.078: INFO: Pod "pod-dfe1957d-304b-483d-91b6-aeacec834e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.69397ms
Dec  7 18:09:54.099: INFO: Pod "pod-dfe1957d-304b-483d-91b6-aeacec834e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038046195s
Dec  7 18:09:56.113: INFO: Pod "pod-dfe1957d-304b-483d-91b6-aeacec834e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052226192s
STEP: Saw pod success
Dec  7 18:09:56.113: INFO: Pod "pod-dfe1957d-304b-483d-91b6-aeacec834e2f" satisfied condition "Succeeded or Failed"
Dec  7 18:09:56.128: INFO: Trying to get logs from node 10.192.217.92 pod pod-dfe1957d-304b-483d-91b6-aeacec834e2f container test-container: <nil>
STEP: delete the pod
Dec  7 18:09:56.195: INFO: Waiting for pod pod-dfe1957d-304b-483d-91b6-aeacec834e2f to disappear
Dec  7 18:09:56.209: INFO: Pod pod-dfe1957d-304b-483d-91b6-aeacec834e2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:09:56.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3190" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:09:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6519
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8810
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:03.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9701" for this suite.
STEP: Destroying namespace "nsdeletetest-6519" for this suite.
Dec  7 18:10:03.144: INFO: Namespace nsdeletetest-6519 was already deleted
STEP: Destroying namespace "nsdeletetest-8810" for this suite.

• [SLOW TEST:6.915 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":188,"skipped":3150,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:03.165: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e48aa3de-c6b9-43e7-8453-cef15974ecbc
STEP: Creating a pod to test consume secrets
Dec  7 18:10:03.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd" in namespace "projected-2506" to be "Succeeded or Failed"
Dec  7 18:10:03.458: INFO: Pod "pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.287685ms
Dec  7 18:10:05.478: INFO: Pod "pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033299621s
STEP: Saw pod success
Dec  7 18:10:05.478: INFO: Pod "pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd" satisfied condition "Succeeded or Failed"
Dec  7 18:10:05.492: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:10:05.617: INFO: Waiting for pod pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd to disappear
Dec  7 18:10:05.633: INFO: Pod pod-projected-secrets-eb3d2131-f9d3-4efb-8563-4658c2e9dcdd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2506" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3153,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:05.671: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:10:05.905: INFO: Creating deployment "test-recreate-deployment"
Dec  7 18:10:05.925: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  7 18:10:05.964: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  7 18:10:08.003: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  7 18:10:08.017: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  7 18:10:08.052: INFO: Updating deployment test-recreate-deployment
Dec  7 18:10:08.052: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 18:10:08.291: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3637  bea6c2a9-2eeb-43d2-857b-c7c67ad70c02 37566 2 2021-12-07 18:10:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008f5ef68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-12-07 18:10:08 +0000 UTC,LastTransitionTime:2021-12-07 18:10:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-12-07 18:10:08 +0000 UTC,LastTransitionTime:2021-12-07 18:10:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec  7 18:10:08.306: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-3637  057c3004-9566-4ddb-9a48-16fb100841fc 37562 1 2021-12-07 18:10:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bea6c2a9-2eeb-43d2-857b-c7c67ad70c02 0xc0048e06d0 0xc0048e06d1}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bea6c2a9-2eeb-43d2-857b-c7c67ad70c02\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e0768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:10:08.306: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  7 18:10:08.306: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-3637  d24f1ab2-3c85-4355-89d2-335b755e211e 37555 2 2021-12-07 18:10:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bea6c2a9-2eeb-43d2-857b-c7c67ad70c02 0xc0048e05a7 0xc0048e05a8}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bea6c2a9-2eeb-43d2-857b-c7c67ad70c02\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e0668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:10:08.321: INFO: Pod "test-recreate-deployment-85d47dcb4-njd64" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-njd64 test-recreate-deployment-85d47dcb4- deployment-3637  5ad3b3af-daef-4bea-b41a-c2d7b4889d16 37565 0 2021-12-07 18:10:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 057c3004-9566-4ddb-9a48-16fb100841fc 0xc004bac310 0xc004bac311}] []  [{kube-controller-manager Update v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"057c3004-9566-4ddb-9a48-16fb100841fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 18:10:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqf2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqf2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:,StartTime:2021-12-07 18:10:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:08.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3637" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":190,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:08.371: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-1fc1e71f-c405-4f56-a935-ced0df9adafe
STEP: Creating a pod to test consume configMaps
Dec  7 18:10:08.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe" in namespace "configmap-9993" to be "Succeeded or Failed"
Dec  7 18:10:08.711: INFO: Pod "pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.56843ms
Dec  7 18:10:10.730: INFO: Pod "pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035709902s
STEP: Saw pod success
Dec  7 18:10:10.730: INFO: Pod "pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe" satisfied condition "Succeeded or Failed"
Dec  7 18:10:10.743: INFO: Trying to get logs from node 10.192.217.92 pod pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe container agnhost-container: <nil>
STEP: delete the pod
Dec  7 18:10:10.822: INFO: Waiting for pod pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe to disappear
Dec  7 18:10:10.837: INFO: Pod pod-configmaps-9d96ff8e-3141-455b-a4b6-763287fdc1fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:10.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9993" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3234,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3f0d2022-694d-4bc0-ae9b-687b44fab30e
STEP: Creating a pod to test consume secrets
Dec  7 18:10:11.179: INFO: Waiting up to 5m0s for pod "pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5" in namespace "secrets-4630" to be "Succeeded or Failed"
Dec  7 18:10:11.196: INFO: Pod "pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.99823ms
Dec  7 18:10:13.213: INFO: Pod "pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033973348s
Dec  7 18:10:15.231: INFO: Pod "pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052278596s
STEP: Saw pod success
Dec  7 18:10:15.231: INFO: Pod "pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5" satisfied condition "Succeeded or Failed"
Dec  7 18:10:15.244: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:10:15.342: INFO: Waiting for pod pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5 to disappear
Dec  7 18:10:15.357: INFO: Pod pod-secrets-ea3f590e-2fee-4889-aabe-e8985c5410a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:15.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4630" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3254,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:15.401: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  7 18:10:15.720: INFO: The status of Pod labelsupdate1f12d45e-8883-491c-979f-82ac875f9dbc is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:10:17.737: INFO: The status of Pod labelsupdate1f12d45e-8883-491c-979f-82ac875f9dbc is Running (Ready = true)
Dec  7 18:10:18.334: INFO: Successfully updated pod "labelsupdate1f12d45e-8883-491c-979f-82ac875f9dbc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:20.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1224" for this suite.

• [SLOW TEST:5.041 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:20.442: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:20.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5102" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":194,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:20.756: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:10:21.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:10:23.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497421, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497421, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497421, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497421, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:10:26.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:26.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6229" for this suite.
STEP: Destroying namespace "webhook-6229-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.411 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":195,"skipped":3325,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Dec  7 18:10:27.450: INFO: The status of Pod pod-hostip-49669500-f5a3-4579-8006-d4668bd5665c is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:10:29.473: INFO: The status of Pod pod-hostip-49669500-f5a3-4579-8006-d4668bd5665c is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:10:31.475: INFO: The status of Pod pod-hostip-49669500-f5a3-4579-8006-d4668bd5665c is Running (Ready = true)
Dec  7 18:10:31.505: INFO: Pod pod-hostip-49669500-f5a3-4579-8006-d4668bd5665c has hostIP: 10.192.217.92
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:31.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8622" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3332,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:31.549: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:10:32.618: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:10:34.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497432, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497432, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497432, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497432, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:10:37.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
Dec  7 18:10:38.883: INFO: Waiting for webhook configuration to be ready...
Dec  7 18:10:40.089: INFO: Waiting for webhook configuration to be ready...
Dec  7 18:10:41.266: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:42.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-554" for this suite.
STEP: Destroying namespace "webhook-554-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.261 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":197,"skipped":3336,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:42.810: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:10:43.120: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  7 18:10:47.160: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 18:10:47.272: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6605  7bb6f5a7-a8f3-4147-9042-b9e6874c013c 38032 1 2021-12-07 18:10:47 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-12-07 18:10:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040715f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec  7 18:10:47.288: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-6605  82c23025-3760-477d-a8e6-49b19a7b1717 38034 1 2021-12-07 18:10:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 7bb6f5a7-a8f3-4147-9042-b9e6874c013c 0xc0070f2fc7 0xc0070f2fc8}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:10:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bb6f5a7-a8f3-4147-9042-b9e6874c013c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070f3058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:10:47.288: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec  7 18:10:47.288: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6605  0c4407d1-f8c0-47bc-9cda-3fd601d8652e 38033 1 2021-12-07 18:10:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 7bb6f5a7-a8f3-4147-9042-b9e6874c013c 0xc0070f2e97 0xc0070f2e98}] []  [{e2e.test Update apps/v1 2021-12-07 18:10:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:10:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2021-12-07 18:10:47 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"7bb6f5a7-a8f3-4147-9042-b9e6874c013c\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0070f2f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:10:47.304: INFO: Pod "test-cleanup-controller-sjcfx" is available:
&Pod{ObjectMeta:{test-cleanup-controller-sjcfx test-cleanup-controller- deployment-6605  6845e864-2bd6-4dd3-b609-1357a47719f8 38030 0 2021-12-07 18:10:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:598d603739e67a0e472fba3384d715abed203549c39dbb0a924c51d547e52cc3 cni.projectcalico.org/podIP:172.30.30.105/32 cni.projectcalico.org/podIPs:172.30.30.105/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 0c4407d1-f8c0-47bc-9cda-3fd601d8652e 0xc004071957 0xc004071958}] []  [{kube-controller-manager Update v1 2021-12-07 18:10:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c4407d1-f8c0-47bc-9cda-3fd601d8652e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 18:10:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 18:10:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt2n5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt2n5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:10:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.124,PodIP:172.30.30.105,StartTime:2021-12-07 18:10:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 18:10:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://0e2221d743f49ca6666ded71c89537cd009f44a11e4a2c3b98f43eb206cda61e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.30.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 18:10:47.304: INFO: Pod "test-cleanup-deployment-5b4d99b59b-q8qbr" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-q8qbr test-cleanup-deployment-5b4d99b59b- deployment-6605  2bd41b7a-693d-4b0a-906e-15fd45cc2b9b 38037 0 2021-12-07 18:10:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 82c23025-3760-477d-a8e6-49b19a7b1717 0xc004071e27 0xc004071e28}] []  [{kube-controller-manager Update v1 2021-12-07 18:10:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82c23025-3760-477d-a8e6-49b19a7b1717\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtww6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtww6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:47.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6605" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":198,"skipped":3341,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:47.349: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:10:47.585: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  7 18:10:47.630: INFO: The status of Pod pod-logs-websocket-c74d2a6d-5bf9-416a-8bc7-09ce43f2e594 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:10:49.651: INFO: The status of Pod pod-logs-websocket-c74d2a6d-5bf9-416a-8bc7-09ce43f2e594 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2543" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3342,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:49.788: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:50.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9480" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":200,"skipped":3346,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:50.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6189" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":201,"skipped":3346,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:50.727: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  7 18:10:51.004: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:10:55.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7289" for this suite.

• [SLOW TEST:5.041 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3348,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:10:55.771: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  7 18:10:56.062: INFO: The status of Pod pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:10:58.080: INFO: The status of Pod pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  7 18:10:58.660: INFO: Successfully updated pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19"
Dec  7 18:10:58.660: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19" in namespace "pods-641" to be "terminated due to deadline exceeded"
Dec  7 18:10:58.674: INFO: Pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19": Phase="Running", Reason="", readiness=true. Elapsed: 13.537091ms
Dec  7 18:11:00.697: INFO: Pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19": Phase="Running", Reason="", readiness=true. Elapsed: 2.036221142s
Dec  7 18:11:02.725: INFO: Pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.06423491s
Dec  7 18:11:02.725: INFO: Pod "pod-update-activedeadlineseconds-076c9542-26e1-47cf-b8ae-287bb60e9f19" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:11:02.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-641" for this suite.

• [SLOW TEST:7.016 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3357,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:11:02.787: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:11:03.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-7113 version'
Dec  7 18:11:03.128: INFO: stderr: ""
Dec  7 18:11:03.128: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.4\", GitCommit:\"b695d79d4f967c403a96986f1750a35eb75e75f1\", GitTreeState:\"clean\", BuildDate:\"2021-11-17T15:48:33Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.4+IKS\", GitCommit:\"10b75797f8e720a48b4ac9b049a24eb048880345\", GitTreeState:\"clean\", BuildDate:\"2021-11-18T18:15:44Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:11:03.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7113" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":204,"skipped":3358,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:11:03.170: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:11:04.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:11:07.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:11:07.245: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3156-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:11:10.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9612" for this suite.
STEP: Destroying namespace "webhook-9612-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.829 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":205,"skipped":3370,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:11:11.000: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:11:11.646: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  7 18:11:13.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497471, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497471, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497471, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497471, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:11:16.756: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:11:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4243" for this suite.
STEP: Destroying namespace "webhook-4243-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":206,"skipped":3376,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:11:17.166: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Dec  7 18:13:18.035: INFO: Successfully updated pod "var-expansion-f77742be-2cbf-41ae-b353-e993c7385d5b"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec  7 18:13:20.074: INFO: Deleting pod "var-expansion-f77742be-2cbf-41ae-b353-e993c7385d5b" in namespace "var-expansion-8172"
Dec  7 18:13:20.098: INFO: Wait up to 5m0s for pod "var-expansion-f77742be-2cbf-41ae-b353-e993c7385d5b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:13:52.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8172" for this suite.

• [SLOW TEST:155.015 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":207,"skipped":3384,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:13:52.181: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4806
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-c8ecdb3a-c336-4d0e-b15f-37b6e730e41c
STEP: Creating configMap with name cm-test-opt-upd-008bfc52-9139-429c-b012-226c118c010c
STEP: Creating the pod
Dec  7 18:13:52.524: INFO: The status of Pod pod-configmaps-72ede34f-e79c-4cab-97d5-27dd9fc25490 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:13:54.550: INFO: The status of Pod pod-configmaps-72ede34f-e79c-4cab-97d5-27dd9fc25490 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:13:56.546: INFO: The status of Pod pod-configmaps-72ede34f-e79c-4cab-97d5-27dd9fc25490 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-c8ecdb3a-c336-4d0e-b15f-37b6e730e41c
STEP: Updating configmap cm-test-opt-upd-008bfc52-9139-429c-b012-226c118c010c
STEP: Creating configMap with name cm-test-opt-create-323d6eaf-1913-40b0-af9c-ec652b8235ab
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:15:28.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4806" for this suite.

• [SLOW TEST:96.200 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":3393,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:15:28.382: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:15:28.649: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  7 18:15:33.677: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  7 18:15:33.677: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  7 18:15:35.698: INFO: Creating deployment "test-rollover-deployment"
Dec  7 18:15:35.729: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  7 18:15:37.789: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  7 18:15:37.820: INFO: Ensure that both replica sets have 1 created replica
Dec  7 18:15:37.853: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  7 18:15:37.890: INFO: Updating deployment test-rollover-deployment
Dec  7 18:15:37.890: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  7 18:15:39.919: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  7 18:15:39.948: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  7 18:15:39.979: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:39.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497738, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:42.019: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:42.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497740, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:44.015: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:44.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497740, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:46.020: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:46.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497740, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:48.015: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:48.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497740, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:50.013: INFO: all replica sets need to contain the pod-template-hash label
Dec  7 18:15:50.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497740, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497735, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:15:52.019: INFO: 
Dec  7 18:15:52.019: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 18:15:52.062: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5174  ccd72e0e-831b-4d3d-a75d-4c5a875117c3 39063 2 2021-12-07 18:15:35 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-07 18:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d31af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-12-07 18:15:35 +0000 UTC,LastTransitionTime:2021-12-07 18:15:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-12-07 18:15:50 +0000 UTC,LastTransitionTime:2021-12-07 18:15:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  7 18:15:52.077: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-5174  a8638950-a5b3-400e-b0a3-db391133e4b6 39053 2 2021-12-07 18:15:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ccd72e0e-831b-4d3d-a75d-4c5a875117c3 0xc003eb2630 0xc003eb2631}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd72e0e-831b-4d3d-a75d-4c5a875117c3\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:15:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb26c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:15:52.077: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  7 18:15:52.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5174  f474c6f6-3341-44ff-a369-39ff7419cbe3 39062 2 2021-12-07 18:15:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ccd72e0e-831b-4d3d-a75d-4c5a875117c3 0xc003eb23e7 0xc003eb23e8}] []  [{e2e.test Update apps/v1 2021-12-07 18:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:15:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd72e0e-831b-4d3d-a75d-4c5a875117c3\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:15:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003eb24a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:15:52.078: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5174  4c0f1c26-eb5f-4c2f-8fe4-3fcb8dc382b2 39015 2 2021-12-07 18:15:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ccd72e0e-831b-4d3d-a75d-4c5a875117c3 0xc003eb2517 0xc003eb2518}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:15:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd72e0e-831b-4d3d-a75d-4c5a875117c3\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:15:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb25c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:15:52.091: INFO: Pod "test-rollover-deployment-98c5f4599-86kwm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-86kwm test-rollover-deployment-98c5f4599- deployment-5174  74bcdf5f-eedf-4ae8-94e0-07a3d63d975f 39037 0 2021-12-07 18:15:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:ac528f618ac669a340db32aeb6839dbb2469955002a6bb23c834efee571394b4 cni.projectcalico.org/podIP:172.30.34.186/32 cni.projectcalico.org/podIPs:172.30.34.186/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 a8638950-a5b3-400e-b0a3-db391133e4b6 0xc003eb2c40 0xc003eb2c41}] []  [{kube-controller-manager Update v1 2021-12-07 18:15:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8638950-a5b3-400e-b0a3-db391133e4b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 18:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 18:15:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kgp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kgp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:15:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:15:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:15:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:15:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.186,StartTime:2021-12-07 18:15:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 18:15:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://722f0f4881499e59bfa252f9f2b32143ffab4699e5e77e69b6c4d4508e68b066,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:15:52.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5174" for this suite.

• [SLOW TEST:23.765 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":209,"skipped":3395,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:15:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:08.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-199" for this suite.

• [SLOW TEST:16.634 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":210,"skipped":3400,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:08.782: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:16:09.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8" in namespace "projected-7044" to be "Succeeded or Failed"
Dec  7 18:16:09.068: INFO: Pod "downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.698884ms
Dec  7 18:16:11.088: INFO: Pod "downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.033209276s
Dec  7 18:16:13.111: INFO: Pod "downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056395573s
STEP: Saw pod success
Dec  7 18:16:13.111: INFO: Pod "downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8" satisfied condition "Succeeded or Failed"
Dec  7 18:16:13.124: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8 container client-container: <nil>
STEP: delete the pod
Dec  7 18:16:13.207: INFO: Waiting for pod downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8 to disappear
Dec  7 18:16:13.220: INFO: Pod downwardapi-volume-dd56d9f1-d4da-4fcd-8b69-ce78859596e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:13.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7044" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":3401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:13.259: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-7739
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:13.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7739" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":212,"skipped":3429,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:13.787: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:16:14.023: INFO: Creating ReplicaSet my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8
Dec  7 18:16:14.085: INFO: Pod name my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8: Found 0 pods out of 1
Dec  7 18:16:19.115: INFO: Pod name my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8: Found 1 pods out of 1
Dec  7 18:16:19.115: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8" is running
Dec  7 18:16:19.128: INFO: Pod "my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8-cmn9d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 18:16:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 18:16:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 18:16:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-07 18:16:14 +0000 UTC Reason: Message:}])
Dec  7 18:16:19.128: INFO: Trying to dial the pod
Dec  7 18:16:24.223: INFO: Controller my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8: Got expected result from replica 1 [my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8-cmn9d]: "my-hostname-basic-ca34ccfe-d863-496d-aa2e-b63c7a415bd8-cmn9d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7338" for this suite.

• [SLOW TEST:10.487 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":213,"skipped":3435,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:24.275: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-684.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-684.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-684.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 18:16:36.824: INFO: DNS probes using dns-684/dns-test-1ea7259d-2deb-4280-b2c0-fc40926af2de succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:36.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-684" for this suite.

• [SLOW TEST:12.690 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":214,"skipped":3443,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:36.968: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:37.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1721" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":215,"skipped":3451,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:37.288: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b0a1ade7-0434-4b1b-a5a4-e5d542685c1c
STEP: Creating a pod to test consume secrets
Dec  7 18:16:37.582: INFO: Waiting up to 5m0s for pod "pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67" in namespace "secrets-1316" to be "Succeeded or Failed"
Dec  7 18:16:37.596: INFO: Pod "pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67": Phase="Pending", Reason="", readiness=false. Elapsed: 13.841351ms
Dec  7 18:16:39.619: INFO: Pod "pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036501039s
Dec  7 18:16:41.639: INFO: Pod "pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056640958s
STEP: Saw pod success
Dec  7 18:16:41.639: INFO: Pod "pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67" satisfied condition "Succeeded or Failed"
Dec  7 18:16:41.652: INFO: Trying to get logs from node 10.192.217.124 pod pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:16:41.840: INFO: Waiting for pod pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67 to disappear
Dec  7 18:16:41.854: INFO: Pod pod-secrets-d0019faf-0429-48a3-b22c-758394ceda67 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:41.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1316" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":3453,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:41.900: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  7 18:16:42.184: INFO: Waiting up to 5m0s for pod "pod-fb4020d3-1b04-48d8-8aaf-6863aa856245" in namespace "emptydir-507" to be "Succeeded or Failed"
Dec  7 18:16:42.197: INFO: Pod "pod-fb4020d3-1b04-48d8-8aaf-6863aa856245": Phase="Pending", Reason="", readiness=false. Elapsed: 12.942246ms
Dec  7 18:16:44.217: INFO: Pod "pod-fb4020d3-1b04-48d8-8aaf-6863aa856245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033367975s
Dec  7 18:16:46.239: INFO: Pod "pod-fb4020d3-1b04-48d8-8aaf-6863aa856245": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05483718s
STEP: Saw pod success
Dec  7 18:16:46.239: INFO: Pod "pod-fb4020d3-1b04-48d8-8aaf-6863aa856245" satisfied condition "Succeeded or Failed"
Dec  7 18:16:46.255: INFO: Trying to get logs from node 10.192.217.124 pod pod-fb4020d3-1b04-48d8-8aaf-6863aa856245 container test-container: <nil>
STEP: delete the pod
Dec  7 18:16:46.323: INFO: Waiting for pod pod-fb4020d3-1b04-48d8-8aaf-6863aa856245 to disappear
Dec  7 18:16:46.338: INFO: Pod pod-fb4020d3-1b04-48d8-8aaf-6863aa856245 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:46.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-507" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:46.378: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec  7 18:16:46.642: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-219  efc16ff6-54ed-4e21-bd30-b922ada186d2 39474 0 2021-12-07 18:16:46 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-12-07 18:16:46 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wh6qj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wh6qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 18:16:46.658: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:16:48.680: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:16:50.675: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec  7 18:16:50.675: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-219 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:16:50.675: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Verifying customized DNS server is configured on pod...
Dec  7 18:16:50.939: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-219 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:16:50.939: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:16:51.165: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:51.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-219" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":218,"skipped":3499,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:51.239: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-41
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Dec  7 18:16:51.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 create -f -'
Dec  7 18:16:51.743: INFO: stderr: ""
Dec  7 18:16:51.743: INFO: stdout: "pod/pause created\n"
Dec  7 18:16:51.743: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  7 18:16:51.743: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-41" to be "running and ready"
Dec  7 18:16:51.760: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.090833ms
Dec  7 18:16:53.782: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038683019s
Dec  7 18:16:55.803: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.059954881s
Dec  7 18:16:55.803: INFO: Pod "pause" satisfied condition "running and ready"
Dec  7 18:16:55.803: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  7 18:16:55.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 label pods pause testing-label=testing-label-value'
Dec  7 18:16:55.908: INFO: stderr: ""
Dec  7 18:16:55.908: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  7 18:16:55.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 get pod pause -L testing-label'
Dec  7 18:16:55.989: INFO: stderr: ""
Dec  7 18:16:55.989: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  7 18:16:55.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 label pods pause testing-label-'
Dec  7 18:16:56.088: INFO: stderr: ""
Dec  7 18:16:56.088: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  7 18:16:56.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 get pod pause -L testing-label'
Dec  7 18:16:56.159: INFO: stderr: ""
Dec  7 18:16:56.159: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Dec  7 18:16:56.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 delete --grace-period=0 --force -f -'
Dec  7 18:16:56.282: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  7 18:16:56.282: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  7 18:16:56.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 get rc,svc -l name=pause --no-headers'
Dec  7 18:16:56.371: INFO: stderr: "No resources found in kubectl-41 namespace.\n"
Dec  7 18:16:56.371: INFO: stdout: ""
Dec  7 18:16:56.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-41 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  7 18:16:56.450: INFO: stderr: ""
Dec  7 18:16:56.450: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:16:56.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-41" for this suite.

• [SLOW TEST:5.256 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":219,"skipped":3499,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:16:56.495: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1721
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Dec  7 18:16:56.771: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  7 18:17:01.801: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:17:01.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1721" for this suite.

• [SLOW TEST:5.487 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":220,"skipped":3505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:17:01.983: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:17:03.222: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:17:05.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497823, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497823, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497823, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774497823, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:17:08.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:17:08.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3230" for this suite.
STEP: Destroying namespace "webhook-3230-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":221,"skipped":3538,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:17:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4673
Dec  7 18:17:09.013: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:17:11.029: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec  7 18:17:11.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec  7 18:17:11.355: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec  7 18:17:11.355: INFO: stdout: "iptables"
Dec  7 18:17:11.355: INFO: proxyMode: iptables
Dec  7 18:17:11.401: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec  7 18:17:11.415: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4673
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4673
I1207 18:17:11.463757      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4673, replica count: 3
I1207 18:17:14.514990      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 18:17:14.555: INFO: Creating new exec pod
Dec  7 18:17:17.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec execpod-affinity76bjv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec  7 18:17:17.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec  7 18:17:17.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:17:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec execpod-affinity76bjv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.238.49 80'
Dec  7 18:17:18.280: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.21.238.49 80\nConnection to 172.21.238.49 80 port [tcp/http] succeeded!\n"
Dec  7 18:17:18.280: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:17:18.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec execpod-affinity76bjv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.238.49:80/ ; done'
Dec  7 18:17:18.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n"
Dec  7 18:17:18.780: INFO: stdout: "\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2\naffinity-clusterip-timeout-7hwl2"
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Received response from host: affinity-clusterip-timeout-7hwl2
Dec  7 18:17:18.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec execpod-affinity76bjv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.238.49:80/'
Dec  7 18:17:19.124: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n"
Dec  7 18:17:19.124: INFO: stdout: "affinity-clusterip-timeout-7hwl2"
Dec  7 18:17:39.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-4673 exec execpod-affinity76bjv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.238.49:80/'
Dec  7 18:17:39.460: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.238.49:80/\n"
Dec  7 18:17:39.460: INFO: stdout: "affinity-clusterip-timeout-9lhdr"
Dec  7 18:17:39.460: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4673, will wait for the garbage collector to delete the pods
Dec  7 18:17:39.596: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 20.783381ms
Dec  7 18:17:39.699: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.215869ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:17:42.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4673" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:34.070 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":222,"skipped":3539,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:17:42.804: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6399
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-896
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:17:59.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3322" for this suite.
STEP: Destroying namespace "nsdeletetest-6399" for this suite.
Dec  7 18:17:59.691: INFO: Namespace nsdeletetest-6399 was already deleted
STEP: Destroying namespace "nsdeletetest-896" for this suite.

• [SLOW TEST:16.917 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":223,"skipped":3540,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:17:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  7 18:17:59.986: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  7 18:18:05.018: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:18:06.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6500" for this suite.

• [SLOW TEST:6.424 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":224,"skipped":3546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:06.146: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  7 18:18:07.733: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W1207 18:18:07.732982      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec  7 18:18:07.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8203" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":225,"skipped":3596,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:07.784: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:18:15.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5680" for this suite.

• [SLOW TEST:7.344 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":226,"skipped":3609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:15.129: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:18:15.442: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef" in namespace "downward-api-8081" to be "Succeeded or Failed"
Dec  7 18:18:15.456: INFO: Pod "downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef": Phase="Pending", Reason="", readiness=false. Elapsed: 13.890203ms
Dec  7 18:18:17.477: INFO: Pod "downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035257905s
Dec  7 18:18:19.497: INFO: Pod "downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055284379s
STEP: Saw pod success
Dec  7 18:18:19.497: INFO: Pod "downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef" satisfied condition "Succeeded or Failed"
Dec  7 18:18:19.510: INFO: Trying to get logs from node 10.192.217.124 pod downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef container client-container: <nil>
STEP: delete the pod
Dec  7 18:18:19.630: INFO: Waiting for pod downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef to disappear
Dec  7 18:18:19.644: INFO: Pod downwardapi-volume-36c51b4d-3918-401c-86c4-b3535b2788ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:18:19.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8081" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":3651,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-mmbx
STEP: Creating a pod to test atomic-volume-subpath
Dec  7 18:18:20.030: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mmbx" in namespace "subpath-958" to be "Succeeded or Failed"
Dec  7 18:18:20.071: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Pending", Reason="", readiness=false. Elapsed: 40.780847ms
Dec  7 18:18:22.088: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.057722832s
Dec  7 18:18:24.105: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.07422962s
Dec  7 18:18:26.121: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 6.090788663s
Dec  7 18:18:28.135: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 8.10442644s
Dec  7 18:18:30.166: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 10.13563783s
Dec  7 18:18:32.182: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 12.151606476s
Dec  7 18:18:34.196: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 14.165896535s
Dec  7 18:18:36.211: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 16.181004597s
Dec  7 18:18:38.225: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 18.194919858s
Dec  7 18:18:40.252: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 20.221990732s
Dec  7 18:18:42.268: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Running", Reason="", readiness=true. Elapsed: 22.237817538s
Dec  7 18:18:44.283: INFO: Pod "pod-subpath-test-downwardapi-mmbx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.252745015s
STEP: Saw pod success
Dec  7 18:18:44.283: INFO: Pod "pod-subpath-test-downwardapi-mmbx" satisfied condition "Succeeded or Failed"
Dec  7 18:18:44.294: INFO: Trying to get logs from node 10.192.217.124 pod pod-subpath-test-downwardapi-mmbx container test-container-subpath-downwardapi-mmbx: <nil>
STEP: delete the pod
Dec  7 18:18:44.413: INFO: Waiting for pod pod-subpath-test-downwardapi-mmbx to disappear
Dec  7 18:18:44.421: INFO: Pod pod-subpath-test-downwardapi-mmbx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mmbx
Dec  7 18:18:44.422: INFO: Deleting pod "pod-subpath-test-downwardapi-mmbx" in namespace "subpath-958"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:18:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-958" for this suite.

• [SLOW TEST:24.775 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":228,"skipped":3653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:44.461: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-0dc76ae6-cccb-4bb3-aa3e-72f919815fc6
STEP: Creating a pod to test consume secrets
Dec  7 18:18:44.723: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064" in namespace "projected-236" to be "Succeeded or Failed"
Dec  7 18:18:44.733: INFO: Pod "pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064": Phase="Pending", Reason="", readiness=false. Elapsed: 9.770543ms
Dec  7 18:18:46.748: INFO: Pod "pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024913817s
Dec  7 18:18:48.763: INFO: Pod "pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039857811s
STEP: Saw pod success
Dec  7 18:18:48.763: INFO: Pod "pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064" satisfied condition "Succeeded or Failed"
Dec  7 18:18:48.772: INFO: Trying to get logs from node 10.192.217.124 pod pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:18:48.830: INFO: Waiting for pod pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064 to disappear
Dec  7 18:18:48.839: INFO: Pod pod-projected-secrets-22ad87ea-77bd-4a2a-a0d1-3cde4d2c8064 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:18:48.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-236" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":3678,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:18:48.872: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec  7 18:18:49.083: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 18:19:49.182: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:19:49.193: INFO: Starting informer...
STEP: Starting pods...
Dec  7 18:19:49.456: INFO: Pod1 is running on 10.192.217.92. Tainting Node
Dec  7 18:19:51.714: INFO: Pod2 is running on 10.192.217.92. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec  7 18:19:59.183: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec  7 18:20:18.264: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:20:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7961" for this suite.

• [SLOW TEST:89.494 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":230,"skipped":3698,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:20:18.365: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  7 18:20:18.643: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  7 18:20:18.677: INFO: Waiting for terminating namespaces to be deleted...
Dec  7 18:20:18.690: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.108 before test
Dec  7 18:20:18.737: INFO: catalog-operator-6c4b4d7c9-xgg9d from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.737: INFO: 	Container catalog-operator ready: true, restart count 0
Dec  7 18:20:18.737: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.737: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:20:18.737: INFO: calico-kube-controllers-749944fc4-s4m78 from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  7 18:20:18.738: INFO: calico-node-wl22v from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:20:18.738: INFO: calico-typha-7d788c697f-mvhs4 from kube-system started at 2021-12-07 15:54:49 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:20:18.738: INFO: coredns-b58d5f584-lngfv from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:20:18.738: INFO: dashboard-metrics-scraper-6747f89c97-qtgnt from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container dashboard-metrics-scraper ready: false, restart count 0
Dec  7 18:20:18.738: INFO: ibm-keepalived-watcher-bjzrb from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:20:18.738: INFO: ibm-master-proxy-static-10.192.217.108 from kube-system started at 2021-12-07 15:54:14 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:20:18.738: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:20:18.738: INFO: ibm-storage-watcher-65b9c4d74f-sr7kw from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container ibm-storage-watcher-container ready: false, restart count 0
Dec  7 18:20:18.738: INFO: konnectivity-agent-4b249 from kube-system started at 2021-12-07 16:03:06 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:20:18.738: INFO: metrics-server-b9bc976b6-5wkrb from kube-system started at 2021-12-07 15:56:17 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.738: INFO: 	Container metrics-server ready: true, restart count 0
Dec  7 18:20:18.738: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Dec  7 18:20:18.738: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9 from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.739: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:20:18.739: INFO: sonobuoy-e2e-job-3b8635ba80e64550 from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.739: INFO: 	Container e2e ready: true, restart count 0
Dec  7 18:20:18.739: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:20:18.739: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.739: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:20:18.739: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:20:18.739: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.124 before test
Dec  7 18:20:18.846: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-12-07 15:57:45 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.846: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Dec  7 18:20:18.847: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-z4mpb from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:20:18.847: INFO: olm-operator-785cdc5884-8lwrq from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container olm-operator ready: true, restart count 0
Dec  7 18:20:18.847: INFO: calico-node-d5dvk from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:20:18.847: INFO: calico-typha-7d788c697f-l2q8w from kube-system started at 2021-12-07 15:54:59 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:20:18.847: INFO: coredns-autoscaler-689fb74d49-kfqhc from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container autoscaler ready: true, restart count 0
Dec  7 18:20:18.847: INFO: coredns-b58d5f584-2kckm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:20:18.847: INFO: coredns-b58d5f584-ghv8w from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:20:18.847: INFO: ibm-file-plugin-bbfc75b87-2cw56 from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container ibm-file-plugin-container ready: false, restart count 0
Dec  7 18:20:18.847: INFO: ibm-keepalived-watcher-gjxv9 from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:20:18.847: INFO: ibm-master-proxy-static-10.192.217.124 from kube-system started at 2021-12-07 15:54:35 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:20:18.847: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:20:18.847: INFO: konnectivity-agent-nz7z5 from kube-system started at 2021-12-07 16:03:10 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:20:18.847: INFO: kubernetes-dashboard-54c47dd995-blkbm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  7 18:20:18.847: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-9vnhs from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:20:18.847: INFO: sonobuoy from sonobuoy started at 2021-12-07 17:19:42 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  7 18:20:18.847: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-7vsjp from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:20:18.847: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:20:18.847: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.92 before test
Dec  7 18:20:18.885: INFO: calico-node-gnsxh from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.885: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:20:18.885: INFO: ibm-keepalived-watcher-8d77z from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.885: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:20:18.885: INFO: ibm-master-proxy-static-10.192.217.92 from kube-system started at 2021-12-07 15:54:06 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.885: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:20:18.885: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:20:18.885: INFO: konnectivity-agent-pcnkz from kube-system started at 2021-12-07 16:03:13 +0000 UTC (1 container statuses recorded)
Dec  7 18:20:18.885: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:20:18.885: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-vgqvj from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:20:18.885: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:20:18.885: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f0ba79d0-691c-4f72-b208-1ad1168a8eb1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.192.217.92 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f0ba79d0-691c-4f72-b208-1ad1168a8eb1 off the node 10.192.217.92
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f0ba79d0-691c-4f72-b208-1ad1168a8eb1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:25:27.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1312" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.901 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":231,"skipped":3707,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:25:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-4d3edbb5-7f61-4ead-a0a6-8a2d03236ab8 in namespace container-probe-5769
Dec  7 18:25:31.537: INFO: Started pod liveness-4d3edbb5-7f61-4ead-a0a6-8a2d03236ab8 in namespace container-probe-5769
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 18:25:31.550: INFO: Initial restart count of pod liveness-4d3edbb5-7f61-4ead-a0a6-8a2d03236ab8 is 0
Dec  7 18:25:49.728: INFO: Restart count of pod container-probe-5769/liveness-4d3edbb5-7f61-4ead-a0a6-8a2d03236ab8 is now 1 (18.178045354s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:25:49.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5769" for this suite.

• [SLOW TEST:22.544 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":3710,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:25:49.810: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4463
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec  7 18:25:50.077: INFO: Found 0 stateful pods, waiting for 3
Dec  7 18:26:00.097: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:26:00.097: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:26:00.097: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec  7 18:26:00.191: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  7 18:26:10.306: INFO: Updating stateful set ss2
Dec  7 18:26:10.336: INFO: Waiting for Pod statefulset-4463/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Dec  7 18:26:20.477: INFO: Found 2 stateful pods, waiting for 3
Dec  7 18:26:30.498: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:26:30.498: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:26:30.498: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  7 18:26:30.579: INFO: Updating stateful set ss2
Dec  7 18:26:30.611: INFO: Waiting for Pod statefulset-4463/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec  7 18:26:40.702: INFO: Updating stateful set ss2
Dec  7 18:26:40.734: INFO: Waiting for StatefulSet statefulset-4463/ss2 to complete update
Dec  7 18:26:40.734: INFO: Waiting for Pod statefulset-4463/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:26:50.767: INFO: Deleting all statefulset in ns statefulset-4463
Dec  7 18:26:50.781: INFO: Scaling statefulset ss2 to 0
Dec  7 18:27:00.853: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:27:00.871: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:00.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4463" for this suite.

• [SLOW TEST:71.160 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":233,"skipped":3721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:27:01.242: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-28cd767e-5b4d-42fe-9138-eff7f5bec417" in namespace "security-context-test-860" to be "Succeeded or Failed"
Dec  7 18:27:01.258: INFO: Pod "busybox-readonly-false-28cd767e-5b4d-42fe-9138-eff7f5bec417": Phase="Pending", Reason="", readiness=false. Elapsed: 16.067942ms
Dec  7 18:27:03.279: INFO: Pod "busybox-readonly-false-28cd767e-5b4d-42fe-9138-eff7f5bec417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037198836s
Dec  7 18:27:03.279: INFO: Pod "busybox-readonly-false-28cd767e-5b4d-42fe-9138-eff7f5bec417" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:03.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-860" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":3745,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:03.313: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec  7 18:27:03.585: INFO: The status of Pod pod-update-5cd0d887-051f-4866-97fd-604aefd508e0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:27:05.611: INFO: The status of Pod pod-update-5cd0d887-051f-4866-97fd-604aefd508e0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  7 18:27:06.205: INFO: Successfully updated pod "pod-update-5cd0d887-051f-4866-97fd-604aefd508e0"
STEP: verifying the updated pod is in kubernetes
Dec  7 18:27:06.234: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:06.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4769" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":3747,"failed":0}
S
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:06.266: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9341
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9341
STEP: Deleting pre-stop pod
Dec  7 18:27:15.700: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:15.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9341" for this suite.

• [SLOW TEST:9.519 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":236,"skipped":3748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  7 18:27:16.083: INFO: The status of Pod annotationupdatee0cccca9-1d23-464b-b6b8-63872568d69d is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:27:18.101: INFO: The status of Pod annotationupdatee0cccca9-1d23-464b-b6b8-63872568d69d is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:27:20.101: INFO: The status of Pod annotationupdatee0cccca9-1d23-464b-b6b8-63872568d69d is Running (Ready = true)
Dec  7 18:27:20.749: INFO: Successfully updated pod "annotationupdatee0cccca9-1d23-464b-b6b8-63872568d69d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9330" for this suite.

• [SLOW TEST:7.066 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:22.863: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  7 18:27:23.075: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:27:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8731" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":238,"skipped":3852,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:27:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2219
Dec  7 18:27:27.596: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:27:29.614: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec  7 18:27:29.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec  7 18:27:30.043: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec  7 18:27:30.043: INFO: stdout: "iptables"
Dec  7 18:27:30.043: INFO: proxyMode: iptables
Dec  7 18:27:30.092: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec  7 18:27:30.107: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2219
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2219
I1207 18:27:30.181249      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2219, replica count: 3
I1207 18:27:33.231885      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 18:27:33.277: INFO: Creating new exec pod
Dec  7 18:27:36.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec  7 18:27:36.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec  7 18:27:36.664: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:27:36.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.209.27 80'
Dec  7 18:27:36.931: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.209.27 80\nConnection to 172.21.209.27 80 port [tcp/http] succeeded!\n"
Dec  7 18:27:36.931: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:27:36.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 32041'
Dec  7 18:27:37.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 32041\nConnection to 10.192.217.92 32041 port [tcp/*] succeeded!\n"
Dec  7 18:27:37.177: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:27:37.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.124 32041'
Dec  7 18:27:37.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.124 32041\nConnection to 10.192.217.124 32041 port [tcp/*] succeeded!\n"
Dec  7 18:27:37.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 18:27:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.192.217.108:32041/ ; done'
Dec  7 18:27:37.742: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n"
Dec  7 18:27:37.742: INFO: stdout: "\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp\naffinity-nodeport-timeout-gwrlp"
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.742: INFO: Received response from host: affinity-nodeport-timeout-gwrlp
Dec  7 18:27:37.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.192.217.108:32041/'
Dec  7 18:27:37.986: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n"
Dec  7 18:27:37.987: INFO: stdout: "affinity-nodeport-timeout-gwrlp"
Dec  7 18:27:57.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.192.217.108:32041/'
Dec  7 18:27:58.254: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n"
Dec  7 18:27:58.254: INFO: stdout: "affinity-nodeport-timeout-gwrlp"
Dec  7 18:28:18.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-2219 exec execpod-affinity8jhjd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.192.217.108:32041/'
Dec  7 18:28:18.525: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.192.217.108:32041/\n"
Dec  7 18:28:18.525: INFO: stdout: "affinity-nodeport-timeout-q8vbh"
Dec  7 18:28:18.525: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2219, will wait for the garbage collector to delete the pods
Dec  7 18:28:18.707: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 17.300661ms
Dec  7 18:28:18.808: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.395516ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:21.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2219" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:54.297 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":239,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:21.635: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Dec  7 18:28:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-2029 api-versions'
Dec  7 18:28:21.929: INFO: stderr: ""
Dec  7 18:28:21.929: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2029" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":240,"skipped":3915,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-14
STEP: Creating secret with name secret-test-aac15518-e3d0-4426-892b-a2404999753f
STEP: Creating a pod to test consume secrets
Dec  7 18:28:22.470: INFO: Waiting up to 5m0s for pod "pod-secrets-deefb111-4bf9-451c-a661-927c9189670f" in namespace "secrets-1112" to be "Succeeded or Failed"
Dec  7 18:28:22.480: INFO: Pod "pod-secrets-deefb111-4bf9-451c-a661-927c9189670f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845371ms
Dec  7 18:28:24.494: INFO: Pod "pod-secrets-deefb111-4bf9-451c-a661-927c9189670f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023820337s
STEP: Saw pod success
Dec  7 18:28:24.494: INFO: Pod "pod-secrets-deefb111-4bf9-451c-a661-927c9189670f" satisfied condition "Succeeded or Failed"
Dec  7 18:28:24.505: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-deefb111-4bf9-451c-a661-927c9189670f container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:28:24.604: INFO: Waiting for pod pod-secrets-deefb111-4bf9-451c-a661-927c9189670f to disappear
Dec  7 18:28:24.614: INFO: Pod pod-secrets-deefb111-4bf9-451c-a661-927c9189670f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:24.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1112" for this suite.
STEP: Destroying namespace "secret-namespace-14" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":3932,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:24.664: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-3597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Dec  7 18:28:24.975: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Dec  7 18:28:27.019: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Dec  7 18:28:29.072: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:31.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3597" for this suite.

• [SLOW TEST:6.459 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":242,"skipped":3953,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:31.123: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec  7 18:28:31.359: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Dec  7 18:28:31.794: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  7 18:28:33.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:28:35.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:28:37.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:28:39.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:28:41.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774498511, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  7 18:28:46.254: INFO: Waited 2.264588216s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Dec  7 18:28:46.487: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:47.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2746" for this suite.

• [SLOW TEST:15.934 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":243,"skipped":3955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:47.058: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-a713d515-625d-4529-9aad-b13044985565
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:47.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8619" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":244,"skipped":4008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:47.353: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-1857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:28:48.240: INFO: Checking APIGroup: apiregistration.k8s.io
Dec  7 18:28:48.245: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec  7 18:28:48.245: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec  7 18:28:48.245: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec  7 18:28:48.245: INFO: Checking APIGroup: apps
Dec  7 18:28:48.250: INFO: PreferredVersion.GroupVersion: apps/v1
Dec  7 18:28:48.250: INFO: Versions found [{apps/v1 v1}]
Dec  7 18:28:48.250: INFO: apps/v1 matches apps/v1
Dec  7 18:28:48.250: INFO: Checking APIGroup: events.k8s.io
Dec  7 18:28:48.256: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec  7 18:28:48.256: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.256: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec  7 18:28:48.256: INFO: Checking APIGroup: authentication.k8s.io
Dec  7 18:28:48.261: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec  7 18:28:48.261: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec  7 18:28:48.261: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec  7 18:28:48.261: INFO: Checking APIGroup: authorization.k8s.io
Dec  7 18:28:48.267: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec  7 18:28:48.267: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec  7 18:28:48.268: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec  7 18:28:48.268: INFO: Checking APIGroup: autoscaling
Dec  7 18:28:48.273: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec  7 18:28:48.273: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec  7 18:28:48.274: INFO: autoscaling/v1 matches autoscaling/v1
Dec  7 18:28:48.274: INFO: Checking APIGroup: batch
Dec  7 18:28:48.279: INFO: PreferredVersion.GroupVersion: batch/v1
Dec  7 18:28:48.279: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec  7 18:28:48.279: INFO: batch/v1 matches batch/v1
Dec  7 18:28:48.279: INFO: Checking APIGroup: certificates.k8s.io
Dec  7 18:28:48.284: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec  7 18:28:48.284: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec  7 18:28:48.284: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec  7 18:28:48.284: INFO: Checking APIGroup: networking.k8s.io
Dec  7 18:28:48.292: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec  7 18:28:48.292: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec  7 18:28:48.292: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec  7 18:28:48.292: INFO: Checking APIGroup: policy
Dec  7 18:28:48.310: INFO: PreferredVersion.GroupVersion: policy/v1
Dec  7 18:28:48.310: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Dec  7 18:28:48.310: INFO: policy/v1 matches policy/v1
Dec  7 18:28:48.310: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec  7 18:28:48.329: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec  7 18:28:48.329: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec  7 18:28:48.329: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec  7 18:28:48.329: INFO: Checking APIGroup: storage.k8s.io
Dec  7 18:28:48.350: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec  7 18:28:48.350: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.350: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec  7 18:28:48.350: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec  7 18:28:48.410: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec  7 18:28:48.410: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec  7 18:28:48.410: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec  7 18:28:48.410: INFO: Checking APIGroup: apiextensions.k8s.io
Dec  7 18:28:48.435: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec  7 18:28:48.435: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec  7 18:28:48.435: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec  7 18:28:48.435: INFO: Checking APIGroup: scheduling.k8s.io
Dec  7 18:28:48.472: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec  7 18:28:48.472: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec  7 18:28:48.472: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec  7 18:28:48.473: INFO: Checking APIGroup: coordination.k8s.io
Dec  7 18:28:48.480: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec  7 18:28:48.480: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec  7 18:28:48.480: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec  7 18:28:48.480: INFO: Checking APIGroup: node.k8s.io
Dec  7 18:28:48.485: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec  7 18:28:48.485: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.485: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec  7 18:28:48.485: INFO: Checking APIGroup: discovery.k8s.io
Dec  7 18:28:48.491: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec  7 18:28:48.491: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.491: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec  7 18:28:48.491: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec  7 18:28:48.497: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Dec  7 18:28:48.497: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.497: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Dec  7 18:28:48.497: INFO: Checking APIGroup: crd.projectcalico.org
Dec  7 18:28:48.502: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec  7 18:28:48.502: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec  7 18:28:48.502: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Dec  7 18:28:48.502: INFO: Checking APIGroup: operators.coreos.com
Dec  7 18:28:48.508: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Dec  7 18:28:48.508: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Dec  7 18:28:48.508: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Dec  7 18:28:48.508: INFO: Checking APIGroup: snapshot.storage.k8s.io
Dec  7 18:28:48.517: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Dec  7 18:28:48.517: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.517: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Dec  7 18:28:48.517: INFO: Checking APIGroup: ibm.com
Dec  7 18:28:48.522: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Dec  7 18:28:48.522: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Dec  7 18:28:48.522: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Dec  7 18:28:48.522: INFO: Checking APIGroup: metrics.k8s.io
Dec  7 18:28:48.527: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec  7 18:28:48.527: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec  7 18:28:48.527: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1857" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":245,"skipped":4061,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:48.576: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Dec  7 18:28:48.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec  7 18:28:48.937: INFO: stderr: ""
Dec  7 18:28:48.937: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Dec  7 18:28:48.937: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec  7 18:28:48.937: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5029" to be "running and ready, or succeeded"
Dec  7 18:28:48.955: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.60753ms
Dec  7 18:28:50.974: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.037743654s
Dec  7 18:28:50.974: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec  7 18:28:50.974: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec  7 18:28:50.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator'
Dec  7 18:28:51.124: INFO: stderr: ""
Dec  7 18:28:51.124: INFO: stdout: "I1207 18:28:50.283993       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/s85k 289\nI1207 18:28:50.484156       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/h5nw 390\nI1207 18:28:50.684996       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bvg 316\nI1207 18:28:50.884346       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kkkl 307\nI1207 18:28:51.084700       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nxsd 599\n"
STEP: limiting log lines
Dec  7 18:28:51.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator --tail=1'
Dec  7 18:28:51.238: INFO: stderr: ""
Dec  7 18:28:51.238: INFO: stdout: "I1207 18:28:51.084700       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nxsd 599\n"
Dec  7 18:28:51.238: INFO: got output "I1207 18:28:51.084700       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nxsd 599\n"
STEP: limiting log bytes
Dec  7 18:28:51.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator --limit-bytes=1'
Dec  7 18:28:51.341: INFO: stderr: ""
Dec  7 18:28:51.341: INFO: stdout: "I"
Dec  7 18:28:51.341: INFO: got output "I"
STEP: exposing timestamps
Dec  7 18:28:51.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator --tail=1 --timestamps'
Dec  7 18:28:51.446: INFO: stderr: ""
Dec  7 18:28:51.446: INFO: stdout: "2021-12-07T18:28:51.285237992Z I1207 18:28:51.285071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2tlr 432\n"
Dec  7 18:28:51.446: INFO: got output "2021-12-07T18:28:51.285237992Z I1207 18:28:51.285071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2tlr 432\n"
STEP: restricting to a time range
Dec  7 18:28:53.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator --since=1s'
Dec  7 18:28:54.054: INFO: stderr: ""
Dec  7 18:28:54.054: INFO: stdout: "I1207 18:28:53.084106       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/v6vp 542\nI1207 18:28:53.284477       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dc4t 347\nI1207 18:28:53.484844       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/l7kt 537\nI1207 18:28:53.684129       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6pjp 561\nI1207 18:28:53.884518       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/hgfd 298\n"
Dec  7 18:28:54.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 logs logs-generator logs-generator --since=24h'
Dec  7 18:28:54.162: INFO: stderr: ""
Dec  7 18:28:54.162: INFO: stdout: "I1207 18:28:50.283993       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/s85k 289\nI1207 18:28:50.484156       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/h5nw 390\nI1207 18:28:50.684996       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bvg 316\nI1207 18:28:50.884346       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kkkl 307\nI1207 18:28:51.084700       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/nxsd 599\nI1207 18:28:51.285071       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2tlr 432\nI1207 18:28:51.484427       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/s47g 331\nI1207 18:28:51.684763       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/xjcg 377\nI1207 18:28:51.884043       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/kfss 401\nI1207 18:28:52.084417       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/jxk 562\nI1207 18:28:52.284788       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/pmht 495\nI1207 18:28:52.484063       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/k7x 219\nI1207 18:28:52.684450       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/hvfh 570\nI1207 18:28:52.884813       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/6t7 219\nI1207 18:28:53.084106       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/v6vp 542\nI1207 18:28:53.284477       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dc4t 347\nI1207 18:28:53.484844       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/l7kt 537\nI1207 18:28:53.684129       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6pjp 561\nI1207 18:28:53.884518       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/hgfd 298\nI1207 18:28:54.084880       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/9g92 272\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Dec  7 18:28:54.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-5029 delete pod logs-generator'
Dec  7 18:28:55.609: INFO: stderr: ""
Dec  7 18:28:55.609: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:55.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5029" for this suite.

• [SLOW TEST:7.089 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":246,"skipped":4070,"failed":0}
SS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:55.665: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Dec  7 18:28:55.922: INFO: created test-event-1
Dec  7 18:28:55.945: INFO: created test-event-2
Dec  7 18:28:55.964: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec  7 18:28:55.977: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec  7 18:28:56.083: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:28:56.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5134" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":247,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:28:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:28:56.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:28:59.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:28:59.847: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3799-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:03.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4816" for this suite.
STEP: Destroying namespace "webhook-4816-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":248,"skipped":4113,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4350
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-2ca88f51-b443-4064-a931-6243a444c142
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:08.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4350" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:08.320: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  7 18:29:08.563: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  7 18:29:08.602: INFO: Waiting for terminating namespaces to be deleted...
Dec  7 18:29:08.616: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.108 before test
Dec  7 18:29:08.645: INFO: catalog-operator-6c4b4d7c9-xgg9d from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container catalog-operator ready: true, restart count 0
Dec  7 18:29:08.645: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:29:08.645: INFO: calico-kube-controllers-749944fc4-s4m78 from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  7 18:29:08.645: INFO: calico-node-wl22v from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:29:08.645: INFO: calico-typha-7d788c697f-mvhs4 from kube-system started at 2021-12-07 15:54:49 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:29:08.645: INFO: coredns-b58d5f584-lngfv from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:29:08.645: INFO: dashboard-metrics-scraper-6747f89c97-qtgnt from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  7 18:29:08.645: INFO: ibm-keepalived-watcher-bjzrb from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:29:08.645: INFO: ibm-master-proxy-static-10.192.217.108 from kube-system started at 2021-12-07 15:54:14 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:29:08.645: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:29:08.645: INFO: ibm-storage-watcher-65b9c4d74f-sr7kw from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Dec  7 18:29:08.645: INFO: konnectivity-agent-4b249 from kube-system started at 2021-12-07 16:03:06 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:29:08.645: INFO: metrics-server-b9bc976b6-5wkrb from kube-system started at 2021-12-07 15:56:17 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container metrics-server ready: true, restart count 0
Dec  7 18:29:08.645: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Dec  7 18:29:08.645: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9 from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:29:08.645: INFO: sonobuoy-e2e-job-3b8635ba80e64550 from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container e2e ready: true, restart count 0
Dec  7 18:29:08.645: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:29:08.645: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.645: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:29:08.646: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:29:08.646: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.124 before test
Dec  7 18:29:08.677: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-12-07 15:57:45 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Dec  7 18:29:08.677: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-z4mpb from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:29:08.677: INFO: olm-operator-785cdc5884-8lwrq from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container olm-operator ready: true, restart count 0
Dec  7 18:29:08.677: INFO: calico-node-d5dvk from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:29:08.677: INFO: calico-typha-7d788c697f-l2q8w from kube-system started at 2021-12-07 15:54:59 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:29:08.677: INFO: coredns-autoscaler-689fb74d49-kfqhc from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container autoscaler ready: true, restart count 0
Dec  7 18:29:08.677: INFO: coredns-b58d5f584-2kckm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:29:08.677: INFO: coredns-b58d5f584-ghv8w from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:29:08.677: INFO: ibm-file-plugin-bbfc75b87-2cw56 from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Dec  7 18:29:08.677: INFO: ibm-keepalived-watcher-gjxv9 from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:29:08.677: INFO: ibm-master-proxy-static-10.192.217.124 from kube-system started at 2021-12-07 15:54:35 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:29:08.677: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:29:08.677: INFO: konnectivity-agent-nz7z5 from kube-system started at 2021-12-07 16:03:10 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.677: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:29:08.678: INFO: kubernetes-dashboard-54c47dd995-blkbm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.678: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  7 18:29:08.678: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-9vnhs from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.678: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:29:08.678: INFO: sonobuoy from sonobuoy started at 2021-12-07 17:19:42 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.678: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  7 18:29:08.678: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-7vsjp from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.678: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:29:08.678: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:29:08.678: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.92 before test
Dec  7 18:29:08.703: INFO: pod-configmaps-0d3cbbfe-931e-44d7-9341-be4d4869f50c from configmap-4350 started at 2021-12-07 18:29:04 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.703: INFO: 	Container agnhost-container ready: true, restart count 0
Dec  7 18:29:08.703: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Dec  7 18:29:08.703: INFO: calico-node-gnsxh from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.703: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:29:08.703: INFO: calico-typha-7d788c697f-b6hn8 from kube-system started at 2021-12-07 18:20:29 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.703: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:29:08.703: INFO: ibm-keepalived-watcher-8d77z from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.704: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:29:08.704: INFO: ibm-master-proxy-static-10.192.217.92 from kube-system started at 2021-12-07 15:54:06 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.704: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:29:08.704: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:29:08.704: INFO: konnectivity-agent-pcnkz from kube-system started at 2021-12-07 16:03:13 +0000 UTC (1 container statuses recorded)
Dec  7 18:29:08.704: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:29:08.704: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-vgqvj from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:29:08.704: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:29:08.704: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16be8c750e4a9d9f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:09.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1387" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":250,"skipped":4179,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:09.850: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8105
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:29:10.196: INFO: Found 0 stateful pods, waiting for 1
Dec  7 18:29:20.218: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Dec  7 18:29:20.311: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:29:20.311: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Dec  7 18:29:30.331: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:29:30.331: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:29:30.426: INFO: Deleting all statefulset in ns statefulset-8105
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:30.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8105" for this suite.

• [SLOW TEST:20.661 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":251,"skipped":4197,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:30.512: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-f5af5e98-b556-4109-92a8-56a4f0335363
STEP: Creating a pod to test consume configMaps
Dec  7 18:29:30.809: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa" in namespace "projected-8129" to be "Succeeded or Failed"
Dec  7 18:29:30.828: INFO: Pod "pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa": Phase="Pending", Reason="", readiness=false. Elapsed: 19.094249ms
Dec  7 18:29:32.850: INFO: Pod "pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041080222s
STEP: Saw pod success
Dec  7 18:29:32.850: INFO: Pod "pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa" satisfied condition "Succeeded or Failed"
Dec  7 18:29:32.862: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa container agnhost-container: <nil>
STEP: delete the pod
Dec  7 18:29:32.927: INFO: Waiting for pod pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa to disappear
Dec  7 18:29:32.941: INFO: Pod pod-projected-configmaps-40cf2590-0b7c-450e-8b7e-d1d66bedb7aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:32.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8129" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4199,"failed":0}
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:32.983: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Dec  7 18:29:33.266: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:29:35.285: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:29:37.283: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Dec  7 18:29:37.338: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:29:39.354: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  7 18:29:39.366: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:39.366: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:39.540: INFO: Exec stderr: ""
Dec  7 18:29:39.540: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:39.689: INFO: Exec stderr: ""
Dec  7 18:29:39.689: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:39.689: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:39.847: INFO: Exec stderr: ""
Dec  7 18:29:39.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:39.847: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.007: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  7 18:29:40.007: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.007: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.162: INFO: Exec stderr: ""
Dec  7 18:29:40.162: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.162: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.325: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  7 18:29:40.325: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.325: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.517: INFO: Exec stderr: ""
Dec  7 18:29:40.517: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.517: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.677: INFO: Exec stderr: ""
Dec  7 18:29:40.677: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.677: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:40.843: INFO: Exec stderr: ""
Dec  7 18:29:40.843: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6399 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:29:40.843: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:29:41.005: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:41.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6399" for this suite.

• [SLOW TEST:8.086 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":4203,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:41.069: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:29:41.749: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:29:44.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:57.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8340" for this suite.
STEP: Destroying namespace "webhook-8340-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.497 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":254,"skipped":4205,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:29:57.566: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:29:57.845: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e" in namespace "projected-1249" to be "Succeeded or Failed"
Dec  7 18:29:57.861: INFO: Pod "downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.978739ms
Dec  7 18:29:59.880: INFO: Pod "downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034071728s
STEP: Saw pod success
Dec  7 18:29:59.880: INFO: Pod "downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e" satisfied condition "Succeeded or Failed"
Dec  7 18:29:59.893: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e container client-container: <nil>
STEP: delete the pod
Dec  7 18:29:59.958: INFO: Waiting for pod downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e to disappear
Dec  7 18:29:59.972: INFO: Pod downwardapi-volume-28a5339e-4a63-4820-8361-fdc5b104dd2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:29:59.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1249" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6750/configmap-test-4414fdbc-9b97-417d-a33e-4e3771a852ea
STEP: Creating a pod to test consume configMaps
Dec  7 18:30:00.315: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e" in namespace "configmap-6750" to be "Succeeded or Failed"
Dec  7 18:30:00.328: INFO: Pod "pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.253375ms
Dec  7 18:30:02.347: INFO: Pod "pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.032621451s
Dec  7 18:30:04.369: INFO: Pod "pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054562324s
STEP: Saw pod success
Dec  7 18:30:04.369: INFO: Pod "pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e" satisfied condition "Succeeded or Failed"
Dec  7 18:30:04.384: INFO: Trying to get logs from node 10.192.217.92 pod pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e container env-test: <nil>
STEP: delete the pod
Dec  7 18:30:04.482: INFO: Waiting for pod pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e to disappear
Dec  7 18:30:04.495: INFO: Pod pod-configmaps-1ef35a79-5a77-491b-9939-7892a1baec7e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:04.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6750" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4238,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:04.546: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-115b3290-ee37-4de8-a564-ea2e4ca0264a
STEP: Creating secret with name secret-projected-all-test-volume-b2a236de-0ff0-49dc-b6b5-99a0ba34be83
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  7 18:30:04.851: INFO: Waiting up to 5m0s for pod "projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4" in namespace "projected-6778" to be "Succeeded or Failed"
Dec  7 18:30:04.869: INFO: Pod "projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.266655ms
Dec  7 18:30:06.889: INFO: Pod "projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038506112s
STEP: Saw pod success
Dec  7 18:30:06.889: INFO: Pod "projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4" satisfied condition "Succeeded or Failed"
Dec  7 18:30:06.905: INFO: Trying to get logs from node 10.192.217.92 pod projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  7 18:30:06.979: INFO: Waiting for pod projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4 to disappear
Dec  7 18:30:06.994: INFO: Pod projected-volume-a59c61a6-5ce9-440f-93ac-3888339b80b4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:06.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6778" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4242,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:07.032: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-3737
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec  7 18:30:07.345: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec  7 18:30:07.413: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:07.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3737" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":258,"skipped":4243,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:07.545: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:30:07.873: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:30:09.891: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:11.894: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:13.893: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:15.895: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:17.893: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:19.892: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:21.893: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:23.898: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:25.896: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:27.895: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = false)
Dec  7 18:30:29.890: INFO: The status of Pod test-webserver-84ed1c5b-8b69-4f88-9170-4abe71754abf is Running (Ready = true)
Dec  7 18:30:29.903: INFO: Container started at 2021-12-07 18:30:09 +0000 UTC, pod became ready at 2021-12-07 18:30:27 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:29.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-694" for this suite.

• [SLOW TEST:22.404 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4254,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:29.949: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Dec  7 18:30:30.262: INFO: Waiting up to 5m0s for pod "var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0" in namespace "var-expansion-6403" to be "Succeeded or Failed"
Dec  7 18:30:30.274: INFO: Pod "var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.290764ms
Dec  7 18:30:32.293: INFO: Pod "var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031256606s
STEP: Saw pod success
Dec  7 18:30:32.293: INFO: Pod "var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0" satisfied condition "Succeeded or Failed"
Dec  7 18:30:32.307: INFO: Trying to get logs from node 10.192.217.92 pod var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0 container dapi-container: <nil>
STEP: delete the pod
Dec  7 18:30:32.387: INFO: Waiting for pod var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0 to disappear
Dec  7 18:30:32.401: INFO: Pod var-expansion-54c16012-9e28-4a9f-b396-ff262fb0a9a0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:32.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6403" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":4260,"failed":0}

------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:32.438: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-6576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec  7 18:30:32.702: INFO: Waiting up to 5m0s for pod "security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8" in namespace "security-context-6576" to be "Succeeded or Failed"
Dec  7 18:30:32.723: INFO: Pod "security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.66417ms
Dec  7 18:30:34.752: INFO: Pod "security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050804946s
Dec  7 18:30:36.774: INFO: Pod "security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072699878s
STEP: Saw pod success
Dec  7 18:30:36.774: INFO: Pod "security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8" satisfied condition "Succeeded or Failed"
Dec  7 18:30:36.789: INFO: Trying to get logs from node 10.192.217.92 pod security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8 container test-container: <nil>
STEP: delete the pod
Dec  7 18:30:36.863: INFO: Waiting for pod security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8 to disappear
Dec  7 18:30:36.882: INFO: Pod security-context-840d7ee1-b4bd-45ee-9e10-dd25103d4ea8 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6576" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":261,"skipped":4260,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:36.925: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-5703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:37.172: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-1761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-5703
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:39.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1761" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:30:39.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5703" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":262,"skipped":4271,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:30:39.778: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7694
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  7 18:30:40.016: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  7 18:30:40.134: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:30:42.153: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:30:44.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:46.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:48.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:50.151: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:52.152: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:54.155: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:56.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:30:58.158: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:31:00.161: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  7 18:31:00.189: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  7 18:31:00.216: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  7 18:31:02.311: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  7 18:31:02.311: INFO: Breadth first check of 172.30.11.51 on host 10.192.217.108...
Dec  7 18:31:02.328: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.171:9080/dial?request=hostname&protocol=http&host=172.30.11.51&port=8083&tries=1'] Namespace:pod-network-test-7694 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:31:02.328: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:31:02.518: INFO: Waiting for responses: map[]
Dec  7 18:31:02.518: INFO: reached 172.30.11.51 after 0/1 tries
Dec  7 18:31:02.518: INFO: Breadth first check of 172.30.30.79 on host 10.192.217.124...
Dec  7 18:31:02.534: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.171:9080/dial?request=hostname&protocol=http&host=172.30.30.79&port=8083&tries=1'] Namespace:pod-network-test-7694 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:31:02.534: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:31:02.709: INFO: Waiting for responses: map[]
Dec  7 18:31:02.710: INFO: reached 172.30.30.79 after 0/1 tries
Dec  7 18:31:02.710: INFO: Breadth first check of 172.30.34.173 on host 10.192.217.92...
Dec  7 18:31:02.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.171:9080/dial?request=hostname&protocol=http&host=172.30.34.173&port=8083&tries=1'] Namespace:pod-network-test-7694 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:31:02.724: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:31:02.895: INFO: Waiting for responses: map[]
Dec  7 18:31:02.895: INFO: reached 172.30.34.173 after 0/1 tries
Dec  7 18:31:02.895: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:31:02.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7694" for this suite.

• [SLOW TEST:23.179 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:31:02.957: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-183f800f-52b2-4e69-9509-4258542baa46
STEP: Creating a pod to test consume configMaps
Dec  7 18:31:03.245: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a" in namespace "projected-405" to be "Succeeded or Failed"
Dec  7 18:31:03.259: INFO: Pod "pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.917643ms
Dec  7 18:31:05.279: INFO: Pod "pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033413212s
STEP: Saw pod success
Dec  7 18:31:05.279: INFO: Pod "pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a" satisfied condition "Succeeded or Failed"
Dec  7 18:31:05.297: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a container agnhost-container: <nil>
STEP: delete the pod
Dec  7 18:31:05.370: INFO: Waiting for pod pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a to disappear
Dec  7 18:31:05.385: INFO: Pod pod-projected-configmaps-2128d4fd-d252-4587-a098-47cb1140252a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:31:05.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-405" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4289,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:31:05.427: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  7 18:31:05.734: INFO: The status of Pod labelsupdate7e8f7523-b827-4ca0-9632-92df5656cbe8 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:31:07.749: INFO: The status of Pod labelsupdate7e8f7523-b827-4ca0-9632-92df5656cbe8 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:31:09.753: INFO: The status of Pod labelsupdate7e8f7523-b827-4ca0-9632-92df5656cbe8 is Running (Ready = true)
Dec  7 18:31:10.338: INFO: Successfully updated pod "labelsupdate7e8f7523-b827-4ca0-9632-92df5656cbe8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:31:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9527" for this suite.

• [SLOW TEST:7.016 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:31:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:36:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9657" for this suite.

• [SLOW TEST:300.400 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":266,"skipped":4326,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:36:12.844: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-0b242c7a-92a8-40e2-850d-2d0b17d01c27
STEP: Creating a pod to test consume secrets
Dec  7 18:36:13.133: INFO: Waiting up to 5m0s for pod "pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba" in namespace "secrets-995" to be "Succeeded or Failed"
Dec  7 18:36:13.152: INFO: Pod "pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba": Phase="Pending", Reason="", readiness=false. Elapsed: 19.119618ms
Dec  7 18:36:15.171: INFO: Pod "pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037539718s
STEP: Saw pod success
Dec  7 18:36:15.171: INFO: Pod "pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba" satisfied condition "Succeeded or Failed"
Dec  7 18:36:15.190: INFO: Trying to get logs from node 10.192.217.92 pod pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba container secret-env-test: <nil>
STEP: delete the pod
Dec  7 18:36:15.335: INFO: Waiting for pod pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba to disappear
Dec  7 18:36:15.349: INFO: Pod pod-secrets-df13512a-1697-4d3c-8ab5-5970d8a307ba no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:36:15.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-995" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:36:15.386: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-9986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:36:15.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9986" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":268,"skipped":4378,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:36:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:36:32.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4028" for this suite.

• [SLOW TEST:16.644 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":269,"skipped":4388,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:36:32.332: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:36:32.614: INFO: The status of Pod server-envvars-3e1f219b-7fa1-4175-b3c2-5b14ec5811c2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:36:34.632: INFO: The status of Pod server-envvars-3e1f219b-7fa1-4175-b3c2-5b14ec5811c2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:36:36.636: INFO: The status of Pod server-envvars-3e1f219b-7fa1-4175-b3c2-5b14ec5811c2 is Running (Ready = true)
Dec  7 18:36:36.707: INFO: Waiting up to 5m0s for pod "client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79" in namespace "pods-4675" to be "Succeeded or Failed"
Dec  7 18:36:36.720: INFO: Pod "client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79": Phase="Pending", Reason="", readiness=false. Elapsed: 12.636478ms
Dec  7 18:36:38.741: INFO: Pod "client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033778565s
Dec  7 18:36:40.762: INFO: Pod "client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05468274s
STEP: Saw pod success
Dec  7 18:36:40.762: INFO: Pod "client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79" satisfied condition "Succeeded or Failed"
Dec  7 18:36:40.791: INFO: Trying to get logs from node 10.192.217.92 pod client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79 container env3cont: <nil>
STEP: delete the pod
Dec  7 18:36:40.860: INFO: Waiting for pod client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79 to disappear
Dec  7 18:36:40.875: INFO: Pod client-envvars-c5dbd467-1332-41af-bf53-3b8967ee1f79 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:36:40.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4675" for this suite.

• [SLOW TEST:8.587 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4392,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:36:40.920: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-6294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:01.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6294" for this suite.

• [SLOW TEST:80.444 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":271,"skipped":4409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:01.369: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:38:01.644: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9" in namespace "security-context-test-9735" to be "Succeeded or Failed"
Dec  7 18:38:01.657: INFO: Pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.263874ms
Dec  7 18:38:03.672: INFO: Pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027919946s
Dec  7 18:38:05.690: INFO: Pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046660889s
Dec  7 18:38:07.710: INFO: Pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066548384s
Dec  7 18:38:07.710: INFO: Pod "alpine-nnp-false-2d1eb92a-bc77-413c-9f69-82ec07821be9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:07.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9735" for this suite.

• [SLOW TEST:6.416 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":4461,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:07.786: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Dec  7 18:38:08.633: INFO: created pod pod-service-account-defaultsa
Dec  7 18:38:08.633: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  7 18:38:08.654: INFO: created pod pod-service-account-mountsa
Dec  7 18:38:08.654: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  7 18:38:08.673: INFO: created pod pod-service-account-nomountsa
Dec  7 18:38:08.674: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  7 18:38:08.697: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  7 18:38:08.697: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  7 18:38:08.720: INFO: created pod pod-service-account-mountsa-mountspec
Dec  7 18:38:08.720: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  7 18:38:08.747: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  7 18:38:08.747: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  7 18:38:08.772: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  7 18:38:08.772: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  7 18:38:08.792: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  7 18:38:08.792: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  7 18:38:08.820: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  7 18:38:08.820: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:08.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5919" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":273,"skipped":4465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:08.870: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:38:09.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c" in namespace "downward-api-6064" to be "Succeeded or Failed"
Dec  7 18:38:09.190: INFO: Pod "downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.064776ms
Dec  7 18:38:11.229: INFO: Pod "downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069792646s
Dec  7 18:38:13.246: INFO: Pod "downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086113378s
STEP: Saw pod success
Dec  7 18:38:13.246: INFO: Pod "downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c" satisfied condition "Succeeded or Failed"
Dec  7 18:38:13.264: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c container client-container: <nil>
STEP: delete the pod
Dec  7 18:38:13.346: INFO: Waiting for pod downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c to disappear
Dec  7 18:38:13.359: INFO: Pod downwardapi-volume-518dca36-bd88-42e5-907c-6cad537dcf3c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:13.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6064" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:13.404: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:18.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9736" for this suite.

• [SLOW TEST:5.699 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":275,"skipped":4533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:19.108: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec  7 18:38:19.350: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:38:23.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4477" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":276,"skipped":4590,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:38:23.221: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-b5b3af36-dd16-4b89-a106-94ebdfdb5646 in namespace container-probe-6373
Dec  7 18:38:25.548: INFO: Started pod test-webserver-b5b3af36-dd16-4b89-a106-94ebdfdb5646 in namespace container-probe-6373
STEP: checking the pod's current state and verifying that restartCount is present
Dec  7 18:38:25.562: INFO: Initial restart count of pod test-webserver-b5b3af36-dd16-4b89-a106-94ebdfdb5646 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:42:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6373" for this suite.

• [SLOW TEST:242.835 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":4600,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:42:26.056: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6151
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  7 18:42:26.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  7 18:42:26.398: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:42:28.412: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:30.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:32.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:34.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:36.419: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:38.415: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:40.419: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:42.416: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:44.417: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:46.427: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  7 18:42:46.456: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  7 18:42:46.480: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  7 18:42:50.616: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  7 18:42:50.617: INFO: Going to poll 172.30.11.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  7 18:42:50.627: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.11.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6151 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:42:50.627: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:42:51.806: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  7 18:42:51.806: INFO: Going to poll 172.30.30.84 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  7 18:42:51.822: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.30.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6151 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:42:51.822: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:42:53.002: INFO: Found all 1 expected endpoints: [netserver-1]
Dec  7 18:42:53.002: INFO: Going to poll 172.30.34.187 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec  7 18:42:53.021: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.34.187 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6151 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:42:53.021: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:42:54.178: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:42:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6151" for this suite.

• [SLOW TEST:28.162 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":4614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:42:54.220: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2948
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  7 18:42:54.460: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  7 18:42:54.574: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:42:56.593: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:42:58.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:00.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:02.607: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:04.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:06.594: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:08.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:10.596: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:12.595: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 18:43:14.595: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  7 18:43:14.632: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  7 18:43:14.656: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  7 18:43:18.739: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  7 18:43:18.739: INFO: Breadth first check of 172.30.11.49 on host 10.192.217.108...
Dec  7 18:43:18.753: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.190:9080/dial?request=hostname&protocol=udp&host=172.30.11.49&port=8081&tries=1'] Namespace:pod-network-test-2948 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:43:18.753: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:43:18.928: INFO: Waiting for responses: map[]
Dec  7 18:43:18.928: INFO: reached 172.30.11.49 after 0/1 tries
Dec  7 18:43:18.928: INFO: Breadth first check of 172.30.30.82 on host 10.192.217.124...
Dec  7 18:43:18.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.190:9080/dial?request=hostname&protocol=udp&host=172.30.30.82&port=8081&tries=1'] Namespace:pod-network-test-2948 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:43:18.942: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:43:19.121: INFO: Waiting for responses: map[]
Dec  7 18:43:19.122: INFO: reached 172.30.30.82 after 0/1 tries
Dec  7 18:43:19.122: INFO: Breadth first check of 172.30.34.188 on host 10.192.217.92...
Dec  7 18:43:19.134: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.34.190:9080/dial?request=hostname&protocol=udp&host=172.30.34.188&port=8081&tries=1'] Namespace:pod-network-test-2948 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 18:43:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:43:19.344: INFO: Waiting for responses: map[]
Dec  7 18:43:19.344: INFO: reached 172.30.34.188 after 0/1 tries
Dec  7 18:43:19.344: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:19.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2948" for this suite.

• [SLOW TEST:25.166 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":4696,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:19.387: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5756
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:43:19.621: INFO: Creating simple deployment test-new-deployment
Dec  7 18:43:19.670: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Dec  7 18:43:21.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499399, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499399, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499399, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499399, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Dec  7 18:43:23.875: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5756  5abb515f-1c7b-467a-b7cd-040f354cfa87 46060 3 2021-12-07 18:43:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-12-07 18:43:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:43:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005948e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-12-07 18:43:21 +0000 UTC,LastTransitionTime:2021-12-07 18:43:19 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-12-07 18:43:23 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  7 18:43:23.900: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-5756  83775948-51b0-4fdd-a315-619a39e15ca9 46056 3 2021-12-07 18:43:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5abb515f-1c7b-467a-b7cd-040f354cfa87 0xc007adbf77 0xc007adbf78}] []  [{kube-controller-manager Update apps/v1 2021-12-07 18:43:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5abb515f-1c7b-467a-b7cd-040f354cfa87\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-12-07 18:43:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0077b0008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  7 18:43:23.920: INFO: Pod "test-new-deployment-847dcfb7fb-7mhns" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-7mhns test-new-deployment-847dcfb7fb- deployment-5756  453c4b94-06bd-42e9-84ed-b4dab3956b0e 46064 0 2021-12-07 18:43:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 83775948-51b0-4fdd-a315-619a39e15ca9 0xc0050b2a17 0xc0050b2a18}] []  [{kube-controller-manager Update v1 2021-12-07 18:43:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83775948-51b0-4fdd-a315-619a39e15ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9flp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9flp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 18:43:23.922: INFO: Pod "test-new-deployment-847dcfb7fb-qzkd8" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-qzkd8 test-new-deployment-847dcfb7fb- deployment-5756  4c46fce3-b638-4fcd-ac0d-193c9aabaa1e 46036 0 2021-12-07 18:43:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9b70c9e5971e30cc820e03a5d48aaa40ca7bb31fd8300029b8b1797143ff5f16 cni.projectcalico.org/podIP:172.30.34.136/32 cni.projectcalico.org/podIPs:172.30.34.136/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 83775948-51b0-4fdd-a315-619a39e15ca9 0xc0050b2ba0 0xc0050b2ba1}] []  [{kube-controller-manager Update v1 2021-12-07 18:43:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83775948-51b0-4fdd-a315-619a39e15ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-12-07 18:43:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-12-07 18:43:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qr2nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qr2nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.92,PodIP:172.30.34.136,StartTime:2021-12-07 18:43:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-07 18:43:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://846e65e1649d48e68ae9c3149a5cbd005d9dd74f724d1462a6d852924e52fd96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.34.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 18:43:23.922: INFO: Pod "test-new-deployment-847dcfb7fb-w4z2c" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-w4z2c test-new-deployment-847dcfb7fb- deployment-5756  17343c7d-61c7-4c9f-9397-b0a2f98b2929 46067 0 2021-12-07 18:43:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 83775948-51b0-4fdd-a315-619a39e15ca9 0xc0050b2dc7 0xc0050b2dc8}] []  [{kube-controller-manager Update v1 2021-12-07 18:43:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83775948-51b0-4fdd-a315-619a39e15ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ghvkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ghvkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.92,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  7 18:43:23.922: INFO: Pod "test-new-deployment-847dcfb7fb-wkj4k" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-wkj4k test-new-deployment-847dcfb7fb- deployment-5756  240f406b-a48a-47a3-a9ab-fd014cc23457 46065 0 2021-12-07 18:43:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 83775948-51b0-4fdd-a315-619a39e15ca9 0xc0050b31a0 0xc0050b31a1}] []  [{kube-controller-manager Update v1 2021-12-07 18:43:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83775948-51b0-4fdd-a315-619a39e15ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-12-07 18:43:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhgnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhgnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.192.217.108,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-07 18:43:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.192.217.108,PodIP:,StartTime:2021-12-07 18:43:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:23.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5756" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":280,"skipped":4704,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:23.960: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:43:24.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:43:26.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499404, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499404, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499404, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499404, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:43:29.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:29.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8591" for this suite.
STEP: Destroying namespace "webhook-8591-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":281,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:30.074: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:43:30.898: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec  7 18:43:32.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499410, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499410, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499411, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774499410, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:43:36.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
Dec  7 18:43:41.142: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:51.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5324" for this suite.
STEP: Destroying namespace "webhook-5324-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:21.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":282,"skipped":4733,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:51.874: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c54e60e1-7607-4f43-99b7-98388534b1b8
STEP: Creating a pod to test consume configMaps
Dec  7 18:43:52.175: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588" in namespace "projected-4523" to be "Succeeded or Failed"
Dec  7 18:43:52.186: INFO: Pod "pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588": Phase="Pending", Reason="", readiness=false. Elapsed: 11.228124ms
Dec  7 18:43:54.202: INFO: Pod "pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026840925s
STEP: Saw pod success
Dec  7 18:43:54.202: INFO: Pod "pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588" satisfied condition "Succeeded or Failed"
Dec  7 18:43:54.218: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  7 18:43:54.379: INFO: Waiting for pod pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588 to disappear
Dec  7 18:43:54.393: INFO: Pod pod-projected-configmaps-d8e7da77-6464-4f60-ae62-b5144143f588 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:54.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4523" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":4736,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:54.453: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:43:54.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949" in namespace "downward-api-6140" to be "Succeeded or Failed"
Dec  7 18:43:54.730: INFO: Pod "downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949": Phase="Pending", Reason="", readiness=false. Elapsed: 12.944481ms
Dec  7 18:43:56.753: INFO: Pod "downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036197874s
Dec  7 18:43:58.771: INFO: Pod "downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054383987s
STEP: Saw pod success
Dec  7 18:43:58.771: INFO: Pod "downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949" satisfied condition "Succeeded or Failed"
Dec  7 18:43:58.785: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949 container client-container: <nil>
STEP: delete the pod
Dec  7 18:43:58.851: INFO: Waiting for pod downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949 to disappear
Dec  7 18:43:58.862: INFO: Pod downwardapi-volume-39c76582-d74d-4bab-91b4-4f143a2e0949 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:43:58.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6140" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":4748,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:43:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Dec  7 18:43:59.185: INFO: created test-pod-1
Dec  7 18:43:59.203: INFO: created test-pod-2
Dec  7 18:43:59.221: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Dec  7 18:43:59.332: INFO: Pod quantity 3 is different from expected quantity 0
Dec  7 18:44:00.353: INFO: Pod quantity 3 is different from expected quantity 0
Dec  7 18:44:01.355: INFO: Pod quantity 3 is different from expected quantity 0
Dec  7 18:44:02.351: INFO: Pod quantity 2 is different from expected quantity 0
Dec  7 18:44:03.348: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:44:04.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5766" for this suite.

• [SLOW TEST:5.487 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":285,"skipped":4750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:44:04.394: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3331
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-2c1f3f6d-68a5-4bf4-bcd3-8a1f0e00dfe9
STEP: Creating configMap with name cm-test-opt-upd-2864bdce-fe88-4d24-80ae-f25d5c28892a
STEP: Creating the pod
Dec  7 18:44:04.795: INFO: The status of Pod pod-projected-configmaps-4964ffbb-b46c-4433-93a1-d2ea94106e79 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:44:06.821: INFO: The status of Pod pod-projected-configmaps-4964ffbb-b46c-4433-93a1-d2ea94106e79 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:44:08.817: INFO: The status of Pod pod-projected-configmaps-4964ffbb-b46c-4433-93a1-d2ea94106e79 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-2c1f3f6d-68a5-4bf4-bcd3-8a1f0e00dfe9
STEP: Updating configmap cm-test-opt-upd-2864bdce-fe88-4d24-80ae-f25d5c28892a
STEP: Creating configMap with name cm-test-opt-create-e3ee1bf6-7d8c-4a5a-98d4-3c074187656a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:45:20.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3331" for this suite.

• [SLOW TEST:75.728 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":4784,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:45:20.123: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-mxvn
STEP: Creating a pod to test atomic-volume-subpath
Dec  7 18:45:20.417: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mxvn" in namespace "subpath-2952" to be "Succeeded or Failed"
Dec  7 18:45:20.432: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Pending", Reason="", readiness=false. Elapsed: 15.099386ms
Dec  7 18:45:22.454: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 2.037070396s
Dec  7 18:45:24.471: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 4.053765611s
Dec  7 18:45:26.493: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 6.076168366s
Dec  7 18:45:28.511: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 8.093717007s
Dec  7 18:45:30.527: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 10.109633143s
Dec  7 18:45:32.551: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 12.134331487s
Dec  7 18:45:34.569: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 14.151894245s
Dec  7 18:45:36.593: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 16.176226914s
Dec  7 18:45:38.614: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 18.196893454s
Dec  7 18:45:40.631: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Running", Reason="", readiness=true. Elapsed: 20.213614318s
Dec  7 18:45:42.650: INFO: Pod "pod-subpath-test-projected-mxvn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.233178546s
STEP: Saw pod success
Dec  7 18:45:42.650: INFO: Pod "pod-subpath-test-projected-mxvn" satisfied condition "Succeeded or Failed"
Dec  7 18:45:42.663: INFO: Trying to get logs from node 10.192.217.92 pod pod-subpath-test-projected-mxvn container test-container-subpath-projected-mxvn: <nil>
STEP: delete the pod
Dec  7 18:45:42.724: INFO: Waiting for pod pod-subpath-test-projected-mxvn to disappear
Dec  7 18:45:42.737: INFO: Pod pod-subpath-test-projected-mxvn no longer exists
STEP: Deleting pod pod-subpath-test-projected-mxvn
Dec  7 18:45:42.737: INFO: Deleting pod "pod-subpath-test-projected-mxvn" in namespace "subpath-2952"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:45:42.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2952" for this suite.

• [SLOW TEST:22.677 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":287,"skipped":4794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:45:42.801: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Dec  7 18:45:43.113: INFO: pods: 0 < 3
Dec  7 18:45:45.130: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec  7 18:45:49.416: INFO: running pods: 2 < 3
Dec  7 18:45:51.431: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:45:53.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9819" for this suite.

• [SLOW TEST:10.833 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":288,"skipped":4854,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:45:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  7 18:45:53.930: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 18:46:54.039: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:46:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-8813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec  7 18:46:58.498: INFO: found a healthy node: 10.192.217.92
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:47:12.836: INFO: pods created so far: [1 1 1]
Dec  7 18:47:12.836: INFO: length of pods created so far: 3
Dec  7 18:47:16.873: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:47:23.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8813" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:47:24.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5981" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:90.646 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":289,"skipped":4869,"failed":0}
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:47:24.280: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Dec  7 18:47:24.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-7770 cluster-info'
Dec  7 18:47:24.705: INFO: stderr: ""
Dec  7 18:47:24.705: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:47:24.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7770" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":290,"skipped":4869,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:47:24.777: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:47:25.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057" in namespace "projected-8744" to be "Succeeded or Failed"
Dec  7 18:47:25.068: INFO: Pod "downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057": Phase="Pending", Reason="", readiness=false. Elapsed: 11.084677ms
Dec  7 18:47:27.085: INFO: Pod "downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057": Phase="Running", Reason="", readiness=true. Elapsed: 2.027774328s
Dec  7 18:47:29.107: INFO: Pod "downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049974737s
STEP: Saw pod success
Dec  7 18:47:29.107: INFO: Pod "downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057" satisfied condition "Succeeded or Failed"
Dec  7 18:47:29.120: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057 container client-container: <nil>
STEP: delete the pod
Dec  7 18:47:29.272: INFO: Waiting for pod downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057 to disappear
Dec  7 18:47:29.313: INFO: Pod downwardapi-volume-6a8bad4e-341f-4b60-8f64-9070e3be5057 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:47:29.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8744" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":4876,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:47:29.385: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9020
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-a0d354cb-58b6-4270-818c-875aba2557ec
STEP: Creating secret with name s-test-opt-upd-2d5ef02a-de8e-4d22-891d-2f7e476923d7
STEP: Creating the pod
Dec  7 18:47:29.803: INFO: The status of Pod pod-secrets-6c26eb0a-c74c-4f9e-bf04-f070f3040c42 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:47:31.818: INFO: The status of Pod pod-secrets-6c26eb0a-c74c-4f9e-bf04-f070f3040c42 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:47:33.822: INFO: The status of Pod pod-secrets-6c26eb0a-c74c-4f9e-bf04-f070f3040c42 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a0d354cb-58b6-4270-818c-875aba2557ec
STEP: Updating secret s-test-opt-upd-2d5ef02a-de8e-4d22-891d-2f7e476923d7
STEP: Creating secret with name s-test-opt-create-81240f53-e0f2-4c22-8342-c6f614d1ded6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:48:59.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9020" for this suite.

• [SLOW TEST:89.968 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":4878,"failed":0}
SSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:48:59.353: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:03.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5477" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":4883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:03.727: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-90301c9b-967b-49af-a78b-105e96118178
STEP: Creating a pod to test consume configMaps
Dec  7 18:49:04.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522" in namespace "configmap-3346" to be "Succeeded or Failed"
Dec  7 18:49:04.033: INFO: Pod "pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522": Phase="Pending", Reason="", readiness=false. Elapsed: 21.809689ms
Dec  7 18:49:06.054: INFO: Pod "pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043098257s
STEP: Saw pod success
Dec  7 18:49:06.055: INFO: Pod "pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522" satisfied condition "Succeeded or Failed"
Dec  7 18:49:06.092: INFO: Trying to get logs from node 10.192.217.92 pod pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 18:49:06.173: INFO: Waiting for pod pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522 to disappear
Dec  7 18:49:06.184: INFO: Pod pod-configmaps-5e4a4c9d-eab9-4fbe-92f8-a47ff98a0522 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:06.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3346" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":4940,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:06.222: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Dec  7 18:49:26.772: INFO: EndpointSlice for Service endpointslice-854/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:36.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-854" for this suite.

• [SLOW TEST:30.631 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":295,"skipped":4960,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4593
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:49:37.048: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:40.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4593" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":296,"skipped":4968,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:40.350: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:49:40.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495" in namespace "downward-api-6696" to be "Succeeded or Failed"
Dec  7 18:49:40.605: INFO: Pod "downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495": Phase="Pending", Reason="", readiness=false. Elapsed: 12.43778ms
Dec  7 18:49:42.620: INFO: Pod "downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02750091s
Dec  7 18:49:44.638: INFO: Pod "downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045125718s
STEP: Saw pod success
Dec  7 18:49:44.638: INFO: Pod "downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495" satisfied condition "Succeeded or Failed"
Dec  7 18:49:44.648: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495 container client-container: <nil>
STEP: delete the pod
Dec  7 18:49:44.768: INFO: Waiting for pod downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495 to disappear
Dec  7 18:49:44.779: INFO: Pod downwardapi-volume-535bcc63-1773-4a91-bbc4-6508c7b41495 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:44.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6696" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":4985,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:44.805: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Dec  7 18:49:45.120: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:49:47.137: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:49:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9818" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":298,"skipped":4985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:49:48.221: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4468
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec  7 18:49:48.433: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 18:49:51.783: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:50:09.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4468" for this suite.

• [SLOW TEST:21.017 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":299,"skipped":5030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:50:09.239: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec  7 18:50:10.699: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1207 18:50:10.699294      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:50:10.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-907" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":300,"skipped":5063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:50:10.737: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Dec  7 18:50:11.030: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:50:13.048: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:50:15.047: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  7 18:50:16.125: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:50:17.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9127" for this suite.

• [SLOW TEST:6.497 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":301,"skipped":5103,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:50:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4030
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec  7 18:50:17.504: INFO: Found 0 stateful pods, waiting for 3
Dec  7 18:50:27.533: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:50:27.533: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:50:27.533: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  7 18:50:27.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-4030 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:50:27.854: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:50:27.854: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:50:27.854: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec  7 18:50:37.986: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  7 18:50:48.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-4030 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 18:50:48.327: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 18:50:48.327: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 18:50:48.327: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 18:50:58.431: INFO: Waiting for StatefulSet statefulset-4030/ss2 to complete update
STEP: Rolling back to a previous revision
Dec  7 18:51:08.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-4030 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  7 18:51:08.742: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  7 18:51:08.742: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  7 18:51:08.742: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  7 18:51:18.876: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  7 18:51:28.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=statefulset-4030 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  7 18:51:29.224: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  7 18:51:29.224: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  7 18:51:29.224: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  7 18:51:39.332: INFO: Waiting for StatefulSet statefulset-4030/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:51:49.371: INFO: Deleting all statefulset in ns statefulset-4030
Dec  7 18:51:49.384: INFO: Scaling statefulset ss2 to 0
Dec  7 18:51:59.480: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:51:59.495: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:51:59.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4030" for this suite.

• [SLOW TEST:102.358 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":302,"skipped":5118,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:51:59.595: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  7 18:51:59.837: INFO: Waiting up to 5m0s for pod "pod-80fab76c-9264-4db7-8a99-8facdf7bb544" in namespace "emptydir-6535" to be "Succeeded or Failed"
Dec  7 18:51:59.851: INFO: Pod "pod-80fab76c-9264-4db7-8a99-8facdf7bb544": Phase="Pending", Reason="", readiness=false. Elapsed: 13.098815ms
Dec  7 18:52:01.869: INFO: Pod "pod-80fab76c-9264-4db7-8a99-8facdf7bb544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031153613s
STEP: Saw pod success
Dec  7 18:52:01.869: INFO: Pod "pod-80fab76c-9264-4db7-8a99-8facdf7bb544" satisfied condition "Succeeded or Failed"
Dec  7 18:52:01.882: INFO: Trying to get logs from node 10.192.217.92 pod pod-80fab76c-9264-4db7-8a99-8facdf7bb544 container test-container: <nil>
STEP: delete the pod
Dec  7 18:52:02.033: INFO: Waiting for pod pod-80fab76c-9264-4db7-8a99-8facdf7bb544 to disappear
Dec  7 18:52:02.047: INFO: Pod pod-80fab76c-9264-4db7-8a99-8facdf7bb544 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:52:02.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6535" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5121,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:52:02.084: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9950 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9950;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9950 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9950;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9950.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9950.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9950.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9950.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9950.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9950.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.131_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9950 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9950;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9950 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9950;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9950.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9950.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9950.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9950.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9950.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9950.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9950.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9950.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9950.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 131.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.131_udp@PTR;check="$$(dig +tcp +noall +answer +search 131.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.131_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 18:52:06.491: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.512: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.579: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.621: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.787: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.807: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.828: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.853: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.876: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.919: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:06.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:07.073: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:12.098: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.118: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.176: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.217: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.238: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.258: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.425: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.447: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.467: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.509: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.529: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.549: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.570: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:12.715: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:17.100: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.121: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.145: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.208: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.229: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.250: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.400: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.436: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.487: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.506: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.526: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.547: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.567: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.587: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:17.719: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:22.117: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.140: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.161: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.181: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.222: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.278: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.416: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.442: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.462: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.503: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.524: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.546: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:22.693: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:27.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.115: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.136: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.220: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.394: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.413: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.437: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.460: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.485: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.506: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.526: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.547: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:27.671: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:32.097: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.119: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.221: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.312: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.517: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.539: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.560: INFO: Unable to read jessie_udp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.581: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950 from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.602: INFO: Unable to read jessie_udp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.646: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc from pod dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae: the server could not find the requested resource (get pods dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae)
Dec  7 18:52:32.797: INFO: Lookups using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9950 wheezy_tcp@dns-test-service.dns-9950 wheezy_udp@dns-test-service.dns-9950.svc wheezy_tcp@dns-test-service.dns-9950.svc wheezy_udp@_http._tcp.dns-test-service.dns-9950.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9950.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9950 jessie_tcp@dns-test-service.dns-9950 jessie_udp@dns-test-service.dns-9950.svc jessie_tcp@dns-test-service.dns-9950.svc jessie_udp@_http._tcp.dns-test-service.dns-9950.svc jessie_tcp@_http._tcp.dns-test-service.dns-9950.svc]

Dec  7 18:52:37.688: INFO: DNS probes using dns-9950/dns-test-35737e44-10b9-4baa-98c7-a9401676b5ae succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:52:37.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9950" for this suite.

• [SLOW TEST:35.867 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":304,"skipped":5135,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:52:37.951: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:52:38.250: INFO: Create a RollingUpdate DaemonSet
Dec  7 18:52:38.275: INFO: Check that daemon pods launch on every node of the cluster
Dec  7 18:52:38.307: INFO: Number of nodes with available pods: 0
Dec  7 18:52:38.307: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:52:39.351: INFO: Number of nodes with available pods: 0
Dec  7 18:52:39.351: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:52:40.343: INFO: Number of nodes with available pods: 0
Dec  7 18:52:40.343: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:52:41.345: INFO: Number of nodes with available pods: 3
Dec  7 18:52:41.345: INFO: Number of running nodes: 3, number of available pods: 3
Dec  7 18:52:41.345: INFO: Update the DaemonSet to trigger a rollout
Dec  7 18:52:41.380: INFO: Updating DaemonSet daemon-set
Dec  7 18:52:44.457: INFO: Roll back the DaemonSet before rollout is complete
Dec  7 18:52:44.502: INFO: Updating DaemonSet daemon-set
Dec  7 18:52:44.502: INFO: Make sure DaemonSet rollback is complete
Dec  7 18:52:44.519: INFO: Wrong image for pod: daemon-set-9n945. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Dec  7 18:52:44.519: INFO: Pod daemon-set-9n945 is not available
Dec  7 18:52:49.560: INFO: Pod daemon-set-cj89z is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4065, will wait for the garbage collector to delete the pods
Dec  7 18:52:49.699: INFO: Deleting DaemonSet.extensions daemon-set took: 25.700211ms
Dec  7 18:52:49.801: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.089984ms
Dec  7 18:52:51.617: INFO: Number of nodes with available pods: 0
Dec  7 18:52:51.617: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 18:52:51.631: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49157"},"items":null}

Dec  7 18:52:51.643: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49158"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:52:51.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4065" for this suite.

• [SLOW TEST:13.806 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":305,"skipped":5140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:52:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec  7 18:52:51.972: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  7 18:52:52.012: INFO: Waiting for terminating namespaces to be deleted...
Dec  7 18:52:52.021: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.108 before test
Dec  7 18:52:52.074: INFO: catalog-operator-6c4b4d7c9-xgg9d from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container catalog-operator ready: true, restart count 0
Dec  7 18:52:52.074: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:52:52.074: INFO: calico-kube-controllers-749944fc4-s4m78 from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  7 18:52:52.074: INFO: calico-node-wl22v from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:52:52.074: INFO: calico-typha-7d788c697f-mvhs4 from kube-system started at 2021-12-07 15:54:49 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:52:52.074: INFO: coredns-b58d5f584-lngfv from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:52:52.074: INFO: dashboard-metrics-scraper-6747f89c97-qtgnt from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Dec  7 18:52:52.074: INFO: ibm-keepalived-watcher-bjzrb from kube-system started at 2021-12-07 15:54:26 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:52:52.074: INFO: ibm-master-proxy-static-10.192.217.108 from kube-system started at 2021-12-07 15:54:14 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:52:52.074: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:52:52.074: INFO: ibm-storage-watcher-65b9c4d74f-sr7kw from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.074: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Dec  7 18:52:52.075: INFO: konnectivity-agent-4b249 from kube-system started at 2021-12-07 16:03:06 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.075: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:52:52.075: INFO: metrics-server-b9bc976b6-5wkrb from kube-system started at 2021-12-07 15:56:17 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.075: INFO: 	Container metrics-server ready: true, restart count 0
Dec  7 18:52:52.075: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Dec  7 18:52:52.075: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9 from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.075: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:52:52.075: INFO: sonobuoy-e2e-job-3b8635ba80e64550 from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.075: INFO: 	Container e2e ready: true, restart count 0
Dec  7 18:52:52.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:52:52.075: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:52:52.075: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:52:52.075: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.124 before test
Dec  7 18:52:52.117: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-12-07 15:57:45 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Dec  7 18:52:52.117: INFO: ibm-cloud-provider-ip-128-168-71-34-6867485f55-z4mpb from ibm-system started at 2021-12-07 15:58:54 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container ibm-cloud-provider-ip-128-168-71-34 ready: true, restart count 0
Dec  7 18:52:52.117: INFO: olm-operator-785cdc5884-8lwrq from ibm-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container olm-operator ready: true, restart count 0
Dec  7 18:52:52.117: INFO: calico-node-d5dvk from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:52:52.117: INFO: calico-typha-7d788c697f-l2q8w from kube-system started at 2021-12-07 15:54:59 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:52:52.117: INFO: coredns-autoscaler-689fb74d49-kfqhc from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container autoscaler ready: true, restart count 0
Dec  7 18:52:52.117: INFO: coredns-b58d5f584-2kckm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:52:52.117: INFO: coredns-b58d5f584-ghv8w from kube-system started at 2021-12-07 16:03:41 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container coredns ready: true, restart count 0
Dec  7 18:52:52.117: INFO: ibm-file-plugin-bbfc75b87-2cw56 from kube-system started at 2021-12-07 18:19:52 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Dec  7 18:52:52.117: INFO: ibm-keepalived-watcher-gjxv9 from kube-system started at 2021-12-07 15:54:38 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:52:52.117: INFO: ibm-master-proxy-static-10.192.217.124 from kube-system started at 2021-12-07 15:54:35 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:52:52.117: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:52:52.117: INFO: konnectivity-agent-nz7z5 from kube-system started at 2021-12-07 16:03:10 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:52:52.117: INFO: kubernetes-dashboard-54c47dd995-blkbm from kube-system started at 2021-12-07 18:19:51 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  7 18:52:52.117: INFO: public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-9vnhs from kube-system started at 2021-12-07 15:58:16 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container nginx-ingress ready: true, restart count 0
Dec  7 18:52:52.117: INFO: sonobuoy from sonobuoy started at 2021-12-07 17:19:42 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  7 18:52:52.117: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-7vsjp from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.117: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:52:52.117: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  7 18:52:52.117: INFO: 
Logging pods the apiserver thinks is on node 10.192.217.92 before test
Dec  7 18:52:52.150: INFO: calico-node-gnsxh from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container calico-node ready: true, restart count 0
Dec  7 18:52:52.150: INFO: calico-typha-7d788c697f-b6hn8 from kube-system started at 2021-12-07 18:20:29 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container calico-typha ready: true, restart count 0
Dec  7 18:52:52.150: INFO: ibm-keepalived-watcher-8d77z from kube-system started at 2021-12-07 15:54:24 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container keepalived-watcher ready: true, restart count 0
Dec  7 18:52:52.150: INFO: ibm-master-proxy-static-10.192.217.92 from kube-system started at 2021-12-07 15:54:06 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Dec  7 18:52:52.150: INFO: 	Container pause ready: true, restart count 0
Dec  7 18:52:52.150: INFO: konnectivity-agent-pcnkz from kube-system started at 2021-12-07 16:03:13 +0000 UTC (1 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container konnectivity-agent ready: true, restart count 0
Dec  7 18:52:52.150: INFO: sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-vgqvj from sonobuoy started at 2021-12-07 17:19:49 +0000 UTC (2 container statuses recorded)
Dec  7 18:52:52.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  7 18:52:52.150: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.192.217.108
STEP: verifying the node has the label node 10.192.217.124
STEP: verifying the node has the label node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod catalog-operator-6c4b4d7c9-xgg9d requesting resource cpu=10m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod ibm-cloud-provider-ip-128-168-71-34-6867485f55-b6k7n requesting resource cpu=5m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod ibm-cloud-provider-ip-128-168-71-34-6867485f55-z4mpb requesting resource cpu=5m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod olm-operator-785cdc5884-8lwrq requesting resource cpu=10m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod calico-kube-controllers-749944fc4-s4m78 requesting resource cpu=10m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod calico-node-d5dvk requesting resource cpu=250m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod calico-node-gnsxh requesting resource cpu=250m on Node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod calico-node-wl22v requesting resource cpu=250m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod calico-typha-7d788c697f-b6hn8 requesting resource cpu=250m on Node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod calico-typha-7d788c697f-l2q8w requesting resource cpu=250m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod calico-typha-7d788c697f-mvhs4 requesting resource cpu=250m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod coredns-autoscaler-689fb74d49-kfqhc requesting resource cpu=20m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod coredns-b58d5f584-2kckm requesting resource cpu=100m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod coredns-b58d5f584-ghv8w requesting resource cpu=100m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod coredns-b58d5f584-lngfv requesting resource cpu=100m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod dashboard-metrics-scraper-6747f89c97-qtgnt requesting resource cpu=1m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod ibm-file-plugin-bbfc75b87-2cw56 requesting resource cpu=50m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod ibm-keepalived-watcher-8d77z requesting resource cpu=5m on Node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod ibm-keepalived-watcher-bjzrb requesting resource cpu=5m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod ibm-keepalived-watcher-gjxv9 requesting resource cpu=5m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod ibm-master-proxy-static-10.192.217.108 requesting resource cpu=25m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod ibm-master-proxy-static-10.192.217.124 requesting resource cpu=25m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod ibm-master-proxy-static-10.192.217.92 requesting resource cpu=25m on Node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod ibm-storage-watcher-65b9c4d74f-sr7kw requesting resource cpu=50m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod konnectivity-agent-4b249 requesting resource cpu=10m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod konnectivity-agent-nz7z5 requesting resource cpu=10m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod konnectivity-agent-pcnkz requesting resource cpu=10m on Node 10.192.217.92
Dec  7 18:52:52.354: INFO: Pod kubernetes-dashboard-54c47dd995-blkbm requesting resource cpu=50m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod metrics-server-b9bc976b6-5wkrb requesting resource cpu=121m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-4mgd9 requesting resource cpu=10m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod public-crc6nnvugt0l0nrclc8f80-alb1-947d94cfb-9vnhs requesting resource cpu=10m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod sonobuoy-e2e-job-3b8635ba80e64550 requesting resource cpu=0m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-7vsjp requesting resource cpu=0m on Node 10.192.217.124
Dec  7 18:52:52.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-nqg7q requesting resource cpu=0m on Node 10.192.217.108
Dec  7 18:52:52.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-6131eb62b436425c-vgqvj requesting resource cpu=0m on Node 10.192.217.92
STEP: Starting Pods to consume most of the cluster CPU.
Dec  7 18:52:52.354: INFO: Creating a pod which consumes cpu=2144m on Node 10.192.217.108
Dec  7 18:52:52.396: INFO: Creating a pod which consumes cpu=2117m on Node 10.192.217.124
Dec  7 18:52:52.418: INFO: Creating a pod which consumes cpu=2359m on Node 10.192.217.92
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494.16be8dc086be5c8a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3207/filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494 to 10.192.217.124]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494.16be8dc0d9c30bd8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494.16be8dc0df117fc9], Reason = [Created], Message = [Created container filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494.16be8dc0ec34ba59], Reason = [Started], Message = [Started container filler-pod-4e568ec7-441a-4b98-b009-af3c8e98f494]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa.16be8dc084f94f9f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3207/filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa to 10.192.217.108]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa.16be8dc0c7e66695], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa.16be8dc0cb0f65f8], Reason = [Created], Message = [Created container filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa.16be8dc0d4985aeb], Reason = [Started], Message = [Started container filler-pod-6f5531d5-4ec3-440d-955f-4543082251aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151.16be8dc087b703c8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3207/filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151 to 10.192.217.92]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151.16be8dc0c958f513], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151.16be8dc0cba5b5cd], Reason = [Created], Message = [Created container filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151.16be8dc0d50bde1e], Reason = [Started], Message = [Started container filler-pod-8fa8d885-bb61-4f10-a659-4ef25446b151]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16be8dc17c969d10], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.192.217.108
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.192.217.124
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.192.217.92
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:52:57.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3207" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.035 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":306,"skipped":5183,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:52:57.795: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-94521cf3-0545-476f-8e46-ec9ec8187370
STEP: Creating a pod to test consume secrets
Dec  7 18:52:58.078: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc" in namespace "projected-4639" to be "Succeeded or Failed"
Dec  7 18:52:58.094: INFO: Pod "pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.365634ms
Dec  7 18:53:00.113: INFO: Pod "pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035290926s
STEP: Saw pod success
Dec  7 18:53:00.113: INFO: Pod "pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc" satisfied condition "Succeeded or Failed"
Dec  7 18:53:00.126: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:53:00.200: INFO: Waiting for pod pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc to disappear
Dec  7 18:53:00.213: INFO: Pod pod-projected-secrets-48324c49-830f-43d9-8d2b-cadc84f1e8fc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:00.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4639" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":5194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:00.249: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9324
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9324
I1207 18:53:00.551628      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9324, replica count: 2
Dec  7 18:53:03.602: INFO: Creating new exec pod
I1207 18:53:03.602498      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 18:53:06.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9324 exec execpod7bnv7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  7 18:53:06.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  7 18:53:06.960: INFO: stdout: "externalname-service-8xkf5"
Dec  7 18:53:06.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9324 exec execpod7bnv7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.218 80'
Dec  7 18:53:07.209: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.218 80\nConnection to 172.21.253.218 80 port [tcp/http] succeeded!\n"
Dec  7 18:53:07.209: INFO: stdout: ""
Dec  7 18:53:08.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-9324 exec execpod7bnv7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.253.218 80'
Dec  7 18:53:08.464: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.253.218 80\nConnection to 172.21.253.218 80 port [tcp/http] succeeded!\n"
Dec  7 18:53:08.464: INFO: stdout: "externalname-service-8xkf5"
Dec  7 18:53:08.464: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:08.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9324" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.310 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":308,"skipped":5237,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:08.560: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec  7 18:53:08.826: INFO: The status of Pod annotationupdatedba59d37-9c3b-4639-b85e-396958386e19 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:53:10.845: INFO: The status of Pod annotationupdatedba59d37-9c3b-4639-b85e-396958386e19 is Running (Ready = true)
Dec  7 18:53:11.437: INFO: Successfully updated pod "annotationupdatedba59d37-9c3b-4639-b85e-396958386e19"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:13.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9741" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":309,"skipped":5245,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:13.544: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-strn
STEP: Creating a pod to test atomic-volume-subpath
Dec  7 18:53:13.827: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-strn" in namespace "subpath-7876" to be "Succeeded or Failed"
Dec  7 18:53:13.862: INFO: Pod "pod-subpath-test-secret-strn": Phase="Pending", Reason="", readiness=false. Elapsed: 35.356804ms
Dec  7 18:53:15.881: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 2.054332711s
Dec  7 18:53:17.899: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 4.071906815s
Dec  7 18:53:19.919: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 6.092146902s
Dec  7 18:53:21.940: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 8.113100615s
Dec  7 18:53:23.958: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 10.131147737s
Dec  7 18:53:25.978: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 12.150656683s
Dec  7 18:53:27.996: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 14.168851031s
Dec  7 18:53:30.013: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 16.185565722s
Dec  7 18:53:32.031: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 18.204321787s
Dec  7 18:53:34.052: INFO: Pod "pod-subpath-test-secret-strn": Phase="Running", Reason="", readiness=true. Elapsed: 20.225355983s
Dec  7 18:53:36.073: INFO: Pod "pod-subpath-test-secret-strn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.245593431s
STEP: Saw pod success
Dec  7 18:53:36.073: INFO: Pod "pod-subpath-test-secret-strn" satisfied condition "Succeeded or Failed"
Dec  7 18:53:36.088: INFO: Trying to get logs from node 10.192.217.92 pod pod-subpath-test-secret-strn container test-container-subpath-secret-strn: <nil>
STEP: delete the pod
Dec  7 18:53:36.167: INFO: Waiting for pod pod-subpath-test-secret-strn to disappear
Dec  7 18:53:36.181: INFO: Pod pod-subpath-test-secret-strn no longer exists
STEP: Deleting pod pod-subpath-test-secret-strn
Dec  7 18:53:36.181: INFO: Deleting pod "pod-subpath-test-secret-strn" in namespace "subpath-7876"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:36.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7876" for this suite.

• [SLOW TEST:22.688 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":310,"skipped":5247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:36.232: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3047
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:53:36.485: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c" in namespace "projected-3047" to be "Succeeded or Failed"
Dec  7 18:53:36.501: INFO: Pod "downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.491065ms
Dec  7 18:53:38.520: INFO: Pod "downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035152058s
Dec  7 18:53:40.535: INFO: Pod "downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050157617s
STEP: Saw pod success
Dec  7 18:53:40.535: INFO: Pod "downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c" satisfied condition "Succeeded or Failed"
Dec  7 18:53:40.550: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c container client-container: <nil>
STEP: delete the pod
Dec  7 18:53:40.626: INFO: Waiting for pod downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c to disappear
Dec  7 18:53:40.640: INFO: Pod downwardapi-volume-33fec5fb-3178-48a7-a182-c718fb78851c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:40.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3047" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:40.676: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-5833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  7 18:53:40.968: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  7 18:53:40.982: INFO: starting watch
STEP: patching
STEP: updating
Dec  7 18:53:41.022: INFO: waiting for watch events with expected annotations
Dec  7 18:53:41.022: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:41.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5833" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":312,"skipped":5319,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:41.131: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  7 18:53:41.479: INFO: Number of nodes with available pods: 0
Dec  7 18:53:41.479: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:53:42.528: INFO: Number of nodes with available pods: 0
Dec  7 18:53:42.528: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:53:43.516: INFO: Number of nodes with available pods: 0
Dec  7 18:53:43.516: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:53:44.518: INFO: Number of nodes with available pods: 3
Dec  7 18:53:44.518: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Dec  7 18:53:44.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49711"},"items":null}

Dec  7 18:53:44.681: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49712"},"items":[{"metadata":{"name":"daemon-set-678fd","generateName":"daemon-set-","namespace":"daemonsets-4588","uid":"59b97fa3-4f0f-4b6b-8b9f-39a05f7ffd4b","resourceVersion":"49712","creationTimestamp":"2021-12-07T18:53:41Z","deletionTimestamp":"2021-12-07T18:54:14Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"05221e84cb3d9dbd4eb05a603917d4a7525f5d18d19339718ae8ada9b6d46b63","cni.projectcalico.org/podIP":"172.30.11.5/32","cni.projectcalico.org/podIPs":"172.30.11.5/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d549d2ec-81c1-4c21-aadc-a9d15f80a915","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d549d2ec-81c1-4c21-aadc-a9d15f80a915\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.11.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g8q9t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g8q9t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.192.217.108","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.192.217.108"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:43Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:43Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"}],"hostIP":"10.192.217.108","podIP":"172.30.11.5","podIPs":[{"ip":"172.30.11.5"}],"startTime":"2021-12-07T18:53:41Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-12-07T18:53:42Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://92dd982f95940bcd30d58cf0d8f763bcdb34e8dbad4b2fdceca199279566e6bb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-p9zt4","generateName":"daemon-set-","namespace":"daemonsets-4588","uid":"8db27e3a-74ae-47c6-a484-876b7ac0ad5c","resourceVersion":"49707","creationTimestamp":"2021-12-07T18:53:41Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e6352ed69c9c3a2eca363211297c7553cb055b9eaa090c72347e51e79ec29274","cni.projectcalico.org/podIP":"172.30.30.90/32","cni.projectcalico.org/podIPs":"172.30.30.90/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d549d2ec-81c1-4c21-aadc-a9d15f80a915","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d549d2ec-81c1-4c21-aadc-a9d15f80a915\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.30.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qg7pw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qg7pw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.192.217.124","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.192.217.124"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"}],"hostIP":"10.192.217.124","podIP":"172.30.30.90","podIPs":[{"ip":"172.30.30.90"}],"startTime":"2021-12-07T18:53:41Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-12-07T18:53:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://94eff6060c2b591ac25e3861eac39e859b8641c7d165805af880c8bd8d86bac7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wd8jc","generateName":"daemon-set-","namespace":"daemonsets-4588","uid":"2ba759dd-64db-4689-b052-274017fbc9f6","resourceVersion":"49702","creationTimestamp":"2021-12-07T18:53:41Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5bc62f448de5303a58a1f56c2500930e7b00cc00b691c98346514df776979f9a","cni.projectcalico.org/podIP":"172.30.34.170/32","cni.projectcalico.org/podIPs":"172.30.34.170/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d549d2ec-81c1-4c21-aadc-a9d15f80a915","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d549d2ec-81c1-4c21-aadc-a9d15f80a915\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-12-07T18:53:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.34.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c7dwr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c7dwr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.192.217.92","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.192.217.92"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:43Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:43Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-12-07T18:53:41Z"}],"hostIP":"10.192.217.92","podIP":"172.30.34.170","podIPs":[{"ip":"172.30.34.170"}],"startTime":"2021-12-07T18:53:41Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-12-07T18:53:42Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://259908bd9c1839a8ea1cfd0d1efdae75bfb2defe6825dc3f1255d4fce003e227","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:53:44.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4588" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":313,"skipped":5326,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:53:44.772: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  7 18:53:46.079: INFO: Pod name wrapped-volume-race-180054fb-8665-4b0b-9d82-86786098c2b4: Found 0 pods out of 5
Dec  7 18:53:51.106: INFO: Pod name wrapped-volume-race-180054fb-8665-4b0b-9d82-86786098c2b4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-180054fb-8665-4b0b-9d82-86786098c2b4 in namespace emptydir-wrapper-3298, will wait for the garbage collector to delete the pods
Dec  7 18:53:51.279: INFO: Deleting ReplicationController wrapped-volume-race-180054fb-8665-4b0b-9d82-86786098c2b4 took: 31.651613ms
Dec  7 18:53:51.380: INFO: Terminating ReplicationController wrapped-volume-race-180054fb-8665-4b0b-9d82-86786098c2b4 pods took: 100.642914ms
STEP: Creating RC which spawns configmap-volume pods
Dec  7 18:53:55.249: INFO: Pod name wrapped-volume-race-baff2d41-6a46-4cb5-9720-95743f9043f5: Found 0 pods out of 5
Dec  7 18:54:00.283: INFO: Pod name wrapped-volume-race-baff2d41-6a46-4cb5-9720-95743f9043f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-baff2d41-6a46-4cb5-9720-95743f9043f5 in namespace emptydir-wrapper-3298, will wait for the garbage collector to delete the pods
Dec  7 18:54:00.483: INFO: Deleting ReplicationController wrapped-volume-race-baff2d41-6a46-4cb5-9720-95743f9043f5 took: 53.050107ms
Dec  7 18:54:00.583: INFO: Terminating ReplicationController wrapped-volume-race-baff2d41-6a46-4cb5-9720-95743f9043f5 pods took: 100.68391ms
STEP: Creating RC which spawns configmap-volume pods
Dec  7 18:54:04.758: INFO: Pod name wrapped-volume-race-a8dfafdc-e5f5-4a2b-9185-04e8a13d119d: Found 0 pods out of 5
Dec  7 18:54:09.798: INFO: Pod name wrapped-volume-race-a8dfafdc-e5f5-4a2b-9185-04e8a13d119d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a8dfafdc-e5f5-4a2b-9185-04e8a13d119d in namespace emptydir-wrapper-3298, will wait for the garbage collector to delete the pods
Dec  7 18:54:09.966: INFO: Deleting ReplicationController wrapped-volume-race-a8dfafdc-e5f5-4a2b-9185-04e8a13d119d took: 29.110427ms
Dec  7 18:54:10.168: INFO: Terminating ReplicationController wrapped-volume-race-a8dfafdc-e5f5-4a2b-9185-04e8a13d119d pods took: 201.097228ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:54:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3298" for this suite.

• [SLOW TEST:30.784 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":314,"skipped":5331,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:54:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-586ec191-e891-4f62-8c3d-99a156ac5eba
STEP: Creating a pod to test consume secrets
Dec  7 18:54:15.822: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39" in namespace "projected-4438" to be "Succeeded or Failed"
Dec  7 18:54:15.835: INFO: Pod "pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.803595ms
Dec  7 18:54:17.855: INFO: Pod "pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033558757s
STEP: Saw pod success
Dec  7 18:54:17.855: INFO: Pod "pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39" satisfied condition "Succeeded or Failed"
Dec  7 18:54:17.870: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39 container secret-volume-test: <nil>
STEP: delete the pod
Dec  7 18:54:17.962: INFO: Waiting for pod pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39 to disappear
Dec  7 18:54:17.975: INFO: Pod pod-projected-secrets-1ca54cc5-6ebb-4d73-a7ba-d894c821ef39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:54:17.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4438" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5335,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:54:18.014: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec  7 18:54:18.289: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:54:20.306: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec  7 18:54:20.367: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec  7 18:54:22.387: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec  7 18:54:22.435: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  7 18:54:22.449: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  7 18:54:24.451: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  7 18:54:24.475: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  7 18:54:26.450: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  7 18:54:26.468: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:54:26.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-112" for this suite.

• [SLOW TEST:8.523 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5337,"failed":0}
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:54:26.538: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:54:26.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5689" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":317,"skipped":5337,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:54:26.908: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  7 18:54:27.186: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 18:55:27.280: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Dec  7 18:55:27.385: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec  7 18:55:27.404: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec  7 18:55:27.460: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec  7 18:55:27.485: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec  7 18:55:27.534: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec  7 18:55:27.561: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:55:45.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4571" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.111 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":318,"skipped":5349,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:55:46.019: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-f851e79e-1556-463a-8bbf-7109a99827fc
STEP: Creating a pod to test consume configMaps
Dec  7 18:55:46.317: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667" in namespace "projected-8523" to be "Succeeded or Failed"
Dec  7 18:55:46.332: INFO: Pod "pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667": Phase="Pending", Reason="", readiness=false. Elapsed: 14.329583ms
Dec  7 18:55:48.351: INFO: Pod "pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033648653s
STEP: Saw pod success
Dec  7 18:55:48.351: INFO: Pod "pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667" satisfied condition "Succeeded or Failed"
Dec  7 18:55:48.365: INFO: Trying to get logs from node 10.192.217.92 pod pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667 container agnhost-container: <nil>
STEP: delete the pod
Dec  7 18:55:48.440: INFO: Waiting for pod pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667 to disappear
Dec  7 18:55:48.459: INFO: Pod pod-projected-configmaps-4d12075f-9622-432d-b6b0-6744da1e3667 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:55:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8523" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":5362,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:55:48.498: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:55:49.441: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:55:51.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500149, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500149, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500149, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500149, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:55:54.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:55:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9398" for this suite.
STEP: Destroying namespace "webhook-9398-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.269 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":320,"skipped":5362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:55:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:55:56.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a" in namespace "projected-4958" to be "Succeeded or Failed"
Dec  7 18:55:56.037: INFO: Pod "downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.996159ms
Dec  7 18:55:58.056: INFO: Pod "downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035534668s
Dec  7 18:56:00.074: INFO: Pod "downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053740855s
STEP: Saw pod success
Dec  7 18:56:00.074: INFO: Pod "downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a" satisfied condition "Succeeded or Failed"
Dec  7 18:56:00.087: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a container client-container: <nil>
STEP: delete the pod
Dec  7 18:56:00.157: INFO: Waiting for pod downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a to disappear
Dec  7 18:56:00.171: INFO: Pod downwardapi-volume-708e2016-a545-48ea-9c9f-275825ccdf3a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:56:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4958" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5397,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:56:00.206: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:56:00.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73744225-db61-4885-9599-30918f7b557b" in namespace "downward-api-9" to be "Succeeded or Failed"
Dec  7 18:56:00.472: INFO: Pod "downwardapi-volume-73744225-db61-4885-9599-30918f7b557b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.284511ms
Dec  7 18:56:02.492: INFO: Pod "downwardapi-volume-73744225-db61-4885-9599-30918f7b557b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037095944s
Dec  7 18:56:04.513: INFO: Pod "downwardapi-volume-73744225-db61-4885-9599-30918f7b557b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058190302s
STEP: Saw pod success
Dec  7 18:56:04.513: INFO: Pod "downwardapi-volume-73744225-db61-4885-9599-30918f7b557b" satisfied condition "Succeeded or Failed"
Dec  7 18:56:04.528: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-73744225-db61-4885-9599-30918f7b557b container client-container: <nil>
STEP: delete the pod
Dec  7 18:56:04.609: INFO: Waiting for pod downwardapi-volume-73744225-db61-4885-9599-30918f7b557b to disappear
Dec  7 18:56:04.623: INFO: Pod downwardapi-volume-73744225-db61-4885-9599-30918f7b557b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:56:04.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5412,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:56:04.662: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-5688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec  7 18:56:04.883: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 18:57:04.967: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 18:57:04.977: INFO: Starting informer...
STEP: Starting pod...
Dec  7 18:57:05.237: INFO: Pod is running on 10.192.217.92. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec  7 18:57:05.287: INFO: Pod wasn't evicted. Proceeding
Dec  7 18:57:05.287: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec  7 18:58:20.334: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:58:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5688" for this suite.

• [SLOW TEST:135.725 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":323,"skipped":5424,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:58:20.388: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2113
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2113
Dec  7 18:58:20.678: INFO: Found 0 stateful pods, waiting for 1
Dec  7 18:58:30.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Dec  7 18:58:30.771: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Dec  7 18:58:30.811: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Dec  7 18:58:30.817: INFO: Observed &StatefulSet event: ADDED
Dec  7 18:58:30.817: INFO: Found Statefulset ss in namespace statefulset-2113 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec  7 18:58:30.817: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Dec  7 18:58:30.817: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec  7 18:58:30.844: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Dec  7 18:58:30.855: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Dec  7 18:58:30.855: INFO: Deleting all statefulset in ns statefulset-2113
Dec  7 18:58:30.871: INFO: Scaling statefulset ss to 0
Dec  7 18:58:40.965: INFO: Waiting for statefulset status.replicas updated to 0
Dec  7 18:58:40.979: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:58:41.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2113" for this suite.

• [SLOW TEST:20.696 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":324,"skipped":5424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:58:41.087: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec  7 18:58:41.344: INFO: Waiting up to 5m0s for pod "downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95" in namespace "downward-api-8872" to be "Succeeded or Failed"
Dec  7 18:58:41.358: INFO: Pod "downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95": Phase="Pending", Reason="", readiness=false. Elapsed: 13.920445ms
Dec  7 18:58:43.374: INFO: Pod "downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029802449s
Dec  7 18:58:45.401: INFO: Pod "downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056672534s
STEP: Saw pod success
Dec  7 18:58:45.401: INFO: Pod "downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95" satisfied condition "Succeeded or Failed"
Dec  7 18:58:45.420: INFO: Trying to get logs from node 10.192.217.92 pod downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95 container dapi-container: <nil>
STEP: delete the pod
Dec  7 18:58:45.552: INFO: Waiting for pod downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95 to disappear
Dec  7 18:58:45.566: INFO: Pod downward-api-92fd142c-9c19-4ed4-bb69-6a1a07613a95 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:58:45.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8872" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":325,"skipped":5449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:58:45.621: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3262.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 140.205.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.205.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.205.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.205.140_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3262.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3262.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3262.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3262.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3262.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 140.205.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.205.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.205.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.205.140_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 18:58:50.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.050: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.070: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.090: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.229: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.248: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.267: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.289: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:50.413: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:58:55.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.455: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.499: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.645: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.668: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.691: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.710: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:58:55.838: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:59:00.436: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.456: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.496: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.646: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.667: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.686: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.705: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:00.821: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:59:05.434: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.455: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.508: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.667: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.687: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.706: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.735: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:05.857: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:59:10.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.458: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.478: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.637: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.660: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.680: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.700: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:10.843: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:59:15.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.456: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.475: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.494: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.639: INFO: Unable to read jessie_udp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.658: INFO: Unable to read jessie_tcp@dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.709: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local from pod dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95: the server could not find the requested resource (get pods dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95)
Dec  7 18:59:15.842: INFO: Lookups using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 failed for: [wheezy_udp@dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@dns-test-service.dns-3262.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_udp@dns-test-service.dns-3262.svc.cluster.local jessie_tcp@dns-test-service.dns-3262.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3262.svc.cluster.local]

Dec  7 18:59:20.835: INFO: DNS probes using dns-3262/dns-test-7a134f05-4970-452c-8c69-783d4f9d1e95 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3262" for this suite.

• [SLOW TEST:35.441 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":326,"skipped":5503,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:21.064: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec  7 18:59:21.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a" in namespace "downward-api-5892" to be "Succeeded or Failed"
Dec  7 18:59:21.343: INFO: Pod "downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.810235ms
Dec  7 18:59:23.362: INFO: Pod "downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035351772s
Dec  7 18:59:25.389: INFO: Pod "downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061653704s
STEP: Saw pod success
Dec  7 18:59:25.389: INFO: Pod "downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a" satisfied condition "Succeeded or Failed"
Dec  7 18:59:25.404: INFO: Trying to get logs from node 10.192.217.92 pod downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a container client-container: <nil>
STEP: delete the pod
Dec  7 18:59:25.518: INFO: Waiting for pod downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a to disappear
Dec  7 18:59:25.532: INFO: Pod downwardapi-volume-dabc607f-1c03-4cd3-b7d4-8d4f8903358a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:25.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5892" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:25.568: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  7 18:59:25.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7604  b847b39a-1561-4a27-9483-fa5f5f9157ff 51650 0 2021-12-07 18:59:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-07 18:59:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:59:25.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7604  b847b39a-1561-4a27-9483-fa5f5f9157ff 51651 0 2021-12-07 18:59:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-07 18:59:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  7 18:59:25.911: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7604  b847b39a-1561-4a27-9483-fa5f5f9157ff 51652 0 2021-12-07 18:59:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-07 18:59:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 18:59:25.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7604  b847b39a-1561-4a27-9483-fa5f5f9157ff 51653 0 2021-12-07 18:59:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-07 18:59:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:25.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7604" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":328,"skipped":5547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:25.947: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  7 18:59:26.312: INFO: Number of nodes with available pods: 0
Dec  7 18:59:26.312: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:59:27.354: INFO: Number of nodes with available pods: 0
Dec  7 18:59:27.354: INFO: Node 10.192.217.108 is running more than one daemon pod
Dec  7 18:59:28.347: INFO: Number of nodes with available pods: 2
Dec  7 18:59:28.347: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 18:59:29.350: INFO: Number of nodes with available pods: 3
Dec  7 18:59:29.350: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  7 18:59:29.441: INFO: Number of nodes with available pods: 2
Dec  7 18:59:29.441: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 18:59:30.475: INFO: Number of nodes with available pods: 2
Dec  7 18:59:30.475: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 18:59:31.475: INFO: Number of nodes with available pods: 2
Dec  7 18:59:31.475: INFO: Node 10.192.217.92 is running more than one daemon pod
Dec  7 18:59:32.515: INFO: Number of nodes with available pods: 3
Dec  7 18:59:32.515: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2261, will wait for the garbage collector to delete the pods
Dec  7 18:59:32.663: INFO: Deleting DaemonSet.extensions daemon-set took: 28.955699ms
Dec  7 18:59:32.764: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.618596ms
Dec  7 18:59:34.785: INFO: Number of nodes with available pods: 0
Dec  7 18:59:34.785: INFO: Number of running nodes: 0, number of available pods: 0
Dec  7 18:59:34.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51808"},"items":null}

Dec  7 18:59:34.811: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51808"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:34.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2261" for this suite.

• [SLOW TEST:8.964 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":329,"skipped":5596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:34.916: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  7 18:59:35.674: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec  7 18:59:37.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500375, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500375, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500375, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500375, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 18:59:40.772: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec  7 18:59:42.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=webhook-9914 attach --namespace=webhook-9914 to-be-attached-pod -i -c=container1'
Dec  7 18:59:43.149: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9914" for this suite.
STEP: Destroying namespace "webhook-9914-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.448 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":330,"skipped":5621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:43.366: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6836
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6836
I1207 18:59:43.683045      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6836, replica count: 2
Dec  7 18:59:46.734: INFO: Creating new exec pod
I1207 18:59:46.734438      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  7 18:59:49.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  7 18:59:50.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  7 18:59:50.088: INFO: stdout: ""
Dec  7 18:59:51.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  7 18:59:51.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  7 18:59:51.391: INFO: stdout: ""
Dec  7 18:59:52.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec  7 18:59:52.359: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  7 18:59:52.359: INFO: stdout: "externalname-service-7drx5"
Dec  7 18:59:52.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.79.150 80'
Dec  7 18:59:52.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.79.150 80\nConnection to 172.21.79.150 80 port [tcp/http] succeeded!\n"
Dec  7 18:59:52.606: INFO: stdout: "externalname-service-7drx5"
Dec  7 18:59:52.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.108 31857'
Dec  7 18:59:52.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.108 31857\nConnection to 10.192.217.108 31857 port [tcp/*] succeeded!\n"
Dec  7 18:59:52.871: INFO: stdout: ""
Dec  7 18:59:53.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.108 31857'
Dec  7 18:59:54.126: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.108 31857\nConnection to 10.192.217.108 31857 port [tcp/*] succeeded!\n"
Dec  7 18:59:54.126: INFO: stdout: ""
Dec  7 18:59:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.108 31857'
Dec  7 18:59:55.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.108 31857\nConnection to 10.192.217.108 31857 port [tcp/*] succeeded!\n"
Dec  7 18:59:55.138: INFO: stdout: "externalname-service-vf2fm"
Dec  7 18:59:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 31857'
Dec  7 18:59:55.410: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 31857\nConnection to 10.192.217.92 31857 port [tcp/*] succeeded!\n"
Dec  7 18:59:55.410: INFO: stdout: ""
Dec  7 18:59:56.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-6836 exec execpodjhc4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.192.217.92 31857'
Dec  7 18:59:56.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.192.217.92 31857\nConnection to 10.192.217.92 31857 port [tcp/*] succeeded!\n"
Dec  7 18:59:56.664: INFO: stdout: "externalname-service-7drx5"
Dec  7 18:59:56.664: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 18:59:56.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6836" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.427 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":331,"skipped":5689,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 18:59:56.793: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 19:00:01.183: INFO: DNS probes using dns-test-3df69a71-ca39-4cd1-a774-f1b403dc7e16 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 19:00:05.483: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:05.502: INFO: File jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:05.503: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:10.526: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:10.545: INFO: File jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:10.545: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:15.523: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:15.546: INFO: File jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:15.546: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:20.523: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:20.544: INFO: File jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:20.544: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:25.535: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:25.554: INFO: File jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:25.554: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:30.525: INFO: File wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local from pod  dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  7 19:00:30.547: INFO: Lookups using dns-2438/dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a failed for: [wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local]

Dec  7 19:00:35.545: INFO: DNS probes using dns-test-f6b0019c-4059-4d04-9264-d83409d4d48a succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2438.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2438.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 19:00:39.842: INFO: DNS probes using dns-test-5de47e74-5554-4f0d-b9a6-7ad0b3ae2440 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:00:39.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2438" for this suite.

• [SLOW TEST:43.187 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":332,"skipped":5694,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:00:39.982: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1508
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  7 19:00:40.214: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec  7 19:00:40.341: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:00:42.362: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:44.360: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:46.362: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:48.359: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:50.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:52.370: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:54.362: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:56.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:00:58.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec  7 19:01:00.363: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec  7 19:01:00.395: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec  7 19:01:00.427: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec  7 19:01:02.564: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec  7 19:01:02.564: INFO: Going to poll 172.30.11.19 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  7 19:01:02.578: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.11.19:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:02.578: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 19:01:02.769: INFO: Found all 1 expected endpoints: [netserver-0]
Dec  7 19:01:02.769: INFO: Going to poll 172.30.30.99 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  7 19:01:02.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.30.99:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:02.784: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 19:01:02.965: INFO: Found all 1 expected endpoints: [netserver-1]
Dec  7 19:01:02.965: INFO: Going to poll 172.30.34.134 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec  7 19:01:02.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.34.134:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1508 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:02.980: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 19:01:03.222: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:01:03.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1508" for this suite.

• [SLOW TEST:23.283 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":5703,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:01:03.265: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-1130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Dec  7 19:01:03.545: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:05.565: INFO: The status of Pod pod1 is Running (Ready = false)
Dec  7 19:01:07.564: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.192.217.92 on the node which pod1 resides and expect scheduled
Dec  7 19:01:07.600: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:09.619: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:11.620: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:13.619: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.192.217.92 but use UDP protocol on the node which pod2 resides
Dec  7 19:01:13.656: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:15.677: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:17.676: INFO: The status of Pod pod3 is Running (Ready = true)
Dec  7 19:01:17.714: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:01:19.734: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Dec  7 19:01:19.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.192.217.92 http://127.0.0.1:54323/hostname] Namespace:hostport-1130 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:19.748: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.192.217.92, port: 54323
Dec  7 19:01:19.924: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.192.217.92:54323/hostname] Namespace:hostport-1130 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:19.924: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.192.217.92, port: 54323 UDP
Dec  7 19:01:20.113: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.192.217.92 54323] Namespace:hostport-1130 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec  7 19:01:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:01:25.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1130" for this suite.

• [SLOW TEST:22.084 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":334,"skipped":5712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:01:25.350: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7924.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7924.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7924.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 19:01:29.825: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.846: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.866: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.887: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.952: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.972: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:29.992: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:30.013: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:30.050: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:01:35.081: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.101: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.126: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.148: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.207: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.229: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.250: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.275: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:35.318: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:01:40.091: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.111: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.130: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.149: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.208: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.229: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.248: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.271: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:40.311: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:01:45.074: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.095: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.116: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.136: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.210: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.232: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.255: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.275: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:45.326: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:01:50.074: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.096: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.117: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.140: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.205: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.226: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.248: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.686: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:50.726: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:01:55.071: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.092: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.114: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.135: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.205: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.228: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.248: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.271: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local from pod dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f: the server could not find the requested resource (get pods dns-test-333a4c82-9c97-4a7c-b17c-20626968916f)
Dec  7 19:01:55.314: INFO: Lookups using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7924.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7924.svc.cluster.local jessie_udp@dns-test-service-2.dns-7924.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7924.svc.cluster.local]

Dec  7 19:02:00.314: INFO: DNS probes using dns-7924/dns-test-333a4c82-9c97-4a7c-b17c-20626968916f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:02:00.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7924" for this suite.

• [SLOW TEST:35.154 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":335,"skipped":5774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:02:00.506: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec  7 19:02:00.760: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  7 19:03:00.862: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:00.873: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 19:03:01.158: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec  7 19:03:01.170: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:01.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1347" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5527" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.990 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":336,"skipped":5841,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4052
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Dec  7 19:03:01.709: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=kubectl-4052 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:01.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4052" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":337,"skipped":5850,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:01.808: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-6991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec  7 19:03:02.065: INFO: Waiting up to 5m0s for pod "security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25" in namespace "security-context-6991" to be "Succeeded or Failed"
Dec  7 19:03:02.079: INFO: Pod "security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 13.407368ms
Dec  7 19:03:04.099: INFO: Pod "security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033846854s
Dec  7 19:03:06.118: INFO: Pod "security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052551595s
STEP: Saw pod success
Dec  7 19:03:06.118: INFO: Pod "security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25" satisfied condition "Succeeded or Failed"
Dec  7 19:03:06.133: INFO: Trying to get logs from node 10.192.217.92 pod security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25 container test-container: <nil>
STEP: delete the pod
Dec  7 19:03:06.237: INFO: Waiting for pod security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25 to disappear
Dec  7 19:03:06.252: INFO: Pod security-context-f6e690a0-b5a1-4448-be4a-fbeb931f5c25 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:06.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6991" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":338,"skipped":5866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:06.288: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1989
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 19:03:06.518: INFO: Creating pod...
Dec  7 19:03:06.572: INFO: Pod Quantity: 1 Status: Pending
Dec  7 19:03:07.587: INFO: Pod Quantity: 1 Status: Pending
Dec  7 19:03:08.590: INFO: Pod Status: Running
Dec  7 19:03:08.590: INFO: Creating service...
Dec  7 19:03:08.621: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/DELETE
Dec  7 19:03:08.675: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  7 19:03:08.675: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/GET
Dec  7 19:03:08.695: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  7 19:03:08.695: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/HEAD
Dec  7 19:03:08.715: INFO: http.Client request:HEAD | StatusCode:200
Dec  7 19:03:08.715: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/OPTIONS
Dec  7 19:03:08.736: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  7 19:03:08.736: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/PATCH
Dec  7 19:03:08.758: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  7 19:03:08.758: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/POST
Dec  7 19:03:08.780: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  7 19:03:08.780: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/pods/agnhost/proxy/some/path/with/PUT
Dec  7 19:03:08.801: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec  7 19:03:08.801: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/DELETE
Dec  7 19:03:08.836: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec  7 19:03:08.836: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/GET
Dec  7 19:03:08.868: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec  7 19:03:08.868: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/HEAD
Dec  7 19:03:08.900: INFO: http.Client request:HEAD | StatusCode:200
Dec  7 19:03:08.900: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/OPTIONS
Dec  7 19:03:08.931: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec  7 19:03:08.931: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/PATCH
Dec  7 19:03:08.963: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec  7 19:03:08.963: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/POST
Dec  7 19:03:08.993: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec  7 19:03:08.993: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1989/services/test-service/proxy/some/path/with/PUT
Dec  7 19:03:09.025: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:09.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1989" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":339,"skipped":5932,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:09.073: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  7 19:03:09.980: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec  7 19:03:12.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500589, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500589, loc:(*time.Location)(0xa0a1d40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500590, loc:(*time.Location)(0xa0a1d40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63774500589, loc:(*time.Location)(0xa0a1d40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  7 19:03:15.091: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 19:03:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
Dec  7 19:03:18.799: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=e2e-test-crd-webhook-9098-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-2358.svc:9443/crdconvert?timeout=30s": EOF
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:20.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2358" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:11.698 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":340,"skipped":5941,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:20.774: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9889.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  7 19:03:25.389: INFO: DNS probes using dns-9889/dns-test-3233e903-b1c1-4a04-af3f-ca364bc93363 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:25.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9889" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":341,"skipped":5950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:25.482: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:29.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2508" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":5990,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:29.845: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec  7 19:03:30.138: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec  7 19:03:30.152: INFO: starting watch
STEP: patching
STEP: updating
Dec  7 19:03:30.208: INFO: waiting for watch events with expected annotations
Dec  7 19:03:30.208: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:30.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9073" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":343,"skipped":6010,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:30.354: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2264
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec  7 19:03:30.570: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  7 19:03:33.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2264 --namespace=crd-publish-openapi-2264 create -f -'
Dec  7 19:03:34.260: INFO: stderr: ""
Dec  7 19:03:34.260: INFO: stdout: "e2e-test-crd-publish-openapi-345-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  7 19:03:34.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2264 --namespace=crd-publish-openapi-2264 delete e2e-test-crd-publish-openapi-345-crds test-cr'
Dec  7 19:03:34.412: INFO: stderr: ""
Dec  7 19:03:34.412: INFO: stdout: "e2e-test-crd-publish-openapi-345-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec  7 19:03:34.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2264 --namespace=crd-publish-openapi-2264 apply -f -'
Dec  7 19:03:34.639: INFO: stderr: ""
Dec  7 19:03:34.639: INFO: stdout: "e2e-test-crd-publish-openapi-345-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  7 19:03:34.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2264 --namespace=crd-publish-openapi-2264 delete e2e-test-crd-publish-openapi-345-crds test-cr'
Dec  7 19:03:34.734: INFO: stderr: ""
Dec  7 19:03:34.734: INFO: stdout: "e2e-test-crd-publish-openapi-345-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  7 19:03:34.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=crd-publish-openapi-2264 explain e2e-test-crd-publish-openapi-345-crds'
Dec  7 19:03:35.960: INFO: stderr: ""
Dec  7 19:03:35.960: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-345-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:40.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2264" for this suite.

• [SLOW TEST:9.741 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":344,"skipped":6013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:40.095: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  7 19:03:40.421: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53188 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 19:03:40.422: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53189 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 19:03:40.422: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53190 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  7 19:03:50.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53213 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 19:03:50.572: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53214 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec  7 19:03:50.572: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9525  dfefd235-6511-40be-9477-711d8a203663 53215 0 2021-12-07 19:03:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-07 19:03:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:03:50.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9525" for this suite.

• [SLOW TEST:10.511 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":345,"skipped":6036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec  7 19:03:50.606: INFO: >>> kubeConfig: /tmp/kubeconfig-742025024
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-8711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8711 to expose endpoints map[]
Dec  7 19:03:50.895: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec  7 19:03:51.927: INFO: successfully validated that service endpoint-test2 in namespace services-8711 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8711
Dec  7 19:03:51.973: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:03:53.994: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8711 to expose endpoints map[pod1:[80]]
Dec  7 19:03:54.054: INFO: successfully validated that service endpoint-test2 in namespace services-8711 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Dec  7 19:03:54.054: INFO: Creating new exec pod
Dec  7 19:03:57.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  7 19:03:57.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  7 19:03:57.363: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 19:03:57.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.187 80'
Dec  7 19:03:57.654: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.189.187 80\nConnection to 172.21.189.187 80 port [tcp/http] succeeded!\n"
Dec  7 19:03:57.654: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8711
Dec  7 19:03:57.694: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec  7 19:03:59.716: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8711 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  7 19:03:59.787: INFO: successfully validated that service endpoint-test2 in namespace services-8711 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Dec  7 19:04:00.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  7 19:04:01.045: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  7 19:04:01.045: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 19:04:01.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.187 80'
Dec  7 19:04:01.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.189.187 80\nConnection to 172.21.189.187 80 port [tcp/http] succeeded!\n"
Dec  7 19:04:01.307: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8711 to expose endpoints map[pod2:[80]]
Dec  7 19:04:01.409: INFO: successfully validated that service endpoint-test2 in namespace services-8711 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Dec  7 19:04:02.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec  7 19:04:03.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec  7 19:04:03.695: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec  7 19:04:03.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-742025024 --namespace=services-8711 exec execpodphrcs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.187 80'
Dec  7 19:04:03.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.189.187 80\nConnection to 172.21.189.187 80 port [tcp/http] succeeded!\n"
Dec  7 19:04:03.958: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8711
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8711 to expose endpoints map[]
Dec  7 19:04:04.050: INFO: successfully validated that service endpoint-test2 in namespace services-8711 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec  7 19:04:04.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8711" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.542 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":346,"skipped":6077,"failed":0}
SSSSSSSSSSDec  7 19:04:04.148: INFO: Running AfterSuite actions on all nodes
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec  7 19:04:04.148: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Dec  7 19:04:04.148: INFO: Running AfterSuite actions on node 1
Dec  7 19:04:04.148: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6087,"failed":0}

Ran 346 of 6433 Specs in 6235.871 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6087 Skipped
PASS

Ginkgo ran 1 suite in 1h43m57.87941744s
Test Suite Passed
