I1116 19:29:57.684898      25 e2e.go:129] Starting e2e run "06d4c95c-5f5c-47a0-b4ae-055bf8744e52" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1637090997 - Will randomize all specs
Will run 346 of 6432 specs

Nov 16 19:29:59.637: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
E1116 19:29:59.638931      25 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Nov 16 19:29:59.639: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 16 19:29:59.701: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 16 19:29:59.833: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 16 19:29:59.833: INFO: expected 15 pod replicas in namespace 'kube-system', 15 are Running and Ready.
Nov 16 19:29:59.833: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 16 19:29:59.859: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Nov 16 19:29:59.859: INFO: e2e test version: v1.22.3
Nov 16 19:29:59.864: INFO: kube-apiserver version: v1.22.3+IKS
Nov 16 19:29:59.864: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:29:59.887: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:29:59.888: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
W1116 19:30:00.006347      25 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Nov 16 19:30:00.007: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Nov 16 19:30:00.072: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
W1116 19:30:00.085104      25 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W1116 19:30:00.106531      25 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Nov 16 19:30:00.159: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:30:00.850: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Nov 16 19:30:02.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 19:30:04.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 19:30:06.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687800, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:30:09.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:30:10.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4978" for this suite.
STEP: Destroying namespace "webhook-4978-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.553 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":1,"skipped":21,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:30:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:30:10.661: INFO: Creating deployment "webserver-deployment"
Nov 16 19:30:10.677: INFO: Waiting for observed generation 1
Nov 16 19:30:12.710: INFO: Waiting for all required pods to come up
Nov 16 19:30:12.724: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 16 19:30:20.755: INFO: Waiting for deployment "webserver-deployment" to complete
Nov 16 19:30:20.789: INFO: Updating deployment "webserver-deployment" with a non-existent image
Nov 16 19:30:20.834: INFO: Updating deployment webserver-deployment
Nov 16 19:30:20.834: INFO: Waiting for observed generation 2
Nov 16 19:30:22.863: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 16 19:30:22.885: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 16 19:30:22.900: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:30:22.936: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 16 19:30:22.936: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 16 19:30:22.950: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:30:22.974: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Nov 16 19:30:22.975: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Nov 16 19:30:23.011: INFO: Updating deployment webserver-deployment
Nov 16 19:30:23.011: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Nov 16 19:30:23.042: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 16 19:30:25.116: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 19:30:25.141: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9962  b94c09fb-781a-491f-8954-9d9cb1b0f71c 19271 3 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003108e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-11-16 19:30:23 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-11-16 19:30:23 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Nov 16 19:30:25.158: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9962  d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 19261 3 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b94c09fb-781a-491f-8954-9d9cb1b0f71c 0xc002ffd447 0xc002ffd448}] []  [{kube-controller-manager Update apps/v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b94c09fb-781a-491f-8954-9d9cb1b0f71c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ffd4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:30:25.158: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Nov 16 19:30:25.158: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-9962  a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 19260 3 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b94c09fb-781a-491f-8954-9d9cb1b0f71c 0xc002ffd547 0xc002ffd548}] []  [{kube-controller-manager Update apps/v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b94c09fb-781a-491f-8954-9d9cb1b0f71c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:30:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ffd5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Nov 16 19:30:25.182: INFO: Pod "webserver-deployment-795d758f88-2tjh5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2tjh5 webserver-deployment-795d758f88- deployment-9962  8e55cb50-e475-45c2-952b-78fca8b24e91 19193 0 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:092ce685e8ac9d5229ac5a5338caa08632f2dd5f17fb366072c7824528478c97 cni.projectcalico.org/podIP:172.30.184.226/32 cni.projectcalico.org/podIPs:172.30.184.226/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc002ffdaa7 0xc002ffdaa8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5hrz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5hrz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.182: INFO: Pod "webserver-deployment-795d758f88-4tc9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4tc9x webserver-deployment-795d758f88- deployment-9962  f53f17ca-162c-4555-bde9-2d5df839855e 19182 0 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:01358b893a4b581a252b0f923feddff81a19d130af4c3758c17922bfe8a55c34 cni.projectcalico.org/podIP:172.30.184.225/32 cni.projectcalico.org/podIPs:172.30.184.225/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc002ffdcb7 0xc002ffdcb8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xv4wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xv4wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.182: INFO: Pod "webserver-deployment-795d758f88-4x6wh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4x6wh webserver-deployment-795d758f88- deployment-9962  0dbc00d3-faed-4057-b362-f8ebb8700ff9 19315 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc002ffdec7 0xc002ffdec8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk2kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk2kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.183: INFO: Pod "webserver-deployment-795d758f88-88bsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-88bsv webserver-deployment-795d758f88- deployment-9962  896574cd-a836-453f-9ccf-83c4f7f75b93 19198 0 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:7aaf73f12e976768f62d0e51ea7a737244895edc383035531811d89f9aac3502 cni.projectcalico.org/podIP:172.30.9.1/32 cni.projectcalico.org/podIPs:172.30.9.1/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc0033500d7 0xc0033500d8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bw6lb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bw6lb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.183: INFO: Pod "webserver-deployment-795d758f88-8fbdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8fbdz webserver-deployment-795d758f88- deployment-9962  e79f3faa-a7b3-4d0e-af20-0dd27f7d76db 19281 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc0033502e7 0xc0033502e8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fq2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fq2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.183: INFO: Pod "webserver-deployment-795d758f88-bglds" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bglds webserver-deployment-795d758f88- deployment-9962  3393647e-1bb9-4b6e-802e-2e4666b655a9 19334 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:b176d191be12c9582542a81ff6974ddd3f6e7dac2dd06371b0e86eff91aba76d cni.projectcalico.org/podIP:172.30.184.229/32 cni.projectcalico.org/podIPs:172.30.184.229/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc0033504d7 0xc0033504d8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c242z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c242z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.183: INFO: Pod "webserver-deployment-795d758f88-fpcxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fpcxq webserver-deployment-795d758f88- deployment-9962  637c101d-cf08-49a6-9430-abe8366d1b5b 19186 0 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:75e18f7cd0d95dbbbef42ba7fadd97bb7d3d5d75270f6be931aae17d92a110eb cni.projectcalico.org/podIP:172.30.9.2/32 cni.projectcalico.org/podIPs:172.30.9.2/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc003350707 0xc003350708}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvdcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvdcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.184: INFO: Pod "webserver-deployment-795d758f88-fschb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fschb webserver-deployment-795d758f88- deployment-9962  e9b0883d-7334-483e-b591-ea4406b3a222 19320 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:b09f76a210629a3365efa84cd0d1e23d2b778db1fcb48ec80c510d2d881a5ed9 cni.projectcalico.org/podIP:172.30.148.167/32 cni.projectcalico.org/podIPs:172.30.148.167/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc003350917 0xc003350918}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vtdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vtdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.184: INFO: Pod "webserver-deployment-795d758f88-lq7kj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lq7kj webserver-deployment-795d758f88- deployment-9962  3420e5b3-2b26-4726-9a6b-f2c46f28f96d 19312 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc003350b27 0xc003350b28}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vnklr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vnklr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.184: INFO: Pod "webserver-deployment-795d758f88-npt8d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-npt8d webserver-deployment-795d758f88- deployment-9962  defff0a1-cad3-41e6-b7ca-84cd20873818 19314 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc003350d17 0xc003350d18}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkpr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkpr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.184: INFO: Pod "webserver-deployment-795d758f88-pk8w7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pk8w7 webserver-deployment-795d758f88- deployment-9962  97f7cd05-5333-4489-94bb-17c6331e88c7 19277 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc003350f07 0xc003350f08}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fknkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fknkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.185: INFO: Pod "webserver-deployment-795d758f88-qzq4p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qzq4p webserver-deployment-795d758f88- deployment-9962  9064609e-c5e4-4fae-b766-75a3fe68111e 19308 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc0033510f7 0xc0033510f8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7h4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7h4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.185: INFO: Pod "webserver-deployment-795d758f88-wfmlm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wfmlm webserver-deployment-795d758f88- deployment-9962  946bb185-d623-4c68-894f-4ccfbf6ac8b2 19181 0 2021-11-16 19:30:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:0225ebb91960778a9ecade610f491d5702503db72d4d0b3bd2dc9b3e1bb5943a cni.projectcalico.org/podIP:172.30.148.165/32 cni.projectcalico.org/podIPs:172.30.148.165/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7 0xc0033512e7 0xc0033512e8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3e8e2c6-6ba8-4cfb-9dc2-9886e10037b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvl6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvl6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.185: INFO: Pod "webserver-deployment-847dcfb7fb-2hfmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2hfmx webserver-deployment-847dcfb7fb- deployment-9962  815597c9-32e9-4460-93c9-2d6cc05fec2b 19344 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:37b3d0fe4169702e0b2ccac9290a44b0e1791e8281cc4e003cd9a441f7ad8e1a cni.projectcalico.org/podIP:172.30.9.10/32 cni.projectcalico.org/podIPs:172.30.9.10/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003351517 0xc003351518}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q22c7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q22c7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.185: INFO: Pod "webserver-deployment-847dcfb7fb-2srd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2srd2 webserver-deployment-847dcfb7fb- deployment-9962  45f7ab18-b60c-4308-9da7-f858cf0d71c3 19270 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003351707 0xc003351708}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssxnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssxnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.185: INFO: Pod "webserver-deployment-847dcfb7fb-4qq6m" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4qq6m webserver-deployment-847dcfb7fb- deployment-9962  e06c34e9-b9a5-4b4e-9212-deb86bea401e 19078 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:0c1b3dac055bd2b3afa28db4d5d1a71cd81d5fe0115bc7d4bb24667eb0215c0f cni.projectcalico.org/podIP:172.30.148.162/32 cni.projectcalico.org/podIPs:172.30.148.162/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033518d7 0xc0033518d8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.148.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxkrj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxkrj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:172.30.148.162,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://a71284ec1d57a6633124f795c96ff5e91164a70291ac52d22d80301a48f267e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.148.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.188: INFO: Pod "webserver-deployment-847dcfb7fb-6tvgv" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6tvgv webserver-deployment-847dcfb7fb- deployment-9962  753a6c3e-5217-4fb7-8a3c-8236f68b62c6 19110 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:756c42d65aa8d3d00241378d7818db4970c14791467822ce689d160e2f9b02a1 cni.projectcalico.org/podIP:172.30.148.164/32 cni.projectcalico.org/podIPs:172.30.148.164/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003351ae7 0xc003351ae8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.148.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p54w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p54w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:172.30.148.164,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://8673562781b7aff7b7d3c85dee2e442699b0a8f76bc699d0b8653ab05ae8915a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.148.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.188: INFO: Pod "webserver-deployment-847dcfb7fb-8pqnj" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8pqnj webserver-deployment-847dcfb7fb- deployment-9962  44bf73fa-095f-4137-9638-58581bacb343 19107 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:dccdeeeaac70e658dedd135c1f5a62941392ccf874b7c1c49e2379b77ebe989a cni.projectcalico.org/podIP:172.30.9.62/32 cni.projectcalico.org/podIPs:172.30.9.62/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003351d17 0xc003351d18}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf2gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf2gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.62,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://d309980bb4bb79fe95085566cb448eb2a1abdf1964f417461bf22fb49eb9c625,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.189: INFO: Pod "webserver-deployment-847dcfb7fb-gg5fz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gg5fz webserver-deployment-847dcfb7fb- deployment-9962  6d74340e-390d-4fdb-a1be-9846d7a1b453 19313 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003351f20 0xc003351f21}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rh4jw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rh4jw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.189: INFO: Pod "webserver-deployment-847dcfb7fb-hddpk" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hddpk webserver-deployment-847dcfb7fb- deployment-9962  19605001-509f-4f4e-92f5-8159817d9f88 19075 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:5c1aac7736695ebb036e269ef52c460766cad961d308c33b57a3d9548cf33bfc cni.projectcalico.org/podIP:172.30.148.163/32 cni.projectcalico.org/podIPs:172.30.148.163/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033920e7 0xc0033920e8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.148.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbnt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbnt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:172.30.148.163,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://6a9cc3d811ad38dc6567c3f373ab8b4bf4323a99eabe749f00349f00ffa852c5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.148.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.189: INFO: Pod "webserver-deployment-847dcfb7fb-j847l" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-j847l webserver-deployment-847dcfb7fb- deployment-9962  0b43ec49-441d-414e-a765-6eac0f25297c 19324 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:ca13ac1faace6b8744376ed5b6c1c6b8891e32133a197cd68218575e8a35091e cni.projectcalico.org/podIP:172.30.9.6/32 cni.projectcalico.org/podIPs:172.30.9.6/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003392317 0xc003392318}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l5fw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l5fw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.189: INFO: Pod "webserver-deployment-847dcfb7fb-k7ks7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-k7ks7 webserver-deployment-847dcfb7fb- deployment-9962  2c8d032a-06c5-487f-94f1-46ff8a96947d 19297 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003392507 0xc003392508}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w62hw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w62hw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.190: INFO: Pod "webserver-deployment-847dcfb7fb-kf97d" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kf97d webserver-deployment-847dcfb7fb- deployment-9962  8b1df262-7afd-4565-8753-534607f9eac1 19285 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033926d7 0xc0033926d8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdbb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdbb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.190: INFO: Pod "webserver-deployment-847dcfb7fb-mrv7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mrv7w webserver-deployment-847dcfb7fb- deployment-9962  66307440-eda2-4876-8311-28ca8f2c04d2 19335 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:188bcfe4a4dee0a60ef6e08c7c01a8c8c776ff6d9179676ba1222b73f7e4a592 cni.projectcalico.org/podIP:172.30.9.9/32 cni.projectcalico.org/podIPs:172.30.9.9/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033928c7 0xc0033928c8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfhlm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfhlm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.191: INFO: Pod "webserver-deployment-847dcfb7fb-nrx4c" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nrx4c webserver-deployment-847dcfb7fb- deployment-9962  49b6f50a-0633-49f5-9b34-7f412fe75402 19088 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:49fa57d5eed5e3e3239aa7e35c56fb15b04aaffa1bb8521415097eec7bd102b2 cni.projectcalico.org/podIP:172.30.184.223/32 cni.projectcalico.org/podIPs:172.30.184.223/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003392ab7 0xc003392ab8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.184.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ktb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ktb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:172.30.184.223,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://983fc9cc247d31ad4245c345f6c58a6860ca2201893c8c33a72cf6c5eec9768d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.184.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.191: INFO: Pod "webserver-deployment-847dcfb7fb-pkjwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pkjwx webserver-deployment-847dcfb7fb- deployment-9962  87c4b2f1-cc41-4b4c-aefb-7ea693d8c2bc 19351 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:aff3e9d6846238f7f96fcc82c4b1ddd2bb8dc31c4d107770612dfb1584f149d7 cni.projectcalico.org/podIP:172.30.148.169/32 cni.projectcalico.org/podIPs:172.30.148.169/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003392cc7 0xc003392cc8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8rh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8rh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.191: INFO: Pod "webserver-deployment-847dcfb7fb-q7t9d" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q7t9d webserver-deployment-847dcfb7fb- deployment-9962  51513428-0de1-4f10-aa93-f646d1000510 19091 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:348cbe660667c3f4389ced0331fe8b1d58e48915ca05baf1d1e861ce3e2567ec cni.projectcalico.org/podIP:172.30.184.222/32 cni.projectcalico.org/podIPs:172.30.184.222/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003392eb7 0xc003392eb8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.184.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b7hxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7hxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:172.30.184.222,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://387fca059327fabcfcae19ff1ece2dee0f2af8e4816991eea022a70d20bbccf3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.184.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.192: INFO: Pod "webserver-deployment-847dcfb7fb-qkjb8" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qkjb8 webserver-deployment-847dcfb7fb- deployment-9962  7c9461b4-ca35-429d-91e7-c1b45ae63426 19114 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:a1e1fd4a1f157af4c8f95f8e1be43840432fb8d2be3bce41008de92036fead56 cni.projectcalico.org/podIP:172.30.184.224/32 cni.projectcalico.org/podIPs:172.30.184.224/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033930c7 0xc0033930c8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.184.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vql2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vql2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:172.30.184.224,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://1ca656d1f973b88a95101cc91236821d9243e09790eee12748afb40c10177a4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.184.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.192: INFO: Pod "webserver-deployment-847dcfb7fb-tgwhh" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tgwhh webserver-deployment-847dcfb7fb- deployment-9962  2f0458e6-1b83-4308-9bda-8cc7ad0582b5 19350 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:32056665455755d2e16fb76fc84b73e149b0297d4cf046c8b0e27047992f2e29 cni.projectcalico.org/podIP:172.30.184.230/32 cni.projectcalico.org/podIPs:172.30.184.230/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033932d7 0xc0033932d8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slggf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slggf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.192: INFO: Pod "webserver-deployment-847dcfb7fb-thlhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-thlhn webserver-deployment-847dcfb7fb- deployment-9962  309574a5-2760-41ff-aacc-60e3035ce7a7 19284 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc0033934c7 0xc0033934c8}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-958qw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-958qw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.192: INFO: Pod "webserver-deployment-847dcfb7fb-tvz28" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tvz28 webserver-deployment-847dcfb7fb- deployment-9962  8818c7b3-c21a-4b47-aaba-91abf41cd9e6 19322 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:51b93d41f384e195c147aa7321a06a71d5691f9891ee6589f978c57dc9ca082e cni.projectcalico.org/podIP:172.30.184.227/32 cni.projectcalico.org/podIPs:172.30.184.227/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003393697 0xc003393698}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8592,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8592,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.193: INFO: Pod "webserver-deployment-847dcfb7fb-tzcgk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tzcgk webserver-deployment-847dcfb7fb- deployment-9962  0be9196b-a43c-4429-9828-fa50ed1daae5 19328 0 2021-11-16 19:30:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7e0d81eab95793f42eda40fb2e915d88a6ae74d1699bc4d439d0197d05f42fd4 cni.projectcalico.org/podIP:172.30.148.168/32 cni.projectcalico.org/podIPs:172.30.148.168/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003393887 0xc003393888}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 19:30:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2021-11-16 19:30:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kfv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kfv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:,StartTime:2021-11-16 19:30:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 19:30:25.195: INFO: Pod "webserver-deployment-847dcfb7fb-v9dc9" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-v9dc9 webserver-deployment-847dcfb7fb- deployment-9962  bed249b5-251b-4c17-81b5-e185a1222ddd 19103 0 2021-11-16 19:30:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:61a5b65802ba576a329bb6eda4ce12b8ab35639019549d494c9d149ecc59a72f cni.projectcalico.org/podIP:172.30.9.61/32 cni.projectcalico.org/podIPs:172.30.9.61/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb a2e437c4-6523-4f54-b1e7-4fa9508dbd0a 0xc003393a97 0xc003393a98}] []  [{kube-controller-manager Update v1 2021-11-16 19:30:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2e437c4-6523-4f54-b1e7-4fa9508dbd0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:30:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxsxg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxsxg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:30:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.61,StartTime:2021-11-16 19:30:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://eb09c9d409107d9628c9b444da4e8cc66a68166f8d05425c548a3b4c78f5b416,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:30:25.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9962" for this suite.

â€¢ [SLOW TEST:14.787 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":2,"skipped":33,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:30:25.230: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-8968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 19:30:25.506: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 19:31:25.620: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Nov 16 19:31:25.698: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 16 19:31:25.715: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 16 19:31:25.780: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 16 19:31:25.799: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 16 19:31:25.850: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 16 19:31:25.869: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:31:44.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8968" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:79.198 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":3,"skipped":81,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:31:44.433: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 19:31:44.720: INFO: Waiting up to 5m0s for pod "pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d" in namespace "emptydir-6430" to be "Succeeded or Failed"
Nov 16 19:31:44.736: INFO: Pod "pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.092417ms
Nov 16 19:31:46.754: INFO: Pod "pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d": Phase="Running", Reason="", readiness=true. Elapsed: 2.034291135s
Nov 16 19:31:48.772: INFO: Pod "pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051655578s
STEP: Saw pod success
Nov 16 19:31:48.772: INFO: Pod "pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d" satisfied condition "Succeeded or Failed"
Nov 16 19:31:48.786: INFO: Trying to get logs from node 10.193.87.27 pod pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d container test-container: <nil>
STEP: delete the pod
Nov 16 19:31:48.921: INFO: Waiting for pod pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d to disappear
Nov 16 19:31:48.935: INFO: Pod pod-5133cb82-bd35-4ab3-9799-1f3eded5e26d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:31:48.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6430" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":99,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:31:48.980: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 16 19:31:49.269: INFO: Waiting up to 5m0s for pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c" in namespace "downward-api-1840" to be "Succeeded or Failed"
Nov 16 19:31:49.281: INFO: Pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048982ms
Nov 16 19:31:51.322: INFO: Pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053199432s
Nov 16 19:31:53.347: INFO: Pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07796077s
Nov 16 19:31:55.418: INFO: Pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.149642167s
STEP: Saw pod success
Nov 16 19:31:55.418: INFO: Pod "downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c" satisfied condition "Succeeded or Failed"
Nov 16 19:31:55.462: INFO: Trying to get logs from node 10.193.87.27 pod downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c container dapi-container: <nil>
STEP: delete the pod
Nov 16 19:31:55.581: INFO: Waiting for pod downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c to disappear
Nov 16 19:31:55.595: INFO: Pod downward-api-a1fbf2c6-a171-4a1c-891e-e29d629ec63c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:31:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1840" for this suite.

â€¢ [SLOW TEST:6.653 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:31:55.646: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:31:56.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:31:58.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687916, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687916, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687916, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772687916, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:32:01.572: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:01.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1607" for this suite.
STEP: Destroying namespace "webhook-1607-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.427 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":6,"skipped":147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:02.073: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:32:02.409: INFO: The status of Pod pod-secrets-3ee23b40-be21-43e5-9147-6759c0d1fc6f is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:32:04.432: INFO: The status of Pod pod-secrets-3ee23b40-be21-43e5-9147-6759c0d1fc6f is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:32:06.453: INFO: The status of Pod pod-secrets-3ee23b40-be21-43e5-9147-6759c0d1fc6f is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:06.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4080" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":7,"skipped":176,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8576
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Nov 16 19:32:06.875: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:29.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8576" for this suite.

â€¢ [SLOW TEST:22.594 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":8,"skipped":176,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:29.217: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:35.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9745" for this suite.

â€¢ [SLOW TEST:6.510 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":9,"skipped":180,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:35.728: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3567
STEP: Creating secret with name secret-test-4d208d25-cb6e-429a-8aef-33ee3341812e
STEP: Creating a pod to test consume secrets
Nov 16 19:32:36.279: INFO: Waiting up to 5m0s for pod "pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b" in namespace "secrets-4844" to be "Succeeded or Failed"
Nov 16 19:32:36.296: INFO: Pod "pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.748029ms
Nov 16 19:32:38.353: INFO: Pod "pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073595861s
Nov 16 19:32:40.374: INFO: Pod "pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095319972s
STEP: Saw pod success
Nov 16 19:32:40.375: INFO: Pod "pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b" satisfied condition "Succeeded or Failed"
Nov 16 19:32:40.390: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:32:40.515: INFO: Waiting for pod pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b to disappear
Nov 16 19:32:40.529: INFO: Pod pod-secrets-f36cb3b5-d15a-44b8-85f9-02e91db7d62b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4844" for this suite.
STEP: Destroying namespace "secret-namespace-3567" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":192,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:40.601: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 16 19:32:40.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9945 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 16 19:32:41.199: INFO: stderr: ""
Nov 16 19:32:41.199: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Nov 16 19:32:46.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9945 get pod e2e-test-httpd-pod -o json'
Nov 16 19:32:46.382: INFO: stderr: ""
Nov 16 19:32:46.382: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"276d80c598ff94ca3a4840f9ac041ec8e0e0dc6f2c127383e9eb52e80a486acc\",\n            \"cni.projectcalico.org/podIP\": \"172.30.9.23/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.9.23/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-11-16T19:32:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9945\",\n        \"resourceVersion\": \"20626\",\n        \"uid\": \"48370562-931c-43c7-9227-b06108376ffd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fc72w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.193.87.27\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fc72w\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-16T19:32:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-16T19:32:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-16T19:32:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-11-16T19:32:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://477e5c1a994b3001891a4c47e37c3575c7ec5c32fe33d3f7e2771f1734f4748f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-11-16T19:32:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.193.87.27\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.9.23\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.9.23\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-11-16T19:32:41Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 16 19:32:46.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9945 replace -f -'
Nov 16 19:32:46.729: INFO: stderr: ""
Nov 16 19:32:46.729: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Nov 16 19:32:46.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9945 delete pods e2e-test-httpd-pod'
Nov 16 19:32:49.121: INFO: stderr: ""
Nov 16 19:32:49.121: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9945" for this suite.

â€¢ [SLOW TEST:8.570 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":11,"skipped":193,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:32:49.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e" in namespace "downward-api-2575" to be "Succeeded or Failed"
Nov 16 19:32:49.459: INFO: Pod "downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.873484ms
Nov 16 19:32:51.483: INFO: Pod "downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03964861s
Nov 16 19:32:53.505: INFO: Pod "downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061837337s
STEP: Saw pod success
Nov 16 19:32:53.505: INFO: Pod "downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e" satisfied condition "Succeeded or Failed"
Nov 16 19:32:53.520: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e container client-container: <nil>
STEP: delete the pod
Nov 16 19:32:53.596: INFO: Waiting for pod downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e to disappear
Nov 16 19:32:53.612: INFO: Pod downwardapi-volume-422ce02e-eb31-44fe-8eab-40b301f41c1e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:32:53.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2575" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":201,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:32:53.659: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5436
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1d94998e-1956-481d-82ca-ba306e87bd8b
STEP: Creating the pod
Nov 16 19:32:53.987: INFO: The status of Pod pod-projected-configmaps-bcbe887e-cf2d-456e-b61c-23c0dc863fa1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:32:56.010: INFO: The status of Pod pod-projected-configmaps-bcbe887e-cf2d-456e-b61c-23c0dc863fa1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:32:58.008: INFO: The status of Pod pod-projected-configmaps-bcbe887e-cf2d-456e-b61c-23c0dc863fa1 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-1d94998e-1956-481d-82ca-ba306e87bd8b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:34:27.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5436" for this suite.

â€¢ [SLOW TEST:93.999 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":213,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:34:27.667: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:36:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8042" for this suite.

â€¢ [SLOW TEST:94.387 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":14,"skipped":227,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:36:02.055: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-9283
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9283 to expose endpoints map[]
Nov 16 19:36:02.359: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Nov 16 19:36:03.395: INFO: successfully validated that service multi-endpoint-test in namespace services-9283 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9283
Nov 16 19:36:03.442: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:05.465: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:07.462: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:09.461: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9283 to expose endpoints map[pod1:[100]]
Nov 16 19:36:09.520: INFO: successfully validated that service multi-endpoint-test in namespace services-9283 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9283
Nov 16 19:36:09.561: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:11.585: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:13.595: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:36:15.582: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9283 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 16 19:36:15.663: INFO: successfully validated that service multi-endpoint-test in namespace services-9283 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Nov 16 19:36:15.663: INFO: Creating new exec pod
Nov 16 19:36:20.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9283 exec execpodt5dzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Nov 16 19:36:21.106: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Nov 16 19:36:21.106: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:36:21.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9283 exec execpodt5dzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.10 80'
Nov 16 19:36:21.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.10 80\nConnection to 172.21.57.10 80 port [tcp/http] succeeded!\n"
Nov 16 19:36:21.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:36:21.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9283 exec execpodt5dzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Nov 16 19:36:21.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Nov 16 19:36:21.785: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:36:21.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9283 exec execpodt5dzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.57.10 81'
Nov 16 19:36:22.096: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.57.10 81\nConnection to 172.21.57.10 81 port [tcp/*] succeeded!\n"
Nov 16 19:36:22.096: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9283
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9283 to expose endpoints map[pod2:[101]]
Nov 16 19:36:22.215: INFO: successfully validated that service multi-endpoint-test in namespace services-9283 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9283
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9283 to expose endpoints map[]
Nov 16 19:36:22.295: INFO: successfully validated that service multi-endpoint-test in namespace services-9283 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:36:22.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9283" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:20.350 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":15,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:36:22.406: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Nov 16 19:36:22.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 create -f -'
Nov 16 19:36:23.055: INFO: stderr: ""
Nov 16 19:36:23.055: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 19:36:23.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:23.201: INFO: stderr: ""
Nov 16 19:36:23.201: INFO: stdout: "update-demo-nautilus-6bkvw update-demo-nautilus-gz7dw "
Nov 16 19:36:23.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-6bkvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:23.319: INFO: stderr: ""
Nov 16 19:36:23.319: INFO: stdout: ""
Nov 16 19:36:23.319: INFO: update-demo-nautilus-6bkvw is created but not running
Nov 16 19:36:28.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:28.467: INFO: stderr: ""
Nov 16 19:36:28.467: INFO: stdout: "update-demo-nautilus-6bkvw update-demo-nautilus-gz7dw "
Nov 16 19:36:28.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-6bkvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:28.551: INFO: stderr: ""
Nov 16 19:36:28.551: INFO: stdout: ""
Nov 16 19:36:28.551: INFO: update-demo-nautilus-6bkvw is created but not running
Nov 16 19:36:33.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:33.709: INFO: stderr: ""
Nov 16 19:36:33.709: INFO: stdout: "update-demo-nautilus-6bkvw update-demo-nautilus-gz7dw "
Nov 16 19:36:33.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-6bkvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:33.834: INFO: stderr: ""
Nov 16 19:36:33.834: INFO: stdout: "true"
Nov 16 19:36:33.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-6bkvw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:33.982: INFO: stderr: ""
Nov 16 19:36:33.982: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:33.982: INFO: validating pod update-demo-nautilus-6bkvw
Nov 16 19:36:34.034: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:34.034: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:34.034: INFO: update-demo-nautilus-6bkvw is verified up and running
Nov 16 19:36:34.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:34.149: INFO: stderr: ""
Nov 16 19:36:34.149: INFO: stdout: "true"
Nov 16 19:36:34.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:34.259: INFO: stderr: ""
Nov 16 19:36:34.259: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:34.259: INFO: validating pod update-demo-nautilus-gz7dw
Nov 16 19:36:34.302: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:34.302: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:34.302: INFO: update-demo-nautilus-gz7dw is verified up and running
STEP: scaling down the replication controller
Nov 16 19:36:34.307: INFO: scanned /root for discovery docs: <nil>
Nov 16 19:36:34.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Nov 16 19:36:35.494: INFO: stderr: ""
Nov 16 19:36:35.494: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 19:36:35.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:35.613: INFO: stderr: ""
Nov 16 19:36:35.613: INFO: stdout: "update-demo-nautilus-6bkvw update-demo-nautilus-gz7dw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 16 19:36:40.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:40.741: INFO: stderr: ""
Nov 16 19:36:40.741: INFO: stdout: "update-demo-nautilus-gz7dw "
Nov 16 19:36:40.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:40.841: INFO: stderr: ""
Nov 16 19:36:40.841: INFO: stdout: "true"
Nov 16 19:36:40.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:40.945: INFO: stderr: ""
Nov 16 19:36:40.945: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:40.945: INFO: validating pod update-demo-nautilus-gz7dw
Nov 16 19:36:40.964: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:40.964: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:40.964: INFO: update-demo-nautilus-gz7dw is verified up and running
STEP: scaling up the replication controller
Nov 16 19:36:40.968: INFO: scanned /root for discovery docs: <nil>
Nov 16 19:36:40.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Nov 16 19:36:42.137: INFO: stderr: ""
Nov 16 19:36:42.137: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 19:36:42.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:42.240: INFO: stderr: ""
Nov 16 19:36:42.240: INFO: stdout: "update-demo-nautilus-gz7dw update-demo-nautilus-lfsx6 "
Nov 16 19:36:42.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:42.363: INFO: stderr: ""
Nov 16 19:36:42.363: INFO: stdout: "true"
Nov 16 19:36:42.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:42.484: INFO: stderr: ""
Nov 16 19:36:42.484: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:42.484: INFO: validating pod update-demo-nautilus-gz7dw
Nov 16 19:36:42.504: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:42.504: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:42.504: INFO: update-demo-nautilus-gz7dw is verified up and running
Nov 16 19:36:42.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-lfsx6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:42.603: INFO: stderr: ""
Nov 16 19:36:42.603: INFO: stdout: ""
Nov 16 19:36:42.603: INFO: update-demo-nautilus-lfsx6 is created but not running
Nov 16 19:36:47.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 19:36:47.708: INFO: stderr: ""
Nov 16 19:36:47.708: INFO: stdout: "update-demo-nautilus-gz7dw update-demo-nautilus-lfsx6 "
Nov 16 19:36:47.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:47.833: INFO: stderr: ""
Nov 16 19:36:47.834: INFO: stdout: "true"
Nov 16 19:36:47.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-gz7dw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:47.946: INFO: stderr: ""
Nov 16 19:36:47.946: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:47.946: INFO: validating pod update-demo-nautilus-gz7dw
Nov 16 19:36:47.966: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:47.966: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:47.966: INFO: update-demo-nautilus-gz7dw is verified up and running
Nov 16 19:36:47.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-lfsx6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 19:36:48.067: INFO: stderr: ""
Nov 16 19:36:48.067: INFO: stdout: "true"
Nov 16 19:36:48.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods update-demo-nautilus-lfsx6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 19:36:48.171: INFO: stderr: ""
Nov 16 19:36:48.171: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 19:36:48.171: INFO: validating pod update-demo-nautilus-lfsx6
Nov 16 19:36:48.216: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 19:36:48.216: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 19:36:48.216: INFO: update-demo-nautilus-lfsx6 is verified up and running
STEP: using delete to clean up resources
Nov 16 19:36:48.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 delete --grace-period=0 --force -f -'
Nov 16 19:36:48.344: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:36:48.344: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 19:36:48.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get rc,svc -l name=update-demo --no-headers'
Nov 16 19:36:48.447: INFO: stderr: "No resources found in kubectl-4219 namespace.\n"
Nov 16 19:36:48.447: INFO: stdout: ""
Nov 16 19:36:48.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4219 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 19:36:48.585: INFO: stderr: ""
Nov 16 19:36:48.585: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:36:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4219" for this suite.

â€¢ [SLOW TEST:26.229 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":16,"skipped":244,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:36:48.635: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 19:36:48.908: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 19:36:48.949: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 19:36:48.964: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.24 before test
Nov 16 19:36:48.996: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-11-16 18:07:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 19:36:48.996: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 19:36:48.996: INFO: calico-node-5cnhh from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:36:48.996: INFO: calico-typha-d5b48569-hcfb8 from kube-system started at 2021-11-16 18:05:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:36:48.996: INFO: coredns-b58d5f584-g92hx from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container coredns ready: true, restart count 0
Nov 16 19:36:48.996: INFO: ibm-keepalived-watcher-bl7zr from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:36:48.996: INFO: ibm-master-proxy-static-10.193.87.24 from kube-system started at 2021-11-16 18:05:01 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:36:48.996: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:36:48.996: INFO: konnectivity-agent-wtjm7 from kube-system started at 2021-11-16 18:13:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 19:36:48.996: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9 from kube-system started at 2021-11-16 18:08:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 19:36:48.996: INFO: update-demo-nautilus-gz7dw from kubectl-4219 started at 2021-11-16 19:36:23 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container update-demo ready: true, restart count 0
Nov 16 19:36:48.996: INFO: sonobuoy-e2e-job-54c8cae784204424 from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container e2e ready: true, restart count 0
Nov 16 19:36:48.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:36:48.996: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:48.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:36:48.996: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 19:36:48.996: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.27 before test
Nov 16 19:36:49.069: INFO: calico-node-smn5q from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:36:49.069: INFO: calico-typha-d5b48569-c7zlf from kube-system started at 2021-11-16 18:05:43 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:36:49.069: INFO: coredns-b58d5f584-qhb8l from kube-system started at 2021-11-16 18:13:58 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container coredns ready: true, restart count 0
Nov 16 19:36:49.069: INFO: ibm-keepalived-watcher-rb6gd from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:36:49.069: INFO: ibm-master-proxy-static-10.193.87.27 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:36:49.069: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:36:49.069: INFO: konnectivity-agent-ln55x from kube-system started at 2021-11-16 18:13:25 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 19:36:49.069: INFO: metrics-server-64bbdfc744-gc97k from kube-system started at 2021-11-16 18:06:37 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container metrics-server ready: true, restart count 0
Nov 16 19:36:49.069: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 16 19:36:49.069: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-mr4bt from kube-system started at 2021-11-16 18:08:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 19:36:49.069: INFO: update-demo-nautilus-lfsx6 from kubectl-4219 started at 2021-11-16 19:36:41 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container update-demo ready: true, restart count 0
Nov 16 19:36:49.069: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j22wg from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:49.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:36:49.069: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 19:36:49.069: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.28 before test
Nov 16 19:36:49.100: INFO: catalog-operator-7489d5857-vlxrz from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 19:36:49.100: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-sf6d4 from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 19:36:49.100: INFO: olm-operator-7b6cd6c94c-5vgk4 from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 19:36:49.100: INFO: calico-kube-controllers-75488ccc5b-pf6m8 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 19:36:49.100: INFO: calico-node-sx675 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 19:36:49.100: INFO: calico-typha-d5b48569-s7xpk from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 19:36:49.100: INFO: coredns-autoscaler-689fb74d49-ww9hg from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container autoscaler ready: true, restart count 0
Nov 16 19:36:49.100: INFO: coredns-b58d5f584-ntqv9 from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container coredns ready: true, restart count 0
Nov 16 19:36:49.100: INFO: dashboard-metrics-scraper-6747f89c97-pzthq from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 16 19:36:49.100: INFO: ibm-file-plugin-fd44cd466-zjcs4 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 19:36:49.100: INFO: ibm-keepalived-watcher-mfkdl from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.100: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 19:36:49.101: INFO: ibm-master-proxy-static-10.193.87.28 from kube-system started at 2021-11-16 18:04:49 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 19:36:49.101: INFO: 	Container pause ready: true, restart count 0
Nov 16 19:36:49.101: INFO: ibm-storage-watcher-765888f8c9-dffqn from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 19:36:49.101: INFO: konnectivity-agent-g2pw7 from kube-system started at 2021-11-16 18:13:28 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 19:36:49.101: INFO: kubernetes-dashboard-54c47dd995-czt2b from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 16 19:36:49.101: INFO: sonobuoy from sonobuoy started at 2021-11-16 19:29:33 +0000 UTC (1 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 19:36:49.101: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-hpqln from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 19:36:49.101: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 19:36:49.101: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-63f6fa8a-817b-42b3-87c1-4906800271f6 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-63f6fa8a-817b-42b3-87c1-4906800271f6 off the node 10.193.87.28
STEP: verifying the node doesn't have the label kubernetes.io/e2e-63f6fa8a-817b-42b3-87c1-4906800271f6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:36:57.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5482" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:8.803 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":17,"skipped":264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:36:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 19:36:57.728: INFO: Waiting up to 5m0s for pod "pod-9ba803f3-1e42-435f-a806-27febaa66534" in namespace "emptydir-739" to be "Succeeded or Failed"
Nov 16 19:36:57.744: INFO: Pod "pod-9ba803f3-1e42-435f-a806-27febaa66534": Phase="Pending", Reason="", readiness=false. Elapsed: 16.630913ms
Nov 16 19:36:59.765: INFO: Pod "pod-9ba803f3-1e42-435f-a806-27febaa66534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036990098s
Nov 16 19:37:01.788: INFO: Pod "pod-9ba803f3-1e42-435f-a806-27febaa66534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060175669s
STEP: Saw pod success
Nov 16 19:37:01.788: INFO: Pod "pod-9ba803f3-1e42-435f-a806-27febaa66534" satisfied condition "Succeeded or Failed"
Nov 16 19:37:01.804: INFO: Trying to get logs from node 10.193.87.27 pod pod-9ba803f3-1e42-435f-a806-27febaa66534 container test-container: <nil>
STEP: delete the pod
Nov 16 19:37:01.952: INFO: Waiting for pod pod-9ba803f3-1e42-435f-a806-27febaa66534 to disappear
Nov 16 19:37:01.967: INFO: Pod pod-9ba803f3-1e42-435f-a806-27febaa66534 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:01.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-739" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":293,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:02.019: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-rx5w
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 19:37:02.347: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rx5w" in namespace "subpath-3360" to be "Succeeded or Failed"
Nov 16 19:37:02.364: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 16.34812ms
Nov 16 19:37:04.386: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038170564s
Nov 16 19:37:06.409: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.061630487s
Nov 16 19:37:08.433: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 6.08543102s
Nov 16 19:37:10.455: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 8.106817744s
Nov 16 19:37:12.477: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 10.129431247s
Nov 16 19:37:14.498: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 12.15076819s
Nov 16 19:37:16.520: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 14.172293864s
Nov 16 19:37:18.555: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 16.207106267s
Nov 16 19:37:20.576: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 18.227997499s
Nov 16 19:37:22.596: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 20.248732781s
Nov 16 19:37:24.620: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Running", Reason="", readiness=true. Elapsed: 22.272386941s
Nov 16 19:37:26.641: INFO: Pod "pod-subpath-test-projected-rx5w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.293009643s
STEP: Saw pod success
Nov 16 19:37:26.641: INFO: Pod "pod-subpath-test-projected-rx5w" satisfied condition "Succeeded or Failed"
Nov 16 19:37:26.655: INFO: Trying to get logs from node 10.193.87.27 pod pod-subpath-test-projected-rx5w container test-container-subpath-projected-rx5w: <nil>
STEP: delete the pod
Nov 16 19:37:26.739: INFO: Waiting for pod pod-subpath-test-projected-rx5w to disappear
Nov 16 19:37:26.755: INFO: Pod pod-subpath-test-projected-rx5w no longer exists
STEP: Deleting pod pod-subpath-test-projected-rx5w
Nov 16 19:37:26.755: INFO: Deleting pod "pod-subpath-test-projected-rx5w" in namespace "subpath-3360"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:26.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3360" for this suite.

â€¢ [SLOW TEST:24.802 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":19,"skipped":298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-6543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 19:37:27.125: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 19:37:27.147: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 19:37:27.214: INFO: waiting for watch events with expected annotations
Nov 16 19:37:27.214: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:27.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6543" for this suite.
â€¢{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":20,"skipped":324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:27.444: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5052
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 16 19:37:27.739: INFO: Waiting up to 5m0s for pod "pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c" in namespace "emptydir-5052" to be "Succeeded or Failed"
Nov 16 19:37:27.760: INFO: Pod "pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.859271ms
Nov 16 19:37:29.781: INFO: Pod "pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042092633s
Nov 16 19:37:31.805: INFO: Pod "pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065620459s
STEP: Saw pod success
Nov 16 19:37:31.805: INFO: Pod "pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c" satisfied condition "Succeeded or Failed"
Nov 16 19:37:31.821: INFO: Trying to get logs from node 10.193.87.27 pod pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c container test-container: <nil>
STEP: delete the pod
Nov 16 19:37:31.899: INFO: Waiting for pod pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c to disappear
Nov 16 19:37:31.914: INFO: Pod pod-9e547dbc-f3e6-4f82-838d-5a9520edaa2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:31.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5052" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":369,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:31.965: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6495" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":22,"skipped":373,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:32.300: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 16 19:37:42.871: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1116 19:37:42.871418      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 19:37:42.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fjwl" in namespace "gc-9770"
Nov 16 19:37:42.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fshx" in namespace "gc-9770"
Nov 16 19:37:42.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vplv" in namespace "gc-9770"
Nov 16 19:37:42.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tltm" in namespace "gc-9770"
Nov 16 19:37:43.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-7knjp" in namespace "gc-9770"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:37:43.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9770" for this suite.

â€¢ [SLOW TEST:10.810 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":23,"skipped":381,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:37:43.110: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9239
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-88195be5-4930-4255-b6b7-e0edbb55ea4c
STEP: Creating configMap with name cm-test-opt-upd-ad18ee87-bd42-4519-8fae-a85311bb6d03
STEP: Creating the pod
Nov 16 19:37:43.517: INFO: The status of Pod pod-configmaps-5f48d4cc-1bb0-4b0d-aa3a-de48a641a318 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:37:45.536: INFO: The status of Pod pod-configmaps-5f48d4cc-1bb0-4b0d-aa3a-de48a641a318 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:37:47.539: INFO: The status of Pod pod-configmaps-5f48d4cc-1bb0-4b0d-aa3a-de48a641a318 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-88195be5-4930-4255-b6b7-e0edbb55ea4c
STEP: Updating configmap cm-test-opt-upd-ad18ee87-bd42-4519-8fae-a85311bb6d03
STEP: Creating configMap with name cm-test-opt-create-7bba78c6-9133-4f62-9f12-836171753f46
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:39:17.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9239" for this suite.

â€¢ [SLOW TEST:94.480 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:39:17.591: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-8895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Nov 16 19:39:19.955: INFO: pods: 0 < 3
Nov 16 19:39:21.976: INFO: running pods: 1 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Nov 16 19:39:24.224: INFO: running pods: 2 < 3
Nov 16 19:39:26.251: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:39:28.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8895" for this suite.

â€¢ [SLOW TEST:10.843 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":25,"skipped":442,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:39:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3510 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3510;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3510 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3510;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3510.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3510.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3510.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3510.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3510.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3510.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3510.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 80.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.80_udp@PTR;check="$$(dig +tcp +noall +answer +search 80.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.80_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3510 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3510;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3510 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3510;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3510.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3510.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3510.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3510.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3510.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3510.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3510.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3510.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3510.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 80.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.80_udp@PTR;check="$$(dig +tcp +noall +answer +search 80.109.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.109.80_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:39:40.881: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.900: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.921: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.941: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.961: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.980: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:40.999: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.020: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.173: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.191: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.226: INFO: Unable to read jessie_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.245: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.265: INFO: Unable to read jessie_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.284: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.302: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.322: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:41.437: INFO: Lookups using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3510 wheezy_tcp@dns-test-service.dns-3510 wheezy_udp@dns-test-service.dns-3510.svc wheezy_tcp@dns-test-service.dns-3510.svc wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3510 jessie_tcp@dns-test-service.dns-3510 jessie_udp@dns-test-service.dns-3510.svc jessie_tcp@dns-test-service.dns-3510.svc jessie_udp@_http._tcp.dns-test-service.dns-3510.svc jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc]

Nov 16 19:39:46.461: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.481: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.502: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.522: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.542: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.561: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.579: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.600: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.732: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.752: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.772: INFO: Unable to read jessie_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.790: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.808: INFO: Unable to read jessie_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.845: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:46.985: INFO: Lookups using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3510 wheezy_tcp@dns-test-service.dns-3510 wheezy_udp@dns-test-service.dns-3510.svc wheezy_tcp@dns-test-service.dns-3510.svc wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3510 jessie_tcp@dns-test-service.dns-3510 jessie_udp@dns-test-service.dns-3510.svc jessie_tcp@dns-test-service.dns-3510.svc jessie_udp@_http._tcp.dns-test-service.dns-3510.svc jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc]

Nov 16 19:39:51.459: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.501: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.520: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.580: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.601: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.734: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.773: INFO: Unable to read jessie_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.791: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.808: INFO: Unable to read jessie_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.825: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.844: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:51.976: INFO: Lookups using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3510 wheezy_tcp@dns-test-service.dns-3510 wheezy_udp@dns-test-service.dns-3510.svc wheezy_tcp@dns-test-service.dns-3510.svc wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3510 jessie_tcp@dns-test-service.dns-3510 jessie_udp@dns-test-service.dns-3510.svc jessie_tcp@dns-test-service.dns-3510.svc jessie_udp@_http._tcp.dns-test-service.dns-3510.svc jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc]

Nov 16 19:39:56.479: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.498: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.517: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.605: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.624: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.758: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.778: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.796: INFO: Unable to read jessie_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.835: INFO: Unable to read jessie_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.877: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:56.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:39:57.015: INFO: Lookups using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3510 wheezy_tcp@dns-test-service.dns-3510 wheezy_udp@dns-test-service.dns-3510.svc wheezy_tcp@dns-test-service.dns-3510.svc wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3510 jessie_tcp@dns-test-service.dns-3510 jessie_udp@dns-test-service.dns-3510.svc jessie_tcp@dns-test-service.dns-3510.svc jessie_udp@_http._tcp.dns-test-service.dns-3510.svc jessie_tcp@_http._tcp.dns-test-service.dns-3510.svc]

Nov 16 19:40:01.459: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.478: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.519: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510 from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.537: INFO: Unable to read wheezy_udp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.556: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.577: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.600: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc from pod dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15: the server could not find the requested resource (get pods dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15)
Nov 16 19:40:01.995: INFO: Lookups using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3510 wheezy_tcp@dns-test-service.dns-3510 wheezy_udp@dns-test-service.dns-3510.svc wheezy_tcp@dns-test-service.dns-3510.svc wheezy_udp@_http._tcp.dns-test-service.dns-3510.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3510.svc]

Nov 16 19:40:07.016: INFO: DNS probes using dns-3510/dns-test-50ac1cb9-06d2-482b-8790-d27d0b65cc15 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:07.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3510" for this suite.

â€¢ [SLOW TEST:38.815 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":26,"skipped":462,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:07.251: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:40:07.490: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 19:40:07.536: INFO: The status of Pod pod-exec-websocket-e607423f-1bfa-4296-8dc5-33579e0ec711 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:40:09.558: INFO: The status of Pod pod-exec-websocket-e607423f-1bfa-4296-8dc5-33579e0ec711 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:40:11.559: INFO: The status of Pod pod-exec-websocket-e607423f-1bfa-4296-8dc5-33579e0ec711 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:11.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2972" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":473,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:11.791: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 19:40:12.537: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 19:40:12.662: INFO: waiting for watch events with expected annotations
Nov 16 19:40:12.662: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:12.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9823" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":28,"skipped":522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:12.953: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Nov 16 19:40:13.271: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:13.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7962" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":29,"skipped":566,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:13.425: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Nov 16 19:40:13.772: INFO: running pods: 0 < 3
Nov 16 19:40:15.794: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:17.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3945" for this suite.
â€¢{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":30,"skipped":575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:17.857: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9716
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:40:18.154: INFO: Found 0 stateful pods, waiting for 1
Nov 16 19:40:28.187: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Nov 16 19:40:28.271: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:40:28.271: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Nov 16 19:40:38.310: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:40:38.310: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 19:40:38.401: INFO: Deleting all statefulset in ns statefulset-9716
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:38.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9716" for this suite.

â€¢ [SLOW TEST:20.649 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":31,"skipped":597,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:38.506: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8027
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8027
I1116 19:40:38.895826      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8027, replica count: 2
I1116 19:40:41.947240      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 19:40:41.947: INFO: Creating new exec pod
Nov 16 19:40:47.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 16 19:40:47.370: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 19:40:47.370: INFO: stdout: ""
Nov 16 19:40:48.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 16 19:40:48.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 19:40:48.715: INFO: stdout: "externalname-service-j5pb7"
Nov 16 19:40:48.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.153.246 80'
Nov 16 19:40:49.098: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 172.21.153.246 80\nConnection to 172.21.153.246 80 port [tcp/http] succeeded!\n"
Nov 16 19:40:49.098: INFO: stdout: "externalname-service-8tb2l"
Nov 16 19:40:49.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31308'
Nov 16 19:40:49.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 31308\nConnection to 10.193.87.28 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:49.432: INFO: stdout: ""
Nov 16 19:40:50.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31308'
Nov 16 19:40:50.769: INFO: stderr: "+ nc -v -t -w 2 10.193.87.28 31308\n+ echo hostName\nConnection to 10.193.87.28 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:50.769: INFO: stdout: ""
Nov 16 19:40:51.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31308'
Nov 16 19:40:51.738: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 31308\nConnection to 10.193.87.28 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:51.739: INFO: stdout: ""
Nov 16 19:40:52.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31308'
Nov 16 19:40:52.739: INFO: stderr: "+ nc -v -t -w 2 10.193.87.28 31308\n+ echo hostName\nConnection to 10.193.87.28 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:52.739: INFO: stdout: ""
Nov 16 19:40:53.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31308'
Nov 16 19:40:53.819: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 31308\nConnection to 10.193.87.28 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:53.819: INFO: stdout: "externalname-service-j5pb7"
Nov 16 19:40:53.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8027 exec execpodbl5cq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.24 31308'
Nov 16 19:40:54.133: INFO: stderr: "+ + nc -vecho -t -w hostName 2\n 10.193.87.24 31308\nConnection to 10.193.87.24 31308 port [tcp/*] succeeded!\n"
Nov 16 19:40:54.133: INFO: stdout: "externalname-service-8tb2l"
Nov 16 19:40:54.133: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:54.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8027" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:15.758 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":32,"skipped":597,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:54.265: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:40:54.552: INFO: The status of Pod busybox-readonly-fs5c7abc04-f1d1-410c-a364-f9bf223f59fe is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:40:56.572: INFO: The status of Pod busybox-readonly-fs5c7abc04-f1d1-410c-a364-f9bf223f59fe is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:40:56.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3268" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":617,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:40:56.717: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e4a6663a-62f6-4e18-89c5-a68066fcc0e2
STEP: Creating a pod to test consume configMaps
Nov 16 19:40:57.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344" in namespace "configmap-9428" to be "Succeeded or Failed"
Nov 16 19:40:57.036: INFO: Pod "pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344": Phase="Pending", Reason="", readiness=false. Elapsed: 18.632424ms
Nov 16 19:40:59.062: INFO: Pod "pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045005019s
Nov 16 19:41:01.086: INFO: Pod "pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068316816s
STEP: Saw pod success
Nov 16 19:41:01.086: INFO: Pod "pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344" satisfied condition "Succeeded or Failed"
Nov 16 19:41:01.111: INFO: Trying to get logs from node 10.193.87.28 pod pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:41:01.281: INFO: Waiting for pod pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344 to disappear
Nov 16 19:41:01.294: INFO: Pod pod-configmaps-73d903dc-abd9-406e-99ea-2d6bf4101344 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:41:01.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9428" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:41:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5190
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-5190
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5190
Nov 16 19:41:01.710: INFO: Found 0 stateful pods, waiting for 1
Nov 16 19:41:11.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 16 19:41:11.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:41:12.082: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:41:12.082: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:41:12.082: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:41:12.099: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 19:41:22.142: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:41:22.142: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:41:22.239: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 16 19:41:22.239: INFO: ss-0  10.193.87.28  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  }]
Nov 16 19:41:22.239: INFO: ss-1                Pending         []
Nov 16 19:41:22.239: INFO: 
Nov 16 19:41:22.239: INFO: StatefulSet ss has not reached scale 3, at 2
Nov 16 19:41:23.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982878114s
Nov 16 19:41:24.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956146522s
Nov 16 19:41:25.307: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935706569s
Nov 16 19:41:26.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.912572352s
Nov 16 19:41:27.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.889837695s
Nov 16 19:41:28.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.869017711s
Nov 16 19:41:29.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.848877023s
Nov 16 19:41:30.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.828188558s
Nov 16 19:41:31.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 808.891157ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5190
Nov 16 19:41:32.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:41:32.813: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:41:32.813: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:41:32.813: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:41:32.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:41:33.356: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 19:41:33.356: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:41:33.356: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:41:33.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:41:33.716: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 16 19:41:33.716: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:41:33.716: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:41:33.733: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Nov 16 19:41:43.763: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:41:43.763: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:41:43.763: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 16 19:41:43.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:41:44.125: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:41:44.125: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:41:44.125: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:41:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:41:44.413: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:41:44.413: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:41:44.413: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:41:44.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-5190 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:41:44.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:41:44.737: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:41:44.737: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:41:44.737: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:41:44.755: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Nov 16 19:41:54.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:41:54.792: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:41:54.792: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 19:41:54.844: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 16 19:41:54.844: INFO: ss-0  10.193.87.28  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  }]
Nov 16 19:41:54.844: INFO: ss-1  10.193.87.27  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  }]
Nov 16 19:41:54.845: INFO: ss-2  10.193.87.24  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  }]
Nov 16 19:41:54.845: INFO: 
Nov 16 19:41:54.845: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 19:41:55.868: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Nov 16 19:41:55.868: INFO: ss-0  10.193.87.28  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:01 +0000 UTC  }]
Nov 16 19:41:55.868: INFO: ss-1  10.193.87.27  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  }]
Nov 16 19:41:55.868: INFO: ss-2  10.193.87.24  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 19:41:22 +0000 UTC  }]
Nov 16 19:41:55.868: INFO: 
Nov 16 19:41:55.868: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 16 19:41:56.888: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.956996776s
Nov 16 19:41:57.907: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.937898069s
Nov 16 19:41:58.946: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.901156976s
Nov 16 19:41:59.966: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.879618675s
Nov 16 19:42:00.987: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.860436056s
Nov 16 19:42:02.007: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.838755956s
Nov 16 19:42:03.025: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.819386036s
Nov 16 19:42:04.045: INFO: Verifying statefulset ss doesn't scale past 0 for another 801.0607ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5190
Nov 16 19:42:05.064: INFO: Scaling statefulset ss to 0
Nov 16 19:42:05.114: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 19:42:05.129: INFO: Deleting all statefulset in ns statefulset-5190
Nov 16 19:42:05.142: INFO: Scaling statefulset ss to 0
Nov 16 19:42:05.185: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:42:05.197: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:05.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5190" for this suite.

â€¢ [SLOW TEST:63.914 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":35,"skipped":660,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:22.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-295" for this suite.

â€¢ [SLOW TEST:16.752 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":36,"skipped":666,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:22.053: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1584
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 16 19:42:22.320: INFO: Waiting up to 5m0s for pod "pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d" in namespace "emptydir-1584" to be "Succeeded or Failed"
Nov 16 19:42:22.335: INFO: Pod "pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.396365ms
Nov 16 19:42:24.357: INFO: Pod "pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036566457s
Nov 16 19:42:26.380: INFO: Pod "pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059480546s
STEP: Saw pod success
Nov 16 19:42:26.380: INFO: Pod "pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d" satisfied condition "Succeeded or Failed"
Nov 16 19:42:26.395: INFO: Trying to get logs from node 10.193.87.27 pod pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d container test-container: <nil>
STEP: delete the pod
Nov 16 19:42:26.483: INFO: Waiting for pod pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d to disappear
Nov 16 19:42:26.499: INFO: Pod pod-3586d37d-b6bb-466e-8c19-0e64a2933a3d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:26.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1584" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":678,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6478.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6478.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6478.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6478.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6478.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6478.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:42:31.095: INFO: DNS probes using dns-6478/dns-test-ddd726f4-1e08-4232-af9e-2624ebcefc77 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:31.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6478" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":38,"skipped":680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1116 19:42:32.675850      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 19:42:32.675: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:32.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1140" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":39,"skipped":707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:32.724: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Nov 16 19:42:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 create -f -'
Nov 16 19:42:33.248: INFO: stderr: ""
Nov 16 19:42:33.248: INFO: stdout: "pod/pause created\n"
Nov 16 19:42:33.248: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 16 19:42:33.248: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1510" to be "running and ready"
Nov 16 19:42:33.265: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.197821ms
Nov 16 19:42:35.289: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040923866s
Nov 16 19:42:37.309: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.061430849s
Nov 16 19:42:37.309: INFO: Pod "pause" satisfied condition "running and ready"
Nov 16 19:42:37.309: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 16 19:42:37.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 label pods pause testing-label=testing-label-value'
Nov 16 19:42:37.437: INFO: stderr: ""
Nov 16 19:42:37.437: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 16 19:42:37.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 get pod pause -L testing-label'
Nov 16 19:42:37.535: INFO: stderr: ""
Nov 16 19:42:37.535: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 16 19:42:37.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 label pods pause testing-label-'
Nov 16 19:42:37.676: INFO: stderr: ""
Nov 16 19:42:37.676: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 16 19:42:37.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 get pod pause -L testing-label'
Nov 16 19:42:37.766: INFO: stderr: ""
Nov 16 19:42:37.766: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Nov 16 19:42:37.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 delete --grace-period=0 --force -f -'
Nov 16 19:42:37.919: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 19:42:37.919: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 16 19:42:37.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 get rc,svc -l name=pause --no-headers'
Nov 16 19:42:38.039: INFO: stderr: "No resources found in kubectl-1510 namespace.\n"
Nov 16 19:42:38.039: INFO: stdout: ""
Nov 16 19:42:38.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1510 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 19:42:38.135: INFO: stderr: ""
Nov 16 19:42:38.135: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1510" for this suite.

â€¢ [SLOW TEST:5.456 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":40,"skipped":825,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:38.180: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1116 19:42:48.562778      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 19:42:48.562: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:48.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2437" for this suite.

â€¢ [SLOW TEST:10.436 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":41,"skipped":838,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:48.622: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 19:42:49.000: INFO: Number of nodes with available pods: 0
Nov 16 19:42:49.000: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 19:42:50.041: INFO: Number of nodes with available pods: 0
Nov 16 19:42:50.041: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 19:42:51.042: INFO: Number of nodes with available pods: 1
Nov 16 19:42:51.042: INFO: Node 10.193.87.27 is running more than one daemon pod
Nov 16 19:42:52.041: INFO: Number of nodes with available pods: 3
Nov 16 19:42:52.041: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 16 19:42:52.130: INFO: Number of nodes with available pods: 2
Nov 16 19:42:52.130: INFO: Node 10.193.87.27 is running more than one daemon pod
Nov 16 19:42:53.169: INFO: Number of nodes with available pods: 2
Nov 16 19:42:53.169: INFO: Node 10.193.87.27 is running more than one daemon pod
Nov 16 19:42:54.169: INFO: Number of nodes with available pods: 3
Nov 16 19:42:54.169: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8890, will wait for the garbage collector to delete the pods
Nov 16 19:42:54.279: INFO: Deleting DaemonSet.extensions daemon-set took: 19.942056ms
Nov 16 19:42:54.380: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.253261ms
Nov 16 19:42:57.203: INFO: Number of nodes with available pods: 0
Nov 16 19:42:57.203: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 19:42:57.217: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23932"},"items":null}

Nov 16 19:42:57.232: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23932"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:42:57.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8890" for this suite.

â€¢ [SLOW TEST:8.724 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":42,"skipped":853,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:42:57.345: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Nov 16 19:42:57.629: INFO: The status of Pod pod-hostip-6d72e0cb-62bc-4414-ba79-1bbc3ab31c82 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:42:59.652: INFO: The status of Pod pod-hostip-6d72e0cb-62bc-4414-ba79-1bbc3ab31c82 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:43:01.653: INFO: The status of Pod pod-hostip-6d72e0cb-62bc-4414-ba79-1bbc3ab31c82 is Running (Ready = true)
Nov 16 19:43:01.683: INFO: Pod pod-hostip-6d72e0cb-62bc-4414-ba79-1bbc3ab31c82 has hostIP: 10.193.87.27
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6379" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":856,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:01.737: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5939
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 19:43:01.996: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 19:43:02.152: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:43:04.174: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:43:06.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:08.172: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:10.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:12.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:14.171: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:16.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:18.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:20.173: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:43:22.177: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 19:43:22.206: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 16 19:43:24.225: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 19:43:24.256: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 19:43:26.343: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 19:43:26.343: INFO: Breadth first check of 172.30.148.184 on host 10.193.87.24...
Nov 16 19:43:26.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.50:9080/dial?request=hostname&protocol=http&host=172.30.148.184&port=8083&tries=1'] Namespace:pod-network-test-5939 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:43:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:43:26.599: INFO: Waiting for responses: map[]
Nov 16 19:43:26.599: INFO: reached 172.30.148.184 after 0/1 tries
Nov 16 19:43:26.599: INFO: Breadth first check of 172.30.9.55 on host 10.193.87.27...
Nov 16 19:43:26.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.50:9080/dial?request=hostname&protocol=http&host=172.30.9.55&port=8083&tries=1'] Namespace:pod-network-test-5939 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:43:26.616: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:43:26.874: INFO: Waiting for responses: map[]
Nov 16 19:43:26.874: INFO: reached 172.30.9.55 after 0/1 tries
Nov 16 19:43:26.874: INFO: Breadth first check of 172.30.184.253 on host 10.193.87.28...
Nov 16 19:43:26.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.50:9080/dial?request=hostname&protocol=http&host=172.30.184.253&port=8083&tries=1'] Namespace:pod-network-test-5939 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:43:26.894: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:43:27.139: INFO: Waiting for responses: map[]
Nov 16 19:43:27.139: INFO: reached 172.30.184.253 after 0/1 tries
Nov 16 19:43:27.139: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:27.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5939" for this suite.

â€¢ [SLOW TEST:25.452 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":868,"failed":0}
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:27.190: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 16 19:43:31.539: INFO: &Pod{ObjectMeta:{send-events-4dfe3ce2-f40a-454e-8faa-880daf334cd7  events-2919  0bab1e2e-e09a-4aeb-b65e-7ff199714919 24122 0 2021-11-16 19:43:27 +0000 UTC <nil> <nil> map[name:foo time:440830275] map[cni.projectcalico.org/containerID:d05cf6fa074e600feb006e4b57336f54273c86c15e2f9b53f54f44c5d27cba4a cni.projectcalico.org/podIP:172.30.184.254/32 cni.projectcalico.org/podIPs:172.30.184.254/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-11-16 19:43:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:43:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:43:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.184.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8kctm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8kctm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:43:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:43:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:43:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:43:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:172.30.184.254,StartTime:2021-11-16 19:43:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:43:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://842fc6ae48a2105e83accf9a01671163d8cf562771869a79e8cff696236533e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.184.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Nov 16 19:43:33.594: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 16 19:43:35.615: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:35.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2919" for this suite.

â€¢ [SLOW TEST:8.498 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":45,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:35.689: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2c5d06d8-c571-443b-8248-062f8f875021
STEP: Creating a pod to test consume secrets
Nov 16 19:43:36.040: INFO: Waiting up to 5m0s for pod "pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6" in namespace "secrets-2836" to be "Succeeded or Failed"
Nov 16 19:43:36.081: INFO: Pod "pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6": Phase="Pending", Reason="", readiness=false. Elapsed: 40.48675ms
Nov 16 19:43:38.103: INFO: Pod "pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062444743s
Nov 16 19:43:40.123: INFO: Pod "pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083267758s
STEP: Saw pod success
Nov 16 19:43:40.123: INFO: Pod "pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6" satisfied condition "Succeeded or Failed"
Nov 16 19:43:40.138: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:43:40.204: INFO: Waiting for pod pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6 to disappear
Nov 16 19:43:40.219: INFO: Pod pod-secrets-a626821f-58d9-4eb2-868e-065845d4eaf6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:40.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2836" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":925,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Nov 16 19:43:40.519: INFO: created test-event-1
Nov 16 19:43:40.537: INFO: created test-event-2
Nov 16 19:43:40.554: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Nov 16 19:43:40.568: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Nov 16 19:43:40.688: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:40.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7789" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":47,"skipped":941,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:40.748: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:43:41.351: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Nov 16 19:43:43.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688621, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688621, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688621, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688621, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:43:46.466: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:43:46.484: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:50.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5153" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:9.617 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":48,"skipped":959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-32
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-14cd33f8-3e32-45cc-8f7f-12ef59e77da5
STEP: Creating secret with name s-test-opt-upd-f4195302-c381-42ff-bf2b-d5144da339b7
STEP: Creating the pod
Nov 16 19:43:50.751: INFO: The status of Pod pod-secrets-a4b0fbd5-88c6-4162-829e-81e3f216aea2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:43:52.783: INFO: The status of Pod pod-secrets-a4b0fbd5-88c6-4162-829e-81e3f216aea2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:43:54.771: INFO: The status of Pod pod-secrets-a4b0fbd5-88c6-4162-829e-81e3f216aea2 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-14cd33f8-3e32-45cc-8f7f-12ef59e77da5
STEP: Updating secret s-test-opt-upd-f4195302-c381-42ff-bf2b-d5144da339b7
STEP: Creating secret with name s-test-opt-create-7ff0e4c2-a6c0-407b-8600-fd9c08324655
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:43:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-32" for this suite.

â€¢ [SLOW TEST:8.782 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":1003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:43:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9003
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:43:59.453: INFO: created pod
Nov 16 19:43:59.453: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9003" to be "Succeeded or Failed"
Nov 16 19:43:59.468: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.579021ms
Nov 16 19:44:01.493: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039343186s
Nov 16 19:44:03.513: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05973719s
STEP: Saw pod success
Nov 16 19:44:03.513: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Nov 16 19:44:33.514: INFO: polling logs
Nov 16 19:44:33.624: INFO: Pod logs: 
2021/11/16 19:44:01 OK: Got token
2021/11/16 19:44:01 validating with in-cluster discovery
2021/11/16 19:44:01 OK: got issuer https://kubernetes.default.svc
2021/11/16 19:44:01 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9003:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1637092439, NotBefore:1637091839, IssuedAt:1637091839, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9003", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7892d4e-96b3-41df-a822-b9c23a9f2567"}}}
2021/11/16 19:44:01 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2021/11/16 19:44:01 OK: Validated signature on JWT
2021/11/16 19:44:01 OK: Got valid claims from token!
2021/11/16 19:44:01 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9003:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1637092439, NotBefore:1637091839, IssuedAt:1637091839, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9003", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7892d4e-96b3-41df-a822-b9c23a9f2567"}}}

Nov 16 19:44:33.624: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:33.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9003" for this suite.

â€¢ [SLOW TEST:34.552 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":50,"skipped":1028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:33.716: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4e0ccf4e-a665-4e3b-8a3d-2befcb5a138d
STEP: Creating a pod to test consume configMaps
Nov 16 19:44:34.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f" in namespace "configmap-9803" to be "Succeeded or Failed"
Nov 16 19:44:34.028: INFO: Pod "pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.687934ms
Nov 16 19:44:36.047: INFO: Pod "pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033838633s
Nov 16 19:44:38.068: INFO: Pod "pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054781154s
STEP: Saw pod success
Nov 16 19:44:38.068: INFO: Pod "pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f" satisfied condition "Succeeded or Failed"
Nov 16 19:44:38.083: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f container agnhost-container: <nil>
STEP: delete the pod
Nov 16 19:44:38.151: INFO: Waiting for pod pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f to disappear
Nov 16 19:44:38.167: INFO: Pod pod-configmaps-f2eeae3f-75ce-4388-becc-c26a30a8782f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:38.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9803" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":51,"skipped":1062,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:38.217: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:44:38.525: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86" in namespace "projected-6624" to be "Succeeded or Failed"
Nov 16 19:44:38.543: INFO: Pod "downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86": Phase="Pending", Reason="", readiness=false. Elapsed: 17.891562ms
Nov 16 19:44:40.566: INFO: Pod "downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040923314s
Nov 16 19:44:42.588: INFO: Pod "downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062428792s
STEP: Saw pod success
Nov 16 19:44:42.588: INFO: Pod "downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86" satisfied condition "Succeeded or Failed"
Nov 16 19:44:42.603: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86 container client-container: <nil>
STEP: delete the pod
Nov 16 19:44:42.676: INFO: Waiting for pod downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86 to disappear
Nov 16 19:44:42.692: INFO: Pod downwardapi-volume-da915b6d-d56f-421a-bb7a-eca4a7db1d86 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:42.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6624" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":1070,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:42.737: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:44:43.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:44:46.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4817" for this suite.
STEP: Destroying namespace "webhook-4817-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":53,"skipped":1084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:47.265: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3698
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:44:47.517: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:50.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3698" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":54,"skipped":1107,"failed":0}

------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Nov 16 19:44:51.230: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Nov 16 19:44:51.252: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 16 19:44:51.253: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Nov 16 19:44:51.289: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Nov 16 19:44:51.289: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Nov 16 19:44:51.324: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Nov 16 19:44:51.324: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Nov 16 19:44:58.469: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:44:58.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-653" for this suite.

â€¢ [SLOW TEST:7.601 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":55,"skipped":1107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:44:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8037
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Nov 16 19:44:58.847: INFO: Found 0 stateful pods, waiting for 3
Nov 16 19:45:08.879: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:45:08.879: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:45:08.879: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 19:45:08.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-8037 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:45:09.393: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:45:09.393: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:45:09.393: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Nov 16 19:45:19.515: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 16 19:45:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-8037 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:45:29.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:45:29.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:45:29.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:45:50.063: INFO: Waiting for StatefulSet statefulset-8037/ss2 to complete update
Nov 16 19:45:50.063: INFO: Waiting for Pod statefulset-8037/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Nov 16 19:46:00.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-8037 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 19:46:00.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 19:46:00.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 19:46:00.469: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 19:46:10.591: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 16 19:46:20.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-8037 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 19:46:21.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 19:46:21.041: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 19:46:21.041: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 19:46:31.156: INFO: Waiting for StatefulSet statefulset-8037/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 19:46:41.203: INFO: Deleting all statefulset in ns statefulset-8037
Nov 16 19:46:41.216: INFO: Scaling statefulset ss2 to 0
Nov 16 19:46:51.308: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 19:46:51.321: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:46:51.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8037" for this suite.

â€¢ [SLOW TEST:112.877 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":56,"skipped":1154,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:46:51.429: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mxg5l in namespace proxy-2525
I1116 19:46:51.750751      25 runners.go:190] Created replication controller with name: proxy-service-mxg5l, namespace: proxy-2525, replica count: 1
I1116 19:46:52.802890      25 runners.go:190] proxy-service-mxg5l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 19:46:53.803811      25 runners.go:190] proxy-service-mxg5l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 19:46:54.804199      25 runners.go:190] proxy-service-mxg5l Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 19:46:54.829: INFO: setup took 3.144427962s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 16 19:46:54.900: INFO: (0) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 69.286586ms)
Nov 16 19:46:54.900: INFO: (0) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 69.276037ms)
Nov 16 19:46:54.900: INFO: (0) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 69.602469ms)
Nov 16 19:46:54.900: INFO: (0) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 69.876567ms)
Nov 16 19:46:54.900: INFO: (0) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 69.463272ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 107.086209ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 107.095442ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 106.765799ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 107.607029ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 106.846091ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 107.159665ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 107.392361ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 106.82542ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 107.578024ms)
Nov 16 19:46:54.937: INFO: (0) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 107.053998ms)
Nov 16 19:46:54.941: INFO: (0) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 110.717877ms)
Nov 16 19:46:54.963: INFO: (1) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 21.307796ms)
Nov 16 19:46:54.966: INFO: (1) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 24.513628ms)
Nov 16 19:46:54.967: INFO: (1) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 25.429892ms)
Nov 16 19:46:54.968: INFO: (1) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 25.879144ms)
Nov 16 19:46:54.968: INFO: (1) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 26.095611ms)
Nov 16 19:46:54.969: INFO: (1) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 26.596094ms)
Nov 16 19:46:54.969: INFO: (1) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 26.944531ms)
Nov 16 19:46:54.969: INFO: (1) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 26.987101ms)
Nov 16 19:46:54.969: INFO: (1) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 27.119376ms)
Nov 16 19:46:54.969: INFO: (1) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 27.287413ms)
Nov 16 19:46:54.970: INFO: (1) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 28.263274ms)
Nov 16 19:46:54.973: INFO: (1) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 30.948748ms)
Nov 16 19:46:54.973: INFO: (1) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 30.849199ms)
Nov 16 19:46:54.973: INFO: (1) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 31.332817ms)
Nov 16 19:46:54.974: INFO: (1) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 31.572187ms)
Nov 16 19:46:54.974: INFO: (1) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 31.674855ms)
Nov 16 19:46:55.000: INFO: (2) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 26.051965ms)
Nov 16 19:46:55.000: INFO: (2) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 25.774305ms)
Nov 16 19:46:55.001: INFO: (2) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 26.448031ms)
Nov 16 19:46:55.002: INFO: (2) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 28.275439ms)
Nov 16 19:46:55.003: INFO: (2) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 28.56262ms)
Nov 16 19:46:55.003: INFO: (2) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 28.236266ms)
Nov 16 19:46:55.003: INFO: (2) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 28.594614ms)
Nov 16 19:46:55.004: INFO: (2) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 29.614076ms)
Nov 16 19:46:55.004: INFO: (2) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 29.311645ms)
Nov 16 19:46:55.004: INFO: (2) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 29.840198ms)
Nov 16 19:46:55.007: INFO: (2) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 32.786199ms)
Nov 16 19:46:55.010: INFO: (2) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 35.488267ms)
Nov 16 19:46:55.011: INFO: (2) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 36.679345ms)
Nov 16 19:46:55.011: INFO: (2) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 36.969343ms)
Nov 16 19:46:55.012: INFO: (2) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 37.161105ms)
Nov 16 19:46:55.012: INFO: (2) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 37.986363ms)
Nov 16 19:46:55.069: INFO: (3) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 56.337269ms)
Nov 16 19:46:55.070: INFO: (3) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 52.203849ms)
Nov 16 19:46:55.085: INFO: (3) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 72.400029ms)
Nov 16 19:46:55.103: INFO: (3) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 86.497074ms)
Nov 16 19:46:55.105: INFO: (3) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 88.350122ms)
Nov 16 19:46:55.105: INFO: (3) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 89.845917ms)
Nov 16 19:46:55.105: INFO: (3) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 86.419835ms)
Nov 16 19:46:55.106: INFO: (3) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 92.418046ms)
Nov 16 19:46:55.107: INFO: (3) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 88.742383ms)
Nov 16 19:46:55.116: INFO: (3) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 100.052274ms)
Nov 16 19:46:55.117: INFO: (3) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 98.547724ms)
Nov 16 19:46:55.118: INFO: (3) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 103.667043ms)
Nov 16 19:46:55.119: INFO: (3) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 102.075197ms)
Nov 16 19:46:55.125: INFO: (3) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 110.321675ms)
Nov 16 19:46:55.125: INFO: (3) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 109.623993ms)
Nov 16 19:46:55.130: INFO: (3) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 116.313232ms)
Nov 16 19:46:55.145: INFO: (4) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 13.240626ms)
Nov 16 19:46:55.146: INFO: (4) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 14.708541ms)
Nov 16 19:46:55.146: INFO: (4) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 14.328779ms)
Nov 16 19:46:55.146: INFO: (4) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 14.456839ms)
Nov 16 19:46:55.146: INFO: (4) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 14.442595ms)
Nov 16 19:46:55.149: INFO: (4) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 17.346814ms)
Nov 16 19:46:55.149: INFO: (4) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 17.1863ms)
Nov 16 19:46:55.149: INFO: (4) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 17.681458ms)
Nov 16 19:46:55.149: INFO: (4) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 18.309437ms)
Nov 16 19:46:55.150: INFO: (4) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 18.667195ms)
Nov 16 19:46:55.150: INFO: (4) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 18.871482ms)
Nov 16 19:46:55.150: INFO: (4) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 18.568393ms)
Nov 16 19:46:55.151: INFO: (4) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 19.724227ms)
Nov 16 19:46:55.152: INFO: (4) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 21.690002ms)
Nov 16 19:46:55.152: INFO: (4) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 21.574701ms)
Nov 16 19:46:55.154: INFO: (4) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 23.831284ms)
Nov 16 19:46:55.175: INFO: (5) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 20.495743ms)
Nov 16 19:46:55.177: INFO: (5) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 22.02708ms)
Nov 16 19:46:55.177: INFO: (5) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 22.183114ms)
Nov 16 19:46:55.177: INFO: (5) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 23.081373ms)
Nov 16 19:46:55.242: INFO: (5) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 87.337587ms)
Nov 16 19:46:55.242: INFO: (5) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 86.645829ms)
Nov 16 19:46:55.242: INFO: (5) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 86.892549ms)
Nov 16 19:46:55.242: INFO: (5) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 87.148098ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 87.170279ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 87.549219ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 87.122805ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 87.385947ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 87.123159ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 87.85407ms)
Nov 16 19:46:55.243: INFO: (5) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 88.186615ms)
Nov 16 19:46:55.253: INFO: (5) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 97.835371ms)
Nov 16 19:46:55.279: INFO: (6) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 25.424253ms)
Nov 16 19:46:55.280: INFO: (6) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 25.438873ms)
Nov 16 19:46:55.280: INFO: (6) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 25.279676ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 26.968598ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 27.091353ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 27.261662ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 27.541377ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 27.653242ms)
Nov 16 19:46:55.281: INFO: (6) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 27.264516ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 39.575428ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 39.794589ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 39.427832ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 39.579681ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 39.513261ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 39.928126ms)
Nov 16 19:46:55.294: INFO: (6) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 40.364561ms)
Nov 16 19:46:55.307: INFO: (7) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 12.535949ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 20.16845ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 19.913817ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 19.607589ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 19.74215ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 19.584186ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 19.222229ms)
Nov 16 19:46:55.314: INFO: (7) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 19.915659ms)
Nov 16 19:46:55.316: INFO: (7) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 21.483419ms)
Nov 16 19:46:55.317: INFO: (7) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 23.046668ms)
Nov 16 19:46:55.318: INFO: (7) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 22.745018ms)
Nov 16 19:46:55.318: INFO: (7) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 22.925105ms)
Nov 16 19:46:55.320: INFO: (7) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 25.092091ms)
Nov 16 19:46:55.320: INFO: (7) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 25.347198ms)
Nov 16 19:46:55.320: INFO: (7) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 25.479834ms)
Nov 16 19:46:55.320: INFO: (7) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 25.619317ms)
Nov 16 19:46:55.330: INFO: (8) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 9.67907ms)
Nov 16 19:46:55.333: INFO: (8) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.270236ms)
Nov 16 19:46:55.333: INFO: (8) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 12.672995ms)
Nov 16 19:46:55.333: INFO: (8) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 12.433457ms)
Nov 16 19:46:55.334: INFO: (8) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.652879ms)
Nov 16 19:46:55.334: INFO: (8) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.606367ms)
Nov 16 19:46:55.337: INFO: (8) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 14.783265ms)
Nov 16 19:46:55.337: INFO: (8) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 15.57854ms)
Nov 16 19:46:55.337: INFO: (8) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 15.979736ms)
Nov 16 19:46:55.341: INFO: (8) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.938084ms)
Nov 16 19:46:55.341: INFO: (8) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 20.318113ms)
Nov 16 19:46:55.341: INFO: (8) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 20.192197ms)
Nov 16 19:46:55.341: INFO: (8) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 19.946438ms)
Nov 16 19:46:55.341: INFO: (8) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 20.480531ms)
Nov 16 19:46:55.342: INFO: (8) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 20.442315ms)
Nov 16 19:46:55.344: INFO: (8) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 22.91957ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.806866ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 13.948433ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 13.652381ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 14.238873ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 14.048027ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 13.927844ms)
Nov 16 19:46:55.358: INFO: (9) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 14.775665ms)
Nov 16 19:46:55.361: INFO: (9) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 16.207456ms)
Nov 16 19:46:55.361: INFO: (9) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 16.497188ms)
Nov 16 19:46:55.361: INFO: (9) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 16.77239ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 19.408289ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 20.030912ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.429952ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 19.797878ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 19.560128ms)
Nov 16 19:46:55.364: INFO: (9) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 19.923122ms)
Nov 16 19:46:55.378: INFO: (10) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 13.705183ms)
Nov 16 19:46:55.378: INFO: (10) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.025128ms)
Nov 16 19:46:55.378: INFO: (10) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 13.218612ms)
Nov 16 19:46:55.378: INFO: (10) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 13.565707ms)
Nov 16 19:46:55.379: INFO: (10) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 14.25023ms)
Nov 16 19:46:55.380: INFO: (10) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 14.586116ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 18.165968ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 18.118565ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 18.507642ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 19.423857ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.642983ms)
Nov 16 19:46:55.384: INFO: (10) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 18.62974ms)
Nov 16 19:46:55.385: INFO: (10) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 20.30723ms)
Nov 16 19:46:55.386: INFO: (10) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 20.133302ms)
Nov 16 19:46:55.386: INFO: (10) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 11.496295ms)
Nov 16 19:46:55.391: INFO: (10) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 26.088421ms)
Nov 16 19:46:55.401: INFO: (11) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 9.654511ms)
Nov 16 19:46:55.403: INFO: (11) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 10.97824ms)
Nov 16 19:46:55.404: INFO: (11) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 12.449935ms)
Nov 16 19:46:55.404: INFO: (11) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.180666ms)
Nov 16 19:46:55.404: INFO: (11) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 12.871015ms)
Nov 16 19:46:55.404: INFO: (11) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 12.257484ms)
Nov 16 19:46:55.404: INFO: (11) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 13.304728ms)
Nov 16 19:46:55.405: INFO: (11) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.902914ms)
Nov 16 19:46:55.405: INFO: (11) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 12.955594ms)
Nov 16 19:46:55.405: INFO: (11) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.087232ms)
Nov 16 19:46:55.407: INFO: (11) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 15.924102ms)
Nov 16 19:46:55.409: INFO: (11) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 16.897663ms)
Nov 16 19:46:55.409: INFO: (11) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 17.459528ms)
Nov 16 19:46:55.410: INFO: (11) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 18.58655ms)
Nov 16 19:46:55.410: INFO: (11) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 18.327652ms)
Nov 16 19:46:55.411: INFO: (11) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.104025ms)
Nov 16 19:46:55.420: INFO: (12) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 9.085166ms)
Nov 16 19:46:55.422: INFO: (12) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 10.232163ms)
Nov 16 19:46:55.424: INFO: (12) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 12.162724ms)
Nov 16 19:46:55.425: INFO: (12) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 12.250466ms)
Nov 16 19:46:55.424: INFO: (12) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 12.434967ms)
Nov 16 19:46:55.425: INFO: (12) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 13.223802ms)
Nov 16 19:46:55.425: INFO: (12) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.22561ms)
Nov 16 19:46:55.426: INFO: (12) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.474808ms)
Nov 16 19:46:55.426: INFO: (12) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 12.447752ms)
Nov 16 19:46:55.427: INFO: (12) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 14.183045ms)
Nov 16 19:46:55.427: INFO: (12) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.979487ms)
Nov 16 19:46:55.427: INFO: (12) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 14.813987ms)
Nov 16 19:46:55.429: INFO: (12) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 16.922723ms)
Nov 16 19:46:55.429: INFO: (12) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 16.662928ms)
Nov 16 19:46:55.430: INFO: (12) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 16.188473ms)
Nov 16 19:46:55.430: INFO: (12) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 16.170646ms)
Nov 16 19:46:55.452: INFO: (13) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 21.116399ms)
Nov 16 19:46:55.452: INFO: (13) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 21.39808ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 22.582335ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 22.536919ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 23.04716ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 22.911448ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 22.6338ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 22.920627ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 22.715377ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 23.016611ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 23.085445ms)
Nov 16 19:46:55.453: INFO: (13) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 23.253954ms)
Nov 16 19:46:55.460: INFO: (13) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 29.880504ms)
Nov 16 19:46:55.461: INFO: (13) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 30.255967ms)
Nov 16 19:46:55.461: INFO: (13) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 30.446065ms)
Nov 16 19:46:55.461: INFO: (13) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 30.004225ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 14.727432ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 14.322691ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 15.463659ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 14.117999ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 14.154595ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 15.678365ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 15.314941ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 14.333865ms)
Nov 16 19:46:55.478: INFO: (14) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 15.192707ms)
Nov 16 19:46:55.479: INFO: (14) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 16.468416ms)
Nov 16 19:46:55.485: INFO: (14) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 21.509467ms)
Nov 16 19:46:55.487: INFO: (14) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 23.677066ms)
Nov 16 19:46:55.490: INFO: (14) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 26.715938ms)
Nov 16 19:46:55.491: INFO: (14) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 27.616019ms)
Nov 16 19:46:55.491: INFO: (14) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 27.560851ms)
Nov 16 19:46:55.491: INFO: (14) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 27.321126ms)
Nov 16 19:46:55.502: INFO: (15) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 11.053831ms)
Nov 16 19:46:55.503: INFO: (15) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 11.241659ms)
Nov 16 19:46:55.504: INFO: (15) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 12.58093ms)
Nov 16 19:46:55.504: INFO: (15) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.82646ms)
Nov 16 19:46:55.504: INFO: (15) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 12.684383ms)
Nov 16 19:46:55.504: INFO: (15) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.334967ms)
Nov 16 19:46:55.504: INFO: (15) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 12.441536ms)
Nov 16 19:46:55.514: INFO: (15) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 21.889641ms)
Nov 16 19:46:55.514: INFO: (15) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 21.987972ms)
Nov 16 19:46:55.515: INFO: (15) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 24.339398ms)
Nov 16 19:46:55.516: INFO: (15) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 23.992556ms)
Nov 16 19:46:55.516: INFO: (15) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 24.02818ms)
Nov 16 19:46:55.517: INFO: (15) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 25.643909ms)
Nov 16 19:46:55.517: INFO: (15) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 25.600609ms)
Nov 16 19:46:55.519: INFO: (15) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 27.448051ms)
Nov 16 19:46:55.520: INFO: (15) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 28.548643ms)
Nov 16 19:46:55.531: INFO: (16) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 10.386598ms)
Nov 16 19:46:55.533: INFO: (16) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 11.480751ms)
Nov 16 19:46:55.534: INFO: (16) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.949193ms)
Nov 16 19:46:55.534: INFO: (16) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 11.921022ms)
Nov 16 19:46:55.535: INFO: (16) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 13.056844ms)
Nov 16 19:46:55.535: INFO: (16) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 12.02854ms)
Nov 16 19:46:55.536: INFO: (16) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 12.324917ms)
Nov 16 19:46:55.536: INFO: (16) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.996802ms)
Nov 16 19:46:55.535: INFO: (16) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 15.105718ms)
Nov 16 19:46:55.536: INFO: (16) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 14.277345ms)
Nov 16 19:46:55.538: INFO: (16) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 16.605575ms)
Nov 16 19:46:55.539: INFO: (16) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 16.130145ms)
Nov 16 19:46:55.539: INFO: (16) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 17.398856ms)
Nov 16 19:46:55.539: INFO: (16) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 17.372442ms)
Nov 16 19:46:55.540: INFO: (16) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 17.098867ms)
Nov 16 19:46:55.540: INFO: (16) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.187332ms)
Nov 16 19:46:55.554: INFO: (17) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 13.541792ms)
Nov 16 19:46:55.554: INFO: (17) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.419983ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.553606ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 13.886012ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 13.520244ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 14.45172ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 14.413045ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 14.659933ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 13.724876ms)
Nov 16 19:46:55.555: INFO: (17) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 14.145093ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 18.945146ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 18.230834ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 18.794244ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 18.52463ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 18.96045ms)
Nov 16 19:46:55.560: INFO: (17) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 18.525797ms)
Nov 16 19:46:55.572: INFO: (18) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 10.426287ms)
Nov 16 19:46:55.573: INFO: (18) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 10.38785ms)
Nov 16 19:46:55.573: INFO: (18) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 10.641571ms)
Nov 16 19:46:55.573: INFO: (18) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 11.732123ms)
Nov 16 19:46:55.573: INFO: (18) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 11.6157ms)
Nov 16 19:46:55.573: INFO: (18) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 12.607334ms)
Nov 16 19:46:55.574: INFO: (18) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 12.847792ms)
Nov 16 19:46:55.574: INFO: (18) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 11.855432ms)
Nov 16 19:46:55.575: INFO: (18) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 13.618256ms)
Nov 16 19:46:55.579: INFO: (18) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 16.280867ms)
Nov 16 19:46:55.579: INFO: (18) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 18.299661ms)
Nov 16 19:46:55.580: INFO: (18) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 18.367034ms)
Nov 16 19:46:55.582: INFO: (18) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 21.279714ms)
Nov 16 19:46:55.582: INFO: (18) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 20.418025ms)
Nov 16 19:46:55.583: INFO: (18) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 22.082482ms)
Nov 16 19:46:55.583: INFO: (18) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 21.790234ms)
Nov 16 19:46:55.597: INFO: (19) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 13.163217ms)
Nov 16 19:46:55.599: INFO: (19) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">... (200; 15.321771ms)
Nov 16 19:46:55.600: INFO: (19) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:460/proxy/: tls baz (200; 15.920896ms)
Nov 16 19:46:55.600: INFO: (19) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:160/proxy/: foo (200; 16.575786ms)
Nov 16 19:46:55.601: INFO: (19) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:462/proxy/: tls qux (200; 17.291738ms)
Nov 16 19:46:55.601: INFO: (19) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 17.289241ms)
Nov 16 19:46:55.602: INFO: (19) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g:1080/proxy/rewriteme">test<... (200; 17.883832ms)
Nov 16 19:46:55.602: INFO: (19) /api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/https:proxy-service-mxg5l-x6q4g:443/proxy/tlsrewritem... (200; 18.194024ms)
Nov 16 19:46:55.601: INFO: (19) /api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/: <a href="/api/v1/namespaces/proxy-2525/pods/proxy-service-mxg5l-x6q4g/proxy/rewriteme">test</a> (200; 17.728852ms)
Nov 16 19:46:55.602: INFO: (19) /api/v1/namespaces/proxy-2525/pods/http:proxy-service-mxg5l-x6q4g:162/proxy/: bar (200; 18.623445ms)
Nov 16 19:46:55.603: INFO: (19) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname2/proxy/: bar (200; 18.926497ms)
Nov 16 19:46:55.603: INFO: (19) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname1/proxy/: foo (200; 19.368213ms)
Nov 16 19:46:55.604: INFO: (19) /api/v1/namespaces/proxy-2525/services/proxy-service-mxg5l:portname2/proxy/: bar (200; 19.742597ms)
Nov 16 19:46:55.604: INFO: (19) /api/v1/namespaces/proxy-2525/services/http:proxy-service-mxg5l:portname1/proxy/: foo (200; 20.034047ms)
Nov 16 19:46:55.604: INFO: (19) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname2/proxy/: tls qux (200; 20.65006ms)
Nov 16 19:46:55.605: INFO: (19) /api/v1/namespaces/proxy-2525/services/https:proxy-service-mxg5l:tlsportname1/proxy/: tls baz (200; 21.185199ms)
STEP: deleting ReplicationController proxy-service-mxg5l in namespace proxy-2525, will wait for the garbage collector to delete the pods
Nov 16 19:46:55.694: INFO: Deleting ReplicationController proxy-service-mxg5l took: 22.076393ms
Nov 16 19:46:55.796: INFO: Terminating ReplicationController proxy-service-mxg5l pods took: 101.281173ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:46:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2525" for this suite.

â€¢ [SLOW TEST:7.297 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":57,"skipped":1168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:46:58.729: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Nov 16 19:46:58.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7704 cluster-info'
Nov 16 19:46:59.084: INFO: stderr: ""
Nov 16 19:46:59.084: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:46:59.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7704" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":58,"skipped":1222,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:46:59.117: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2299
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-9ea79a34-ba4c-4551-a468-687aec8538ca
STEP: Creating configMap with name cm-test-opt-upd-ed00b6b4-57e4-46d8-ae15-9b4ccae1469f
STEP: Creating the pod
Nov 16 19:46:59.386: INFO: The status of Pod pod-projected-configmaps-4169799b-9ca6-4b0f-b51e-a576b2fc8448 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:47:01.398: INFO: The status of Pod pod-projected-configmaps-4169799b-9ca6-4b0f-b51e-a576b2fc8448 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:47:03.397: INFO: The status of Pod pod-projected-configmaps-4169799b-9ca6-4b0f-b51e-a576b2fc8448 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-9ea79a34-ba4c-4551-a468-687aec8538ca
STEP: Updating configmap cm-test-opt-upd-ed00b6b4-57e4-46d8-ae15-9b4ccae1469f
STEP: Creating configMap with name cm-test-opt-create-0b471542-62a8-48b6-81ef-9759f5c63ab3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2299" for this suite.

â€¢ [SLOW TEST:85.327 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1228,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:24.450: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:24.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7087" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":60,"skipped":1233,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:24.794: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 19:48:25.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 19:48:27.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688905, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688905, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688905, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772688905, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 19:48:30.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:48:30.486: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6127-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:33.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3624" for this suite.
STEP: Destroying namespace "webhook-3624-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.192 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":61,"skipped":1248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:33.997: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:34.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2698" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":62,"skipped":1273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:48:34.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94" in namespace "projected-1128" to be "Succeeded or Failed"
Nov 16 19:48:34.498: INFO: Pod "downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.612214ms
Nov 16 19:48:36.507: INFO: Pod "downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016043006s
Nov 16 19:48:38.519: INFO: Pod "downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027393902s
STEP: Saw pod success
Nov 16 19:48:38.519: INFO: Pod "downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94" satisfied condition "Succeeded or Failed"
Nov 16 19:48:38.525: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94 container client-container: <nil>
STEP: delete the pod
Nov 16 19:48:38.563: INFO: Waiting for pod downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94 to disappear
Nov 16 19:48:38.569: INFO: Pod downwardapi-volume-4368463a-a4b6-4a4d-a2c8-7034a6e88a94 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:38.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1128" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1326,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-6534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:48:39.574: INFO: Checking APIGroup: apiregistration.k8s.io
Nov 16 19:48:39.576: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Nov 16 19:48:39.576: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Nov 16 19:48:39.576: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Nov 16 19:48:39.576: INFO: Checking APIGroup: apps
Nov 16 19:48:39.578: INFO: PreferredVersion.GroupVersion: apps/v1
Nov 16 19:48:39.578: INFO: Versions found [{apps/v1 v1}]
Nov 16 19:48:39.578: INFO: apps/v1 matches apps/v1
Nov 16 19:48:39.578: INFO: Checking APIGroup: events.k8s.io
Nov 16 19:48:39.580: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Nov 16 19:48:39.580: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.580: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Nov 16 19:48:39.580: INFO: Checking APIGroup: authentication.k8s.io
Nov 16 19:48:39.582: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Nov 16 19:48:39.582: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Nov 16 19:48:39.582: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Nov 16 19:48:39.582: INFO: Checking APIGroup: authorization.k8s.io
Nov 16 19:48:39.584: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Nov 16 19:48:39.584: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Nov 16 19:48:39.584: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Nov 16 19:48:39.584: INFO: Checking APIGroup: autoscaling
Nov 16 19:48:39.586: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Nov 16 19:48:39.586: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Nov 16 19:48:39.586: INFO: autoscaling/v1 matches autoscaling/v1
Nov 16 19:48:39.586: INFO: Checking APIGroup: batch
Nov 16 19:48:39.588: INFO: PreferredVersion.GroupVersion: batch/v1
Nov 16 19:48:39.588: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Nov 16 19:48:39.588: INFO: batch/v1 matches batch/v1
Nov 16 19:48:39.588: INFO: Checking APIGroup: certificates.k8s.io
Nov 16 19:48:39.590: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Nov 16 19:48:39.590: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Nov 16 19:48:39.590: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Nov 16 19:48:39.590: INFO: Checking APIGroup: networking.k8s.io
Nov 16 19:48:39.592: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Nov 16 19:48:39.592: INFO: Versions found [{networking.k8s.io/v1 v1}]
Nov 16 19:48:39.592: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Nov 16 19:48:39.592: INFO: Checking APIGroup: policy
Nov 16 19:48:39.594: INFO: PreferredVersion.GroupVersion: policy/v1
Nov 16 19:48:39.594: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Nov 16 19:48:39.594: INFO: policy/v1 matches policy/v1
Nov 16 19:48:39.594: INFO: Checking APIGroup: rbac.authorization.k8s.io
Nov 16 19:48:39.596: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Nov 16 19:48:39.596: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Nov 16 19:48:39.596: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Nov 16 19:48:39.596: INFO: Checking APIGroup: storage.k8s.io
Nov 16 19:48:39.599: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Nov 16 19:48:39.599: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.599: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Nov 16 19:48:39.599: INFO: Checking APIGroup: admissionregistration.k8s.io
Nov 16 19:48:39.601: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Nov 16 19:48:39.601: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Nov 16 19:48:39.601: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Nov 16 19:48:39.601: INFO: Checking APIGroup: apiextensions.k8s.io
Nov 16 19:48:39.603: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Nov 16 19:48:39.603: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Nov 16 19:48:39.603: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Nov 16 19:48:39.603: INFO: Checking APIGroup: scheduling.k8s.io
Nov 16 19:48:39.605: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Nov 16 19:48:39.605: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Nov 16 19:48:39.605: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Nov 16 19:48:39.605: INFO: Checking APIGroup: coordination.k8s.io
Nov 16 19:48:39.607: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Nov 16 19:48:39.607: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Nov 16 19:48:39.607: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Nov 16 19:48:39.607: INFO: Checking APIGroup: node.k8s.io
Nov 16 19:48:39.609: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Nov 16 19:48:39.609: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.609: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Nov 16 19:48:39.609: INFO: Checking APIGroup: discovery.k8s.io
Nov 16 19:48:39.612: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Nov 16 19:48:39.612: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.612: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Nov 16 19:48:39.612: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Nov 16 19:48:39.615: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Nov 16 19:48:39.615: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.615: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Nov 16 19:48:39.615: INFO: Checking APIGroup: crd.projectcalico.org
Nov 16 19:48:39.618: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Nov 16 19:48:39.618: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Nov 16 19:48:39.618: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Nov 16 19:48:39.618: INFO: Checking APIGroup: operators.coreos.com
Nov 16 19:48:39.621: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Nov 16 19:48:39.621: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Nov 16 19:48:39.621: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Nov 16 19:48:39.621: INFO: Checking APIGroup: snapshot.storage.k8s.io
Nov 16 19:48:39.624: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Nov 16 19:48:39.624: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.624: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Nov 16 19:48:39.624: INFO: Checking APIGroup: ibm.com
Nov 16 19:48:39.626: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Nov 16 19:48:39.626: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Nov 16 19:48:39.626: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Nov 16 19:48:39.626: INFO: Checking APIGroup: metrics.k8s.io
Nov 16 19:48:39.629: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Nov 16 19:48:39.629: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Nov 16 19:48:39.629: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6534" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":64,"skipped":1330,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:39.662: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9191.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9191.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 19:48:44.015: INFO: DNS probes using dns-9191/dns-test-9e319f5e-6c73-4892-90cd-7cde3454bf0e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:44.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9191" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":65,"skipped":1340,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:44.063: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Nov 16 19:48:44.275: INFO: created test-podtemplate-1
Nov 16 19:48:44.287: INFO: created test-podtemplate-2
Nov 16 19:48:44.299: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Nov 16 19:48:44.311: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Nov 16 19:48:44.375: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:44.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7103" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":66,"skipped":1348,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:44.419: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Nov 16 19:48:44.678: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.679: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.703: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.703: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.739: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.739: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.782: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:44.782: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Nov 16 19:48:46.872: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 16 19:48:46.872: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Nov 16 19:48:46.987: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Nov 16 19:48:47.035: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.039: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.040: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.040: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 0
Nov 16 19:48:47.040: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:47.040: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:47.041: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.041: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.041: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.041: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.059: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.059: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.095: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.095: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:47.117: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:47.117: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:47.147: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:47.147: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:49.905: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:49.905: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:49.942: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
STEP: listing Deployments
Nov 16 19:48:49.962: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Nov 16 19:48:49.984: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Nov 16 19:48:50.008: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:50.014: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:50.073: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:50.095: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:52.030: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:52.507: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:52.583: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:52.595: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Nov 16 19:48:54.933: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Nov 16 19:48:55.006: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:55.007: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:55.007: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:55.008: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 1
Nov 16 19:48:55.008: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:55.008: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 3
Nov 16 19:48:55.009: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:55.009: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 2
Nov 16 19:48:55.009: INFO: observed Deployment test-deployment in namespace deployment-9435 with ReadyReplicas 3
STEP: deleting the Deployment
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.041: INFO: observed event type MODIFIED
Nov 16 19:48:55.042: INFO: observed event type MODIFIED
Nov 16 19:48:55.042: INFO: observed event type MODIFIED
Nov 16 19:48:55.042: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 19:48:55.058: INFO: Log out all the ReplicaSets if there is no deployment created
Nov 16 19:48:55.066: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-9435  7104557b-bf80-4dfe-9005-29694364832a 26300 4 2021-11-16 19:48:47 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment da1b426d-f9cb-455d-9fc8-a017593be37a 0xc00518fef7 0xc00518fef8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 19:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da1b426d-f9cb-455d-9fc8-a017593be37a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:48:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00518ff80 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 16 19:48:55.073: INFO: pod: "test-deployment-56c98d85f9-226fh":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-226fh test-deployment-56c98d85f9- deployment-9435  016ebb24-9a6e-4dbd-be0a-ef8de7fd673e 26296 0 2021-11-16 19:48:47 +0000 UTC 2021-11-16 19:48:55 +0000 UTC 0xc002946c58 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:45829df252a190b3a22f52b8d694f28eb9580f1a442bc316cbcd1b3d325f1c71 cni.projectcalico.org/podIP:172.30.9.12/32 cni.projectcalico.org/podIPs:172.30.9.12/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 7104557b-bf80-4dfe-9005-29694364832a 0xc002946ca7 0xc002946ca8}] []  [{kube-controller-manager Update v1 2021-11-16 19:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7104557b-bf80-4dfe-9005-29694364832a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:48:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xmd4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xmd4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.12,StartTime:2021-11-16 19:48:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:containerd://c00439f0ca0dbb825a7b6bbb4b911fe25eed874c40f0c6fa0011a090331060e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 16 19:48:55.073: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-9435  fc212af6-7db3-4b59-8a8f-be67011146ea 26195 3 2021-11-16 19:48:44 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment da1b426d-f9cb-455d-9fc8-a017593be37a 0xc00518ffe7 0xc00518ffe8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 19:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da1b426d-f9cb-455d-9fc8-a017593be37a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:48:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041aa0b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Nov 16 19:48:55.081: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-9435  369fbacc-71a4-4acd-92a6-ea72b05ed04a 26292 2 2021-11-16 19:48:50 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment da1b426d-f9cb-455d-9fc8-a017593be37a 0xc0041aa1b7 0xc0041aa1b8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 19:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da1b426d-f9cb-455d-9fc8-a017593be37a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 19:48:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041aa290 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Nov 16 19:48:55.087: INFO: pod: "test-deployment-d4dfddfbf-762rp":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-762rp test-deployment-d4dfddfbf- deployment-9435  c9da81bb-78f0-461f-b497-ed14797bd65c 26256 0 2021-11-16 19:48:50 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:39a9518f34ea574a482d31ecd6e8d89274b40b0777128a57f00527899a05e954 cni.projectcalico.org/podIP:172.30.148.189/32 cni.projectcalico.org/podIPs:172.30.148.189/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 369fbacc-71a4-4acd-92a6-ea72b05ed04a 0xc0013377c7 0xc0013377c8}] []  [{kube-controller-manager Update v1 2021-11-16 19:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"369fbacc-71a4-4acd-92a6-ea72b05ed04a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:48:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:48:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.148.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp889,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp889,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.24,PodIP:172.30.148.189,StartTime:2021-11-16 19:48:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:48:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://7fab8ca487ff140714822b2be5ada40bc47d8c6b67d7a267031b548d971739d7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.148.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Nov 16 19:48:55.088: INFO: pod: "test-deployment-d4dfddfbf-vmncz":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-vmncz test-deployment-d4dfddfbf- deployment-9435  9e846f03-985c-44bb-ab6e-3382d02dc339 26291 0 2021-11-16 19:48:52 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:c16d663230a8abf184dca5c5c18a7fb1ca99393a8a3e4721049c9da08528f62e cni.projectcalico.org/podIP:172.30.9.6/32 cni.projectcalico.org/podIPs:172.30.9.6/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 369fbacc-71a4-4acd-92a6-ea72b05ed04a 0xc0013379f7 0xc0013379f8}] []  [{kube-controller-manager Update v1 2021-11-16 19:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"369fbacc-71a4-4acd-92a6-ea72b05ed04a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 19:48:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 19:48:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svx49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svx49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 19:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.6,StartTime:2021-11-16 19:48:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 19:48:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b321e72aab9b11ce75fcdce36fe0655d1e7b6e3cf9b5c8d81c57a10bbaa04414,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:48:55.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9435" for this suite.

â€¢ [SLOW TEST:10.700 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":67,"skipped":1354,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:48:55.119: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-8119
STEP: creating replication controller nodeport-test in namespace services-8119
I1116 19:48:55.372313      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8119, replica count: 2
I1116 19:48:58.423887      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 19:48:58.424: INFO: Creating new exec pod
Nov 16 19:49:03.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 16 19:49:03.848: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 16 19:49:03.848: INFO: stdout: ""
Nov 16 19:49:04.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 16 19:49:05.242: INFO: stderr: "+ + nc -v -t -wecho 2 hostName nodeport-test\n 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 16 19:49:05.242: INFO: stdout: ""
Nov 16 19:49:05.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Nov 16 19:49:06.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Nov 16 19:49:06.222: INFO: stdout: "nodeport-test-xspvd"
Nov 16 19:49:06.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.59.34 80'
Nov 16 19:49:06.454: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 172.21.59.34 80\nConnection to 172.21.59.34 80 port [tcp/http] succeeded!\n"
Nov 16 19:49:06.454: INFO: stdout: "nodeport-test-xspvd"
Nov 16 19:49:06.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 30656'
Nov 16 19:49:06.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 30656\nConnection to 10.193.87.28 30656 port [tcp/*] succeeded!\n"
Nov 16 19:49:06.782: INFO: stdout: "nodeport-test-xspvd"
Nov 16 19:49:06.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8119 exec execpod9h9wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.24 30656'
Nov 16 19:49:07.076: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.24 30656\nConnection to 10.193.87.24 30656 port [tcp/*] succeeded!\n"
Nov 16 19:49:07.076: INFO: stdout: "nodeport-test-tt7nt"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:49:07.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8119" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:11.992 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":68,"skipped":1355,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:49:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Nov 16 19:51:07.890: INFO: Successfully updated pod "var-expansion-04ccb7a1-d0ac-48d1-9e0e-d90685dc98c1"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Nov 16 19:51:09.910: INFO: Deleting pod "var-expansion-04ccb7a1-d0ac-48d1-9e0e-d90685dc98c1" in namespace "var-expansion-3716"
Nov 16 19:51:09.924: INFO: Wait up to 5m0s for pod "var-expansion-04ccb7a1-d0ac-48d1-9e0e-d90685dc98c1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:51:41.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3716" for this suite.

â€¢ [SLOW TEST:154.866 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":69,"skipped":1368,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:51:41.978: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2912
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 19:51:42.198: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Creating first CR 
Nov 16 19:51:44.792: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:51:44Z]] name:name1 resourceVersion:26795 uid:3870fa8d-e81c-41ab-ac60-6abff887f4b0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Nov 16 19:51:54.806: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:51:54Z]] name:name2 resourceVersion:26822 uid:f41a557d-a9fe-41bd-8a56-87485f4374f4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Nov 16 19:52:04.826: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:52:04Z]] name:name1 resourceVersion:26837 uid:3870fa8d-e81c-41ab-ac60-6abff887f4b0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Nov 16 19:52:14.844: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:52:14Z]] name:name2 resourceVersion:26852 uid:f41a557d-a9fe-41bd-8a56-87485f4374f4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Nov 16 19:52:24.866: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:52:04Z]] name:name1 resourceVersion:26866 uid:3870fa8d-e81c-41ab-ac60-6abff887f4b0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Nov 16 19:52:34.891: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-11-16T19:51:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-11-16T19:52:14Z]] name:name2 resourceVersion:26880 uid:f41a557d-a9fe-41bd-8a56-87485f4374f4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:52:45.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2912" for this suite.

â€¢ [SLOW TEST:63.485 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":70,"skipped":1383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:52:45.465: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:53:12.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3149" for this suite.

â€¢ [SLOW TEST:26.739 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:53:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:53:16.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6151" for this suite.
â€¢{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:53:16.572: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 16 19:53:16.835: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27050 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 19:53:16.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27052 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 19:53:16.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27053 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 16 19:53:26.923: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27112 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 19:53:26.923: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27114 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 19:53:26.924: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9262  6f37b062-a67a-443f-8e87-819f59319335 27115 0 2021-11-16 19:53:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-11-16 19:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:53:26.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9262" for this suite.

â€¢ [SLOW TEST:10.379 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":73,"skipped":1483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:53:26.953: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 16 19:53:27.160: INFO: PodSpec: initContainers in spec.initContainers
Nov 16 19:54:16.319: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3c897cfb-3f7d-44d5-8791-eb21701f15aa", GenerateName:"", Namespace:"init-container-9017", SelfLink:"", UID:"a5b5c030-68f1-46d1-ad18-e1cb4b2baeef", ResourceVersion:"27228", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63772689207, loc:(*time.Location)(0xa09ece0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"160568725"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2862be8fb18350c1e331870d33f226cddf244e26f199144e12580602801f87d8", "cni.projectcalico.org/podIP":"172.30.9.19/32", "cni.projectcalico.org/podIPs":"172.30.9.19/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00354d980), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00354d998), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00354d9b0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00354d9c8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00354d9e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00354d9f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-p4xz8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005227ec0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4xz8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4xz8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-p4xz8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00490d048), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.193.87.27", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a62460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490d0d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00490d0f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00490d0f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00490d0fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002b67310), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689207, loc:(*time.Location)(0xa09ece0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689207, loc:(*time.Location)(0xa09ece0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689207, loc:(*time.Location)(0xa09ece0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689207, loc:(*time.Location)(0xa09ece0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.193.87.27", PodIP:"172.30.9.19", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.9.19"}}, StartTime:(*v1.Time)(0xc00354da28), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a62540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a625b0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://81dedbb556e0f6b45b8ddab5a712f81aa821d32ee9ae26a9fcc0f8c02a75c8cc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003f60060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005227fe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc00490d17f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:54:16.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9017" for this suite.

â€¢ [SLOW TEST:49.407 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":74,"skipped":1506,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:54:16.360: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3a324818-a094-47a5-847f-c20f92ca75b4
STEP: Creating a pod to test consume secrets
Nov 16 19:54:16.599: INFO: Waiting up to 5m0s for pod "pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669" in namespace "secrets-5514" to be "Succeeded or Failed"
Nov 16 19:54:16.606: INFO: Pod "pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425624ms
Nov 16 19:54:18.616: INFO: Pod "pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017624253s
Nov 16 19:54:20.628: INFO: Pod "pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029381943s
STEP: Saw pod success
Nov 16 19:54:20.628: INFO: Pod "pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669" satisfied condition "Succeeded or Failed"
Nov 16 19:54:20.636: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 19:54:20.681: INFO: Waiting for pod pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669 to disappear
Nov 16 19:54:20.688: INFO: Pod pod-secrets-ae59ae09-27ff-4c27-85bc-df76b4e16669 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:54:20.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5514" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:54:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7012
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Nov 16 19:54:20.956: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:54:22.964: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:54:24.968: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Nov 16 19:54:24.993: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:54:27.003: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 16 19:54:27.010: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:27.010: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:27.211: INFO: Exec stderr: ""
Nov 16 19:54:27.211: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:27.211: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:27.434: INFO: Exec stderr: ""
Nov 16 19:54:27.435: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:27.435: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:27.607: INFO: Exec stderr: ""
Nov 16 19:54:27.607: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:27.608: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:27.879: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 16 19:54:27.879: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:27.880: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:28.090: INFO: Exec stderr: ""
Nov 16 19:54:28.091: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:28.273: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 16 19:54:28.273: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:28.602: INFO: Exec stderr: ""
Nov 16 19:54:28.602: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:28.603: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:28.807: INFO: Exec stderr: ""
Nov 16 19:54:28.807: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:28.807: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:29.000: INFO: Exec stderr: ""
Nov 16 19:54:29.001: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7012 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:54:29.001: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:54:29.201: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:54:29.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7012" for this suite.

â€¢ [SLOW TEST:8.534 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:54:29.254: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8466
Nov 16 19:54:29.481: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:54:31.492: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 16 19:54:31.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 16 19:54:31.891: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 16 19:54:31.891: INFO: stdout: "iptables"
Nov 16 19:54:31.891: INFO: proxyMode: iptables
Nov 16 19:54:31.913: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 16 19:54:31.920: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8466
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8466
I1116 19:54:31.967237      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8466, replica count: 3
I1116 19:54:35.018492      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 19:54:35.054: INFO: Creating new exec pod
Nov 16 19:54:38.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Nov 16 19:54:38.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Nov 16 19:54:38.433: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:54:38.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.186.12 80'
Nov 16 19:54:38.787: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.186.12 80\nConnection to 172.21.186.12 80 port [tcp/http] succeeded!\n"
Nov 16 19:54:38.787: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:54:38.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 32352'
Nov 16 19:54:39.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 32352\nConnection to 10.193.87.28 32352 port [tcp/*] succeeded!\n"
Nov 16 19:54:39.208: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:54:39.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.24 32352'
Nov 16 19:54:39.576: INFO: stderr: "+ + nc -v -t -wecho 2 10.193.87.24 hostName 32352\n\nConnection to 10.193.87.24 32352 port [tcp/*] succeeded!\n"
Nov 16 19:54:39.576: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:54:39.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.193.87.24:32352/ ; done'
Nov 16 19:54:40.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n"
Nov 16 19:54:40.088: INFO: stdout: "\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq\naffinity-nodeport-timeout-wftzq"
Nov 16 19:54:40.088: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.088: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.088: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Received response from host: affinity-nodeport-timeout-wftzq
Nov 16 19:54:40.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.193.87.24:32352/'
Nov 16 19:54:40.371: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n"
Nov 16 19:54:40.371: INFO: stdout: "affinity-nodeport-timeout-wftzq"
Nov 16 19:55:00.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8466 exec execpod-affinityqf5kx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.193.87.24:32352/'
Nov 16 19:55:00.684: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.193.87.24:32352/\n"
Nov 16 19:55:00.684: INFO: stdout: "affinity-nodeport-timeout-dxcls"
Nov 16 19:55:00.684: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8466, will wait for the garbage collector to delete the pods
Nov 16 19:55:00.794: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 20.996913ms
Nov 16 19:55:00.895: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.546562ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:03.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8466" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:34.622 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":77,"skipped":1573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:03.886: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1932
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:04.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1932" for this suite.
â€¢{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":78,"skipped":1618,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:04.281: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 19:55:04.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1" in namespace "downward-api-3152" to be "Succeeded or Failed"
Nov 16 19:55:04.515: INFO: Pod "downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.56798ms
Nov 16 19:55:06.528: INFO: Pod "downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019304069s
Nov 16 19:55:08.538: INFO: Pod "downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029339897s
STEP: Saw pod success
Nov 16 19:55:08.538: INFO: Pod "downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1" satisfied condition "Succeeded or Failed"
Nov 16 19:55:08.545: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1 container client-container: <nil>
STEP: delete the pod
Nov 16 19:55:08.581: INFO: Waiting for pod downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1 to disappear
Nov 16 19:55:08.588: INFO: Pod downwardapi-volume-ed6789b9-8071-4406-a9af-fd3526e536c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:08.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3152" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:08.616: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-366/configmap-test-9d8f7c23-cf78-489e-9390-c89e83f4752b
STEP: Creating a pod to test consume configMaps
Nov 16 19:55:08.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad" in namespace "configmap-366" to be "Succeeded or Failed"
Nov 16 19:55:08.877: INFO: Pod "pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900783ms
Nov 16 19:55:10.889: INFO: Pod "pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018653658s
Nov 16 19:55:12.898: INFO: Pod "pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028030667s
STEP: Saw pod success
Nov 16 19:55:12.898: INFO: Pod "pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad" satisfied condition "Succeeded or Failed"
Nov 16 19:55:12.905: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad container env-test: <nil>
STEP: delete the pod
Nov 16 19:55:12.945: INFO: Waiting for pod pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad to disappear
Nov 16 19:55:12.951: INFO: Pod pod-configmaps-94417333-6e13-4ac3-9d32-1cfd9acc31ad no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:12.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-366" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1650,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 19:55:16.252: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4048" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1656,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:16.325: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Nov 16 19:55:16.576: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 19:55:21.590: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Nov 16 19:55:21.617: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Nov 16 19:55:21.640: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Nov 16 19:55:21.643: INFO: Observed &ReplicaSet event: ADDED
Nov 16 19:55:21.643: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.643: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.643: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.643: INFO: Found replicaset test-rs in namespace replicaset-6752 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 19:55:21.643: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Nov 16 19:55:21.643: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 19:55:21.658: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Nov 16 19:55:21.662: INFO: Observed &ReplicaSet event: ADDED
Nov 16 19:55:21.663: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.664: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.664: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.665: INFO: Observed replicaset test-rs in namespace replicaset-6752 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 19:55:21.665: INFO: Observed &ReplicaSet event: MODIFIED
Nov 16 19:55:21.665: INFO: Found replicaset test-rs in namespace replicaset-6752 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Nov 16 19:55:21.666: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:21.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6752" for this suite.

â€¢ [SLOW TEST:5.371 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":82,"skipped":1663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-e4543710-0346-4bfa-b039-9311f73d7c1e
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3749" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":83,"skipped":1748,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:22.009: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-4c2687d4-2148-47a4-a35e-e747e028b08b
STEP: Creating a pod to test consume configMaps
Nov 16 19:55:22.249: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668" in namespace "projected-7899" to be "Succeeded or Failed"
Nov 16 19:55:22.256: INFO: Pod "pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668": Phase="Pending", Reason="", readiness=false. Elapsed: 6.390944ms
Nov 16 19:55:24.265: INFO: Pod "pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016097294s
Nov 16 19:55:26.276: INFO: Pod "pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026252488s
STEP: Saw pod success
Nov 16 19:55:26.276: INFO: Pod "pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668" satisfied condition "Succeeded or Failed"
Nov 16 19:55:26.282: INFO: Trying to get logs from node 10.193.87.28 pod pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 16 19:55:26.383: INFO: Waiting for pod pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668 to disappear
Nov 16 19:55:26.390: INFO: Pod pod-projected-configmaps-a8e64303-b892-4e42-ba20-2c62fb997668 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:26.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7899" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:26.439: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Nov 16 19:55:26.719: INFO: Waiting up to 5m0s for pod "test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5" in namespace "svcaccounts-9083" to be "Succeeded or Failed"
Nov 16 19:55:26.725: INFO: Pod "test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335669ms
Nov 16 19:55:28.734: INFO: Pod "test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015166283s
Nov 16 19:55:30.744: INFO: Pod "test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025484466s
STEP: Saw pod success
Nov 16 19:55:30.745: INFO: Pod "test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5" satisfied condition "Succeeded or Failed"
Nov 16 19:55:30.751: INFO: Trying to get logs from node 10.193.87.28 pod test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 19:55:30.795: INFO: Waiting for pod test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5 to disappear
Nov 16 19:55:30.802: INFO: Pod test-pod-d076b2f8-8f38-40d2-8699-79ced49883f5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:30.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9083" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":85,"skipped":1796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:30.842: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:31.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1723" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":86,"skipped":1849,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:31.180: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 16 19:55:31.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2466 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Nov 16 19:55:31.727: INFO: stderr: ""
Nov 16 19:55:31.727: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Nov 16 19:55:31.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2466 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Nov 16 19:55:32.008: INFO: stderr: ""
Nov 16 19:55:32.008: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 16 19:55:32.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2466 delete pods e2e-test-httpd-pod'
Nov 16 19:55:35.115: INFO: stderr: ""
Nov 16 19:55:35.115: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:55:35.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2466" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":87,"skipped":1865,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:55:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 19:55:35.405: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 19:56:35.493: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Nov 16 19:56:35.551: INFO: Created pod: pod0-0-sched-preemption-low-priority
Nov 16 19:56:35.563: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Nov 16 19:56:35.593: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Nov 16 19:56:35.614: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Nov 16 19:56:35.646: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Nov 16 19:56:35.659: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:56:45.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1637" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:70.860 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":88,"skipped":1874,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:56:46.011: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-33d7085f-bff1-4a53-aa73-d6eb87a52aa6
STEP: Creating a pod to test consume configMaps
Nov 16 19:56:46.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac" in namespace "configmap-8586" to be "Succeeded or Failed"
Nov 16 19:56:46.272: INFO: Pod "pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.517882ms
Nov 16 19:56:48.284: INFO: Pod "pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020617097s
Nov 16 19:56:50.295: INFO: Pod "pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031852906s
STEP: Saw pod success
Nov 16 19:56:50.295: INFO: Pod "pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac" satisfied condition "Succeeded or Failed"
Nov 16 19:56:50.303: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac container agnhost-container: <nil>
STEP: delete the pod
Nov 16 19:56:50.394: INFO: Waiting for pod pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac to disappear
Nov 16 19:56:50.403: INFO: Pod pod-configmaps-c5cdb428-978f-48f3-8125-d127229320ac no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:56:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8586" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1877,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:56:50.436: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 16 19:56:51.850: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1116 19:56:51.850129      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:56:51.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5738" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":90,"skipped":1898,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:56:51.890: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8518
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 19:56:52.153: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 19:56:52.263: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:56:54.275: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:56:56.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:56:58.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:00.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:02.277: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:04.277: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:06.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:08.279: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:10.277: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:12.276: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 19:57:12.294: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 16 19:57:14.306: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 19:57:14.323: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 19:57:18.374: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 19:57:18.374: INFO: Breadth first check of 172.30.148.139 on host 10.193.87.24...
Nov 16 19:57:18.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.32:9080/dial?request=hostname&protocol=udp&host=172.30.148.139&port=8081&tries=1'] Namespace:pod-network-test-8518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:18.587: INFO: Waiting for responses: map[]
Nov 16 19:57:18.587: INFO: reached 172.30.148.139 after 0/1 tries
Nov 16 19:57:18.587: INFO: Breadth first check of 172.30.9.34 on host 10.193.87.27...
Nov 16 19:57:18.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.32:9080/dial?request=hostname&protocol=udp&host=172.30.9.34&port=8081&tries=1'] Namespace:pod-network-test-8518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:18.597: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:18.799: INFO: Waiting for responses: map[]
Nov 16 19:57:18.800: INFO: reached 172.30.9.34 after 0/1 tries
Nov 16 19:57:18.800: INFO: Breadth first check of 172.30.184.206 on host 10.193.87.28...
Nov 16 19:57:18.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.9.32:9080/dial?request=hostname&protocol=udp&host=172.30.184.206&port=8081&tries=1'] Namespace:pod-network-test-8518 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:18.809: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:19.049: INFO: Waiting for responses: map[]
Nov 16 19:57:19.050: INFO: reached 172.30.184.206 after 0/1 tries
Nov 16 19:57:19.050: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:57:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8518" for this suite.

â€¢ [SLOW TEST:27.199 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:57:19.096: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-843
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 19:57:19.317: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 19:57:19.390: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:57:21.399: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:57:23.401: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:25.423: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:27.404: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:29.403: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:31.403: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:33.401: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:35.404: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:37.400: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 19:57:39.403: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 19:57:39.420: INFO: The status of Pod netserver-1 is Running (Ready = false)
Nov 16 19:57:41.433: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 19:57:41.451: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 19:57:45.527: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 19:57:45.527: INFO: Going to poll 172.30.148.133 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 19:57:45.535: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.148.133:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:45.763: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 16 19:57:45.763: INFO: Going to poll 172.30.9.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 19:57:45.842: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.9.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:45.842: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:46.065: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 16 19:57:46.065: INFO: Going to poll 172.30.184.212 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Nov 16 19:57:46.078: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.184.212:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:57:46.079: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 19:57:46.269: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:57:46.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-843" for this suite.

â€¢ [SLOW TEST:27.217 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1928,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:57:46.318: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 16 19:57:46.606: INFO: Waiting up to 5m0s for pod "pod-696e14b3-240b-49c6-ba75-374ffb61b37c" in namespace "emptydir-2222" to be "Succeeded or Failed"
Nov 16 19:57:46.624: INFO: Pod "pod-696e14b3-240b-49c6-ba75-374ffb61b37c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.946947ms
Nov 16 19:57:48.639: INFO: Pod "pod-696e14b3-240b-49c6-ba75-374ffb61b37c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032795951s
Nov 16 19:57:50.669: INFO: Pod "pod-696e14b3-240b-49c6-ba75-374ffb61b37c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062569757s
STEP: Saw pod success
Nov 16 19:57:50.669: INFO: Pod "pod-696e14b3-240b-49c6-ba75-374ffb61b37c" satisfied condition "Succeeded or Failed"
Nov 16 19:57:50.686: INFO: Trying to get logs from node 10.193.87.27 pod pod-696e14b3-240b-49c6-ba75-374ffb61b37c container test-container: <nil>
STEP: delete the pod
Nov 16 19:57:50.833: INFO: Waiting for pod pod-696e14b3-240b-49c6-ba75-374ffb61b37c to disappear
Nov 16 19:57:50.846: INFO: Pod pod-696e14b3-240b-49c6-ba75-374ffb61b37c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:57:50.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2222" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:57:50.901: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 19:57:51.343: INFO: Number of nodes with available pods: 0
Nov 16 19:57:51.343: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 19:57:52.383: INFO: Number of nodes with available pods: 0
Nov 16 19:57:52.383: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 19:57:53.398: INFO: Number of nodes with available pods: 0
Nov 16 19:57:53.398: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 19:57:54.389: INFO: Number of nodes with available pods: 2
Nov 16 19:57:54.389: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 19:57:55.382: INFO: Number of nodes with available pods: 3
Nov 16 19:57:55.382: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Nov 16 19:57:55.480: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28805"},"items":null}

Nov 16 19:57:55.500: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28807"},"items":[{"metadata":{"name":"daemon-set-bt9qh","generateName":"daemon-set-","namespace":"daemonsets-4294","uid":"dfc760e4-4de0-42d5-8be2-8aaa3fcfb559","resourceVersion":"28806","creationTimestamp":"2021-11-16T19:57:51Z","deletionTimestamp":"2021-11-16T19:58:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"406fa32ec4447d6f0d3ba9ead3498aef5d1c420a7563329557d0a7696f5dcb24","cni.projectcalico.org/podIP":"172.30.9.37/32","cni.projectcalico.org/podIPs":"172.30.9.37/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"57bb1319-0fe7-46fc-b607-cf44db1232bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57bb1319-0fe7-46fc-b607-cf44db1232bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zs7pg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zs7pg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.193.87.27","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.193.87.27"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"}],"hostIP":"10.193.87.27","podIP":"172.30.9.37","podIPs":[{"ip":"172.30.9.37"}],"startTime":"2021-11-16T19:57:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-11-16T19:57:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://14bb23e583d97fe4704d75c3137dfffa0f197133918069cc2611316ff970e67b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fr8kp","generateName":"daemon-set-","namespace":"daemonsets-4294","uid":"f896dd53-f6db-4cc3-93c2-c064f0bdeba2","resourceVersion":"28800","creationTimestamp":"2021-11-16T19:57:51Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9352d52f1f0e2744e599b5de5eeacb2466fb7ebab50d94848317ba05d7a7a1c2","cni.projectcalico.org/podIP":"172.30.184.213/32","cni.projectcalico.org/podIPs":"172.30.184.213/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"57bb1319-0fe7-46fc-b607-cf44db1232bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57bb1319-0fe7-46fc-b607-cf44db1232bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.184.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z77cb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z77cb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.193.87.28","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.193.87.28"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"}],"hostIP":"10.193.87.28","podIP":"172.30.184.213","podIPs":[{"ip":"172.30.184.213"}],"startTime":"2021-11-16T19:57:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-11-16T19:57:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://4f5fdfc91e3c263cfe358e99bd1444e55fc3641047295443a139269c283efc79","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qmzq6","generateName":"daemon-set-","namespace":"daemonsets-4294","uid":"7af4764f-8490-4a91-b135-510231de07aa","resourceVersion":"28807","creationTimestamp":"2021-11-16T19:57:51Z","deletionTimestamp":"2021-11-16T19:58:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"023da2e7256a6a924533aa1dcd20567bcb645990bed8116cf274727a8a72b3bc","cni.projectcalico.org/podIP":"172.30.148.141/32","cni.projectcalico.org/podIPs":"172.30.148.141/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"57bb1319-0fe7-46fc-b607-cf44db1232bc","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57bb1319-0fe7-46fc-b607-cf44db1232bc\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2021-11-16T19:57:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.148.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qlrmt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qlrmt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.193.87.24","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.193.87.24"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2021-11-16T19:57:51Z"}],"hostIP":"10.193.87.24","podIP":"172.30.148.141","podIPs":[{"ip":"172.30.148.141"}],"startTime":"2021-11-16T19:57:51Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2021-11-16T19:57:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://31ba25711c278afb3b5927c260536724bf5eb52f386968f15a0e13a24ab235fe","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:57:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4294" for this suite.
â€¢{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":94,"skipped":1987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:57:55.606: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-8cb2ab0c-81ce-4446-9df1-1ceded19cb63 in namespace container-probe-6883
Nov 16 19:57:59.928: INFO: Started pod liveness-8cb2ab0c-81ce-4446-9df1-1ceded19cb63 in namespace container-probe-6883
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 19:57:59.947: INFO: Initial restart count of pod liveness-8cb2ab0c-81ce-4446-9df1-1ceded19cb63 is 0
Nov 16 19:58:18.151: INFO: Restart count of pod container-probe-6883/liveness-8cb2ab0c-81ce-4446-9df1-1ceded19cb63 is now 1 (18.204043939s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:58:18.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6883" for this suite.

â€¢ [SLOW TEST:22.628 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":2010,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:58:18.236: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 19:58:22.586: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:58:22.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7551" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:58:22.686: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-1748
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1748
STEP: Deleting pre-stop pod
Nov 16 19:58:36.195: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:58:36.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1748" for this suite.

â€¢ [SLOW TEST:13.599 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":97,"skipped":2118,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:58:36.286: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 16 19:58:36.582: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:58:38.601: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:58:40.602: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 16 19:58:40.651: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:58:42.678: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:58:44.673: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 19:58:44.836: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 19:58:44.851: INFO: Pod pod-with-poststart-http-hook still exists
Nov 16 19:58:46.852: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 16 19:58:46.873: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:58:46.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9645" for this suite.

â€¢ [SLOW TEST:10.628 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:58:46.915: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Nov 16 19:58:51.229: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8139 PodName:var-expansion-51a5ae17-a92c-4876-af01-6084952d52d0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:58:51.229: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: test for file in mounted path
Nov 16 19:58:51.489: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8139 PodName:var-expansion-51a5ae17-a92c-4876-af01-6084952d52d0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 19:58:51.489: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: updating the annotation value
Nov 16 19:58:52.217: INFO: Successfully updated pod "var-expansion-51a5ae17-a92c-4876-af01-6084952d52d0"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Nov 16 19:58:52.233: INFO: Deleting pod "var-expansion-51a5ae17-a92c-4876-af01-6084952d52d0" in namespace "var-expansion-8139"
Nov 16 19:58:52.259: INFO: Wait up to 5m0s for pod "var-expansion-51a5ae17-a92c-4876-af01-6084952d52d0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:24.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8139" for this suite.

â€¢ [SLOW TEST:37.428 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":99,"skipped":2168,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:24.344: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9923
STEP: creating service affinity-clusterip in namespace services-9923
STEP: creating replication controller affinity-clusterip in namespace services-9923
I1116 19:59:24.654508      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9923, replica count: 3
I1116 19:59:27.705870      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 19:59:27.729: INFO: Creating new exec pod
Nov 16 19:59:32.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9923 exec execpod-affinityxzr8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Nov 16 19:59:33.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Nov 16 19:59:33.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:59:33.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9923 exec execpod-affinityxzr8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.39.169 80'
Nov 16 19:59:33.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.39.169 80\nConnection to 172.21.39.169 80 port [tcp/http] succeeded!\n"
Nov 16 19:59:33.500: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 19:59:33.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-9923 exec execpod-affinityxzr8t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.39.169:80/ ; done'
Nov 16 19:59:33.967: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.39.169:80/\n"
Nov 16 19:59:33.967: INFO: stdout: "\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p\naffinity-clusterip-gll7p"
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Received response from host: affinity-clusterip-gll7p
Nov 16 19:59:33.968: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9923, will wait for the garbage collector to delete the pods
Nov 16 19:59:34.134: INFO: Deleting ReplicationController affinity-clusterip took: 25.104495ms
Nov 16 19:59:34.235: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.793425ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:37.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9923" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:12.985 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":100,"skipped":2174,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:37.330: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:37.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4757" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":101,"skipped":2194,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:37.615: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-4a84c5e2-a032-407c-b45d-412175f521f5
STEP: Creating a pod to test consume secrets
Nov 16 19:59:37.901: INFO: Waiting up to 5m0s for pod "pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04" in namespace "secrets-6009" to be "Succeeded or Failed"
Nov 16 19:59:37.914: INFO: Pod "pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04": Phase="Pending", Reason="", readiness=false. Elapsed: 12.662632ms
Nov 16 19:59:39.933: INFO: Pod "pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031290014s
Nov 16 19:59:41.958: INFO: Pod "pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056451051s
STEP: Saw pod success
Nov 16 19:59:41.958: INFO: Pod "pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04" satisfied condition "Succeeded or Failed"
Nov 16 19:59:41.972: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04 container secret-env-test: <nil>
STEP: delete the pod
Nov 16 19:59:42.109: INFO: Waiting for pod pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04 to disappear
Nov 16 19:59:42.122: INFO: Pod pod-secrets-93f79aa2-a85c-4e21-8043-bc3329a3ba04 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:42.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6009" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":2210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:42.158: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4164c7d0-f425-4653-bd2a-04a688e8ff80
STEP: Creating a pod to test consume configMaps
Nov 16 19:59:42.440: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe" in namespace "configmap-6912" to be "Succeeded or Failed"
Nov 16 19:59:42.456: INFO: Pod "pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.833861ms
Nov 16 19:59:44.476: INFO: Pod "pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe": Phase="Running", Reason="", readiness=true. Elapsed: 2.035960039s
Nov 16 19:59:46.497: INFO: Pod "pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056200074s
STEP: Saw pod success
Nov 16 19:59:46.497: INFO: Pod "pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe" satisfied condition "Succeeded or Failed"
Nov 16 19:59:46.521: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe container agnhost-container: <nil>
STEP: delete the pod
Nov 16 19:59:46.599: INFO: Waiting for pod pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe to disappear
Nov 16 19:59:46.614: INFO: Pod pod-configmaps-e4175e08-9c35-4262-95d8-dc1dfafaeefe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:46.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6912" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":2238,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:46.652: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Nov 16 19:59:48.982: INFO: running pods: 0 < 1
Nov 16 19:59:51.000: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:53.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3199" for this suite.

â€¢ [SLOW TEST:6.528 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":104,"skipped":2238,"failed":0}
S
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 19:59:53.536: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 19:59:53.571: INFO: waiting for watch events with expected annotations
Nov 16 19:59:53.571: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 19:59:53.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7147" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":105,"skipped":2239,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 19:59:53.699: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 16 19:59:53.978: INFO: The status of Pod labelsupdate05daebbb-d21c-42f2-9813-6dcd4318ed8b is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:59:56.000: INFO: The status of Pod labelsupdate05daebbb-d21c-42f2-9813-6dcd4318ed8b is Pending, waiting for it to be Running (with Ready = true)
Nov 16 19:59:57.996: INFO: The status of Pod labelsupdate05daebbb-d21c-42f2-9813-6dcd4318ed8b is Running (Ready = true)
Nov 16 19:59:58.581: INFO: Successfully updated pod "labelsupdate05daebbb-d21c-42f2-9813-6dcd4318ed8b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2188" for this suite.

â€¢ [SLOW TEST:7.011 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":2240,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:00.713: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 20:00:01.101: INFO: Number of nodes with available pods: 0
Nov 16 20:00:01.101: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:02.188: INFO: Number of nodes with available pods: 0
Nov 16 20:00:02.188: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:03.147: INFO: Number of nodes with available pods: 0
Nov 16 20:00:03.147: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:04.137: INFO: Number of nodes with available pods: 3
Nov 16 20:00:04.137: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Nov 16 20:00:04.163: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Nov 16 20:00:04.192: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Nov 16 20:00:04.199: INFO: Observed &DaemonSet event: ADDED
Nov 16 20:00:04.199: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.200: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.200: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.201: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.201: INFO: Found daemon set daemon-set in namespace daemonsets-2872 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 20:00:04.202: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Nov 16 20:00:04.243: INFO: Observed &DaemonSet event: ADDED
Nov 16 20:00:04.245: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.245: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.246: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.246: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.247: INFO: Observed daemon set daemon-set in namespace daemonsets-2872 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 20:00:04.248: INFO: Observed &DaemonSet event: MODIFIED
Nov 16 20:00:04.248: INFO: Found daemon set daemon-set in namespace daemonsets-2872 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Nov 16 20:00:04.248: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2872, will wait for the garbage collector to delete the pods
Nov 16 20:00:04.343: INFO: Deleting DaemonSet.extensions daemon-set took: 20.310236ms
Nov 16 20:00:04.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.983775ms
Nov 16 20:00:07.361: INFO: Number of nodes with available pods: 0
Nov 16 20:00:07.361: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 20:00:07.376: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29750"},"items":null}

Nov 16 20:00:07.391: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29750"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:07.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2872" for this suite.

â€¢ [SLOW TEST:6.779 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":107,"skipped":2244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 20:00:07.780: INFO: Waiting up to 5m0s for pod "pod-e92bbbc4-e370-45d5-9c56-77963118da31" in namespace "emptydir-2678" to be "Succeeded or Failed"
Nov 16 20:00:07.796: INFO: Pod "pod-e92bbbc4-e370-45d5-9c56-77963118da31": Phase="Pending", Reason="", readiness=false. Elapsed: 15.91357ms
Nov 16 20:00:09.821: INFO: Pod "pod-e92bbbc4-e370-45d5-9c56-77963118da31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040139439s
Nov 16 20:00:11.848: INFO: Pod "pod-e92bbbc4-e370-45d5-9c56-77963118da31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06774714s
STEP: Saw pod success
Nov 16 20:00:11.849: INFO: Pod "pod-e92bbbc4-e370-45d5-9c56-77963118da31" satisfied condition "Succeeded or Failed"
Nov 16 20:00:11.863: INFO: Trying to get logs from node 10.193.87.27 pod pod-e92bbbc4-e370-45d5-9c56-77963118da31 container test-container: <nil>
STEP: delete the pod
Nov 16 20:00:11.931: INFO: Waiting for pod pod-e92bbbc4-e370-45d5-9c56-77963118da31 to disappear
Nov 16 20:00:11.945: INFO: Pod pod-e92bbbc4-e370-45d5-9c56-77963118da31 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:11.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2678" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":108,"skipped":2281,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:11.988: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:00:12.327: INFO: Create a RollingUpdate DaemonSet
Nov 16 20:00:12.347: INFO: Check that daemon pods launch on every node of the cluster
Nov 16 20:00:12.391: INFO: Number of nodes with available pods: 0
Nov 16 20:00:12.391: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:13.435: INFO: Number of nodes with available pods: 0
Nov 16 20:00:13.435: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:14.449: INFO: Number of nodes with available pods: 0
Nov 16 20:00:14.450: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:00:15.426: INFO: Number of nodes with available pods: 2
Nov 16 20:00:15.426: INFO: Node 10.193.87.27 is running more than one daemon pod
Nov 16 20:00:16.456: INFO: Number of nodes with available pods: 3
Nov 16 20:00:16.456: INFO: Number of running nodes: 3, number of available pods: 3
Nov 16 20:00:16.456: INFO: Update the DaemonSet to trigger a rollout
Nov 16 20:00:16.488: INFO: Updating DaemonSet daemon-set
Nov 16 20:00:19.558: INFO: Roll back the DaemonSet before rollout is complete
Nov 16 20:00:19.589: INFO: Updating DaemonSet daemon-set
Nov 16 20:00:19.590: INFO: Make sure DaemonSet rollback is complete
Nov 16 20:00:19.608: INFO: Wrong image for pod: daemon-set-7qtxf. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Nov 16 20:00:19.608: INFO: Pod daemon-set-7qtxf is not available
Nov 16 20:00:24.656: INFO: Pod daemon-set-4jpz2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7169, will wait for the garbage collector to delete the pods
Nov 16 20:00:24.801: INFO: Deleting DaemonSet.extensions daemon-set took: 26.075497ms
Nov 16 20:00:24.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.798614ms
Nov 16 20:00:29.723: INFO: Number of nodes with available pods: 0
Nov 16 20:00:29.723: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 20:00:29.735: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29982"},"items":null}

Nov 16 20:00:29.750: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29982"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:29.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7169" for this suite.

â€¢ [SLOW TEST:17.868 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":109,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:30.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9767" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":110,"skipped":2316,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:30.468: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 16 20:00:30.743: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:00:32.765: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:00:34.772: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 16 20:00:34.829: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:00:36.852: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:00:38.847: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 16 20:00:38.891: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:00:38.907: INFO: Pod pod-with-prestop-http-hook still exists
Nov 16 20:00:40.907: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 16 20:00:40.928: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:40.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1247" for this suite.

â€¢ [SLOW TEST:10.529 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2336,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-7519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Nov 16 20:00:41.342: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:00:43.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7519" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":112,"skipped":2354,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:00:43.464: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-k7wd
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:00:43.773: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-k7wd" in namespace "subpath-4637" to be "Succeeded or Failed"
Nov 16 20:00:43.787: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.776747ms
Nov 16 20:00:45.805: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032072763s
Nov 16 20:00:47.823: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 4.049744199s
Nov 16 20:00:49.845: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 6.07174615s
Nov 16 20:00:51.863: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 8.090387839s
Nov 16 20:00:53.889: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 10.116377698s
Nov 16 20:00:55.917: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 12.143686851s
Nov 16 20:00:57.932: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 14.159203082s
Nov 16 20:00:59.952: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 16.179004969s
Nov 16 20:01:01.976: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 18.202996651s
Nov 16 20:01:04.003: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 20.229683342s
Nov 16 20:01:06.024: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Running", Reason="", readiness=true. Elapsed: 22.251199341s
Nov 16 20:01:08.043: INFO: Pod "pod-subpath-test-configmap-k7wd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.269594657s
STEP: Saw pod success
Nov 16 20:01:08.043: INFO: Pod "pod-subpath-test-configmap-k7wd" satisfied condition "Succeeded or Failed"
Nov 16 20:01:08.056: INFO: Trying to get logs from node 10.193.87.28 pod pod-subpath-test-configmap-k7wd container test-container-subpath-configmap-k7wd: <nil>
STEP: delete the pod
Nov 16 20:01:08.192: INFO: Waiting for pod pod-subpath-test-configmap-k7wd to disappear
Nov 16 20:01:08.208: INFO: Pod pod-subpath-test-configmap-k7wd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-k7wd
Nov 16 20:01:08.208: INFO: Deleting pod "pod-subpath-test-configmap-k7wd" in namespace "subpath-4637"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:01:08.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4637" for this suite.

â€¢ [SLOW TEST:24.801 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":113,"skipped":2364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:01:08.287: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 20:01:08.591: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 20:02:08.691: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:02:08.705: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Nov 16 20:02:11.112: INFO: found a healthy node: 10.193.87.27
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:02:29.435: INFO: pods created so far: [1 1 1]
Nov 16 20:02:29.435: INFO: length of pods created so far: 3
Nov 16 20:02:35.485: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:02:42.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1992" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:02:42.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3599" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:94.530 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":114,"skipped":2448,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:02:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:02:43.063: INFO: Creating ReplicaSet my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa
Nov 16 20:02:43.092: INFO: Pod name my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa: Found 0 pods out of 1
Nov 16 20:02:48.134: INFO: Pod name my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa: Found 1 pods out of 1
Nov 16 20:02:48.134: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa" is running
Nov 16 20:02:48.148: INFO: Pod "my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa-227dj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 20:02:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 20:02:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 20:02:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 20:02:43 +0000 UTC Reason: Message:}])
Nov 16 20:02:48.148: INFO: Trying to dial the pod
Nov 16 20:02:53.231: INFO: Controller my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa: Got expected result from replica 1 [my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa-227dj]: "my-hostname-basic-5b2d064d-fe28-4b42-aaa4-73625fd4effa-227dj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:02:53.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9144" for this suite.

â€¢ [SLOW TEST:10.461 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":115,"skipped":2450,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:02:53.285: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Nov 16 20:02:53.589: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:02:55.610: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:02:57.610: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:02:58.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9045" for this suite.

â€¢ [SLOW TEST:5.426 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":116,"skipped":2462,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:02:58.711: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Nov 16 20:02:58.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 create -f -'
Nov 16 20:02:59.238: INFO: stderr: ""
Nov 16 20:02:59.238: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 16 20:02:59.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 20:02:59.353: INFO: stderr: ""
Nov 16 20:02:59.353: INFO: stdout: "update-demo-nautilus-6mqxp update-demo-nautilus-tv5zp "
Nov 16 20:02:59.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods update-demo-nautilus-6mqxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 20:02:59.462: INFO: stderr: ""
Nov 16 20:02:59.462: INFO: stdout: ""
Nov 16 20:02:59.462: INFO: update-demo-nautilus-6mqxp is created but not running
Nov 16 20:03:04.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Nov 16 20:03:04.565: INFO: stderr: ""
Nov 16 20:03:04.565: INFO: stdout: "update-demo-nautilus-6mqxp update-demo-nautilus-tv5zp "
Nov 16 20:03:04.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods update-demo-nautilus-6mqxp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 20:03:04.686: INFO: stderr: ""
Nov 16 20:03:04.686: INFO: stdout: "true"
Nov 16 20:03:04.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods update-demo-nautilus-6mqxp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 20:03:04.910: INFO: stderr: ""
Nov 16 20:03:04.910: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 20:03:04.910: INFO: validating pod update-demo-nautilus-6mqxp
Nov 16 20:03:04.994: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:03:04.994: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:03:04.994: INFO: update-demo-nautilus-6mqxp is verified up and running
Nov 16 20:03:04.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods update-demo-nautilus-tv5zp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Nov 16 20:03:05.088: INFO: stderr: ""
Nov 16 20:03:05.088: INFO: stdout: "true"
Nov 16 20:03:05.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods update-demo-nautilus-tv5zp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Nov 16 20:03:05.212: INFO: stderr: ""
Nov 16 20:03:05.212: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Nov 16 20:03:05.212: INFO: validating pod update-demo-nautilus-tv5zp
Nov 16 20:03:05.254: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 16 20:03:05.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 16 20:03:05.254: INFO: update-demo-nautilus-tv5zp is verified up and running
STEP: using delete to clean up resources
Nov 16 20:03:05.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 delete --grace-period=0 --force -f -'
Nov 16 20:03:05.372: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 20:03:05.372: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 16 20:03:05.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get rc,svc -l name=update-demo --no-headers'
Nov 16 20:03:05.512: INFO: stderr: "No resources found in kubectl-7369 namespace.\n"
Nov 16 20:03:05.512: INFO: stdout: ""
Nov 16 20:03:05.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7369 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 16 20:03:05.615: INFO: stderr: ""
Nov 16 20:03:05.616: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:03:05.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7369" for this suite.

â€¢ [SLOW TEST:6.939 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":117,"skipped":2483,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:03:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7458
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-7458
Nov 16 20:03:05.954: INFO: Found 0 stateful pods, waiting for 1
Nov 16 20:03:15.983: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Nov 16 20:03:16.055: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Nov 16 20:03:16.082: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Nov 16 20:03:16.091: INFO: Observed &StatefulSet event: ADDED
Nov 16 20:03:16.091: INFO: Found Statefulset ss in namespace statefulset-7458 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 20:03:16.091: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Nov 16 20:03:16.091: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 20:03:16.119: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Nov 16 20:03:16.126: INFO: Observed &StatefulSet event: ADDED
Nov 16 20:03:16.126: INFO: Observed Statefulset ss in namespace statefulset-7458 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 20:03:16.127: INFO: Observed &StatefulSet event: MODIFIED
Nov 16 20:03:16.127: INFO: Found Statefulset ss in namespace statefulset-7458 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 20:03:16.128: INFO: Deleting all statefulset in ns statefulset-7458
Nov 16 20:03:16.143: INFO: Scaling statefulset ss to 0
Nov 16 20:03:26.217: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:03:26.231: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:03:26.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7458" for this suite.

â€¢ [SLOW TEST:20.668 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":118,"skipped":2494,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:03:26.319: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5594
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 16 20:03:26.577: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Nov 16 20:03:26.717: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:03:28.741: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:03:30.765: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:32.741: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:34.736: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:36.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:38.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:40.738: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:42.735: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:44.741: INFO: The status of Pod netserver-0 is Running (Ready = false)
Nov 16 20:03:46.738: INFO: The status of Pod netserver-0 is Running (Ready = true)
Nov 16 20:03:46.767: INFO: The status of Pod netserver-1 is Running (Ready = true)
Nov 16 20:03:46.802: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Nov 16 20:03:50.980: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Nov 16 20:03:50.980: INFO: Going to poll 172.30.148.140 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 20:03:50.995: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.148.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5594 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:03:50.995: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:03:52.259: INFO: Found all 1 expected endpoints: [netserver-0]
Nov 16 20:03:52.259: INFO: Going to poll 172.30.9.2 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 20:03:52.283: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.9.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5594 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:03:52.283: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:03:53.507: INFO: Found all 1 expected endpoints: [netserver-1]
Nov 16 20:03:53.507: INFO: Going to poll 172.30.184.226 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Nov 16 20:03:53.554: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.184.226 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5594 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:03:53.554: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:03:54.755: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:03:54.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5594" for this suite.

â€¢ [SLOW TEST:28.507 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2498,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:03:54.826: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6992
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-6992
Nov 16 20:03:55.130: INFO: Found 0 stateful pods, waiting for 1
Nov 16 20:04:05.166: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 20:04:05.299: INFO: Deleting all statefulset in ns statefulset-6992
Nov 16 20:04:05.310: INFO: Scaling statefulset ss to 0
Nov 16 20:04:15.391: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:04:15.406: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:15.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6992" for this suite.

â€¢ [SLOW TEST:20.662 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":120,"skipped":2501,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:15.490: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:04:15.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095" in namespace "projected-295" to be "Succeeded or Failed"
Nov 16 20:04:15.807: INFO: Pod "downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095": Phase="Pending", Reason="", readiness=false. Elapsed: 45.508886ms
Nov 16 20:04:17.824: INFO: Pod "downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063082361s
Nov 16 20:04:19.848: INFO: Pod "downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087317035s
STEP: Saw pod success
Nov 16 20:04:19.848: INFO: Pod "downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095" satisfied condition "Succeeded or Failed"
Nov 16 20:04:19.864: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095 container client-container: <nil>
STEP: delete the pod
Nov 16 20:04:20.002: INFO: Waiting for pod downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095 to disappear
Nov 16 20:04:20.015: INFO: Pod downwardapi-volume-6048dcc1-fc7b-4150-aeaa-9ad3e8768095 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:20.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-295" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2508,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:20.054: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:04:20.343: INFO: Waiting up to 5m0s for pod "downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280" in namespace "downward-api-3467" to be "Succeeded or Failed"
Nov 16 20:04:20.370: INFO: Pod "downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280": Phase="Pending", Reason="", readiness=false. Elapsed: 27.682255ms
Nov 16 20:04:22.393: INFO: Pod "downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050214519s
Nov 16 20:04:24.419: INFO: Pod "downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076137618s
STEP: Saw pod success
Nov 16 20:04:24.419: INFO: Pod "downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280" satisfied condition "Succeeded or Failed"
Nov 16 20:04:24.434: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280 container client-container: <nil>
STEP: delete the pod
Nov 16 20:04:24.534: INFO: Waiting for pod downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280 to disappear
Nov 16 20:04:24.551: INFO: Pod downwardapi-volume-040c6333-7ac6-4c9a-9c10-6ec6a1df0280 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:24.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3467" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:24.613: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:04:25.575: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:04:27.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689865, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689865, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689865, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689865, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:04:30.733: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:30.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9948" for this suite.
STEP: Destroying namespace "webhook-9948-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.358 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":123,"skipped":2549,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:30.971: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:31.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1663" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":124,"skipped":2550,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:31.492: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:38.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6731" for this suite.

â€¢ [SLOW TEST:7.354 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":125,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:38.847: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8392
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8392
STEP: Waiting until pod test-pod will start running in namespace statefulset-8392
STEP: Creating statefulset with conflicting port in namespace statefulset-8392
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8392
Nov 16 20:04:43.270: INFO: Observed stateful pod in namespace: statefulset-8392, name: ss-0, uid: 376cf3eb-e60e-4fc8-b9f5-64ef62325790, status phase: Pending. Waiting for statefulset controller to delete.
Nov 16 20:04:43.295: INFO: Observed stateful pod in namespace: statefulset-8392, name: ss-0, uid: 376cf3eb-e60e-4fc8-b9f5-64ef62325790, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 20:04:43.307: INFO: Observed stateful pod in namespace: statefulset-8392, name: ss-0, uid: 376cf3eb-e60e-4fc8-b9f5-64ef62325790, status phase: Failed. Waiting for statefulset controller to delete.
Nov 16 20:04:43.312: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8392
STEP: Removing pod with conflicting port in namespace statefulset-8392
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8392 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 20:04:47.454: INFO: Deleting all statefulset in ns statefulset-8392
Nov 16 20:04:47.467: INFO: Scaling statefulset ss to 0
Nov 16 20:04:57.542: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:04:57.553: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:04:57.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8392" for this suite.

â€¢ [SLOW TEST:18.790 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":126,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:04:57.638: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-239
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Nov 16 20:04:57.889: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Nov 16 20:05:13.537: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:05:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:05:34.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-239" for this suite.

â€¢ [SLOW TEST:36.454 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":127,"skipped":2617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:05:34.102: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:05:34.737: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:05:36.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689934, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689934, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689934, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772689934, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:05:39.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:05:40.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8971" for this suite.
STEP: Destroying namespace "webhook-8971-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.103 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":128,"skipped":2658,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:05:40.206: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:05:57.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4880" for this suite.

â€¢ [SLOW TEST:17.506 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":129,"skipped":2678,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:05:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:06:26.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-332" for this suite.

â€¢ [SLOW TEST:28.468 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":130,"skipped":2698,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:06:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6735
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:06:30.592: INFO: DNS probes using dns-test-ed25e6d1-7ac4-4141-bd26-e663b9a8c369 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:06:34.823: INFO: File wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:34.847: INFO: File jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:34.848: INFO: Lookups using dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 failed for: [wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local]

Nov 16 20:06:39.906: INFO: File jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:39.906: INFO: Lookups using dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 failed for: [jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local]

Nov 16 20:06:44.890: INFO: File jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:44.890: INFO: Lookups using dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 failed for: [jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local]

Nov 16 20:06:49.871: INFO: File wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:49.899: INFO: File jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:49.899: INFO: Lookups using dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 failed for: [wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local]

Nov 16 20:06:54.870: INFO: File wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:54.895: INFO: File jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local from pod  dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 16 20:06:54.896: INFO: Lookups using dns-6735/dns-test-a6e54c67-1423-4f89-b04b-cff239034801 failed for: [wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local]

Nov 16 20:06:59.896: INFO: DNS probes using dns-test-a6e54c67-1423-4f89-b04b-cff239034801 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6735.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6735.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:07:04.213: INFO: DNS probes using dns-test-28d29473-9f03-4f1c-b24a-d1d8f8cbb9ec succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:04.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6735" for this suite.

â€¢ [SLOW TEST:38.159 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":131,"skipped":2701,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:04.349: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 20:07:04.631: INFO: The status of Pod pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:07:06.657: INFO: The status of Pod pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:07:08.658: INFO: The status of Pod pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 20:07:09.230: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d"
Nov 16 20:07:09.230: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d" in namespace "pods-738" to be "terminated due to deadline exceeded"
Nov 16 20:07:09.243: INFO: Pod "pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d": Phase="Running", Reason="", readiness=true. Elapsed: 13.323431ms
Nov 16 20:07:11.264: INFO: Pod "pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.033602813s
Nov 16 20:07:11.264: INFO: Pod "pod-update-activedeadlineseconds-cca9ac55-38b6-47cc-921f-701e34402d9d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:11.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-738" for this suite.

â€¢ [SLOW TEST:6.958 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":132,"skipped":2706,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:11.319: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:07:11.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 create -f -'
Nov 16 20:07:12.136: INFO: stderr: ""
Nov 16 20:07:12.137: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Nov 16 20:07:12.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 create -f -'
Nov 16 20:07:12.457: INFO: stderr: ""
Nov 16 20:07:12.458: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 20:07:13.483: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:07:13.483: INFO: Found 0 / 1
Nov 16 20:07:14.476: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:07:14.476: INFO: Found 0 / 1
Nov 16 20:07:15.479: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:07:15.479: INFO: Found 1 / 1
Nov 16 20:07:15.479: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 20:07:15.492: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:07:15.493: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 20:07:15.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 describe pod agnhost-primary-j6zgw'
Nov 16 20:07:15.674: INFO: stderr: ""
Nov 16 20:07:15.674: INFO: stdout: "Name:         agnhost-primary-j6zgw\nNamespace:    kubectl-8879\nPriority:     0\nNode:         10.193.87.27/10.193.87.27\nStart Time:   Tue, 16 Nov 2021 20:07:12 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 882455d34a2155fbaebc6e06e203fef452151be0ee70fc41822670c54b82c36a\n              cni.projectcalico.org/podIP: 172.30.9.16/32\n              cni.projectcalico.org/podIPs: 172.30.9.16/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.9.16\nIPs:\n  IP:           172.30.9.16\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://cff3346a99c32b1f4833466ea18df340ed1bfab2a296fb7a1330ec3b39c29413\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 Nov 2021 20:07:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rqd4q (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rqd4q:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-8879/agnhost-primary-j6zgw to 10.193.87.27\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Nov 16 20:07:15.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 describe rc agnhost-primary'
Nov 16 20:07:15.828: INFO: stderr: ""
Nov 16 20:07:15.828: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8879\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-j6zgw\n"
Nov 16 20:07:15.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 describe service agnhost-primary'
Nov 16 20:07:16.014: INFO: stderr: ""
Nov 16 20:07:16.014: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8879\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.216.110\nIPs:               172.21.216.110\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.9.16:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 16 20:07:16.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 describe node 10.193.87.24'
Nov 16 20:07:16.270: INFO: stderr: ""
Nov 16 20:07:16.270: INFO: stdout: "Name:               10.193.87.24\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-tok\n                    failure-domain.beta.kubernetes.io/zone=tok05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=165.192.104.123\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.193.87.24\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=jp-tok\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-c69uu49t0k6l31sv41j0-kubee2epvgf-default-000003d9\n                    ibm-cloud.kubernetes.io/worker-pool-id=c69uu49t0k6l31sv41j0-4c04381\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.22.2_1528\n                    ibm-cloud.kubernetes.io/zone=tok05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.193.87.24\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2723062\n                    publicVLAN=2723060\n                    topology.kubernetes.io/region=jp-tok\n                    topology.kubernetes.io/zone=tok05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.193.87.24/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.148.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 Nov 2021 18:05:04 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.193.87.24\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 16 Nov 2021 20:07:15 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 16 Nov 2021 18:05:36 +0000   Tue, 16 Nov 2021 18:05:36 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 16 Nov 2021 20:06:53 +0000   Tue, 16 Nov 2021 18:05:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 16 Nov 2021 20:06:53 +0000   Tue, 16 Nov 2021 18:05:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 16 Nov 2021 20:06:53 +0000   Tue, 16 Nov 2021 18:05:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 16 Nov 2021 20:06:53 +0000   Tue, 16 Nov 2021 18:05:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.193.87.24\n  ExternalIP:  165.192.104.123\n  Hostname:    10.193.87.24\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16410264Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93986994917\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13618840Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 fab1f10bcdf14eda98b03c26e81083da\n  System UUID:                1A942520-523D-4531-446D-277A4D105578\n  Boot ID:                    6282a04c-7494-41c7-a83b-c5b35d4e5180\n  Kernel Version:             4.15.0-161-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.7\n  Kubelet Version:            v1.22.2+IKS\n  Kube-Proxy Version:         v1.22.2+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///c69uu49t0k6l31sv41j0/kube-c69uu49t0k6l31sv41j0-kubee2epvgf-default-000003d9\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                     test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         119m\n  ibm-system                  ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         117m\n  kube-system                 calico-node-5cnhh                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         122m\n  kube-system                 calico-typha-d5b48569-hcfb8                                250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         128m\n  kube-system                 coredns-b58d5f584-g92hx                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     113m\n  kube-system                 ibm-keepalived-watcher-bl7zr                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         122m\n  kube-system                 ibm-master-proxy-static-10.193.87.24                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      120m\n  kube-system                 konnectivity-agent-wtjm7                                   10m (0%)      0 (0%)      10Mi (0%)        50Mi (0%)      113m\n  kube-system                 public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         118m\n  sonobuoy                    sonobuoy-e2e-job-54c8cae784204424                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                655m (16%)     300m (7%)\n  memory             399890Ki (2%)  960800Ki (7%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Nov 16 20:07:16.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8879 describe namespace kubectl-8879'
Nov 16 20:07:16.429: INFO: stderr: ""
Nov 16 20:07:16.429: INFO: stdout: "Name:         kubectl-8879\nLabels:       e2e-framework=kubectl\n              e2e-run=06d4c95c-5f5c-47a0-b4ae-055bf8744e52\n              kubernetes.io/metadata.name=kubectl-8879\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:16.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8879" for this suite.

â€¢ [SLOW TEST:5.156 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1094
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":133,"skipped":2728,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 16 20:07:16.762: INFO: Waiting up to 5m0s for pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6" in namespace "downward-api-8570" to be "Succeeded or Failed"
Nov 16 20:07:16.779: INFO: Pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.302774ms
Nov 16 20:07:18.801: INFO: Pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038950182s
Nov 16 20:07:20.819: INFO: Pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056848251s
Nov 16 20:07:22.839: INFO: Pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076207775s
STEP: Saw pod success
Nov 16 20:07:22.839: INFO: Pod "downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6" satisfied condition "Succeeded or Failed"
Nov 16 20:07:22.856: INFO: Trying to get logs from node 10.193.87.28 pod downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6 container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:07:23.015: INFO: Waiting for pod downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6 to disappear
Nov 16 20:07:23.033: INFO: Pod downward-api-21c8a39b-4e33-4476-bef9-eea45c558aa6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:23.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8570" for this suite.

â€¢ [SLOW TEST:6.594 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:23.072: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-671/secret-test-1cd0e405-ec32-44cc-a13b-76c4135bd02a
STEP: Creating a pod to test consume secrets
Nov 16 20:07:23.374: INFO: Waiting up to 5m0s for pod "pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66" in namespace "secrets-671" to be "Succeeded or Failed"
Nov 16 20:07:23.392: INFO: Pod "pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66": Phase="Pending", Reason="", readiness=false. Elapsed: 17.762338ms
Nov 16 20:07:25.418: INFO: Pod "pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043581205s
Nov 16 20:07:27.440: INFO: Pod "pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065854564s
STEP: Saw pod success
Nov 16 20:07:27.440: INFO: Pod "pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66" satisfied condition "Succeeded or Failed"
Nov 16 20:07:27.454: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66 container env-test: <nil>
STEP: delete the pod
Nov 16 20:07:27.644: INFO: Waiting for pod pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66 to disappear
Nov 16 20:07:27.659: INFO: Pod pod-configmaps-db8724ce-a2d5-4690-aebb-a6d9e463ca66 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-671" for this suite.
â€¢{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2787,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:27.697: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 16 20:07:27.969: INFO: The status of Pod annotationupdate8ba04b82-5d3d-490e-941c-600cf0515c44 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:07:30.000: INFO: The status of Pod annotationupdate8ba04b82-5d3d-490e-941c-600cf0515c44 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:07:32.007: INFO: The status of Pod annotationupdate8ba04b82-5d3d-490e-941c-600cf0515c44 is Running (Ready = true)
Nov 16 20:07:32.610: INFO: Successfully updated pod "annotationupdate8ba04b82-5d3d-490e-941c-600cf0515c44"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:07:34.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3923" for this suite.

â€¢ [SLOW TEST:7.032 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2803,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:07:34.729: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 16 20:07:35.799: INFO: Pod name wrapped-volume-race-99808dae-2ad9-4ca5-81c7-4c2d0a9ac735: Found 0 pods out of 5
Nov 16 20:07:40.858: INFO: Pod name wrapped-volume-race-99808dae-2ad9-4ca5-81c7-4c2d0a9ac735: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-99808dae-2ad9-4ca5-81c7-4c2d0a9ac735 in namespace emptydir-wrapper-189, will wait for the garbage collector to delete the pods
Nov 16 20:07:41.033: INFO: Deleting ReplicationController wrapped-volume-race-99808dae-2ad9-4ca5-81c7-4c2d0a9ac735 took: 28.62198ms
Nov 16 20:07:41.133: INFO: Terminating ReplicationController wrapped-volume-race-99808dae-2ad9-4ca5-81c7-4c2d0a9ac735 pods took: 100.601667ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 20:07:45.320: INFO: Pod name wrapped-volume-race-1baefb3c-bcc8-4ca2-8b7f-e1cf71b2ce03: Found 0 pods out of 5
Nov 16 20:07:50.371: INFO: Pod name wrapped-volume-race-1baefb3c-bcc8-4ca2-8b7f-e1cf71b2ce03: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1baefb3c-bcc8-4ca2-8b7f-e1cf71b2ce03 in namespace emptydir-wrapper-189, will wait for the garbage collector to delete the pods
Nov 16 20:07:50.559: INFO: Deleting ReplicationController wrapped-volume-race-1baefb3c-bcc8-4ca2-8b7f-e1cf71b2ce03 took: 33.727145ms
Nov 16 20:07:50.659: INFO: Terminating ReplicationController wrapped-volume-race-1baefb3c-bcc8-4ca2-8b7f-e1cf71b2ce03 pods took: 100.46992ms
STEP: Creating RC which spawns configmap-volume pods
Nov 16 20:07:56.837: INFO: Pod name wrapped-volume-race-f7bf2ce6-58f7-4ee4-a067-ab699df7f794: Found 0 pods out of 5
Nov 16 20:08:01.873: INFO: Pod name wrapped-volume-race-f7bf2ce6-58f7-4ee4-a067-ab699df7f794: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f7bf2ce6-58f7-4ee4-a067-ab699df7f794 in namespace emptydir-wrapper-189, will wait for the garbage collector to delete the pods
Nov 16 20:08:02.054: INFO: Deleting ReplicationController wrapped-volume-race-f7bf2ce6-58f7-4ee4-a067-ab699df7f794 took: 34.56046ms
Nov 16 20:08:02.155: INFO: Terminating ReplicationController wrapped-volume-race-f7bf2ce6-58f7-4ee4-a067-ab699df7f794 pods took: 100.586352ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:08:07.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-189" for this suite.

â€¢ [SLOW TEST:33.205 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":137,"skipped":2806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:08:07.935: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-751, will wait for the garbage collector to delete the pods
Nov 16 20:08:12.299: INFO: Deleting Job.batch foo took: 20.283949ms
Nov 16 20:08:12.400: INFO: Terminating Job.batch foo pods took: 100.474602ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:08:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-751" for this suite.

â€¢ [SLOW TEST:36.826 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":138,"skipped":2834,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:08:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:08:45.041: INFO: Creating simple deployment test-new-deployment
Nov 16 20:08:45.111: INFO: deployment "test-new-deployment" doesn't have the required revision set
Nov 16 20:08:47.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690125, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690125, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690125, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690125, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:08:49.321: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-224  fca72576-fa43-46e4-9946-0a740e785e80 33069 3 2021-11-16 20:08:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2021-11-16 20:08:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:08:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f27438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-11-16 20:08:47 +0000 UTC,LastTransitionTime:2021-11-16 20:08:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-11-16 20:08:49 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:08:49.334: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-224  9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a 33066 3 2021-11-16 20:08:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fca72576-fa43-46e4-9946-0a740e785e80 0xc004f27857 0xc004f27858}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:08:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fca72576-fa43-46e4-9946-0a740e785e80\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:08:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f278e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:08:49.353: INFO: Pod "test-new-deployment-847dcfb7fb-7st75" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-7st75 test-new-deployment-847dcfb7fb- deployment-224  2882a1ee-bd5c-431f-aa3f-087f3d59d5a4 33045 0 2021-11-16 20:08:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:219c6d34214e59560c6f6d1577ac392fa5154b670667614208cad3b0cfc00eb0 cni.projectcalico.org/podIP:172.30.9.27/32 cni.projectcalico.org/podIPs:172.30.9.27/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a 0xc003684307 0xc003684308}] []  [{kube-controller-manager Update v1 2021-11-16 20:08:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 20:08:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 20:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc9pr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc9pr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.27,StartTime:2021-11-16 20:08:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 20:08:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://bc1b91ef1abb8ef4f5c892a772200a664052129f2c27425522d752a2c2000f4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 20:08:49.353: INFO: Pod "test-new-deployment-847dcfb7fb-j4scp" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-j4scp test-new-deployment-847dcfb7fb- deployment-224  4889cfff-1862-4e3f-89b4-b492f40d71cb 33074 0 2021-11-16 20:08:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a 0xc003684530 0xc003684531}] []  [{kube-controller-manager Update v1 2021-11-16 20:08:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbpcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbpcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 20:08:49.354: INFO: Pod "test-new-deployment-847dcfb7fb-jqsdl" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-jqsdl test-new-deployment-847dcfb7fb- deployment-224  bac16f64-86aa-4ad5-901e-95276afe50cd 33077 0 2021-11-16 20:08:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a 0xc0036846a0 0xc0036846a1}] []  [{kube-controller-manager Update v1 2021-11-16 20:08:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fgz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fgz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 20:08:49.355: INFO: Pod "test-new-deployment-847dcfb7fb-rfm78" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-rfm78 test-new-deployment-847dcfb7fb- deployment-224  68d65d1f-eed2-4a4b-aedd-1a27b47dea21 33067 0 2021-11-16 20:08:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a 0xc003684820 0xc003684821}] []  [{kube-controller-manager Update v1 2021-11-16 20:08:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6bff2e-154a-4ef5-9d69-290cf8cc6b8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2021-11-16 20:08:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcgt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcgt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.28,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:08:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.28,PodIP:,StartTime:2021-11-16 20:08:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:08:49.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-224" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":139,"skipped":2837,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:08:49.404: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:09:01.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1280" for this suite.

â€¢ [SLOW TEST:11.735 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":140,"skipped":2847,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:09:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-d2ba4f98-59fa-4f65-a0ca-796dc41f57b1
STEP: Creating a pod to test consume configMaps
Nov 16 20:09:01.450: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7" in namespace "projected-2349" to be "Succeeded or Failed"
Nov 16 20:09:01.464: INFO: Pod "pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.681861ms
Nov 16 20:09:03.489: INFO: Pod "pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7": Phase="Running", Reason="", readiness=true. Elapsed: 2.039408243s
Nov 16 20:09:05.508: INFO: Pod "pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058300648s
STEP: Saw pod success
Nov 16 20:09:05.508: INFO: Pod "pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7" satisfied condition "Succeeded or Failed"
Nov 16 20:09:05.523: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:09:05.647: INFO: Waiting for pod pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7 to disappear
Nov 16 20:09:05.664: INFO: Pod pod-projected-configmaps-99000288-0f9a-468e-8807-3fb6c33dbda7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:09:05.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2349" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2849,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:09:05.703: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Nov 16 20:09:06.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2091 create -f -'
Nov 16 20:09:06.364: INFO: stderr: ""
Nov 16 20:09:06.364: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 20:09:07.379: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:09:07.379: INFO: Found 0 / 1
Nov 16 20:09:08.383: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:09:08.383: INFO: Found 0 / 1
Nov 16 20:09:09.387: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:09:09.387: INFO: Found 1 / 1
Nov 16 20:09:09.387: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 16 20:09:09.402: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:09:09.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 20:09:09.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2091 patch pod agnhost-primary-wgdqv -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 16 20:09:09.587: INFO: stderr: ""
Nov 16 20:09:09.587: INFO: stdout: "pod/agnhost-primary-wgdqv patched\n"
STEP: checking annotations
Nov 16 20:09:09.602: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 20:09:09.602: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:09:09.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2091" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":142,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:09:09.647: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Nov 16 20:09:30.397: INFO: EndpointSlice for Service endpointslice-1837/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:09:40.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1837" for this suite.

â€¢ [SLOW TEST:30.865 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":143,"skipped":2897,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:09:40.513: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:09:41.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78988fc6cd\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:09:43.305: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690181, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:09:46.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:09:46.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1411" for this suite.
STEP: Destroying namespace "webhook-1411-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.448 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":144,"skipped":2902,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:09:46.961: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 in namespace container-probe-8477
Nov 16 20:09:51.302: INFO: Started pod liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 in namespace container-probe-8477
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 20:09:51.319: INFO: Initial restart count of pod liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is 0
Nov 16 20:10:09.537: INFO: Restart count of pod container-probe-8477/liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is now 1 (18.218498456s elapsed)
Nov 16 20:10:29.781: INFO: Restart count of pod container-probe-8477/liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is now 2 (38.461656893s elapsed)
Nov 16 20:10:47.977: INFO: Restart count of pod container-probe-8477/liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is now 3 (56.657668606s elapsed)
Nov 16 20:11:08.194: INFO: Restart count of pod container-probe-8477/liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is now 4 (1m16.87536604s elapsed)
Nov 16 20:12:23.084: INFO: Restart count of pod container-probe-8477/liveness-0120bd3b-4381-4129-b703-0e8cd3c4aa85 is now 5 (2m31.76493049s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:12:23.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8477" for this suite.

â€¢ [SLOW TEST:156.258 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:12:23.219: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3739
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:12:23.495: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:12:31.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3739" for this suite.

â€¢ [SLOW TEST:8.113 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":146,"skipped":2944,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:12:31.333: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:12:42.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7287" for this suite.

â€¢ [SLOW TEST:11.551 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":147,"skipped":2948,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:12:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:12:43.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca" in namespace "downward-api-1245" to be "Succeeded or Failed"
Nov 16 20:12:43.187: INFO: Pod "downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca": Phase="Pending", Reason="", readiness=false. Elapsed: 15.561093ms
Nov 16 20:12:45.210: INFO: Pod "downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038381762s
Nov 16 20:12:47.239: INFO: Pod "downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067557816s
STEP: Saw pod success
Nov 16 20:12:47.239: INFO: Pod "downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca" satisfied condition "Succeeded or Failed"
Nov 16 20:12:47.253: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca container client-container: <nil>
STEP: delete the pod
Nov 16 20:12:47.407: INFO: Waiting for pod downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca to disappear
Nov 16 20:12:47.423: INFO: Pod downwardapi-volume-e4a984aa-0d0f-43c8-875a-ba719fd1adca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:12:47.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1245" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2969,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:12:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3496" for this suite.

â€¢ [SLOW TEST:13.614 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":149,"skipped":2974,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:01.086: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 16 20:13:01.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7092  3410e673-8c0e-4278-b157-693b00fd252a 34025 0 2021-11-16 20:13:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-11-16 20:13:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 20:13:01.444: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7092  3410e673-8c0e-4278-b157-693b00fd252a 34026 0 2021-11-16 20:13:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-11-16 20:13:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:01.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7092" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":150,"skipped":2981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:12.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4960" for this suite.

â€¢ [SLOW TEST:11.502 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":151,"skipped":3005,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:12.998: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:13.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4382" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":152,"skipped":3016,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:13.422: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:17.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9466" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":3029,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:17.908: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 16 20:13:18.207: INFO: The status of Pod labelsupdated58e3308-b02c-497c-8922-be7684daf94e is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:13:20.237: INFO: The status of Pod labelsupdated58e3308-b02c-497c-8922-be7684daf94e is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:13:22.227: INFO: The status of Pod labelsupdated58e3308-b02c-497c-8922-be7684daf94e is Running (Ready = true)
Nov 16 20:13:22.819: INFO: Successfully updated pod "labelsupdated58e3308-b02c-497c-8922-be7684daf94e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:24.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4716" for this suite.

â€¢ [SLOW TEST:7.024 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":3033,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:24.933: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:13:25.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5" in namespace "downward-api-6268" to be "Succeeded or Failed"
Nov 16 20:13:25.225: INFO: Pod "downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.819766ms
Nov 16 20:13:27.249: INFO: Pod "downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04116519s
Nov 16 20:13:29.267: INFO: Pod "downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059452172s
STEP: Saw pod success
Nov 16 20:13:29.267: INFO: Pod "downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5" satisfied condition "Succeeded or Failed"
Nov 16 20:13:29.281: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5 container client-container: <nil>
STEP: delete the pod
Nov 16 20:13:29.356: INFO: Waiting for pod downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5 to disappear
Nov 16 20:13:29.371: INFO: Pod downwardapi-volume-fde74142-7d08-409e-bd9d-7804172e2ef5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:29.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6268" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":3053,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:29.417: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-ee7bf619-e014-4212-ad2c-f59ff694dfc4
STEP: Creating a pod to test consume secrets
Nov 16 20:13:29.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90" in namespace "projected-3307" to be "Succeeded or Failed"
Nov 16 20:13:29.724: INFO: Pod "pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90": Phase="Pending", Reason="", readiness=false. Elapsed: 29.870103ms
Nov 16 20:13:31.745: INFO: Pod "pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051014393s
Nov 16 20:13:33.766: INFO: Pod "pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071556021s
STEP: Saw pod success
Nov 16 20:13:33.766: INFO: Pod "pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90" satisfied condition "Succeeded or Failed"
Nov 16 20:13:33.783: INFO: Trying to get logs from node 10.193.87.28 pod pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:13:33.919: INFO: Waiting for pod pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90 to disappear
Nov 16 20:13:33.938: INFO: Pod pod-projected-secrets-2aa28d19-a8c1-403e-ba5b-50d3e2d9df90 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:33.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3307" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":3057,"failed":0}

------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:33.988: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:13:34.308: INFO: The status of Pod busybox-host-aliasesd361a67d-d172-4ae4-9f38-f73cabbccf21 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:13:36.329: INFO: The status of Pod busybox-host-aliasesd361a67d-d172-4ae4-9f38-f73cabbccf21 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:13:38.331: INFO: The status of Pod busybox-host-aliasesd361a67d-d172-4ae4-9f38-f73cabbccf21 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:38.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4575" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":3057,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:38.434: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:13:40.762: INFO: Deleting pod "var-expansion-cc04f1e1-03f9-4140-87ab-a1a9ca8cae92" in namespace "var-expansion-6491"
Nov 16 20:13:40.800: INFO: Wait up to 5m0s for pod "var-expansion-cc04f1e1-03f9-4140-87ab-a1a9ca8cae92" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:44.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6491" for this suite.

â€¢ [SLOW TEST:6.442 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":158,"skipped":3057,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:44.881: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-68e644f4-4c46-4ba1-9e8d-2fb13e5b62b2
STEP: Creating a pod to test consume configMaps
Nov 16 20:13:45.164: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31" in namespace "projected-8571" to be "Succeeded or Failed"
Nov 16 20:13:45.180: INFO: Pod "pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31": Phase="Pending", Reason="", readiness=false. Elapsed: 16.168672ms
Nov 16 20:13:47.199: INFO: Pod "pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035029814s
Nov 16 20:13:49.220: INFO: Pod "pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055962234s
STEP: Saw pod success
Nov 16 20:13:49.220: INFO: Pod "pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31" satisfied condition "Succeeded or Failed"
Nov 16 20:13:49.235: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:13:49.312: INFO: Waiting for pod pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31 to disappear
Nov 16 20:13:49.327: INFO: Pod pod-projected-configmaps-93e5ddda-5c84-4721-a65a-4f5ed3752f31 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:49.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8571" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":3059,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Nov 16 20:13:49.632: INFO: Waiting up to 5m0s for pod "client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40" in namespace "containers-7343" to be "Succeeded or Failed"
Nov 16 20:13:49.648: INFO: Pod "client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40": Phase="Pending", Reason="", readiness=false. Elapsed: 15.39916ms
Nov 16 20:13:51.670: INFO: Pod "client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038009741s
Nov 16 20:13:53.695: INFO: Pod "client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062838956s
STEP: Saw pod success
Nov 16 20:13:53.695: INFO: Pod "client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40" satisfied condition "Succeeded or Failed"
Nov 16 20:13:53.709: INFO: Trying to get logs from node 10.193.87.27 pod client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:13:53.779: INFO: Waiting for pod client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40 to disappear
Nov 16 20:13:53.794: INFO: Pod client-containers-91c4597e-0d6f-4bcb-badc-803f28217f40 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:53.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7343" for this suite.
â€¢{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":3069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:53.838: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 16 20:13:54.091: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:13:59.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7033" for this suite.

â€¢ [SLOW TEST:5.300 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":161,"skipped":3091,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:13:59.139: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9650
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:13:59.373: INFO: Creating pod...
Nov 16 20:13:59.413: INFO: Pod Quantity: 1 Status: Pending
Nov 16 20:14:00.434: INFO: Pod Quantity: 1 Status: Pending
Nov 16 20:14:01.452: INFO: Pod Quantity: 1 Status: Pending
Nov 16 20:14:02.438: INFO: Pod Status: Running
Nov 16 20:14:02.439: INFO: Creating service...
Nov 16 20:14:02.479: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/DELETE
Nov 16 20:14:02.539: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 16 20:14:02.540: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/GET
Nov 16 20:14:02.569: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 16 20:14:02.569: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/HEAD
Nov 16 20:14:02.594: INFO: http.Client request:HEAD | StatusCode:200
Nov 16 20:14:02.594: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/OPTIONS
Nov 16 20:14:02.617: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 16 20:14:02.617: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/PATCH
Nov 16 20:14:02.635: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 16 20:14:02.635: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/POST
Nov 16 20:14:02.654: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 16 20:14:02.655: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/PUT
Nov 16 20:14:02.675: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Nov 16 20:14:02.675: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/DELETE
Nov 16 20:14:02.737: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Nov 16 20:14:02.737: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/GET
Nov 16 20:14:02.763: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Nov 16 20:14:02.763: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/HEAD
Nov 16 20:14:02.789: INFO: http.Client request:HEAD | StatusCode:200
Nov 16 20:14:02.789: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/OPTIONS
Nov 16 20:14:02.815: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Nov 16 20:14:02.815: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/PATCH
Nov 16 20:14:02.838: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Nov 16 20:14:02.838: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/POST
Nov 16 20:14:02.861: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Nov 16 20:14:02.861: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/PUT
Nov 16 20:14:02.884: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:14:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9650" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":162,"skipped":3098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:14:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6883
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:14:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1648" for this suite.
STEP: Destroying namespace "nsdeletetest-1773" for this suite.
Nov 16 20:14:19.832: INFO: Namespace nsdeletetest-1773 was already deleted
STEP: Destroying namespace "nsdeletetest-6883" for this suite.

â€¢ [SLOW TEST:16.925 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":163,"skipped":3120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:14:19.858: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:14:20.136: INFO: The status of Pod server-envvars-451386e1-715b-4462-a51e-55a7897acda9 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:22.147: INFO: The status of Pod server-envvars-451386e1-715b-4462-a51e-55a7897acda9 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:24.147: INFO: The status of Pod server-envvars-451386e1-715b-4462-a51e-55a7897acda9 is Running (Ready = true)
Nov 16 20:14:24.191: INFO: Waiting up to 5m0s for pod "client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421" in namespace "pods-7333" to be "Succeeded or Failed"
Nov 16 20:14:24.199: INFO: Pod "client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421": Phase="Pending", Reason="", readiness=false. Elapsed: 7.917755ms
Nov 16 20:14:26.212: INFO: Pod "client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020987534s
Nov 16 20:14:28.226: INFO: Pod "client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035263447s
STEP: Saw pod success
Nov 16 20:14:28.227: INFO: Pod "client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421" satisfied condition "Succeeded or Failed"
Nov 16 20:14:28.235: INFO: Trying to get logs from node 10.193.87.28 pod client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421 container env3cont: <nil>
STEP: delete the pod
Nov 16 20:14:28.312: INFO: Waiting for pod client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421 to disappear
Nov 16 20:14:28.321: INFO: Pod client-envvars-e36ad69e-5f9f-43ad-a370-492f95eab421 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:14:28.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7333" for this suite.

â€¢ [SLOW TEST:8.505 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3182,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:14:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:14:29.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:14:31.252: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690469, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690469, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690469, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690469, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:14:34.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:14:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4236" for this suite.
STEP: Destroying namespace "webhook-4236-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":165,"skipped":3186,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:14:39.016: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3930
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3930 to expose endpoints map[]
Nov 16 20:14:39.309: INFO: successfully validated that service endpoint-test2 in namespace services-3930 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3930
Nov 16 20:14:39.338: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:41.352: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:43.351: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3930 to expose endpoints map[pod1:[80]]
Nov 16 20:14:43.389: INFO: successfully validated that service endpoint-test2 in namespace services-3930 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Nov 16 20:14:43.389: INFO: Creating new exec pod
Nov 16 20:14:46.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 20:14:46.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:46.833: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:14:46.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.224.190 80'
Nov 16 20:14:47.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.224.190 80\nConnection to 172.21.224.190 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:47.161: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3930
Nov 16 20:14:47.186: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:49.197: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:14:51.198: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3930 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 16 20:14:51.246: INFO: successfully validated that service endpoint-test2 in namespace services-3930 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Nov 16 20:14:52.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 20:14:52.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:52.564: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:14:52.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.224.190 80'
Nov 16 20:14:52.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.224.190 80\nConnection to 172.21.224.190 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:52.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3930
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3930 to expose endpoints map[pod2:[80]]
Nov 16 20:14:54.040: INFO: successfully validated that service endpoint-test2 in namespace services-3930 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Nov 16 20:14:55.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Nov 16 20:14:55.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:55.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:14:55.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3930 exec execpodnpsrf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.224.190 80'
Nov 16 20:14:55.701: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 172.21.224.190 80\nConnection to 172.21.224.190 80 port [tcp/http] succeeded!\n"
Nov 16 20:14:55.701: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3930
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3930 to expose endpoints map[]
Nov 16 20:14:55.754: INFO: successfully validated that service endpoint-test2 in namespace services-3930 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:14:55.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3930" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:16.819 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":166,"skipped":3206,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:14:55.835: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7913
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:14:56.071: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 20:14:59.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-7913 --namespace=crd-publish-openapi-7913 create -f -'
Nov 16 20:15:00.212: INFO: stderr: ""
Nov 16 20:15:00.212: INFO: stdout: "e2e-test-crd-publish-openapi-7958-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 20:15:00.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-7913 --namespace=crd-publish-openapi-7913 delete e2e-test-crd-publish-openapi-7958-crds test-cr'
Nov 16 20:15:00.411: INFO: stderr: ""
Nov 16 20:15:00.411: INFO: stdout: "e2e-test-crd-publish-openapi-7958-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Nov 16 20:15:00.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-7913 --namespace=crd-publish-openapi-7913 apply -f -'
Nov 16 20:15:00.695: INFO: stderr: ""
Nov 16 20:15:00.695: INFO: stdout: "e2e-test-crd-publish-openapi-7958-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Nov 16 20:15:00.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-7913 --namespace=crd-publish-openapi-7913 delete e2e-test-crd-publish-openapi-7958-crds test-cr'
Nov 16 20:15:00.805: INFO: stderr: ""
Nov 16 20:15:00.805: INFO: stdout: "e2e-test-crd-publish-openapi-7958-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 20:15:00.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-7913 explain e2e-test-crd-publish-openapi-7958-crds'
Nov 16 20:15:01.084: INFO: stderr: ""
Nov 16 20:15:01.084: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7958-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:15:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7913" for this suite.

â€¢ [SLOW TEST:8.975 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":167,"skipped":3213,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:15:04.813: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-pvrf
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:15:05.138: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pvrf" in namespace "subpath-5938" to be "Succeeded or Failed"
Nov 16 20:15:05.154: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.162554ms
Nov 16 20:15:07.172: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.033534922s
Nov 16 20:15:09.196: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 4.057313352s
Nov 16 20:15:11.219: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 6.080071528s
Nov 16 20:15:13.238: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 8.09964795s
Nov 16 20:15:15.265: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 10.126061342s
Nov 16 20:15:17.283: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 12.143851693s
Nov 16 20:15:19.304: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 14.165646832s
Nov 16 20:15:21.324: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 16.185398332s
Nov 16 20:15:23.345: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 18.205891775s
Nov 16 20:15:25.364: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Running", Reason="", readiness=true. Elapsed: 20.224969755s
Nov 16 20:15:27.388: INFO: Pod "pod-subpath-test-configmap-pvrf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.248732042s
STEP: Saw pod success
Nov 16 20:15:27.388: INFO: Pod "pod-subpath-test-configmap-pvrf" satisfied condition "Succeeded or Failed"
Nov 16 20:15:27.404: INFO: Trying to get logs from node 10.193.87.27 pod pod-subpath-test-configmap-pvrf container test-container-subpath-configmap-pvrf: <nil>
STEP: delete the pod
Nov 16 20:15:27.563: INFO: Waiting for pod pod-subpath-test-configmap-pvrf to disappear
Nov 16 20:15:27.575: INFO: Pod pod-subpath-test-configmap-pvrf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pvrf
Nov 16 20:15:27.575: INFO: Deleting pod "pod-subpath-test-configmap-pvrf" in namespace "subpath-5938"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:15:27.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5938" for this suite.

â€¢ [SLOW TEST:22.815 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":168,"skipped":3224,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:15:27.632: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:16:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6939" for this suite.

â€¢ [SLOW TEST:60.341 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:16:27.983: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:16:28.828: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:16:30.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690588, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690588, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690588, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690588, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:16:33.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Nov 16 20:16:38.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=webhook-6681 attach --namespace=webhook-6681 to-be-attached-pod -i -c=container1'
Nov 16 20:16:38.318: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:16:38.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6681" for this suite.
STEP: Destroying namespace "webhook-6681-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":170,"skipped":3298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:16:38.532: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Nov 16 20:16:38.818: INFO: The status of Pod annotationupdate8b2e43b7-e709-4ec0-9cbd-72bf304c8f1b is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:16:40.839: INFO: The status of Pod annotationupdate8b2e43b7-e709-4ec0-9cbd-72bf304c8f1b is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:16:42.837: INFO: The status of Pod annotationupdate8b2e43b7-e709-4ec0-9cbd-72bf304c8f1b is Running (Ready = true)
Nov 16 20:16:43.444: INFO: Successfully updated pod "annotationupdate8b2e43b7-e709-4ec0-9cbd-72bf304c8f1b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:16:45.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7925" for this suite.

â€¢ [SLOW TEST:7.028 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3320,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:16:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1116 20:17:25.956961      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Nov 16 20:17:25.957: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Nov 16 20:17:25.957: INFO: Deleting pod "simpletest.rc-5h6r8" in namespace "gc-6836"
Nov 16 20:17:26.008: INFO: Deleting pod "simpletest.rc-7fll7" in namespace "gc-6836"
Nov 16 20:17:26.053: INFO: Deleting pod "simpletest.rc-hfbgj" in namespace "gc-6836"
Nov 16 20:17:26.094: INFO: Deleting pod "simpletest.rc-k6kvb" in namespace "gc-6836"
Nov 16 20:17:26.137: INFO: Deleting pod "simpletest.rc-kpfnw" in namespace "gc-6836"
Nov 16 20:17:26.180: INFO: Deleting pod "simpletest.rc-nrj5v" in namespace "gc-6836"
Nov 16 20:17:26.226: INFO: Deleting pod "simpletest.rc-rpwnx" in namespace "gc-6836"
Nov 16 20:17:26.271: INFO: Deleting pod "simpletest.rc-tz8gq" in namespace "gc-6836"
Nov 16 20:17:26.322: INFO: Deleting pod "simpletest.rc-v4k5n" in namespace "gc-6836"
Nov 16 20:17:26.380: INFO: Deleting pod "simpletest.rc-wfpbb" in namespace "gc-6836"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:17:26.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6836" for this suite.

â€¢ [SLOW TEST:40.902 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":172,"skipped":3333,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:17:26.463: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:17:26.741: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2" in namespace "security-context-test-2523" to be "Succeeded or Failed"
Nov 16 20:17:26.753: INFO: Pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.548174ms
Nov 16 20:17:28.770: INFO: Pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028762083s
Nov 16 20:17:30.792: INFO: Pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050745538s
Nov 16 20:17:30.792: INFO: Pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2" satisfied condition "Succeeded or Failed"
Nov 16 20:17:30.822: INFO: Got logs for pod "busybox-privileged-false-e56c3044-b702-4077-93ee-63f385288bd2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:17:30.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2523" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3342,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:17:30.866: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-5940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:17:31.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5940" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":174,"skipped":3345,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:17:31.295: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Nov 16 20:17:31.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Nov 16 20:17:31.701: INFO: stderr: ""
Nov 16 20:17:31.701: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Nov 16 20:17:31.701: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Nov 16 20:17:31.701: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2088" to be "running and ready, or succeeded"
Nov 16 20:17:31.720: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.489934ms
Nov 16 20:17:33.743: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042357849s
Nov 16 20:17:35.764: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.063345088s
Nov 16 20:17:35.765: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Nov 16 20:17:35.765: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Nov 16 20:17:35.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator'
Nov 16 20:17:35.947: INFO: stderr: ""
Nov 16 20:17:35.947: INFO: stdout: "I1116 20:17:33.477757       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jm9b 208\nI1116 20:17:33.678073       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/24mw 566\nI1116 20:17:33.878120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/fwr 419\nI1116 20:17:34.078649       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/lf8h 334\nI1116 20:17:34.278037       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/48kt 404\nI1116 20:17:34.478569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/2gdv 443\nI1116 20:17:34.678008       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/55j 585\nI1116 20:17:34.880055       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2pl7 378\nI1116 20:17:35.078601       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/vnq 502\nI1116 20:17:35.277986       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/484 479\nI1116 20:17:35.478467       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/tr6 285\nI1116 20:17:35.677856       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2xf 303\nI1116 20:17:35.878385       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/g9s4 282\n"
STEP: limiting log lines
Nov 16 20:17:35.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator --tail=1'
Nov 16 20:17:36.178: INFO: stderr: ""
Nov 16 20:17:36.178: INFO: stdout: "I1116 20:17:36.079030       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/7wv 403\n"
Nov 16 20:17:36.178: INFO: got output "I1116 20:17:36.079030       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/7wv 403\n"
STEP: limiting log bytes
Nov 16 20:17:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator --limit-bytes=1'
Nov 16 20:17:36.314: INFO: stderr: ""
Nov 16 20:17:36.314: INFO: stdout: "I"
Nov 16 20:17:36.314: INFO: got output "I"
STEP: exposing timestamps
Nov 16 20:17:36.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator --tail=1 --timestamps'
Nov 16 20:17:36.453: INFO: stderr: ""
Nov 16 20:17:36.453: INFO: stdout: "2021-11-16T20:17:36.278769193Z I1116 20:17:36.278460       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/q4v 592\n"
Nov 16 20:17:36.453: INFO: got output "2021-11-16T20:17:36.278769193Z I1116 20:17:36.278460       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/q4v 592\n"
STEP: restricting to a time range
Nov 16 20:17:38.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator --since=1s'
Nov 16 20:17:39.123: INFO: stderr: ""
Nov 16 20:17:39.123: INFO: stdout: "I1116 20:17:38.278365       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/pk6l 290\nI1116 20:17:38.478112       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/s9hp 396\nI1116 20:17:38.678641       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/qgk 563\nI1116 20:17:38.877951       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/twdn 552\nI1116 20:17:39.078816       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/j9z 229\n"
Nov 16 20:17:39.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 logs logs-generator logs-generator --since=24h'
Nov 16 20:17:39.278: INFO: stderr: ""
Nov 16 20:17:39.278: INFO: stdout: "I1116 20:17:33.477757       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/jm9b 208\nI1116 20:17:33.678073       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/24mw 566\nI1116 20:17:33.878120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/fwr 419\nI1116 20:17:34.078649       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/lf8h 334\nI1116 20:17:34.278037       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/48kt 404\nI1116 20:17:34.478569       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/2gdv 443\nI1116 20:17:34.678008       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/55j 585\nI1116 20:17:34.880055       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2pl7 378\nI1116 20:17:35.078601       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/vnq 502\nI1116 20:17:35.277986       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/484 479\nI1116 20:17:35.478467       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/tr6 285\nI1116 20:17:35.677856       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2xf 303\nI1116 20:17:35.878385       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/g9s4 282\nI1116 20:17:36.079030       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/7wv 403\nI1116 20:17:36.278460       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/q4v 592\nI1116 20:17:36.477864       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/2hx 429\nI1116 20:17:36.678600       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/s5dp 572\nI1116 20:17:36.877803       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/8dr5 592\nI1116 20:17:37.078320       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/z5w 239\nI1116 20:17:37.278869       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/mw5d 201\nI1116 20:17:37.478381       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/mjf 480\nI1116 20:17:37.678857       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/bsv 223\nI1116 20:17:37.878365       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/5wzc 485\nI1116 20:17:38.078852       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/98fs 562\nI1116 20:17:38.278365       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/pk6l 290\nI1116 20:17:38.478112       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/s9hp 396\nI1116 20:17:38.678641       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/qgk 563\nI1116 20:17:38.877951       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/twdn 552\nI1116 20:17:39.078816       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/j9z 229\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Nov 16 20:17:39.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2088 delete pod logs-generator'
Nov 16 20:17:40.980: INFO: stderr: ""
Nov 16 20:17:40.980: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:17:40.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2088" for this suite.

â€¢ [SLOW TEST:9.736 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":175,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:17:41.032: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Nov 16 20:17:41.288: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 20:18:41.425: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:18:41.441: INFO: Starting informer...
STEP: Starting pod...
Nov 16 20:18:41.697: INFO: Pod is running on 10.193.87.27. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Nov 16 20:18:41.749: INFO: Pod wasn't evicted. Proceeding
Nov 16 20:18:41.749: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Nov 16 20:19:56.818: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:19:56.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9782" for this suite.

â€¢ [SLOW TEST:135.852 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":176,"skipped":3393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:19:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Nov 16 20:19:57.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4184 create -f -'
Nov 16 20:19:57.434: INFO: stderr: ""
Nov 16 20:19:57.434: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Nov 16 20:19:57.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4184 diff -f -'
Nov 16 20:19:57.752: INFO: rc: 1
Nov 16 20:19:57.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-4184 delete -f -'
Nov 16 20:19:57.888: INFO: stderr: ""
Nov 16 20:19:57.889: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:19:57.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4184" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":177,"skipped":3432,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:19:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1204
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Nov 16 20:20:02.247: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1204 PodName:pod-sharedvolume-1964420a-fb53-4a2a-ab90-e49a32072271 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:20:02.247: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:20:02.620: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:02.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1204" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":178,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:02.673: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:20:03.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:20:05.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690803, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690803, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690803, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690803, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:20:09.033: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:20:09.051: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:12.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9257" for this suite.
STEP: Destroying namespace "webhook-9257-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.171 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":179,"skipped":3473,"failed":0}
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:12.845: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-90
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 16 20:20:13.130: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:18.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-90" for this suite.

â€¢ [SLOW TEST:5.925 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":180,"skipped":3479,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:20:19.072: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7d567aa6-7930-4bb7-9a29-3a3006f19ab1" in namespace "security-context-test-8865" to be "Succeeded or Failed"
Nov 16 20:20:19.086: INFO: Pod "busybox-user-65534-7d567aa6-7930-4bb7-9a29-3a3006f19ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.31371ms
Nov 16 20:20:21.106: INFO: Pod "busybox-user-65534-7d567aa6-7930-4bb7-9a29-3a3006f19ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034400715s
Nov 16 20:20:23.127: INFO: Pod "busybox-user-65534-7d567aa6-7930-4bb7-9a29-3a3006f19ab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055245767s
Nov 16 20:20:23.127: INFO: Pod "busybox-user-65534-7d567aa6-7930-4bb7-9a29-3a3006f19ab1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:23.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8865" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3498,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:23.179: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3548
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Nov 16 20:20:23.431: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:44.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3548" for this suite.

â€¢ [SLOW TEST:21.850 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":182,"skipped":3504,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:45.031: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:20:45.278: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 16 20:20:45.306: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 20:20:50.342: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 20:20:50.342: INFO: Creating deployment "test-rolling-update-deployment"
Nov 16 20:20:50.365: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 16 20:20:50.396: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 16 20:20:52.430: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 16 20:20:52.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690850, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690850, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690850, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772690850, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:20:54.467: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:20:54.511: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2109  bf7bf766-855a-4702-890a-737c35778cdd 36738 1 2021-11-16 20:20:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-11-16 20:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:20:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055f5898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-16 20:20:50 +0000 UTC,LastTransitionTime:2021-11-16 20:20:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-11-16 20:20:52 +0000 UTC,LastTransitionTime:2021-11-16 20:20:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:20:54.524: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-2109  b00be54c-6302-4797-9541-aed8f51b084f 36728 1 2021-11-16 20:20:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bf7bf766-855a-4702-890a-737c35778cdd 0xc0055f5eb7 0xc0055f5eb8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf7bf766-855a-4702-890a-737c35778cdd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:20:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055f5f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:20:54.524: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 16 20:20:54.525: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2109  563a99a8-efa1-4799-b2d1-06a4c91ffe49 36736 2 2021-11-16 20:20:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bf7bf766-855a-4702-890a-737c35778cdd 0xc0055f5cf7 0xc0055f5cf8}] []  [{e2e.test Update apps/v1 2021-11-16 20:20:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:20:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf7bf766-855a-4702-890a-737c35778cdd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:20:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0055f5e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:20:54.541: INFO: Pod "test-rolling-update-deployment-585b757574-d8pg7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-d8pg7 test-rolling-update-deployment-585b757574- deployment-2109  1a5478f6-dd8d-490a-b80f-630109d4f41c 36727 0 2021-11-16 20:20:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:52f8acabfa612150b6c221c878eea602edfe52b3729da2ce84df0da2bac45fc5 cni.projectcalico.org/podIP:172.30.9.63/32 cni.projectcalico.org/podIPs:172.30.9.63/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 b00be54c-6302-4797-9541-aed8f51b084f 0xc0056e84a7 0xc0056e84a8}] []  [{kube-controller-manager Update v1 2021-11-16 20:20:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b00be54c-6302-4797-9541-aed8f51b084f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 20:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 20:20:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqrj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqrj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:20:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:20:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.63,StartTime:2021-11-16 20:20:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 20:20:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://7163630829f58fb7c0bb54313c6e7516ffe4b9a5d75b6d4e2ed284c0c7975fcf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:54.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2109" for this suite.

â€¢ [SLOW TEST:9.559 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":183,"skipped":3510,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:54.590: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-83294933-5164-46e2-9896-43e1590b3d06
STEP: Creating a pod to test consume configMaps
Nov 16 20:20:54.891: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47" in namespace "projected-9864" to be "Succeeded or Failed"
Nov 16 20:20:54.906: INFO: Pod "pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47": Phase="Pending", Reason="", readiness=false. Elapsed: 15.111907ms
Nov 16 20:20:56.929: INFO: Pod "pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038839261s
Nov 16 20:20:58.951: INFO: Pod "pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060606274s
STEP: Saw pod success
Nov 16 20:20:58.951: INFO: Pod "pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47" satisfied condition "Succeeded or Failed"
Nov 16 20:20:58.966: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:20:59.133: INFO: Waiting for pod pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47 to disappear
Nov 16 20:20:59.147: INFO: Pod pod-projected-configmaps-903ea2f5-2a1e-4bc8-a4b9-1a08f6eb2e47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:20:59.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9864" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3519,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:20:59.191: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 20:20:59.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 20:20:59.479: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 20:20:59.494: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.24 before test
Nov 16 20:20:59.525: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-11-16 18:07:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 20:20:59.525: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 20:20:59.525: INFO: calico-node-5cnhh from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:20:59.525: INFO: calico-typha-d5b48569-hcfb8 from kube-system started at 2021-11-16 18:05:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:20:59.525: INFO: coredns-b58d5f584-g92hx from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:20:59.525: INFO: ibm-keepalived-watcher-bl7zr from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:20:59.525: INFO: ibm-master-proxy-static-10.193.87.24 from kube-system started at 2021-11-16 18:05:01 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:20:59.525: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:20:59.525: INFO: konnectivity-agent-wtjm7 from kube-system started at 2021-11-16 18:13:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:20:59.525: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9 from kube-system started at 2021-11-16 18:08:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 20:20:59.525: INFO: sonobuoy-e2e-job-54c8cae784204424 from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container e2e ready: true, restart count 0
Nov 16 20:20:59.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:20:59.525: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:20:59.525: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:20:59.525: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.27 before test
Nov 16 20:20:59.553: INFO: test-rolling-update-deployment-585b757574-d8pg7 from deployment-2109 started at 2021-11-16 20:20:50 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container agnhost ready: true, restart count 0
Nov 16 20:20:59.553: INFO: calico-node-smn5q from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:20:59.553: INFO: calico-typha-d5b48569-bjxkh from kube-system started at 2021-11-16 20:18:43 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:20:59.553: INFO: coredns-b58d5f584-vp4t9 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:20:59.553: INFO: ibm-keepalived-watcher-rb6gd from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:20:59.553: INFO: ibm-master-proxy-static-10.193.87.27 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:20:59.553: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:20:59.553: INFO: konnectivity-agent-ln55x from kube-system started at 2021-11-16 18:13:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:20:59.553: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j22wg from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:20:59.553: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:20:59.553: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.28 before test
Nov 16 20:20:59.600: INFO: catalog-operator-7489d5857-vlxrz from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 20:20:59.600: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-sf6d4 from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 20:20:59.600: INFO: olm-operator-7b6cd6c94c-5vgk4 from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 20:20:59.600: INFO: calico-kube-controllers-75488ccc5b-pf6m8 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 20:20:59.600: INFO: calico-node-sx675 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:20:59.600: INFO: calico-typha-d5b48569-s7xpk from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:20:59.600: INFO: coredns-autoscaler-689fb74d49-ww9hg from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container autoscaler ready: true, restart count 0
Nov 16 20:20:59.600: INFO: coredns-b58d5f584-ntqv9 from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:20:59.600: INFO: dashboard-metrics-scraper-6747f89c97-pzthq from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 16 20:20:59.600: INFO: ibm-file-plugin-fd44cd466-zjcs4 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 20:20:59.600: INFO: ibm-keepalived-watcher-mfkdl from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:20:59.600: INFO: ibm-master-proxy-static-10.193.87.28 from kube-system started at 2021-11-16 18:04:49 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:20:59.600: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:20:59.600: INFO: ibm-storage-watcher-765888f8c9-dffqn from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 20:20:59.600: INFO: konnectivity-agent-g2pw7 from kube-system started at 2021-11-16 18:13:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.600: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:20:59.600: INFO: kubernetes-dashboard-54c47dd995-czt2b from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.601: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 16 20:20:59.601: INFO: metrics-server-64bbdfc744-wxsz6 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.601: INFO: 	Container metrics-server ready: true, restart count 0
Nov 16 20:20:59.601: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 16 20:20:59.601: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bb4n2 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.601: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 20:20:59.601: INFO: sonobuoy from sonobuoy started at 2021-11-16 19:29:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:20:59.601: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 20:20:59.601: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-hpqln from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:20:59.601: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:20:59.601: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16b8205fa409b61d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:21:00.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4087" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":185,"skipped":3534,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:21:00.767: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7191
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Nov 16 20:21:01.087: INFO: Found 0 stateful pods, waiting for 3
Nov 16 20:21:11.131: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:21:11.132: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:21:11.132: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Nov 16 20:21:11.226: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 16 20:21:21.364: INFO: Updating stateful set ss2
Nov 16 20:21:21.394: INFO: Waiting for Pod statefulset-7191/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Nov 16 20:21:31.551: INFO: Found 2 stateful pods, waiting for 3
Nov 16 20:21:41.605: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:21:41.605: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:21:41.605: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 16 20:21:41.704: INFO: Updating stateful set ss2
Nov 16 20:21:41.740: INFO: Waiting for Pod statefulset-7191/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Nov 16 20:21:51.849: INFO: Updating stateful set ss2
Nov 16 20:21:51.879: INFO: Waiting for StatefulSet statefulset-7191/ss2 to complete update
Nov 16 20:21:51.879: INFO: Waiting for Pod statefulset-7191/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 20:22:01.914: INFO: Deleting all statefulset in ns statefulset-7191
Nov 16 20:22:01.928: INFO: Scaling statefulset ss2 to 0
Nov 16 20:22:12.004: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:22:12.020: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:22:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7191" for this suite.

â€¢ [SLOW TEST:71.355 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":186,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:22:12.123: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-5bb2a6ab-e885-4705-a1c5-b6131a56538d
STEP: Creating a pod to test consume configMaps
Nov 16 20:22:12.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2" in namespace "configmap-4972" to be "Succeeded or Failed"
Nov 16 20:22:12.453: INFO: Pod "pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.250023ms
Nov 16 20:22:14.475: INFO: Pod "pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037847044s
Nov 16 20:22:16.496: INFO: Pod "pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059319445s
STEP: Saw pod success
Nov 16 20:22:16.497: INFO: Pod "pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2" satisfied condition "Succeeded or Failed"
Nov 16 20:22:16.511: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:22:16.586: INFO: Waiting for pod pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2 to disappear
Nov 16 20:22:16.601: INFO: Pod pod-configmaps-796a3309-670d-4aa4-b424-9f3c254bc1f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:22:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4972" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3574,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:22:16.647: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:22:17.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2732" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":188,"skipped":3577,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:22:17.070: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9725
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-91f81819-134f-41e8-96a9-23ddc7f021a6
STEP: Creating secret with name s-test-opt-upd-1a9151a8-77b7-4e15-9c9a-3d3c74f09bab
STEP: Creating the pod
Nov 16 20:22:17.426: INFO: The status of Pod pod-projected-secrets-332de5bf-fded-48e9-a612-4dd589d1123f is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:22:19.448: INFO: The status of Pod pod-projected-secrets-332de5bf-fded-48e9-a612-4dd589d1123f is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:22:21.449: INFO: The status of Pod pod-projected-secrets-332de5bf-fded-48e9-a612-4dd589d1123f is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-91f81819-134f-41e8-96a9-23ddc7f021a6
STEP: Updating secret s-test-opt-upd-1a9151a8-77b7-4e15-9c9a-3d3c74f09bab
STEP: Creating secret with name s-test-opt-create-ee593efe-8b7a-42a2-9d9b-2b1177a25727
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:23:41.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9725" for this suite.

â€¢ [SLOW TEST:84.025 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3578,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:23:41.095: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:23:41.419: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:23:43.436: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:23:45.444: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:47.447: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:49.443: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:51.448: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:53.435: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:55.446: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:57.446: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:23:59.441: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:24:01.442: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = false)
Nov 16 20:24:03.441: INFO: The status of Pod test-webserver-8915f5a7-730a-468d-af51-ad004cba1592 is Running (Ready = true)
Nov 16 20:24:03.457: INFO: Container started at 2021-11-16 20:23:43 +0000 UTC, pod became ready at 2021-11-16 20:24:01 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:24:03.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1244" for this suite.

â€¢ [SLOW TEST:22.414 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3581,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:24:03.513: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-ab6e3c7b-0cfd-45e5-a51a-d2c1a018f054 in namespace container-probe-8644
Nov 16 20:24:05.819: INFO: Started pod test-webserver-ab6e3c7b-0cfd-45e5-a51a-d2c1a018f054 in namespace container-probe-8644
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 20:24:05.838: INFO: Initial restart count of pod test-webserver-ab6e3c7b-0cfd-45e5-a51a-d2c1a018f054 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:06.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8644" for this suite.

â€¢ [SLOW TEST:243.071 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:06.594: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-5140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 20:28:06.916: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 20:28:06.934: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 20:28:06.990: INFO: waiting for watch events with expected annotations
Nov 16 20:28:06.990: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:07.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5140" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":192,"skipped":3613,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:07.272: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:28:07.563: INFO: The status of Pod busybox-scheduling-bc5035f8-92aa-4b9f-918d-aec159cfabf7 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:28:09.583: INFO: The status of Pod busybox-scheduling-bc5035f8-92aa-4b9f-918d-aec159cfabf7 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:28:11.582: INFO: The status of Pod busybox-scheduling-bc5035f8-92aa-4b9f-918d-aec159cfabf7 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:11.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7079" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:11.757: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:28:12.636: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:28:14.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691292, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691292, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691292, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691292, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:28:17.741: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Nov 16 20:28:17.836: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:17.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5920" for this suite.
STEP: Destroying namespace "webhook-5920-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.393 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":194,"skipped":3643,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:18.150: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:28:18.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:28:20.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691298, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691298, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691298, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691298, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:28:23.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
Nov 16 20:28:28.943: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:29.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5122" for this suite.
STEP: Destroying namespace "webhook-5122-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:11.350 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":195,"skipped":3657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:29.513: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 20:28:29.780: INFO: Waiting up to 5m0s for pod "pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4" in namespace "emptydir-3386" to be "Succeeded or Failed"
Nov 16 20:28:29.792: INFO: Pod "pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.53151ms
Nov 16 20:28:31.814: INFO: Pod "pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034419982s
Nov 16 20:28:33.833: INFO: Pod "pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052962521s
STEP: Saw pod success
Nov 16 20:28:33.833: INFO: Pod "pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4" satisfied condition "Succeeded or Failed"
Nov 16 20:28:33.847: INFO: Trying to get logs from node 10.193.87.27 pod pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4 container test-container: <nil>
STEP: delete the pod
Nov 16 20:28:33.920: INFO: Waiting for pod pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4 to disappear
Nov 16 20:28:33.935: INFO: Pod pod-1969fdf0-c0e5-4cb4-8201-1e4bbd2025f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:33.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3386" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3754,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:33.977: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:38.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-831" for this suite.
â€¢{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":197,"skipped":3757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3468
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:28:45.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5185" for this suite.
STEP: Destroying namespace "nsdeletetest-180" for this suite.
Nov 16 20:28:45.224: INFO: Namespace nsdeletetest-180 was already deleted
STEP: Destroying namespace "nsdeletetest-3468" for this suite.

â€¢ [SLOW TEST:6.867 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":198,"skipped":3816,"failed":0}
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:28:45.249: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-ac956ee5-f9f8-45fc-95b0-d0bf62ad5cec in namespace container-probe-7592
Nov 16 20:28:49.566: INFO: Started pod busybox-ac956ee5-f9f8-45fc-95b0-d0bf62ad5cec in namespace container-probe-7592
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 20:28:49.579: INFO: Initial restart count of pod busybox-ac956ee5-f9f8-45fc-95b0-d0bf62ad5cec is 0
Nov 16 20:29:38.150: INFO: Restart count of pod container-probe-7592/busybox-ac956ee5-f9f8-45fc-95b0-d0bf62ad5cec is now 1 (48.571393714s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:29:38.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7592" for this suite.

â€¢ [SLOW TEST:52.992 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:29:38.243: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:29:38.517: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6083fcff-9650-4412-a8a8-3e722532cdb0" in namespace "security-context-test-938" to be "Succeeded or Failed"
Nov 16 20:29:38.532: INFO: Pod "busybox-readonly-false-6083fcff-9650-4412-a8a8-3e722532cdb0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.343321ms
Nov 16 20:29:40.552: INFO: Pod "busybox-readonly-false-6083fcff-9650-4412-a8a8-3e722532cdb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034971196s
Nov 16 20:29:42.573: INFO: Pod "busybox-readonly-false-6083fcff-9650-4412-a8a8-3e722532cdb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055763917s
Nov 16 20:29:42.573: INFO: Pod "busybox-readonly-false-6083fcff-9650-4412-a8a8-3e722532cdb0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:29:42.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-938" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3891,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:29:42.619: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Nov 16 20:29:42.886: INFO: Waiting up to 5m0s for pod "client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838" in namespace "containers-3088" to be "Succeeded or Failed"
Nov 16 20:29:42.900: INFO: Pod "client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838": Phase="Pending", Reason="", readiness=false. Elapsed: 13.680422ms
Nov 16 20:29:44.921: INFO: Pod "client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034457144s
Nov 16 20:29:46.942: INFO: Pod "client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05557059s
STEP: Saw pod success
Nov 16 20:29:46.942: INFO: Pod "client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838" satisfied condition "Succeeded or Failed"
Nov 16 20:29:46.956: INFO: Trying to get logs from node 10.193.87.27 pod client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:29:47.036: INFO: Waiting for pod client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838 to disappear
Nov 16 20:29:47.049: INFO: Pod client-containers-d55a5bea-5f88-4417-8c5f-09a365be8838 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:29:47.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3088" for this suite.
â€¢{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3897,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:29:47.095: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8150
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8150
STEP: creating replication controller externalsvc in namespace services-8150
I1116 20:29:47.447445      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8150, replica count: 2
I1116 20:29:50.498958      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Nov 16 20:29:50.593: INFO: Creating new exec pod
Nov 16 20:29:54.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8150 exec execpodbzd49 -- /bin/sh -x -c nslookup clusterip-service.services-8150.svc.cluster.local'
Nov 16 20:29:55.138: INFO: stderr: "+ nslookup clusterip-service.services-8150.svc.cluster.local\n"
Nov 16 20:29:55.138: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8150.svc.cluster.local\tcanonical name = externalsvc.services-8150.svc.cluster.local.\nName:\texternalsvc.services-8150.svc.cluster.local\nAddress: 172.21.56.231\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8150, will wait for the garbage collector to delete the pods
Nov 16 20:29:55.224: INFO: Deleting ReplicationController externalsvc took: 21.769767ms
Nov 16 20:29:55.324: INFO: Terminating ReplicationController externalsvc pods took: 100.162772ms
Nov 16 20:29:58.382: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:29:58.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8150" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:11.370 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":202,"skipped":3912,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:29:58.466: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:29:58.727: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 16 20:30:03.753: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 20:30:03.753: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:30:07.868: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1729  d5acbd1f-0666-42f1-b1cb-429f48aa7d46 38904 1 2021-11-16 20:30:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-11-16 20:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:30:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024bb868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-16 20:30:03 +0000 UTC,LastTransitionTime:2021-11-16 20:30:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2021-11-16 20:30:06 +0000 UTC,LastTransitionTime:2021-11-16 20:30:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:30:07.880: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-1729  d0263034-f8d9-4248-963b-c822ce4a71c7 38894 1 2021-11-16 20:30:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d5acbd1f-0666-42f1-b1cb-429f48aa7d46 0xc0024bbc37 0xc0024bbc38}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5acbd1f-0666-42f1-b1cb-429f48aa7d46\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:30:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033f6008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:30:07.895: INFO: Pod "test-cleanup-deployment-5b4d99b59b-dwq5w" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-dwq5w test-cleanup-deployment-5b4d99b59b- deployment-1729  3e91ab8c-3b40-46e5-b42c-0eaa818bd216 38893 0 2021-11-16 20:30:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/containerID:cc5337a67dad372a368651c86f5eb8caa52be647a6b8c6b17235b0275884bdb1 cni.projectcalico.org/podIP:172.30.9.22/32 cni.projectcalico.org/podIPs:172.30.9.22/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b d0263034-f8d9-4248-963b-c822ce4a71c7 0xc0033f63b7 0xc0033f63b8}] []  [{kube-controller-manager Update v1 2021-11-16 20:30:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0263034-f8d9-4248-963b-c822ce4a71c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 20:30:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 20:30:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hb4rs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hb4rs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:30:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:30:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:30:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.22,StartTime:2021-11-16 20:30:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 20:30:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://1d729f497dd1e3f4a563311747c39b2ad0788c13cc9b7e7b524048daabf03c80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:07.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1729" for this suite.

â€¢ [SLOW TEST:9.479 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":203,"skipped":3914,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Nov 16 20:30:08.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-7544 api-versions'
Nov 16 20:30:08.282: INFO: stderr: ""
Nov 16 20:30:08.282: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:08.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7544" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":204,"skipped":3930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1780
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:30:08.602: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 20:30:13.631: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Nov 16 20:30:13.663: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Nov 16 20:30:13.688: INFO: observed ReplicaSet test-rs in namespace replicaset-1780 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 20:30:13.703: INFO: observed ReplicaSet test-rs in namespace replicaset-1780 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 20:30:13.727: INFO: observed ReplicaSet test-rs in namespace replicaset-1780 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 20:30:13.742: INFO: observed ReplicaSet test-rs in namespace replicaset-1780 with ReadyReplicas 1, AvailableReplicas 1
Nov 16 20:30:16.334: INFO: observed ReplicaSet test-rs in namespace replicaset-1780 with ReadyReplicas 2, AvailableReplicas 2
Nov 16 20:30:16.753: INFO: observed Replicaset test-rs in namespace replicaset-1780 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:16.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1780" for this suite.

â€¢ [SLOW TEST:8.482 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":205,"skipped":3966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:16.810: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:30:17.041: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4986
I1116 20:30:17.069670      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4986, replica count: 1
I1116 20:30:18.121397      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 20:30:19.122145      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1116 20:30:20.122522      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:30:20.261: INFO: Created: latency-svc-cmznd
Nov 16 20:30:20.276: INFO: Got endpoints: latency-svc-cmznd [53.444423ms]
Nov 16 20:30:20.314: INFO: Created: latency-svc-jcltn
Nov 16 20:30:20.322: INFO: Got endpoints: latency-svc-jcltn [45.545504ms]
Nov 16 20:30:20.330: INFO: Created: latency-svc-59d5b
Nov 16 20:30:20.348: INFO: Got endpoints: latency-svc-59d5b [70.302862ms]
Nov 16 20:30:20.348: INFO: Created: latency-svc-978qs
Nov 16 20:30:20.361: INFO: Got endpoints: latency-svc-978qs [82.749717ms]
Nov 16 20:30:20.378: INFO: Created: latency-svc-cq6sf
Nov 16 20:30:20.397: INFO: Got endpoints: latency-svc-cq6sf [118.699893ms]
Nov 16 20:30:20.419: INFO: Created: latency-svc-dvhd9
Nov 16 20:30:20.419: INFO: Got endpoints: latency-svc-dvhd9 [140.852034ms]
Nov 16 20:30:20.428: INFO: Created: latency-svc-9j4qd
Nov 16 20:30:20.435: INFO: Got endpoints: latency-svc-9j4qd [157.228703ms]
Nov 16 20:30:20.443: INFO: Created: latency-svc-tlflp
Nov 16 20:30:20.453: INFO: Got endpoints: latency-svc-tlflp [174.620923ms]
Nov 16 20:30:20.467: INFO: Created: latency-svc-szrjz
Nov 16 20:30:20.473: INFO: Got endpoints: latency-svc-szrjz [194.223146ms]
Nov 16 20:30:20.479: INFO: Created: latency-svc-gfdtf
Nov 16 20:30:20.487: INFO: Got endpoints: latency-svc-gfdtf [208.048066ms]
Nov 16 20:30:20.501: INFO: Created: latency-svc-2pd62
Nov 16 20:30:20.512: INFO: Got endpoints: latency-svc-2pd62 [233.316003ms]
Nov 16 20:30:20.524: INFO: Created: latency-svc-z24dv
Nov 16 20:30:20.532: INFO: Got endpoints: latency-svc-z24dv [253.40884ms]
Nov 16 20:30:20.540: INFO: Created: latency-svc-mc2cl
Nov 16 20:30:20.552: INFO: Got endpoints: latency-svc-mc2cl [273.750824ms]
Nov 16 20:30:20.572: INFO: Created: latency-svc-hht8r
Nov 16 20:30:20.581: INFO: Got endpoints: latency-svc-hht8r [302.244635ms]
Nov 16 20:30:20.591: INFO: Created: latency-svc-dcs52
Nov 16 20:30:20.599: INFO: Got endpoints: latency-svc-dcs52 [321.150427ms]
Nov 16 20:30:20.610: INFO: Created: latency-svc-m4mxj
Nov 16 20:30:20.619: INFO: Got endpoints: latency-svc-m4mxj [340.492277ms]
Nov 16 20:30:20.625: INFO: Created: latency-svc-m29vq
Nov 16 20:30:20.638: INFO: Got endpoints: latency-svc-m29vq [315.450641ms]
Nov 16 20:30:20.646: INFO: Created: latency-svc-z6dzh
Nov 16 20:30:20.660: INFO: Got endpoints: latency-svc-z6dzh [312.423182ms]
Nov 16 20:30:20.669: INFO: Created: latency-svc-bpcjv
Nov 16 20:30:20.679: INFO: Got endpoints: latency-svc-bpcjv [317.740702ms]
Nov 16 20:30:20.688: INFO: Created: latency-svc-mlfng
Nov 16 20:30:20.701: INFO: Got endpoints: latency-svc-mlfng [303.406299ms]
Nov 16 20:30:20.708: INFO: Created: latency-svc-sdp92
Nov 16 20:30:20.718: INFO: Got endpoints: latency-svc-sdp92 [298.55478ms]
Nov 16 20:30:20.730: INFO: Created: latency-svc-mct58
Nov 16 20:30:20.739: INFO: Got endpoints: latency-svc-mct58 [303.202602ms]
Nov 16 20:30:20.747: INFO: Created: latency-svc-w2cgr
Nov 16 20:30:20.761: INFO: Got endpoints: latency-svc-w2cgr [307.883656ms]
Nov 16 20:30:20.777: INFO: Created: latency-svc-hbft6
Nov 16 20:30:20.786: INFO: Got endpoints: latency-svc-hbft6 [313.203095ms]
Nov 16 20:30:20.796: INFO: Created: latency-svc-nm4rr
Nov 16 20:30:20.805: INFO: Got endpoints: latency-svc-nm4rr [317.846331ms]
Nov 16 20:30:20.817: INFO: Created: latency-svc-z2fvv
Nov 16 20:30:20.826: INFO: Got endpoints: latency-svc-z2fvv [313.356086ms]
Nov 16 20:30:20.836: INFO: Created: latency-svc-hhl54
Nov 16 20:30:20.847: INFO: Got endpoints: latency-svc-hhl54 [314.116423ms]
Nov 16 20:30:20.857: INFO: Created: latency-svc-bxztl
Nov 16 20:30:20.869: INFO: Got endpoints: latency-svc-bxztl [316.921739ms]
Nov 16 20:30:20.880: INFO: Created: latency-svc-nv8b2
Nov 16 20:30:20.888: INFO: Got endpoints: latency-svc-nv8b2 [306.748983ms]
Nov 16 20:30:20.897: INFO: Created: latency-svc-gb49s
Nov 16 20:30:20.905: INFO: Got endpoints: latency-svc-gb49s [305.948807ms]
Nov 16 20:30:20.916: INFO: Created: latency-svc-cj7s4
Nov 16 20:30:20.925: INFO: Got endpoints: latency-svc-cj7s4 [305.18221ms]
Nov 16 20:30:20.933: INFO: Created: latency-svc-wktnv
Nov 16 20:30:20.941: INFO: Got endpoints: latency-svc-wktnv [303.123985ms]
Nov 16 20:30:20.951: INFO: Created: latency-svc-pxkg6
Nov 16 20:30:20.957: INFO: Got endpoints: latency-svc-pxkg6 [297.131074ms]
Nov 16 20:30:20.967: INFO: Created: latency-svc-bsbn9
Nov 16 20:30:20.976: INFO: Got endpoints: latency-svc-bsbn9 [296.715274ms]
Nov 16 20:30:20.981: INFO: Created: latency-svc-wcj9s
Nov 16 20:30:20.998: INFO: Got endpoints: latency-svc-wcj9s [296.715199ms]
Nov 16 20:30:21.008: INFO: Created: latency-svc-4sp75
Nov 16 20:30:21.015: INFO: Got endpoints: latency-svc-4sp75 [296.778929ms]
Nov 16 20:30:21.025: INFO: Created: latency-svc-rp7v8
Nov 16 20:30:21.032: INFO: Got endpoints: latency-svc-rp7v8 [292.740052ms]
Nov 16 20:30:21.042: INFO: Created: latency-svc-86bvw
Nov 16 20:30:21.051: INFO: Got endpoints: latency-svc-86bvw [289.513839ms]
Nov 16 20:30:21.066: INFO: Created: latency-svc-87hpr
Nov 16 20:30:21.073: INFO: Got endpoints: latency-svc-87hpr [286.264667ms]
Nov 16 20:30:21.084: INFO: Created: latency-svc-xsgf5
Nov 16 20:30:21.092: INFO: Got endpoints: latency-svc-xsgf5 [286.903423ms]
Nov 16 20:30:21.101: INFO: Created: latency-svc-ccwlh
Nov 16 20:30:21.109: INFO: Got endpoints: latency-svc-ccwlh [283.193394ms]
Nov 16 20:30:21.120: INFO: Created: latency-svc-l9vd5
Nov 16 20:30:21.127: INFO: Got endpoints: latency-svc-l9vd5 [280.761992ms]
Nov 16 20:30:21.138: INFO: Created: latency-svc-6qbw8
Nov 16 20:30:21.145: INFO: Got endpoints: latency-svc-6qbw8 [275.861952ms]
Nov 16 20:30:21.153: INFO: Created: latency-svc-5msq2
Nov 16 20:30:21.159: INFO: Got endpoints: latency-svc-5msq2 [271.172103ms]
Nov 16 20:30:21.168: INFO: Created: latency-svc-t9scz
Nov 16 20:30:21.175: INFO: Got endpoints: latency-svc-t9scz [268.964792ms]
Nov 16 20:30:21.188: INFO: Created: latency-svc-k25sf
Nov 16 20:30:21.198: INFO: Got endpoints: latency-svc-k25sf [273.730943ms]
Nov 16 20:30:21.209: INFO: Created: latency-svc-vf575
Nov 16 20:30:21.216: INFO: Got endpoints: latency-svc-vf575 [274.811916ms]
Nov 16 20:30:21.227: INFO: Created: latency-svc-l858b
Nov 16 20:30:21.234: INFO: Got endpoints: latency-svc-l858b [276.695952ms]
Nov 16 20:30:21.245: INFO: Created: latency-svc-klqmf
Nov 16 20:30:21.253: INFO: Got endpoints: latency-svc-klqmf [276.999012ms]
Nov 16 20:30:21.276: INFO: Created: latency-svc-z8kb7
Nov 16 20:30:21.277: INFO: Got endpoints: latency-svc-z8kb7 [279.47901ms]
Nov 16 20:30:21.277: INFO: Created: latency-svc-qhxhv
Nov 16 20:30:21.286: INFO: Got endpoints: latency-svc-qhxhv [270.797203ms]
Nov 16 20:30:21.295: INFO: Created: latency-svc-8577x
Nov 16 20:30:21.304: INFO: Got endpoints: latency-svc-8577x [272.12018ms]
Nov 16 20:30:21.316: INFO: Created: latency-svc-tmv2h
Nov 16 20:30:21.325: INFO: Got endpoints: latency-svc-tmv2h [274.248656ms]
Nov 16 20:30:21.335: INFO: Created: latency-svc-wp7c7
Nov 16 20:30:21.344: INFO: Got endpoints: latency-svc-wp7c7 [271.269221ms]
Nov 16 20:30:21.357: INFO: Created: latency-svc-7tpjc
Nov 16 20:30:21.366: INFO: Got endpoints: latency-svc-7tpjc [273.679185ms]
Nov 16 20:30:21.398: INFO: Created: latency-svc-z7zc2
Nov 16 20:30:21.398: INFO: Got endpoints: latency-svc-z7zc2 [288.189902ms]
Nov 16 20:30:21.398: INFO: Created: latency-svc-8zhxr
Nov 16 20:30:21.402: INFO: Got endpoints: latency-svc-8zhxr [274.752465ms]
Nov 16 20:30:21.412: INFO: Created: latency-svc-zd96x
Nov 16 20:30:21.420: INFO: Got endpoints: latency-svc-zd96x [274.841079ms]
Nov 16 20:30:21.430: INFO: Created: latency-svc-8th2l
Nov 16 20:30:21.439: INFO: Got endpoints: latency-svc-8th2l [280.00652ms]
Nov 16 20:30:21.445: INFO: Created: latency-svc-9ps6g
Nov 16 20:30:21.452: INFO: Got endpoints: latency-svc-9ps6g [276.61307ms]
Nov 16 20:30:21.461: INFO: Created: latency-svc-phw6s
Nov 16 20:30:21.468: INFO: Got endpoints: latency-svc-phw6s [269.484902ms]
Nov 16 20:30:21.481: INFO: Created: latency-svc-jgsjj
Nov 16 20:30:21.489: INFO: Got endpoints: latency-svc-jgsjj [273.108639ms]
Nov 16 20:30:21.500: INFO: Created: latency-svc-ng4sl
Nov 16 20:30:21.510: INFO: Got endpoints: latency-svc-ng4sl [275.8339ms]
Nov 16 20:30:21.520: INFO: Created: latency-svc-tzbqj
Nov 16 20:30:21.530: INFO: Got endpoints: latency-svc-tzbqj [276.298175ms]
Nov 16 20:30:21.540: INFO: Created: latency-svc-4d5fp
Nov 16 20:30:21.554: INFO: Got endpoints: latency-svc-4d5fp [276.873936ms]
Nov 16 20:30:21.575: INFO: Created: latency-svc-f56hh
Nov 16 20:30:21.607: INFO: Got endpoints: latency-svc-f56hh [321.129412ms]
Nov 16 20:30:21.612: INFO: Created: latency-svc-hhtpw
Nov 16 20:30:21.637: INFO: Got endpoints: latency-svc-hhtpw [333.599506ms]
Nov 16 20:30:21.641: INFO: Created: latency-svc-sqwmp
Nov 16 20:30:21.648: INFO: Got endpoints: latency-svc-sqwmp [323.394615ms]
Nov 16 20:30:21.660: INFO: Created: latency-svc-w25b5
Nov 16 20:30:21.682: INFO: Got endpoints: latency-svc-w25b5 [337.50835ms]
Nov 16 20:30:21.693: INFO: Created: latency-svc-665hh
Nov 16 20:30:21.702: INFO: Got endpoints: latency-svc-665hh [335.356773ms]
Nov 16 20:30:21.710: INFO: Created: latency-svc-28vcs
Nov 16 20:30:21.717: INFO: Got endpoints: latency-svc-28vcs [319.651318ms]
Nov 16 20:30:21.732: INFO: Created: latency-svc-7d66x
Nov 16 20:30:21.743: INFO: Got endpoints: latency-svc-7d66x [340.412443ms]
Nov 16 20:30:21.752: INFO: Created: latency-svc-p28f5
Nov 16 20:30:21.759: INFO: Got endpoints: latency-svc-p28f5 [337.883235ms]
Nov 16 20:30:21.774: INFO: Created: latency-svc-mwv9m
Nov 16 20:30:21.786: INFO: Got endpoints: latency-svc-mwv9m [346.955115ms]
Nov 16 20:30:21.793: INFO: Created: latency-svc-zwvmp
Nov 16 20:30:21.803: INFO: Got endpoints: latency-svc-zwvmp [350.521865ms]
Nov 16 20:30:21.819: INFO: Created: latency-svc-gk54r
Nov 16 20:30:21.829: INFO: Got endpoints: latency-svc-gk54r [361.422417ms]
Nov 16 20:30:21.841: INFO: Created: latency-svc-shb4w
Nov 16 20:30:21.867: INFO: Got endpoints: latency-svc-shb4w [377.708783ms]
Nov 16 20:30:21.886: INFO: Created: latency-svc-6t9rt
Nov 16 20:30:21.900: INFO: Got endpoints: latency-svc-6t9rt [389.158416ms]
Nov 16 20:30:21.912: INFO: Created: latency-svc-s6zjg
Nov 16 20:30:21.922: INFO: Got endpoints: latency-svc-s6zjg [54.737396ms]
Nov 16 20:30:21.932: INFO: Created: latency-svc-xqz7p
Nov 16 20:30:21.946: INFO: Got endpoints: latency-svc-xqz7p [415.584447ms]
Nov 16 20:30:21.958: INFO: Created: latency-svc-vgf4s
Nov 16 20:30:21.970: INFO: Got endpoints: latency-svc-vgf4s [415.50031ms]
Nov 16 20:30:21.978: INFO: Created: latency-svc-ndhpz
Nov 16 20:30:21.986: INFO: Got endpoints: latency-svc-ndhpz [378.361923ms]
Nov 16 20:30:21.995: INFO: Created: latency-svc-hkzdw
Nov 16 20:30:22.003: INFO: Got endpoints: latency-svc-hkzdw [365.699838ms]
Nov 16 20:30:22.014: INFO: Created: latency-svc-zpn45
Nov 16 20:30:22.022: INFO: Got endpoints: latency-svc-zpn45 [373.325618ms]
Nov 16 20:30:22.031: INFO: Created: latency-svc-ngft6
Nov 16 20:30:22.045: INFO: Got endpoints: latency-svc-ngft6 [362.152502ms]
Nov 16 20:30:22.062: INFO: Created: latency-svc-2qkfs
Nov 16 20:30:22.070: INFO: Got endpoints: latency-svc-2qkfs [368.470269ms]
Nov 16 20:30:22.086: INFO: Created: latency-svc-rhcl5
Nov 16 20:30:22.097: INFO: Got endpoints: latency-svc-rhcl5 [379.581362ms]
Nov 16 20:30:22.107: INFO: Created: latency-svc-npdl5
Nov 16 20:30:22.117: INFO: Got endpoints: latency-svc-npdl5 [373.773016ms]
Nov 16 20:30:22.127: INFO: Created: latency-svc-vddjd
Nov 16 20:30:22.141: INFO: Got endpoints: latency-svc-vddjd [381.837685ms]
Nov 16 20:30:22.150: INFO: Created: latency-svc-tbfsb
Nov 16 20:30:22.161: INFO: Got endpoints: latency-svc-tbfsb [374.712893ms]
Nov 16 20:30:22.193: INFO: Created: latency-svc-687ms
Nov 16 20:30:22.210: INFO: Got endpoints: latency-svc-687ms [407.413861ms]
Nov 16 20:30:22.221: INFO: Created: latency-svc-dbbk8
Nov 16 20:30:22.229: INFO: Got endpoints: latency-svc-dbbk8 [399.854957ms]
Nov 16 20:30:22.242: INFO: Created: latency-svc-6rhtt
Nov 16 20:30:22.256: INFO: Got endpoints: latency-svc-6rhtt [356.193743ms]
Nov 16 20:30:22.264: INFO: Created: latency-svc-5bm6b
Nov 16 20:30:22.275: INFO: Got endpoints: latency-svc-5bm6b [352.725994ms]
Nov 16 20:30:22.297: INFO: Created: latency-svc-4p66k
Nov 16 20:30:22.321: INFO: Got endpoints: latency-svc-4p66k [375.624802ms]
Nov 16 20:30:22.336: INFO: Created: latency-svc-c5knc
Nov 16 20:30:22.347: INFO: Got endpoints: latency-svc-c5knc [377.254661ms]
Nov 16 20:30:22.357: INFO: Created: latency-svc-msdng
Nov 16 20:30:22.368: INFO: Got endpoints: latency-svc-msdng [382.473974ms]
Nov 16 20:30:22.395: INFO: Created: latency-svc-wd4fz
Nov 16 20:30:22.443: INFO: Created: latency-svc-q77p2
Nov 16 20:30:22.463: INFO: Got endpoints: latency-svc-q77p2 [441.54697ms]
Nov 16 20:30:22.464: INFO: Got endpoints: latency-svc-wd4fz [460.355805ms]
Nov 16 20:30:22.514: INFO: Created: latency-svc-clvf8
Nov 16 20:30:22.528: INFO: Got endpoints: latency-svc-clvf8 [483.070709ms]
Nov 16 20:30:22.537: INFO: Created: latency-svc-52m2c
Nov 16 20:30:22.547: INFO: Got endpoints: latency-svc-52m2c [476.473696ms]
Nov 16 20:30:22.558: INFO: Created: latency-svc-2k65h
Nov 16 20:30:22.587: INFO: Got endpoints: latency-svc-2k65h [490.453605ms]
Nov 16 20:30:22.624: INFO: Created: latency-svc-5xrtg
Nov 16 20:30:22.624: INFO: Created: latency-svc-z5drq
Nov 16 20:30:22.625: INFO: Got endpoints: latency-svc-5xrtg [508.732594ms]
Nov 16 20:30:22.634: INFO: Created: latency-svc-tgph8
Nov 16 20:30:22.678: INFO: Got endpoints: latency-svc-tgph8 [516.921864ms]
Nov 16 20:30:22.679: INFO: Got endpoints: latency-svc-z5drq [537.694284ms]
Nov 16 20:30:22.681: INFO: Created: latency-svc-5zk87
Nov 16 20:30:22.692: INFO: Got endpoints: latency-svc-5zk87 [482.046774ms]
Nov 16 20:30:22.723: INFO: Created: latency-svc-44hn6
Nov 16 20:30:22.724: INFO: Got endpoints: latency-svc-44hn6 [494.608329ms]
Nov 16 20:30:22.724: INFO: Created: latency-svc-98kcn
Nov 16 20:30:22.734: INFO: Got endpoints: latency-svc-98kcn [477.708735ms]
Nov 16 20:30:22.743: INFO: Created: latency-svc-nq8tf
Nov 16 20:30:22.754: INFO: Got endpoints: latency-svc-nq8tf [478.309962ms]
Nov 16 20:30:22.760: INFO: Created: latency-svc-jn7jk
Nov 16 20:30:22.771: INFO: Got endpoints: latency-svc-jn7jk [449.611966ms]
Nov 16 20:30:22.779: INFO: Created: latency-svc-h87bz
Nov 16 20:30:22.790: INFO: Got endpoints: latency-svc-h87bz [442.772238ms]
Nov 16 20:30:22.800: INFO: Created: latency-svc-cwb55
Nov 16 20:30:22.844: INFO: Got endpoints: latency-svc-cwb55 [475.961556ms]
Nov 16 20:30:22.849: INFO: Created: latency-svc-x8bcl
Nov 16 20:30:22.859: INFO: Got endpoints: latency-svc-x8bcl [394.994212ms]
Nov 16 20:30:22.883: INFO: Created: latency-svc-f72vx
Nov 16 20:30:22.895: INFO: Got endpoints: latency-svc-f72vx [430.689732ms]
Nov 16 20:30:22.907: INFO: Created: latency-svc-x7kq7
Nov 16 20:30:22.914: INFO: Got endpoints: latency-svc-x7kq7 [386.209031ms]
Nov 16 20:30:22.923: INFO: Created: latency-svc-72bs6
Nov 16 20:30:22.959: INFO: Got endpoints: latency-svc-72bs6 [412.313669ms]
Nov 16 20:30:22.974: INFO: Created: latency-svc-gbb2l
Nov 16 20:30:23.016: INFO: Got endpoints: latency-svc-gbb2l [428.373954ms]
Nov 16 20:30:23.017: INFO: Created: latency-svc-ww2mk
Nov 16 20:30:23.017: INFO: Created: latency-svc-4fhnm
Nov 16 20:30:23.017: INFO: Got endpoints: latency-svc-4fhnm [391.554762ms]
Nov 16 20:30:23.023: INFO: Got endpoints: latency-svc-ww2mk [344.965818ms]
Nov 16 20:30:23.052: INFO: Created: latency-svc-44bkq
Nov 16 20:30:23.059: INFO: Got endpoints: latency-svc-44bkq [380.180604ms]
Nov 16 20:30:23.069: INFO: Created: latency-svc-c5crm
Nov 16 20:30:23.081: INFO: Got endpoints: latency-svc-c5crm [388.439741ms]
Nov 16 20:30:23.088: INFO: Created: latency-svc-lczv7
Nov 16 20:30:23.109: INFO: Got endpoints: latency-svc-lczv7 [385.191671ms]
Nov 16 20:30:23.124: INFO: Created: latency-svc-nr8nc
Nov 16 20:30:23.138: INFO: Got endpoints: latency-svc-nr8nc [404.104402ms]
Nov 16 20:30:23.160: INFO: Created: latency-svc-hs9gg
Nov 16 20:30:23.169: INFO: Got endpoints: latency-svc-hs9gg [414.884094ms]
Nov 16 20:30:23.195: INFO: Created: latency-svc-v2mph
Nov 16 20:30:23.200: INFO: Got endpoints: latency-svc-v2mph [429.251501ms]
Nov 16 20:30:23.215: INFO: Created: latency-svc-bnvsf
Nov 16 20:30:23.229: INFO: Got endpoints: latency-svc-bnvsf [439.048884ms]
Nov 16 20:30:23.240: INFO: Created: latency-svc-qmxsw
Nov 16 20:30:23.254: INFO: Got endpoints: latency-svc-qmxsw [410.135845ms]
Nov 16 20:30:23.270: INFO: Created: latency-svc-2dlfg
Nov 16 20:30:23.278: INFO: Got endpoints: latency-svc-2dlfg [419.641473ms]
Nov 16 20:30:23.289: INFO: Created: latency-svc-ktqfx
Nov 16 20:30:23.297: INFO: Got endpoints: latency-svc-ktqfx [401.918955ms]
Nov 16 20:30:23.305: INFO: Created: latency-svc-mbc7r
Nov 16 20:30:23.316: INFO: Got endpoints: latency-svc-mbc7r [402.092096ms]
Nov 16 20:30:23.327: INFO: Created: latency-svc-p8kwd
Nov 16 20:30:23.347: INFO: Got endpoints: latency-svc-p8kwd [387.587202ms]
Nov 16 20:30:23.359: INFO: Created: latency-svc-xgt6v
Nov 16 20:30:23.368: INFO: Got endpoints: latency-svc-xgt6v [351.669985ms]
Nov 16 20:30:23.387: INFO: Created: latency-svc-8fdcb
Nov 16 20:30:23.394: INFO: Got endpoints: latency-svc-8fdcb [376.95752ms]
Nov 16 20:30:23.395: INFO: Created: latency-svc-sx9h4
Nov 16 20:30:23.408: INFO: Got endpoints: latency-svc-sx9h4 [385.122039ms]
Nov 16 20:30:23.408: INFO: Created: latency-svc-m9qfs
Nov 16 20:30:23.426: INFO: Got endpoints: latency-svc-m9qfs [366.805373ms]
Nov 16 20:30:23.432: INFO: Created: latency-svc-9zlw9
Nov 16 20:30:23.440: INFO: Got endpoints: latency-svc-9zlw9 [358.659075ms]
Nov 16 20:30:23.450: INFO: Created: latency-svc-7fcf8
Nov 16 20:30:23.459: INFO: Got endpoints: latency-svc-7fcf8 [349.583718ms]
Nov 16 20:30:23.476: INFO: Created: latency-svc-6nwrm
Nov 16 20:30:23.483: INFO: Got endpoints: latency-svc-6nwrm [344.395092ms]
Nov 16 20:30:23.491: INFO: Created: latency-svc-wcndc
Nov 16 20:30:23.502: INFO: Got endpoints: latency-svc-wcndc [333.163993ms]
Nov 16 20:30:23.509: INFO: Created: latency-svc-b7g7k
Nov 16 20:30:23.521: INFO: Got endpoints: latency-svc-b7g7k [320.748917ms]
Nov 16 20:30:23.528: INFO: Created: latency-svc-t6jll
Nov 16 20:30:23.539: INFO: Got endpoints: latency-svc-t6jll [309.475674ms]
Nov 16 20:30:23.553: INFO: Created: latency-svc-n4d9m
Nov 16 20:30:23.559: INFO: Got endpoints: latency-svc-n4d9m [304.429468ms]
Nov 16 20:30:23.569: INFO: Created: latency-svc-dmvcd
Nov 16 20:30:23.578: INFO: Got endpoints: latency-svc-dmvcd [299.699104ms]
Nov 16 20:30:23.586: INFO: Created: latency-svc-kmc9c
Nov 16 20:30:23.601: INFO: Got endpoints: latency-svc-kmc9c [303.871514ms]
Nov 16 20:30:23.612: INFO: Created: latency-svc-5r8ws
Nov 16 20:30:23.621: INFO: Got endpoints: latency-svc-5r8ws [304.440478ms]
Nov 16 20:30:23.627: INFO: Created: latency-svc-jtxzg
Nov 16 20:30:23.636: INFO: Got endpoints: latency-svc-jtxzg [289.273134ms]
Nov 16 20:30:23.644: INFO: Created: latency-svc-4j5qp
Nov 16 20:30:23.653: INFO: Got endpoints: latency-svc-4j5qp [285.712992ms]
Nov 16 20:30:23.661: INFO: Created: latency-svc-mkgxw
Nov 16 20:30:23.667: INFO: Got endpoints: latency-svc-mkgxw [272.78582ms]
Nov 16 20:30:23.675: INFO: Created: latency-svc-4kqfh
Nov 16 20:30:23.684: INFO: Got endpoints: latency-svc-4kqfh [275.83972ms]
Nov 16 20:30:23.696: INFO: Created: latency-svc-ljzp5
Nov 16 20:30:23.705: INFO: Got endpoints: latency-svc-ljzp5 [277.956576ms]
Nov 16 20:30:23.714: INFO: Created: latency-svc-x4s6s
Nov 16 20:30:23.733: INFO: Got endpoints: latency-svc-x4s6s [293.156521ms]
Nov 16 20:30:23.744: INFO: Created: latency-svc-pfzg4
Nov 16 20:30:23.753: INFO: Got endpoints: latency-svc-pfzg4 [293.591317ms]
Nov 16 20:30:23.760: INFO: Created: latency-svc-tzw6k
Nov 16 20:30:23.769: INFO: Got endpoints: latency-svc-tzw6k [286.245285ms]
Nov 16 20:30:23.781: INFO: Created: latency-svc-7c7ms
Nov 16 20:30:23.794: INFO: Created: latency-svc-m7jwl
Nov 16 20:30:23.795: INFO: Got endpoints: latency-svc-7c7ms [291.893112ms]
Nov 16 20:30:23.810: INFO: Got endpoints: latency-svc-m7jwl [288.512802ms]
Nov 16 20:30:23.824: INFO: Created: latency-svc-rv7xk
Nov 16 20:30:23.837: INFO: Got endpoints: latency-svc-rv7xk [298.427166ms]
Nov 16 20:30:23.847: INFO: Created: latency-svc-vqfst
Nov 16 20:30:23.857: INFO: Got endpoints: latency-svc-vqfst [297.600181ms]
Nov 16 20:30:23.864: INFO: Created: latency-svc-5s899
Nov 16 20:30:23.879: INFO: Got endpoints: latency-svc-5s899 [301.202176ms]
Nov 16 20:30:23.889: INFO: Created: latency-svc-ntlsd
Nov 16 20:30:23.900: INFO: Got endpoints: latency-svc-ntlsd [298.325107ms]
Nov 16 20:30:23.907: INFO: Created: latency-svc-99qn8
Nov 16 20:30:23.914: INFO: Got endpoints: latency-svc-99qn8 [292.887031ms]
Nov 16 20:30:23.936: INFO: Created: latency-svc-xhgrt
Nov 16 20:30:23.966: INFO: Got endpoints: latency-svc-xhgrt [329.640485ms]
Nov 16 20:30:23.977: INFO: Created: latency-svc-5xmf4
Nov 16 20:30:23.984: INFO: Got endpoints: latency-svc-5xmf4 [330.00097ms]
Nov 16 20:30:23.991: INFO: Created: latency-svc-qdhwd
Nov 16 20:30:23.997: INFO: Got endpoints: latency-svc-qdhwd [329.884525ms]
Nov 16 20:30:24.008: INFO: Created: latency-svc-8tv87
Nov 16 20:30:24.016: INFO: Got endpoints: latency-svc-8tv87 [330.965994ms]
Nov 16 20:30:24.022: INFO: Created: latency-svc-z67zn
Nov 16 20:30:24.031: INFO: Got endpoints: latency-svc-z67zn [325.146015ms]
Nov 16 20:30:24.037: INFO: Created: latency-svc-drjjq
Nov 16 20:30:24.048: INFO: Got endpoints: latency-svc-drjjq [313.63977ms]
Nov 16 20:30:24.059: INFO: Created: latency-svc-6cxr9
Nov 16 20:30:24.065: INFO: Got endpoints: latency-svc-6cxr9 [312.501677ms]
Nov 16 20:30:24.077: INFO: Created: latency-svc-vsl4h
Nov 16 20:30:24.109: INFO: Got endpoints: latency-svc-vsl4h [340.223917ms]
Nov 16 20:30:24.122: INFO: Created: latency-svc-w895b
Nov 16 20:30:24.129: INFO: Got endpoints: latency-svc-w895b [333.727255ms]
Nov 16 20:30:24.142: INFO: Created: latency-svc-hpsk9
Nov 16 20:30:24.152: INFO: Got endpoints: latency-svc-hpsk9 [341.53381ms]
Nov 16 20:30:24.162: INFO: Created: latency-svc-c466s
Nov 16 20:30:24.172: INFO: Created: latency-svc-jgwwp
Nov 16 20:30:24.172: INFO: Got endpoints: latency-svc-c466s [334.903754ms]
Nov 16 20:30:24.188: INFO: Created: latency-svc-ntklm
Nov 16 20:30:24.190: INFO: Got endpoints: latency-svc-jgwwp [332.733771ms]
Nov 16 20:30:24.199: INFO: Got endpoints: latency-svc-ntklm [319.327075ms]
Nov 16 20:30:24.224: INFO: Created: latency-svc-74mbs
Nov 16 20:30:24.243: INFO: Got endpoints: latency-svc-74mbs [342.786634ms]
Nov 16 20:30:24.258: INFO: Created: latency-svc-pmjgm
Nov 16 20:30:24.266: INFO: Got endpoints: latency-svc-pmjgm [352.175428ms]
Nov 16 20:30:24.290: INFO: Created: latency-svc-2bsv5
Nov 16 20:30:24.301: INFO: Got endpoints: latency-svc-2bsv5 [335.384636ms]
Nov 16 20:30:24.309: INFO: Created: latency-svc-9g58m
Nov 16 20:30:24.320: INFO: Got endpoints: latency-svc-9g58m [335.647901ms]
Nov 16 20:30:24.325: INFO: Created: latency-svc-ssz7d
Nov 16 20:30:24.334: INFO: Got endpoints: latency-svc-ssz7d [335.994105ms]
Nov 16 20:30:24.344: INFO: Created: latency-svc-crbzf
Nov 16 20:30:24.353: INFO: Got endpoints: latency-svc-crbzf [337.136695ms]
Nov 16 20:30:24.369: INFO: Created: latency-svc-9tg8b
Nov 16 20:30:24.403: INFO: Got endpoints: latency-svc-9tg8b [372.782299ms]
Nov 16 20:30:24.415: INFO: Created: latency-svc-jr9vx
Nov 16 20:30:24.438: INFO: Got endpoints: latency-svc-jr9vx [390.231414ms]
Nov 16 20:30:24.448: INFO: Created: latency-svc-r5zs5
Nov 16 20:30:24.459: INFO: Got endpoints: latency-svc-r5zs5 [391.690343ms]
Nov 16 20:30:24.467: INFO: Created: latency-svc-gp7gt
Nov 16 20:30:24.474: INFO: Got endpoints: latency-svc-gp7gt [364.691436ms]
Nov 16 20:30:24.484: INFO: Created: latency-svc-pdm88
Nov 16 20:30:24.492: INFO: Got endpoints: latency-svc-pdm88 [363.254777ms]
Nov 16 20:30:24.500: INFO: Created: latency-svc-c4ckn
Nov 16 20:30:24.507: INFO: Got endpoints: latency-svc-c4ckn [354.983127ms]
Nov 16 20:30:24.517: INFO: Created: latency-svc-59bsb
Nov 16 20:30:24.528: INFO: Got endpoints: latency-svc-59bsb [356.070651ms]
Nov 16 20:30:24.544: INFO: Created: latency-svc-mcqvf
Nov 16 20:30:24.552: INFO: Got endpoints: latency-svc-mcqvf [362.213908ms]
Nov 16 20:30:24.559: INFO: Created: latency-svc-trdt5
Nov 16 20:30:24.561: INFO: Got endpoints: latency-svc-trdt5 [362.412685ms]
Nov 16 20:30:24.572: INFO: Created: latency-svc-hd5gr
Nov 16 20:30:24.582: INFO: Got endpoints: latency-svc-hd5gr [338.81712ms]
Nov 16 20:30:24.591: INFO: Created: latency-svc-wvtrb
Nov 16 20:30:24.599: INFO: Got endpoints: latency-svc-wvtrb [332.319704ms]
Nov 16 20:30:24.617: INFO: Created: latency-svc-77mth
Nov 16 20:30:24.624: INFO: Created: latency-svc-j7shb
Nov 16 20:30:24.625: INFO: Got endpoints: latency-svc-77mth [323.210795ms]
Nov 16 20:30:24.633: INFO: Got endpoints: latency-svc-j7shb [313.471166ms]
Nov 16 20:30:24.641: INFO: Created: latency-svc-lsktl
Nov 16 20:30:24.661: INFO: Got endpoints: latency-svc-lsktl [327.55563ms]
Nov 16 20:30:24.672: INFO: Created: latency-svc-7fxvc
Nov 16 20:30:24.679: INFO: Got endpoints: latency-svc-7fxvc [325.522841ms]
Nov 16 20:30:24.689: INFO: Created: latency-svc-gdrsq
Nov 16 20:30:24.703: INFO: Got endpoints: latency-svc-gdrsq [299.45835ms]
Nov 16 20:30:24.714: INFO: Created: latency-svc-bxlx6
Nov 16 20:30:24.722: INFO: Got endpoints: latency-svc-bxlx6 [284.103525ms]
Nov 16 20:30:24.739: INFO: Created: latency-svc-b4pff
Nov 16 20:30:24.740: INFO: Got endpoints: latency-svc-b4pff [280.840375ms]
Nov 16 20:30:24.753: INFO: Created: latency-svc-wpc8l
Nov 16 20:30:24.766: INFO: Got endpoints: latency-svc-wpc8l [291.724467ms]
Nov 16 20:30:24.767: INFO: Created: latency-svc-5skzf
Nov 16 20:30:24.775: INFO: Got endpoints: latency-svc-5skzf [282.213934ms]
Nov 16 20:30:24.792: INFO: Created: latency-svc-4hsv6
Nov 16 20:30:24.802: INFO: Got endpoints: latency-svc-4hsv6 [295.447314ms]
Nov 16 20:30:24.811: INFO: Created: latency-svc-rmc2n
Nov 16 20:30:24.819: INFO: Got endpoints: latency-svc-rmc2n [290.730474ms]
Nov 16 20:30:24.819: INFO: Latencies: [45.545504ms 54.737396ms 70.302862ms 82.749717ms 118.699893ms 140.852034ms 157.228703ms 174.620923ms 194.223146ms 208.048066ms 233.316003ms 253.40884ms 268.964792ms 269.484902ms 270.797203ms 271.172103ms 271.269221ms 272.12018ms 272.78582ms 273.108639ms 273.679185ms 273.730943ms 273.750824ms 274.248656ms 274.752465ms 274.811916ms 274.841079ms 275.8339ms 275.83972ms 275.861952ms 276.298175ms 276.61307ms 276.695952ms 276.873936ms 276.999012ms 277.956576ms 279.47901ms 280.00652ms 280.761992ms 280.840375ms 282.213934ms 283.193394ms 284.103525ms 285.712992ms 286.245285ms 286.264667ms 286.903423ms 288.189902ms 288.512802ms 289.273134ms 289.513839ms 290.730474ms 291.724467ms 291.893112ms 292.740052ms 292.887031ms 293.156521ms 293.591317ms 295.447314ms 296.715199ms 296.715274ms 296.778929ms 297.131074ms 297.600181ms 298.325107ms 298.427166ms 298.55478ms 299.45835ms 299.699104ms 301.202176ms 302.244635ms 303.123985ms 303.202602ms 303.406299ms 303.871514ms 304.429468ms 304.440478ms 305.18221ms 305.948807ms 306.748983ms 307.883656ms 309.475674ms 312.423182ms 312.501677ms 313.203095ms 313.356086ms 313.471166ms 313.63977ms 314.116423ms 315.450641ms 316.921739ms 317.740702ms 317.846331ms 319.327075ms 319.651318ms 320.748917ms 321.129412ms 321.150427ms 323.210795ms 323.394615ms 325.146015ms 325.522841ms 327.55563ms 329.640485ms 329.884525ms 330.00097ms 330.965994ms 332.319704ms 332.733771ms 333.163993ms 333.599506ms 333.727255ms 334.903754ms 335.356773ms 335.384636ms 335.647901ms 335.994105ms 337.136695ms 337.50835ms 337.883235ms 338.81712ms 340.223917ms 340.412443ms 340.492277ms 341.53381ms 342.786634ms 344.395092ms 344.965818ms 346.955115ms 349.583718ms 350.521865ms 351.669985ms 352.175428ms 352.725994ms 354.983127ms 356.070651ms 356.193743ms 358.659075ms 361.422417ms 362.152502ms 362.213908ms 362.412685ms 363.254777ms 364.691436ms 365.699838ms 366.805373ms 368.470269ms 372.782299ms 373.325618ms 373.773016ms 374.712893ms 375.624802ms 376.95752ms 377.254661ms 377.708783ms 378.361923ms 379.581362ms 380.180604ms 381.837685ms 382.473974ms 385.122039ms 385.191671ms 386.209031ms 387.587202ms 388.439741ms 389.158416ms 390.231414ms 391.554762ms 391.690343ms 394.994212ms 399.854957ms 401.918955ms 402.092096ms 404.104402ms 407.413861ms 410.135845ms 412.313669ms 414.884094ms 415.50031ms 415.584447ms 419.641473ms 428.373954ms 429.251501ms 430.689732ms 439.048884ms 441.54697ms 442.772238ms 449.611966ms 460.355805ms 475.961556ms 476.473696ms 477.708735ms 478.309962ms 482.046774ms 483.070709ms 490.453605ms 494.608329ms 508.732594ms 516.921864ms 537.694284ms]
Nov 16 20:30:24.820: INFO: 50 %ile: 325.146015ms
Nov 16 20:30:24.820: INFO: 90 %ile: 419.641473ms
Nov 16 20:30:24.820: INFO: 99 %ile: 516.921864ms
Nov 16 20:30:24.820: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:24.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4986" for this suite.

â€¢ [SLOW TEST:8.063 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":206,"skipped":3990,"failed":0}
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:30:25.142: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3" in namespace "security-context-test-557" to be "Succeeded or Failed"
Nov 16 20:30:25.159: INFO: Pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.361524ms
Nov 16 20:30:27.179: INFO: Pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037010351s
Nov 16 20:30:29.198: INFO: Pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056195813s
Nov 16 20:30:31.222: INFO: Pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08007774s
Nov 16 20:30:31.222: INFO: Pod "alpine-nnp-false-b70c59da-2865-477b-ab8c-16ec26eba9f3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:31.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-557" for this suite.

â€¢ [SLOW TEST:6.433 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3991,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:31.307: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:30:31.614: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ebf39aff-d568-4838-be80-6e8b3f7df53a", Controller:(*bool)(0xc00368547e), BlockOwnerDeletion:(*bool)(0xc00368547f)}}
Nov 16 20:30:31.632: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8234f247-c78a-4336-be60-524a742a8dd5", Controller:(*bool)(0xc0036856c6), BlockOwnerDeletion:(*bool)(0xc0036856c7)}}
Nov 16 20:30:31.652: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1bd0637b-0d06-4367-b064-6967d6c61152", Controller:(*bool)(0xc005651786), BlockOwnerDeletion:(*bool)(0xc005651787)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:36.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-261" for this suite.

â€¢ [SLOW TEST:5.465 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":208,"skipped":4004,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-685c8648-9b7e-4789-bec0-d6a960d23912
STEP: Creating a pod to test consume secrets
Nov 16 20:30:37.061: INFO: Waiting up to 5m0s for pod "pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588" in namespace "secrets-5043" to be "Succeeded or Failed"
Nov 16 20:30:37.075: INFO: Pod "pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588": Phase="Pending", Reason="", readiness=false. Elapsed: 13.628716ms
Nov 16 20:30:39.097: INFO: Pod "pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034887536s
Nov 16 20:30:41.116: INFO: Pod "pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054726548s
STEP: Saw pod success
Nov 16 20:30:41.116: INFO: Pod "pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588" satisfied condition "Succeeded or Failed"
Nov 16 20:30:41.130: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:30:41.200: INFO: Waiting for pod pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588 to disappear
Nov 16 20:30:41.214: INFO: Pod pod-secrets-ddeaff97-57b6-4d5f-9d05-1f95f3d85588 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:30:41.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5043" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":4013,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:30:41.259: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:30:41.568: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 20:30:41.659: INFO: Number of nodes with available pods: 0
Nov 16 20:30:41.659: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:30:42.701: INFO: Number of nodes with available pods: 0
Nov 16 20:30:42.701: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:30:43.697: INFO: Number of nodes with available pods: 1
Nov 16 20:30:43.697: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:30:44.698: INFO: Number of nodes with available pods: 3
Nov 16 20:30:44.698: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 16 20:30:44.813: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:44.813: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:45.858: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:45.858: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:46.851: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:46.851: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:47.849: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:47.849: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:48.851: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:48.851: INFO: Pod daemon-set-8qwj9 is not available
Nov 16 20:30:48.851: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:49.854: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:49.854: INFO: Pod daemon-set-8qwj9 is not available
Nov 16 20:30:49.854: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:50.851: INFO: Wrong image for pod: daemon-set-6k59s. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:50.851: INFO: Pod daemon-set-8qwj9 is not available
Nov 16 20:30:50.851: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:51.847: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:52.852: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:52.852: INFO: Pod daemon-set-mxqj4 is not available
Nov 16 20:30:53.849: INFO: Wrong image for pod: daemon-set-97qqr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Nov 16 20:30:53.849: INFO: Pod daemon-set-mxqj4 is not available
Nov 16 20:30:56.848: INFO: Pod daemon-set-nd68f is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 16 20:30:56.899: INFO: Number of nodes with available pods: 2
Nov 16 20:30:56.899: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:30:57.937: INFO: Number of nodes with available pods: 2
Nov 16 20:30:57.937: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:30:58.935: INFO: Number of nodes with available pods: 3
Nov 16 20:30:58.935: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7533, will wait for the garbage collector to delete the pods
Nov 16 20:30:59.086: INFO: Deleting DaemonSet.extensions daemon-set took: 26.117957ms
Nov 16 20:30:59.187: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.132755ms
Nov 16 20:31:01.912: INFO: Number of nodes with available pods: 0
Nov 16 20:31:01.913: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 20:31:01.924: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41084"},"items":null}

Nov 16 20:31:01.938: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41084"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7533" for this suite.

â€¢ [SLOW TEST:20.784 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":210,"skipped":4019,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:02.043: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Nov 16 20:31:02.332: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:31:04.352: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:31:06.354: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 16 20:31:07.426: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:07.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6270" for this suite.

â€¢ [SLOW TEST:5.488 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":211,"skipped":4020,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:07.531: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3048
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:31:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 20:31:12.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-3048 --namespace=crd-publish-openapi-3048 create -f -'
Nov 16 20:31:13.152: INFO: stderr: ""
Nov 16 20:31:13.152: INFO: stdout: "e2e-test-crd-publish-openapi-9836-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 20:31:13.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-3048 --namespace=crd-publish-openapi-3048 delete e2e-test-crd-publish-openapi-9836-crds test-cr'
Nov 16 20:31:13.277: INFO: stderr: ""
Nov 16 20:31:13.277: INFO: stdout: "e2e-test-crd-publish-openapi-9836-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Nov 16 20:31:13.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-3048 --namespace=crd-publish-openapi-3048 apply -f -'
Nov 16 20:31:13.653: INFO: stderr: ""
Nov 16 20:31:13.653: INFO: stdout: "e2e-test-crd-publish-openapi-9836-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Nov 16 20:31:13.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-3048 --namespace=crd-publish-openapi-3048 delete e2e-test-crd-publish-openapi-9836-crds test-cr'
Nov 16 20:31:13.788: INFO: stderr: ""
Nov 16 20:31:13.788: INFO: stdout: "e2e-test-crd-publish-openapi-9836-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Nov 16 20:31:13.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-3048 explain e2e-test-crd-publish-openapi-9836-crds'
Nov 16 20:31:14.024: INFO: stderr: ""
Nov 16 20:31:14.024: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9836-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:18.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3048" for this suite.

â€¢ [SLOW TEST:11.178 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":212,"skipped":4024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 20:31:21.055: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:21.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9952" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":4063,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:21.159: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ad0fd519-ec7b-4249-82da-209d41252ac9
STEP: Creating a pod to test consume configMaps
Nov 16 20:31:21.460: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501" in namespace "projected-3017" to be "Succeeded or Failed"
Nov 16 20:31:21.477: INFO: Pod "pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501": Phase="Pending", Reason="", readiness=false. Elapsed: 17.644202ms
Nov 16 20:31:23.499: INFO: Pod "pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039139349s
Nov 16 20:31:25.521: INFO: Pod "pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061564995s
STEP: Saw pod success
Nov 16 20:31:25.522: INFO: Pod "pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501" satisfied condition "Succeeded or Failed"
Nov 16 20:31:25.536: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:31:25.609: INFO: Waiting for pod pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501 to disappear
Nov 16 20:31:25.623: INFO: Pod pod-projected-configmaps-bd22f2f1-ef85-402c-9694-eab6f9d70501 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:25.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3017" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4073,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:25.670: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Nov 16 20:31:26.015: INFO: Found Service test-service-ss8mk in namespace services-3402 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Nov 16 20:31:26.015: INFO: Service test-service-ss8mk created
STEP: Getting /status
Nov 16 20:31:26.031: INFO: Service test-service-ss8mk has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Nov 16 20:31:26.055: INFO: observed Service test-service-ss8mk in namespace services-3402 with annotations: map[] & LoadBalancer: {[]}
Nov 16 20:31:26.055: INFO: Found Service test-service-ss8mk in namespace services-3402 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Nov 16 20:31:26.055: INFO: Service test-service-ss8mk has service status patched
STEP: updating the ServiceStatus
Nov 16 20:31:26.089: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Nov 16 20:31:26.095: INFO: Observed Service test-service-ss8mk in namespace services-3402 with annotations: map[] & Conditions: {[]}
Nov 16 20:31:26.095: INFO: Observed event: &Service{ObjectMeta:{test-service-ss8mk  services-3402  6b0dc990-95b4-4e28-b8f3-a2b45391f0a6 41309 0 2021-11-16 20:31:25 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-11-16 20:31:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2021-11-16 20:31:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.163.75,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:nil,ClusterIPs:[172.21.163.75],IPFamilies:[],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Nov 16 20:31:26.095: INFO: Found Service test-service-ss8mk in namespace services-3402 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Nov 16 20:31:26.095: INFO: Service test-service-ss8mk has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Nov 16 20:31:26.125: INFO: observed Service test-service-ss8mk in namespace services-3402 with labels: map[test-service-static:true]
Nov 16 20:31:26.126: INFO: observed Service test-service-ss8mk in namespace services-3402 with labels: map[test-service-static:true]
Nov 16 20:31:26.126: INFO: observed Service test-service-ss8mk in namespace services-3402 with labels: map[test-service-static:true]
Nov 16 20:31:26.126: INFO: Found Service test-service-ss8mk in namespace services-3402 with labels: map[test-service:patched test-service-static:true]
Nov 16 20:31:26.126: INFO: Service test-service-ss8mk patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Nov 16 20:31:26.175: INFO: Observed event: ADDED
Nov 16 20:31:26.175: INFO: Observed event: MODIFIED
Nov 16 20:31:26.175: INFO: Observed event: MODIFIED
Nov 16 20:31:26.176: INFO: Observed event: MODIFIED
Nov 16 20:31:26.176: INFO: Found Service test-service-ss8mk in namespace services-3402 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Nov 16 20:31:26.176: INFO: Service test-service-ss8mk deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:26.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3402" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
â€¢{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":215,"skipped":4082,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:26.220: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Nov 16 20:31:26.454: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:31:30.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7166" for this suite.
â€¢{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":216,"skipped":4094,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:31:30.969: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6340.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6340.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6340.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6340.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:31:35.385: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.404: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.421: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.439: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.490: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.508: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.526: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.543: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:35.578: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:31:40.597: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:40.614: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:40.699: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:40.717: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:40.787: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:31:45.600: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:45.618: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:45.711: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:45.730: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:45.806: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:31:50.597: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:50.614: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:50.706: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:50.724: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:50.795: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:31:55.596: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:55.615: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:55.700: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:55.718: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:31:55.788: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:32:00.597: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:32:00.616: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:32:00.709: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:32:00.728: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local from pod dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64: the server could not find the requested resource (get pods dns-test-c72a29e9-b121-462a-b068-fa0744512c64)
Nov 16 20:32:00.799: INFO: Lookups using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6340.svc.cluster.local]

Nov 16 20:32:05.792: INFO: DNS probes using dns-6340/dns-test-c72a29e9-b121-462a-b068-fa0744512c64 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:05.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6340" for this suite.

â€¢ [SLOW TEST:34.981 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":217,"skipped":4109,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 16 20:32:06.231: INFO: Waiting up to 5m0s for pod "downward-api-95041af2-26b9-43a2-99c6-e9070234d048" in namespace "downward-api-1414" to be "Succeeded or Failed"
Nov 16 20:32:06.246: INFO: Pod "downward-api-95041af2-26b9-43a2-99c6-e9070234d048": Phase="Pending", Reason="", readiness=false. Elapsed: 14.861986ms
Nov 16 20:32:08.267: INFO: Pod "downward-api-95041af2-26b9-43a2-99c6-e9070234d048": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035481743s
Nov 16 20:32:10.286: INFO: Pod "downward-api-95041af2-26b9-43a2-99c6-e9070234d048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05466485s
STEP: Saw pod success
Nov 16 20:32:10.286: INFO: Pod "downward-api-95041af2-26b9-43a2-99c6-e9070234d048" satisfied condition "Succeeded or Failed"
Nov 16 20:32:10.299: INFO: Trying to get logs from node 10.193.87.27 pod downward-api-95041af2-26b9-43a2-99c6-e9070234d048 container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:32:10.370: INFO: Waiting for pod downward-api-95041af2-26b9-43a2-99c6-e9070234d048 to disappear
Nov 16 20:32:10.384: INFO: Pod downward-api-95041af2-26b9-43a2-99c6-e9070234d048 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:10.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1414" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4112,"failed":0}
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:10.423: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:10.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-173" for this suite.
â€¢{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":219,"skipped":4119,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4980
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Nov 16 20:32:10.990: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 16 20:32:16.022: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4980" for this suite.

â€¢ [SLOW TEST:5.450 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":220,"skipped":4124,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:16.165: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:16.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1207" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":221,"skipped":4130,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:32:16.784: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 16 20:32:17.890: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1500" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":222,"skipped":4130,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:17.964: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-733f79fa-8c0f-40d5-844a-cae7a27fac3d
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:18.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4582" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":223,"skipped":4143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:18.313: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Nov 16 20:32:18.570: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Nov 16 20:32:19.994: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Nov 16 20:32:22.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:24.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:26.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:28.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:30.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:32.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772691540, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:32:36.186: INFO: Waited 1.997068028s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Nov 16 20:32:36.442: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3561" for this suite.

â€¢ [SLOW TEST:18.684 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":224,"skipped":4169,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:36.997: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-7e0d7847-a9db-4636-9695-eae9ff8fee04
STEP: Creating a pod to test consume secrets
Nov 16 20:32:37.351: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac" in namespace "projected-6670" to be "Succeeded or Failed"
Nov 16 20:32:37.365: INFO: Pod "pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac": Phase="Pending", Reason="", readiness=false. Elapsed: 13.904009ms
Nov 16 20:32:39.382: INFO: Pod "pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac": Phase="Running", Reason="", readiness=true. Elapsed: 2.030882602s
Nov 16 20:32:41.403: INFO: Pod "pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051697667s
STEP: Saw pod success
Nov 16 20:32:41.403: INFO: Pod "pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac" satisfied condition "Succeeded or Failed"
Nov 16 20:32:41.416: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:32:41.490: INFO: Waiting for pod pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac to disappear
Nov 16 20:32:41.502: INFO: Pod pod-projected-secrets-0d3f0c9f-03a2-4ff2-843c-53ae749bdbac no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:41.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6670" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4169,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:41.544: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 20:32:41.808: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 20:32:41.848: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 20:32:41.861: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.24 before test
Nov 16 20:32:41.893: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-11-16 18:07:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 20:32:41.893: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 20:32:41.893: INFO: calico-node-5cnhh from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:41.893: INFO: calico-typha-d5b48569-hcfb8 from kube-system started at 2021-11-16 18:05:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:32:41.893: INFO: coredns-b58d5f584-g92hx from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:32:41.893: INFO: ibm-keepalived-watcher-bl7zr from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:41.893: INFO: ibm-master-proxy-static-10.193.87.24 from kube-system started at 2021-11-16 18:05:01 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:41.893: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:41.893: INFO: konnectivity-agent-wtjm7 from kube-system started at 2021-11-16 18:13:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:32:41.893: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9 from kube-system started at 2021-11-16 18:08:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 20:32:41.893: INFO: sonobuoy-e2e-job-54c8cae784204424 from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container e2e ready: true, restart count 0
Nov 16 20:32:41.893: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:32:41.893: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.893: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:32:41.893: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:32:41.893: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.27 before test
Nov 16 20:32:41.917: INFO: calico-node-smn5q from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.918: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:41.918: INFO: calico-typha-d5b48569-bjxkh from kube-system started at 2021-11-16 20:18:43 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.918: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:32:41.919: INFO: coredns-b58d5f584-vp4t9 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.919: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:32:41.919: INFO: ibm-keepalived-watcher-rb6gd from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.919: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:41.920: INFO: ibm-master-proxy-static-10.193.87.27 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.920: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:41.920: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:41.921: INFO: konnectivity-agent-ln55x from kube-system started at 2021-11-16 18:13:25 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.921: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:32:41.921: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j22wg from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:32:41.922: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 20:32:41.922: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.28 before test
Nov 16 20:32:41.956: INFO: catalog-operator-7489d5857-vlxrz from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.957: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 20:32:41.957: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-sf6d4 from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.957: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 20:32:41.957: INFO: olm-operator-7b6cd6c94c-5vgk4 from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.958: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 20:32:41.958: INFO: calico-kube-controllers-75488ccc5b-pf6m8 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.958: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 20:32:41.958: INFO: calico-node-sx675 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.958: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 20:32:41.959: INFO: calico-typha-d5b48569-s7xpk from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.959: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 20:32:41.959: INFO: coredns-autoscaler-689fb74d49-ww9hg from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.959: INFO: 	Container autoscaler ready: true, restart count 0
Nov 16 20:32:41.959: INFO: coredns-b58d5f584-ntqv9 from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.960: INFO: 	Container coredns ready: true, restart count 0
Nov 16 20:32:41.960: INFO: dashboard-metrics-scraper-6747f89c97-pzthq from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.960: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 16 20:32:41.960: INFO: ibm-file-plugin-fd44cd466-zjcs4 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.961: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 20:32:41.961: INFO: ibm-keepalived-watcher-mfkdl from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.961: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 20:32:41.961: INFO: ibm-master-proxy-static-10.193.87.28 from kube-system started at 2021-11-16 18:04:49 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.961: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 20:32:41.962: INFO: 	Container pause ready: true, restart count 0
Nov 16 20:32:41.962: INFO: ibm-storage-watcher-765888f8c9-dffqn from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.962: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 20:32:41.962: INFO: konnectivity-agent-g2pw7 from kube-system started at 2021-11-16 18:13:28 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.962: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 20:32:41.963: INFO: kubernetes-dashboard-54c47dd995-czt2b from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.963: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 16 20:32:41.963: INFO: metrics-server-64bbdfc744-wxsz6 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.963: INFO: 	Container metrics-server ready: true, restart count 0
Nov 16 20:32:41.963: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 16 20:32:41.964: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bb4n2 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.964: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 20:32:41.964: INFO: sonobuoy from sonobuoy started at 2021-11-16 19:29:33 +0000 UTC (1 container statuses recorded)
Nov 16 20:32:41.964: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 20:32:41.964: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-hpqln from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 20:32:41.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 20:32:41.965: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.193.87.24
STEP: verifying the node has the label node 10.193.87.27
STEP: verifying the node has the label node 10.193.87.28
Nov 16 20:32:42.182: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.193.87.24
Nov 16 20:32:42.183: INFO: Pod catalog-operator-7489d5857-vlxrz requesting resource cpu=10m on Node 10.193.87.28
Nov 16 20:32:42.183: INFO: Pod ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-sf6d4 requesting resource cpu=5m on Node 10.193.87.28
Nov 16 20:32:42.183: INFO: Pod ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b requesting resource cpu=5m on Node 10.193.87.24
Nov 16 20:32:42.183: INFO: Pod olm-operator-7b6cd6c94c-5vgk4 requesting resource cpu=10m on Node 10.193.87.28
Nov 16 20:32:42.183: INFO: Pod calico-kube-controllers-75488ccc5b-pf6m8 requesting resource cpu=10m on Node 10.193.87.28
Nov 16 20:32:42.183: INFO: Pod calico-node-5cnhh requesting resource cpu=250m on Node 10.193.87.24
Nov 16 20:32:42.183: INFO: Pod calico-node-smn5q requesting resource cpu=250m on Node 10.193.87.27
Nov 16 20:32:42.184: INFO: Pod calico-node-sx675 requesting resource cpu=250m on Node 10.193.87.28
Nov 16 20:32:42.184: INFO: Pod calico-typha-d5b48569-bjxkh requesting resource cpu=250m on Node 10.193.87.27
Nov 16 20:32:42.184: INFO: Pod calico-typha-d5b48569-hcfb8 requesting resource cpu=250m on Node 10.193.87.24
Nov 16 20:32:42.184: INFO: Pod calico-typha-d5b48569-s7xpk requesting resource cpu=250m on Node 10.193.87.28
Nov 16 20:32:42.184: INFO: Pod coredns-autoscaler-689fb74d49-ww9hg requesting resource cpu=20m on Node 10.193.87.28
Nov 16 20:32:42.184: INFO: Pod coredns-b58d5f584-g92hx requesting resource cpu=100m on Node 10.193.87.24
Nov 16 20:32:42.184: INFO: Pod coredns-b58d5f584-ntqv9 requesting resource cpu=100m on Node 10.193.87.28
Nov 16 20:32:42.184: INFO: Pod coredns-b58d5f584-vp4t9 requesting resource cpu=100m on Node 10.193.87.27
Nov 16 20:32:42.184: INFO: Pod dashboard-metrics-scraper-6747f89c97-pzthq requesting resource cpu=1m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod ibm-file-plugin-fd44cd466-zjcs4 requesting resource cpu=50m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod ibm-keepalived-watcher-bl7zr requesting resource cpu=5m on Node 10.193.87.24
Nov 16 20:32:42.185: INFO: Pod ibm-keepalived-watcher-mfkdl requesting resource cpu=5m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod ibm-keepalived-watcher-rb6gd requesting resource cpu=5m on Node 10.193.87.27
Nov 16 20:32:42.185: INFO: Pod ibm-master-proxy-static-10.193.87.24 requesting resource cpu=25m on Node 10.193.87.24
Nov 16 20:32:42.185: INFO: Pod ibm-master-proxy-static-10.193.87.27 requesting resource cpu=25m on Node 10.193.87.27
Nov 16 20:32:42.185: INFO: Pod ibm-master-proxy-static-10.193.87.28 requesting resource cpu=25m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod ibm-storage-watcher-765888f8c9-dffqn requesting resource cpu=50m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod konnectivity-agent-g2pw7 requesting resource cpu=10m on Node 10.193.87.28
Nov 16 20:32:42.185: INFO: Pod konnectivity-agent-ln55x requesting resource cpu=10m on Node 10.193.87.27
Nov 16 20:32:42.185: INFO: Pod konnectivity-agent-wtjm7 requesting resource cpu=10m on Node 10.193.87.24
Nov 16 20:32:42.186: INFO: Pod kubernetes-dashboard-54c47dd995-czt2b requesting resource cpu=50m on Node 10.193.87.28
Nov 16 20:32:42.186: INFO: Pod metrics-server-64bbdfc744-wxsz6 requesting resource cpu=121m on Node 10.193.87.28
Nov 16 20:32:42.186: INFO: Pod public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bb4n2 requesting resource cpu=10m on Node 10.193.87.28
Nov 16 20:32:42.186: INFO: Pod public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9 requesting resource cpu=10m on Node 10.193.87.24
Nov 16 20:32:42.186: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.193.87.28
Nov 16 20:32:42.186: INFO: Pod sonobuoy-e2e-job-54c8cae784204424 requesting resource cpu=0m on Node 10.193.87.24
Nov 16 20:32:42.186: INFO: Pod sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-hpqln requesting resource cpu=0m on Node 10.193.87.28
Nov 16 20:32:42.186: INFO: Pod sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j22wg requesting resource cpu=0m on Node 10.193.87.27
Nov 16 20:32:42.186: INFO: Pod sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt requesting resource cpu=0m on Node 10.193.87.24
STEP: Starting Pods to consume most of the cluster CPU.
Nov 16 20:32:42.186: INFO: Creating a pod which consumes cpu=2278m on Node 10.193.87.24
Nov 16 20:32:42.215: INFO: Creating a pod which consumes cpu=2289m on Node 10.193.87.27
Nov 16 20:32:42.238: INFO: Creating a pod which consumes cpu=2053m on Node 10.193.87.28
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d.16b8210336b58db8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8091/filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d to 10.193.87.24]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d.16b8210389c35817], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d.16b821038fcff521], Reason = [Created], Message = [Created container filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d.16b821039fdb34ea], Reason = [Started], Message = [Started container filler-pod-1bc54317-49c2-4b68-bc17-9fe0fb337a9d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c.16b8210337d3f282], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8091/filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c to 10.193.87.27]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c.16b8210383a4a802], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c.16b8210389c41e30], Reason = [Created], Message = [Created container filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c.16b8210398950e69], Reason = [Started], Message = [Started container filler-pod-7ac1d214-89b0-41d5-9413-5c1e218c176c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b.16b82103394f839f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8091/filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b to 10.193.87.28]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b.16b821039e190558], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b.16b82103a1fc8124], Reason = [Created], Message = [Created container filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b.16b82103af9feeea], Reason = [Started], Message = [Started container filler-pod-cfd71c4c-8e9c-447d-8276-608224f0a11b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16b821042d4c5b3a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.193.87.28
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.193.87.24
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.193.87.27
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:47.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8091" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:6.045 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":226,"skipped":4174,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:47.590: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-d8b6c63b-2e86-4b50-97ae-d0a40469ab13
STEP: Creating a pod to test consume secrets
Nov 16 20:32:47.871: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2" in namespace "projected-8815" to be "Succeeded or Failed"
Nov 16 20:32:47.885: INFO: Pod "pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.916262ms
Nov 16 20:32:49.902: INFO: Pod "pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030645616s
Nov 16 20:32:51.922: INFO: Pod "pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051000387s
STEP: Saw pod success
Nov 16 20:32:51.923: INFO: Pod "pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2" satisfied condition "Succeeded or Failed"
Nov 16 20:32:51.936: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:32:52.011: INFO: Waiting for pod pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2 to disappear
Nov 16 20:32:52.023: INFO: Pod pod-projected-secrets-64e125aa-6e4e-4ee2-8e58-ac911adf37e2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:52.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8815" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4191,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:52.066: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:32:52.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2250" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":228,"skipped":4192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:32:52.483: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:37:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-934" for this suite.

â€¢ [SLOW TEST:300.390 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":229,"skipped":4240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:37:52.875: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Nov 16 20:37:53.149: INFO: Waiting up to 5m0s for pod "var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0" in namespace "var-expansion-5365" to be "Succeeded or Failed"
Nov 16 20:37:53.164: INFO: Pod "var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.123053ms
Nov 16 20:37:55.183: INFO: Pod "var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033937435s
Nov 16 20:37:57.204: INFO: Pod "var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054819214s
STEP: Saw pod success
Nov 16 20:37:57.204: INFO: Pod "var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0" satisfied condition "Succeeded or Failed"
Nov 16 20:37:57.219: INFO: Trying to get logs from node 10.193.87.27 pod var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0 container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:37:57.386: INFO: Waiting for pod var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0 to disappear
Nov 16 20:37:57.399: INFO: Pod var-expansion-0f8f4c9e-c0c6-4495-b59a-d85206f77be0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:37:57.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5365" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4279,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:37:57.442: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-7592e503-cf60-4f4d-9f47-9006662e6ce6
STEP: Creating a pod to test consume secrets
Nov 16 20:37:57.720: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa" in namespace "projected-4689" to be "Succeeded or Failed"
Nov 16 20:37:57.733: INFO: Pod "pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.401342ms
Nov 16 20:37:59.759: INFO: Pod "pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa": Phase="Running", Reason="", readiness=true. Elapsed: 2.039572914s
Nov 16 20:38:01.781: INFO: Pod "pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061324541s
STEP: Saw pod success
Nov 16 20:38:01.781: INFO: Pod "pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa" satisfied condition "Succeeded or Failed"
Nov 16 20:38:01.795: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:38:01.875: INFO: Waiting for pod pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa to disappear
Nov 16 20:38:01.889: INFO: Pod pod-projected-secrets-b9820668-ebeb-457c-90aa-cc3365c7bafa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:38:01.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4689" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4284,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:38:01.932: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-62a17d86-0d85-42f4-9e3f-acd5dcb023a2 in namespace container-probe-2737
Nov 16 20:38:06.242: INFO: Started pod liveness-62a17d86-0d85-42f4-9e3f-acd5dcb023a2 in namespace container-probe-2737
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 20:38:06.260: INFO: Initial restart count of pod liveness-62a17d86-0d85-42f4-9e3f-acd5dcb023a2 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:06.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2737" for this suite.

â€¢ [SLOW TEST:245.051 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4286,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:06.983: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:42:11.301: INFO: Deleting pod "var-expansion-30ba126b-9d0d-422f-be64-549d5f7dd9a1" in namespace "var-expansion-264"
Nov 16 20:42:11.328: INFO: Wait up to 5m0s for pod "var-expansion-30ba126b-9d0d-422f-be64-549d5f7dd9a1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:13.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-264" for this suite.

â€¢ [SLOW TEST:6.432 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":233,"skipped":4288,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:13.418: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3683.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3683.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3683.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3683.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 136.121.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.121.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.121.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.121.136_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3683.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3683.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3683.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3683.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3683.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3683.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 136.121.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.121.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.121.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.121.136_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:42:17.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:17.854: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:17.891: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:18.027: INFO: Unable to read jessie_udp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:18.046: INFO: Unable to read jessie_tcp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:18.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:18.197: INFO: Lookups using dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb failed for: [wheezy_udp@dns-test-service.dns-3683.svc.cluster.local wheezy_tcp@dns-test-service.dns-3683.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local jessie_udp@dns-test-service.dns-3683.svc.cluster.local jessie_tcp@dns-test-service.dns-3683.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3683.svc.cluster.local]

Nov 16 20:42:23.218: INFO: Unable to read wheezy_udp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:23.400: INFO: Unable to read jessie_udp@dns-test-service.dns-3683.svc.cluster.local from pod dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb: the server could not find the requested resource (get pods dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb)
Nov 16 20:42:23.561: INFO: Lookups using dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb failed for: [wheezy_udp@dns-test-service.dns-3683.svc.cluster.local jessie_udp@dns-test-service.dns-3683.svc.cluster.local]

Nov 16 20:42:28.561: INFO: DNS probes using dns-3683/dns-test-3be25262-4f67-4432-aeae-8e9a9c7836eb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:28.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3683" for this suite.

â€¢ [SLOW TEST:15.343 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":234,"skipped":4302,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-8313
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Nov 16 20:42:29.116: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Nov 16 20:42:29.192: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:29.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8313" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":235,"skipped":4314,"failed":0}

------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:29.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8402" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":236,"skipped":4314,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:29.673: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Nov 16 20:42:29.937: INFO: Waiting up to 5m0s for pod "var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b" in namespace "var-expansion-7061" to be "Succeeded or Failed"
Nov 16 20:42:29.952: INFO: Pod "var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.41367ms
Nov 16 20:42:31.974: INFO: Pod "var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036736093s
Nov 16 20:42:33.995: INFO: Pod "var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057762893s
STEP: Saw pod success
Nov 16 20:42:33.995: INFO: Pod "var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b" satisfied condition "Succeeded or Failed"
Nov 16 20:42:34.009: INFO: Trying to get logs from node 10.193.87.27 pod var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:42:34.137: INFO: Waiting for pod var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b to disappear
Nov 16 20:42:34.150: INFO: Pod var-expansion-a76be585-7ba3-4472-b1ca-27036da6843b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:34.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7061" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4317,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:34.191: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-2f02fba4-ac8e-444a-b43a-5544cb65fb6c
STEP: Creating a pod to test consume configMaps
Nov 16 20:42:34.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a" in namespace "configmap-8055" to be "Succeeded or Failed"
Nov 16 20:42:34.485: INFO: Pod "pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.915822ms
Nov 16 20:42:36.498: INFO: Pod "pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022047045s
Nov 16 20:42:38.512: INFO: Pod "pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035928768s
STEP: Saw pod success
Nov 16 20:42:38.512: INFO: Pod "pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a" satisfied condition "Succeeded or Failed"
Nov 16 20:42:38.522: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:42:38.605: INFO: Waiting for pod pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a to disappear
Nov 16 20:42:38.617: INFO: Pod pod-configmaps-03e8bc36-9f0a-4644-80be-73a000d1775a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:38.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8055" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:38.661: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-958
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:42:38.879: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Nov 16 20:42:43.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-958 --namespace=crd-publish-openapi-958 create -f -'
Nov 16 20:42:44.239: INFO: stderr: ""
Nov 16 20:42:44.239: INFO: stdout: "e2e-test-crd-publish-openapi-7929-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 20:42:44.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-958 --namespace=crd-publish-openapi-958 delete e2e-test-crd-publish-openapi-7929-crds test-cr'
Nov 16 20:42:44.385: INFO: stderr: ""
Nov 16 20:42:44.385: INFO: stdout: "e2e-test-crd-publish-openapi-7929-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Nov 16 20:42:44.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-958 --namespace=crd-publish-openapi-958 apply -f -'
Nov 16 20:42:44.895: INFO: stderr: ""
Nov 16 20:42:44.895: INFO: stdout: "e2e-test-crd-publish-openapi-7929-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Nov 16 20:42:44.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-958 --namespace=crd-publish-openapi-958 delete e2e-test-crd-publish-openapi-7929-crds test-cr'
Nov 16 20:42:45.022: INFO: stderr: ""
Nov 16 20:42:45.022: INFO: stdout: "e2e-test-crd-publish-openapi-7929-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Nov 16 20:42:45.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-958 explain e2e-test-crd-publish-openapi-7929-crds'
Nov 16 20:42:45.318: INFO: stderr: ""
Nov 16 20:42:45.318: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7929-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:50.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-958" for this suite.

â€¢ [SLOW TEST:11.442 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":239,"skipped":4343,"failed":0}
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:50.104: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 16 20:42:56.471: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1116 20:42:56.471434      25 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:42:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-768" for this suite.

â€¢ [SLOW TEST:6.404 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":240,"skipped":4343,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:42:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 16 20:43:01.338: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-394 pod-service-account-fe431b1c-0b0b-4ed9-9386-e5c7cab59e37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 16 20:43:01.719: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-394 pod-service-account-fe431b1c-0b0b-4ed9-9386-e5c7cab59e37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 16 20:43:02.072: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-394 pod-service-account-fe431b1c-0b0b-4ed9-9386-e5c7cab59e37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:02.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-394" for this suite.

â€¢ [SLOW TEST:5.991 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":241,"skipped":4352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:02.501: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:43:03.101: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Nov 16 20:43:05.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692183, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692183, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692183, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692183, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:43:08.207: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:43:08.224: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:11.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5587" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:9.318 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":242,"skipped":4377,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:11.820: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:43:12.139: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8" in namespace "projected-7651" to be "Succeeded or Failed"
Nov 16 20:43:12.159: INFO: Pod "downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.599776ms
Nov 16 20:43:14.178: INFO: Pod "downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8": Phase="Running", Reason="", readiness=true. Elapsed: 2.038782204s
Nov 16 20:43:16.198: INFO: Pod "downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059187772s
STEP: Saw pod success
Nov 16 20:43:16.199: INFO: Pod "downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8" satisfied condition "Succeeded or Failed"
Nov 16 20:43:16.212: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8 container client-container: <nil>
STEP: delete the pod
Nov 16 20:43:16.296: INFO: Waiting for pod downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8 to disappear
Nov 16 20:43:16.309: INFO: Pod downwardapi-volume-bece73cd-2a45-4b4f-9fe9-74df79b25df8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:16.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7651" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4381,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:16.351: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Nov 16 20:43:16.608: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9031  74a07dda-3a2e-4129-9d4e-1d704c25dc2b 43817 0 2021-11-16 20:43:16 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-11-16 20:43:16 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnh9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnh9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Nov 16 20:43:16.621: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:43:18.641: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:43:20.640: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Nov 16 20:43:20.640: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9031 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:43:20.640: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Verifying customized DNS server is configured on pod...
Nov 16 20:43:20.842: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9031 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:43:20.843: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:43:21.052: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:21.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9031" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":244,"skipped":4381,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:43:21.406: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 16 20:43:26.437: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 16 20:43:26.437: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 16 20:43:28.456: INFO: Creating deployment "test-rollover-deployment"
Nov 16 20:43:28.489: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 16 20:43:30.523: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 16 20:43:30.548: INFO: Ensure that both replica sets have 1 created replica
Nov 16 20:43:30.570: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 16 20:43:30.604: INFO: Updating deployment test-rollover-deployment
Nov 16 20:43:30.604: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 16 20:43:32.638: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 16 20:43:32.664: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 16 20:43:32.690: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:32.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692210, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:34.724: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:34.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692213, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:36.717: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:36.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692213, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:38.721: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:38.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692213, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:40.723: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:40.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692213, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:42.720: INFO: all replica sets need to contain the pod-template-hash label
Nov 16 20:43:42.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692213, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692208, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:43:44.727: INFO: 
Nov 16 20:43:44.727: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:43:44.768: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3408  ecebdc4e-ed66-477a-926d-29f158c0850e 43994 2 2021-11-16 20:43:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-11-16 20:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:43:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072e7b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-11-16 20:43:28 +0000 UTC,LastTransitionTime:2021-11-16 20:43:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-11-16 20:43:43 +0000 UTC,LastTransitionTime:2021-11-16 20:43:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:43:44.779: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-3408  d6fabbf9-5dc2-487d-a4fa-63956bec6114 43984 2 2021-11-16 20:43:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ecebdc4e-ed66-477a-926d-29f158c0850e 0xc00737e1f0 0xc00737e1f1}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecebdc4e-ed66-477a-926d-29f158c0850e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:43:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00737e288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:43:44.779: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 16 20:43:44.780: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3408  d9d7c37a-14d1-4231-9481-8e3e288ec768 43992 2 2021-11-16 20:43:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ecebdc4e-ed66-477a-926d-29f158c0850e 0xc0072e7f97 0xc0072e7f98}] []  [{e2e.test Update apps/v1 2021-11-16 20:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:43:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecebdc4e-ed66-477a-926d-29f158c0850e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:43:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00737e058 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:43:44.780: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3408  216ce108-ea31-4da6-9f2e-be0d1f910ce4 43945 2 2021-11-16 20:43:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ecebdc4e-ed66-477a-926d-29f158c0850e 0xc00737e0d7 0xc00737e0d8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:43:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecebdc4e-ed66-477a-926d-29f158c0850e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:43:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00737e188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:43:44.795: INFO: Pod "test-rollover-deployment-98c5f4599-6tl65" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-6tl65 test-rollover-deployment-98c5f4599- deployment-3408  17d12bf7-917d-4092-a938-fe8a80b8ad62 43968 0 2021-11-16 20:43:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:c73aa63c32d68e56226a5e7fc2469469facd2046db912089b988ba65aa31d47a cni.projectcalico.org/podIP:172.30.9.57/32 cni.projectcalico.org/podIPs:172.30.9.57/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 d6fabbf9-5dc2-487d-a4fa-63956bec6114 0xc00737e8c0 0xc00737e8c1}] []  [{kube-controller-manager Update v1 2021-11-16 20:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6fabbf9-5dc2-487d-a4fa-63956bec6114\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 20:43:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 20:43:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djd5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djd5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:43:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.57,StartTime:2021-11-16 20:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 20:43:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://27b01f815d5edaf26c0f45d1fa9ae20833ca2254dda4abf7a0092c06f9ea14b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:44.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3408" for this suite.

â€¢ [SLOW TEST:23.694 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":245,"skipped":4399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:44.839: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3902
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3902
I1116 20:43:45.205319      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3902, replica count: 2
I1116 20:43:48.256198      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:43:48.256: INFO: Creating new exec pod
Nov 16 20:43:53.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3902 exec execpodpctsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Nov 16 20:43:53.710: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Nov 16 20:43:53.710: INFO: stdout: "externalname-service-c7b4n"
Nov 16 20:43:53.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-3902 exec execpodpctsm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.168.48 80'
Nov 16 20:43:54.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.168.48 80\nConnection to 172.21.168.48 80 port [tcp/http] succeeded!\n"
Nov 16 20:43:54.053: INFO: stdout: "externalname-service-c7b4n"
Nov 16 20:43:54.053: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:54.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3902" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:9.314 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":246,"skipped":4430,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:54.153: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 16 20:43:54.417: INFO: Waiting up to 5m0s for pod "pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1" in namespace "emptydir-8931" to be "Succeeded or Failed"
Nov 16 20:43:54.431: INFO: Pod "pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.109434ms
Nov 16 20:43:56.451: INFO: Pod "pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033842725s
Nov 16 20:43:58.470: INFO: Pod "pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05336994s
STEP: Saw pod success
Nov 16 20:43:58.470: INFO: Pod "pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1" satisfied condition "Succeeded or Failed"
Nov 16 20:43:58.484: INFO: Trying to get logs from node 10.193.87.27 pod pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1 container test-container: <nil>
STEP: delete the pod
Nov 16 20:43:58.559: INFO: Waiting for pod pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1 to disappear
Nov 16 20:43:58.573: INFO: Pod pod-1f4a5605-1313-4192-b96b-9fe85a8a0bd1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:43:58.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8931" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4431,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:43:58.615: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 16 20:43:58.891: INFO: Waiting up to 5m0s for pod "pod-b24b7ddb-090a-4207-a7d2-747e2f28043f" in namespace "emptydir-5711" to be "Succeeded or Failed"
Nov 16 20:43:58.906: INFO: Pod "pod-b24b7ddb-090a-4207-a7d2-747e2f28043f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.52265ms
Nov 16 20:44:00.925: INFO: Pod "pod-b24b7ddb-090a-4207-a7d2-747e2f28043f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033576657s
Nov 16 20:44:02.955: INFO: Pod "pod-b24b7ddb-090a-4207-a7d2-747e2f28043f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06345252s
STEP: Saw pod success
Nov 16 20:44:02.955: INFO: Pod "pod-b24b7ddb-090a-4207-a7d2-747e2f28043f" satisfied condition "Succeeded or Failed"
Nov 16 20:44:02.985: INFO: Trying to get logs from node 10.193.87.27 pod pod-b24b7ddb-090a-4207-a7d2-747e2f28043f container test-container: <nil>
STEP: delete the pod
Nov 16 20:44:03.070: INFO: Waiting for pod pod-b24b7ddb-090a-4207-a7d2-747e2f28043f to disappear
Nov 16 20:44:03.088: INFO: Pod pod-b24b7ddb-090a-4207-a7d2-747e2f28043f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:03.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5711" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4437,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:03.126: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:44:03.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:44:05.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692243, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692243, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692243, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692243, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:44:08.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:21.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5729" for this suite.
STEP: Destroying namespace "webhook-5729-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:18.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":249,"skipped":4445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:21.651: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6865
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:44:21.879: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:22.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6865" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":250,"skipped":4495,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 16 20:44:23.320: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:30.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8491" for this suite.

â€¢ [SLOW TEST:7.040 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4502,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:30.053: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:44:30.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:44:32.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692270, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692270, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692270, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692270, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:44:36.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:44:36.090: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1590-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:39.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6798" for this suite.
STEP: Destroying namespace "webhook-6798-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.624 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":252,"skipped":4506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:39.681: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Nov 16 20:44:39.930: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-1777 proxy --unix-socket=/tmp/kubectl-proxy-unix118938688/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:40.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1777" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":253,"skipped":4559,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-e50099a9-7ff7-4c05-a0c3-14754a5ff343-6422
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:40.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7213" for this suite.
STEP: Destroying namespace "nspatchtest-e50099a9-7ff7-4c05-a0c3-14754a5ff343-6422" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":254,"skipped":4568,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9801.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9801.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9801.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9801.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9801.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9801.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 16 20:44:45.158: INFO: DNS probes using dns-9801/dns-test-226fff86-24fd-4fb7-85ee-977d7d918299 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9801" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":255,"skipped":4586,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:45.261: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-e828ec92-8de3-4a42-a509-d1c90d2338c4
STEP: Creating a pod to test consume secrets
Nov 16 20:44:45.549: INFO: Waiting up to 5m0s for pod "pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413" in namespace "secrets-8763" to be "Succeeded or Failed"
Nov 16 20:44:45.562: INFO: Pod "pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413": Phase="Pending", Reason="", readiness=false. Elapsed: 13.144129ms
Nov 16 20:44:47.586: INFO: Pod "pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037628671s
Nov 16 20:44:49.605: INFO: Pod "pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05647707s
STEP: Saw pod success
Nov 16 20:44:49.605: INFO: Pod "pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413" satisfied condition "Succeeded or Failed"
Nov 16 20:44:49.619: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413 container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:44:49.755: INFO: Waiting for pod pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413 to disappear
Nov 16 20:44:49.769: INFO: Pod pod-secrets-bcd5b3c6-7cb6-4ea3-91f5-f7e0dcf49413 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:49.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8763" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4602,"failed":0}
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:49.807: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:50.044: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-6128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-946
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:56.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6128" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:56.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-946" for this suite.

â€¢ [SLOW TEST:6.783 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":257,"skipped":4605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:56.598: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Nov 16 20:44:56.931: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Nov 16 20:44:56.951: INFO: starting watch
STEP: patching
STEP: updating
Nov 16 20:44:56.999: INFO: waiting for watch events with expected annotations
Nov 16 20:44:56.999: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:44:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2736" for this suite.
â€¢{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":258,"skipped":4633,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:44:57.127: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:01.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4416" for this suite.

â€¢ [SLOW TEST:304.412 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":259,"skipped":4641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:01.540: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:50:02.104: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:50:05.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:15.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9861" for this suite.
STEP: Destroying namespace "webhook-9861-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.335 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":260,"skipped":4674,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:15.877: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:50:16.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114" in namespace "downward-api-4440" to be "Succeeded or Failed"
Nov 16 20:50:16.173: INFO: Pod "downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114": Phase="Pending", Reason="", readiness=false. Elapsed: 22.798823ms
Nov 16 20:50:18.189: INFO: Pod "downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038741842s
Nov 16 20:50:20.205: INFO: Pod "downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054399872s
STEP: Saw pod success
Nov 16 20:50:20.205: INFO: Pod "downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114" satisfied condition "Succeeded or Failed"
Nov 16 20:50:20.220: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114 container client-container: <nil>
STEP: delete the pod
Nov 16 20:50:20.387: INFO: Waiting for pod downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114 to disappear
Nov 16 20:50:20.401: INFO: Pod downwardapi-volume-622bec25-389f-4d24-a2bc-09df7406a114 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:20.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4440" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4683,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:20.436: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:37.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2332" for this suite.

â€¢ [SLOW TEST:16.695 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":262,"skipped":4692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:37.137: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 20:50:37.407: INFO: Waiting up to 5m0s for pod "pod-738d44aa-c701-4876-af00-d5dcc9048eb8" in namespace "emptydir-8840" to be "Succeeded or Failed"
Nov 16 20:50:37.420: INFO: Pod "pod-738d44aa-c701-4876-af00-d5dcc9048eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.243066ms
Nov 16 20:50:39.435: INFO: Pod "pod-738d44aa-c701-4876-af00-d5dcc9048eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027697532s
Nov 16 20:50:41.456: INFO: Pod "pod-738d44aa-c701-4876-af00-d5dcc9048eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049249935s
STEP: Saw pod success
Nov 16 20:50:41.456: INFO: Pod "pod-738d44aa-c701-4876-af00-d5dcc9048eb8" satisfied condition "Succeeded or Failed"
Nov 16 20:50:41.469: INFO: Trying to get logs from node 10.193.87.27 pod pod-738d44aa-c701-4876-af00-d5dcc9048eb8 container test-container: <nil>
STEP: delete the pod
Nov 16 20:50:41.549: INFO: Waiting for pod pod-738d44aa-c701-4876-af00-d5dcc9048eb8 to disappear
Nov 16 20:50:41.562: INFO: Pod pod-738d44aa-c701-4876-af00-d5dcc9048eb8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:41.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8840" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:41.606: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:50:41.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1434" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":264,"skipped":4788,"failed":0}

------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:50:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-3093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Nov 16 20:50:42.335: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:44.365: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:46.360: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.193.87.27 on the node which pod1 resides and expect scheduled
Nov 16 20:50:46.407: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:48.429: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:50.426: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.193.87.27 but use UDP protocol on the node which pod2 resides
Nov 16 20:50:50.469: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:52.499: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:54.486: INFO: The status of Pod pod3 is Running (Ready = true)
Nov 16 20:50:54.518: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:50:56.546: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Nov 16 20:50:56.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.193.87.27 http://127.0.0.1:54323/hostname] Namespace:hostport-3093 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:50:56.563: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.193.87.27, port: 54323
Nov 16 20:50:56.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.193.87.27:54323/hostname] Namespace:hostport-3093 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:50:56.807: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.193.87.27, port: 54323 UDP
Nov 16 20:50:57.034: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.193.87.27 54323] Namespace:hostport-3093 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Nov 16 20:50:57.034: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:51:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3093" for this suite.

â€¢ [SLOW TEST:20.242 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":265,"skipped":4788,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:51:02.291: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1750/configmap-test-ff7dd3ab-6b14-4cd0-9163-41eb8d3b15dd
STEP: Creating a pod to test consume configMaps
Nov 16 20:51:02.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68" in namespace "configmap-1750" to be "Succeeded or Failed"
Nov 16 20:51:02.595: INFO: Pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68": Phase="Pending", Reason="", readiness=false. Elapsed: 14.543525ms
Nov 16 20:51:04.614: INFO: Pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033412043s
Nov 16 20:51:06.645: INFO: Pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064027982s
Nov 16 20:51:08.667: INFO: Pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086366373s
STEP: Saw pod success
Nov 16 20:51:08.667: INFO: Pod "pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68" satisfied condition "Succeeded or Failed"
Nov 16 20:51:08.681: INFO: Trying to get logs from node 10.193.87.24 pod pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68 container env-test: <nil>
STEP: delete the pod
Nov 16 20:51:08.829: INFO: Waiting for pod pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68 to disappear
Nov 16 20:51:08.844: INFO: Pod pod-configmaps-505a6174-0ec1-4a69-8bd9-ec5de4728a68 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:51:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1750" for this suite.

â€¢ [SLOW TEST:6.591 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":4808,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:51:08.882: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-9ba42348-2f78-4b7a-8a88-111e7e59e661
STEP: Creating a pod to test consume configMaps
Nov 16 20:51:09.155: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3" in namespace "projected-7412" to be "Succeeded or Failed"
Nov 16 20:51:09.167: INFO: Pod "pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.895756ms
Nov 16 20:51:11.187: INFO: Pod "pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031579036s
Nov 16 20:51:13.213: INFO: Pod "pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057652635s
STEP: Saw pod success
Nov 16 20:51:13.213: INFO: Pod "pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3" satisfied condition "Succeeded or Failed"
Nov 16 20:51:13.226: INFO: Trying to get logs from node 10.193.87.24 pod pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 20:51:13.300: INFO: Waiting for pod pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3 to disappear
Nov 16 20:51:13.313: INFO: Pod pod-projected-configmaps-5e79058b-1754-4a0d-9f8c-8a6f6769dbb3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:51:13.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7412" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4808,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:51:13.358: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1253
STEP: creating service affinity-nodeport-transition in namespace services-1253
STEP: creating replication controller affinity-nodeport-transition in namespace services-1253
I1116 20:51:13.668584      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1253, replica count: 3
I1116 20:51:16.719383      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:51:16.783: INFO: Creating new exec pod
Nov 16 20:51:21.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Nov 16 20:51:22.224: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Nov 16 20:51:22.224: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:51:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.159.198 80'
Nov 16 20:51:22.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.159.198 80\nConnection to 172.21.159.198 80 port [tcp/http] succeeded!\n"
Nov 16 20:51:22.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:51:22.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.24 31563'
Nov 16 20:51:22.928: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.24 31563\nConnection to 10.193.87.24 31563 port [tcp/*] succeeded!\n"
Nov 16 20:51:22.928: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:51:22.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 31563'
Nov 16 20:51:23.246: INFO: stderr: "+ + ncecho -v -t hostName -w 2\n 10.193.87.28 31563\nConnection to 10.193.87.28 31563 port [tcp/*] succeeded!\n"
Nov 16 20:51:23.246: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:51:23.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.193.87.24:31563/ ; done'
Nov 16 20:51:23.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n"
Nov 16 20:51:23.847: INFO: stdout: "\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s"
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.847: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:23.848: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:53.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.193.87.24:31563/ ; done'
Nov 16 20:51:54.366: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n"
Nov 16 20:51:54.366: INFO: stdout: "\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-w9cj4\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-w9cj4\naffinity-nodeport-transition-w9cj4\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-9rv4s\naffinity-nodeport-transition-w9cj4"
Nov 16 20:51:54.366: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-w9cj4
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-w9cj4
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-w9cj4
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-9rv4s
Nov 16 20:51:54.367: INFO: Received response from host: affinity-nodeport-transition-w9cj4
Nov 16 20:51:54.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1253 exec execpod-affinitymnzvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.193.87.24:31563/ ; done'
Nov 16 20:51:54.916: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:31563/\n"
Nov 16 20:51:54.917: INFO: stdout: "\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b\naffinity-nodeport-transition-57t5b"
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.917: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Received response from host: affinity-nodeport-transition-57t5b
Nov 16 20:51:54.918: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1253, will wait for the garbage collector to delete the pods
Nov 16 20:51:55.057: INFO: Deleting ReplicationController affinity-nodeport-transition took: 24.032277ms
Nov 16 20:51:55.158: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.846229ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:51:58.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1253" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:44.833 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":268,"skipped":4819,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:51:58.192: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-226
STEP: creating service affinity-nodeport in namespace services-226
STEP: creating replication controller affinity-nodeport in namespace services-226
I1116 20:51:58.585739      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-226, replica count: 3
I1116 20:52:01.638924      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:52:01.691: INFO: Creating new exec pod
Nov 16 20:52:04.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-226 exec execpod-affinity4txnn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Nov 16 20:52:06.270: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Nov 16 20:52:06.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:52:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-226 exec execpod-affinity4txnn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.187.58 80'
Nov 16 20:52:06.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.187.58 80\nConnection to 172.21.187.58 80 port [tcp/http] succeeded!\n"
Nov 16 20:52:06.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:52:06.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-226 exec execpod-affinity4txnn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.28 30397'
Nov 16 20:52:06.961: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.28 30397\nConnection to 10.193.87.28 30397 port [tcp/*] succeeded!\n"
Nov 16 20:52:06.961: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:52:06.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-226 exec execpod-affinity4txnn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.193.87.24 30397'
Nov 16 20:52:07.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.193.87.24 30397\nConnection to 10.193.87.24 30397 port [tcp/*] succeeded!\n"
Nov 16 20:52:07.305: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:52:07.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-226 exec execpod-affinity4txnn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.193.87.24:30397/ ; done'
Nov 16 20:52:07.841: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.193.87.24:30397/\n"
Nov 16 20:52:07.841: INFO: stdout: "\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7\naffinity-nodeport-jbms7"
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Received response from host: affinity-nodeport-jbms7
Nov 16 20:52:07.841: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-226, will wait for the garbage collector to delete the pods
Nov 16 20:52:08.019: INFO: Deleting ReplicationController affinity-nodeport took: 30.375999ms
Nov 16 20:52:08.120: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.373369ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:52:11.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:13.057 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":269,"skipped":4830,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:52:11.251: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7541
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-cd836573-9674-443b-b758-7c808bc19932
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:52:15.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7541" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4843,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:52:15.750: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3338
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-aac475d2-32d0-4d7f-9bf1-a2848f17c882
STEP: Creating the pod
Nov 16 20:52:16.118: INFO: The status of Pod pod-configmaps-cd4546b7-11f2-4c57-9b3f-1c6063b74639 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:52:18.134: INFO: The status of Pod pod-configmaps-cd4546b7-11f2-4c57-9b3f-1c6063b74639 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:52:20.137: INFO: The status of Pod pod-configmaps-cd4546b7-11f2-4c57-9b3f-1c6063b74639 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-aac475d2-32d0-4d7f-9bf1-a2848f17c882
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:53:49.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3338" for this suite.

â€¢ [SLOW TEST:94.181 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":4846,"failed":0}
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:53:49.931: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-5837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:53:54.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5837" for this suite.
â€¢{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":272,"skipped":4846,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:53:54.423: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:53:59.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5989" for this suite.

â€¢ [SLOW TEST:5.457 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":273,"skipped":4867,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:53:59.880: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-2337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Nov 16 20:54:00.210: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 20:55:00.323: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:55:00.335: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:55:00.694: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Nov 16 20:55:00.708: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:55:00.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5141" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:55:00.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2337" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:61.165 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":274,"skipped":4868,"failed":0}
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:55:01.045: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-f778c278-29ec-43a2-b331-9761044491a7
STEP: Creating secret with name secret-projected-all-test-volume-69ee5ac7-8e14-4c20-bf1b-27c6aa6609f2
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 16 20:55:01.367: INFO: Waiting up to 5m0s for pod "projected-volume-417fb6f4-0589-430b-933d-695e70067db4" in namespace "projected-7995" to be "Succeeded or Failed"
Nov 16 20:55:01.393: INFO: Pod "projected-volume-417fb6f4-0589-430b-933d-695e70067db4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.555728ms
Nov 16 20:55:03.418: INFO: Pod "projected-volume-417fb6f4-0589-430b-933d-695e70067db4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050931229s
Nov 16 20:55:05.438: INFO: Pod "projected-volume-417fb6f4-0589-430b-933d-695e70067db4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070735794s
STEP: Saw pod success
Nov 16 20:55:05.438: INFO: Pod "projected-volume-417fb6f4-0589-430b-933d-695e70067db4" satisfied condition "Succeeded or Failed"
Nov 16 20:55:05.452: INFO: Trying to get logs from node 10.193.87.27 pod projected-volume-417fb6f4-0589-430b-933d-695e70067db4 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 16 20:55:05.576: INFO: Waiting for pod projected-volume-417fb6f4-0589-430b-933d-695e70067db4 to disappear
Nov 16 20:55:05.592: INFO: Pod projected-volume-417fb6f4-0589-430b-933d-695e70067db4 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:55:05.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7995" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":275,"skipped":4868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:55:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 16 20:55:05.937: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:55:07.961: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:55:09.963: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 16 20:55:10.018: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:55:12.045: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:55:14.038: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Nov 16 20:55:14.080: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:55:14.095: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:55:16.096: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:55:16.116: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 16 20:55:18.097: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 16 20:55:18.117: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:55:18.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5120" for this suite.

â€¢ [SLOW TEST:12.556 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4926,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:55:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8496
STEP: creating service affinity-clusterip-transition in namespace services-8496
STEP: creating replication controller affinity-clusterip-transition in namespace services-8496
I1116 20:55:18.492062      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8496, replica count: 3
I1116 20:55:21.544335      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 20:55:21.584: INFO: Creating new exec pod
Nov 16 20:55:26.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8496 exec execpod-affinity99764 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Nov 16 20:55:27.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Nov 16 20:55:27.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:55:27.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8496 exec execpod-affinity99764 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.132.56 80'
Nov 16 20:55:27.527: INFO: stderr: "+ + nc -v -t -w 2 172.21.132.56 80echo\n hostName\nConnection to 172.21.132.56 80 port [tcp/http] succeeded!\n"
Nov 16 20:55:27.527: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 20:55:27.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8496 exec execpod-affinity99764 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.132.56:80/ ; done'
Nov 16 20:55:28.136: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n"
Nov 16 20:55:28.136: INFO: stdout: "\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v"
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:28.136: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8496 exec execpod-affinity99764 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.132.56:80/ ; done'
Nov 16 20:55:58.667: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n"
Nov 16 20:55:58.667: INFO: stdout: "\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-5vvtn\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-5vvtn\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-b9pb2\naffinity-clusterip-transition-f589v"
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-5vvtn
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-5vvtn
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-b9pb2
Nov 16 20:55:58.667: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:58.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-8496 exec execpod-affinity99764 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.132.56:80/ ; done'
Nov 16 20:55:59.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.132.56:80/\n"
Nov 16 20:55:59.192: INFO: stdout: "\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v\naffinity-clusterip-transition-f589v"
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Received response from host: affinity-clusterip-transition-f589v
Nov 16 20:55:59.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8496, will wait for the garbage collector to delete the pods
Nov 16 20:55:59.329: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.982474ms
Nov 16 20:55:59.431: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.491024ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:02.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8496" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:44.050 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":277,"skipped":4933,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:02.250: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Nov 16 20:56:02.535: INFO: created test-pod-1
Nov 16 20:56:02.558: INFO: created test-pod-2
Nov 16 20:56:02.580: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Nov 16 20:56:02.695: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 20:56:03.717: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 20:56:04.715: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 20:56:05.716: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 20:56:06.744: INFO: Pod quantity 3 is different from expected quantity 0
Nov 16 20:56:07.713: INFO: Pod quantity 1 is different from expected quantity 0
Nov 16 20:56:08.717: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:09.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-482" for this suite.

â€¢ [SLOW TEST:7.513 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":278,"skipped":4939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:09.765: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 20:56:10.051: INFO: The status of Pod pod-update-531febd3-c93c-4f5e-8225-ad582ced6602 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:56:12.074: INFO: The status of Pod pod-update-531febd3-c93c-4f5e-8225-ad582ced6602 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:56:14.072: INFO: The status of Pod pod-update-531febd3-c93c-4f5e-8225-ad582ced6602 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 16 20:56:14.646: INFO: Successfully updated pod "pod-update-531febd3-c93c-4f5e-8225-ad582ced6602"
STEP: verifying the updated pod is in kubernetes
Nov 16 20:56:14.676: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:14.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6159" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":4978,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:14.722: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Nov 16 20:56:15.561: INFO: created pod pod-service-account-defaultsa
Nov 16 20:56:15.561: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 16 20:56:15.582: INFO: created pod pod-service-account-mountsa
Nov 16 20:56:15.582: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 16 20:56:15.606: INFO: created pod pod-service-account-nomountsa
Nov 16 20:56:15.606: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 16 20:56:15.628: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 16 20:56:15.629: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 16 20:56:15.650: INFO: created pod pod-service-account-mountsa-mountspec
Nov 16 20:56:15.650: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 16 20:56:15.671: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 16 20:56:15.671: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 16 20:56:15.693: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 16 20:56:15.693: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 16 20:56:15.713: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 16 20:56:15.713: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 16 20:56:15.734: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 16 20:56:15.734: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:15.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6903" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":280,"skipped":4980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:15.777: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:56:16.077: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c" in namespace "projected-3274" to be "Succeeded or Failed"
Nov 16 20:56:16.095: INFO: Pod "downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.704134ms
Nov 16 20:56:18.116: INFO: Pod "downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039525317s
Nov 16 20:56:20.139: INFO: Pod "downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062366442s
STEP: Saw pod success
Nov 16 20:56:20.139: INFO: Pod "downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c" satisfied condition "Succeeded or Failed"
Nov 16 20:56:20.155: INFO: Trying to get logs from node 10.193.87.24 pod downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c container client-container: <nil>
STEP: delete the pod
Nov 16 20:56:20.304: INFO: Waiting for pod downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c to disappear
Nov 16 20:56:20.318: INFO: Pod downwardapi-volume-14224602-a56b-4ffd-8bc8-46c9dd10736c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:20.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3274" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5016,"failed":0}
SS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-6614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 16 20:56:20.635: INFO: Waiting up to 5m0s for pod "security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78" in namespace "security-context-6614" to be "Succeeded or Failed"
Nov 16 20:56:20.651: INFO: Pod "security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.461531ms
Nov 16 20:56:22.674: INFO: Pod "security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039651073s
Nov 16 20:56:24.698: INFO: Pod "security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062928694s
STEP: Saw pod success
Nov 16 20:56:24.698: INFO: Pod "security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78" satisfied condition "Succeeded or Failed"
Nov 16 20:56:24.713: INFO: Trying to get logs from node 10.193.87.24 pod security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78 container test-container: <nil>
STEP: delete the pod
Nov 16 20:56:24.807: INFO: Waiting for pod security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78 to disappear
Nov 16 20:56:24.823: INFO: Pod security-context-f273de39-6e9c-4cfe-94b4-0139e7c8ea78 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:24.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6614" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":282,"skipped":5018,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:24.870: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:56:25.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:56:27.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692985, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692985, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692985, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772692985, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:56:30.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:56:30.900: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1794-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:33.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4791" for this suite.
STEP: Destroying namespace "webhook-4791-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.227 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":283,"skipped":5019,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:34.100: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 16 20:56:34.386: INFO: Waiting up to 5m0s for pod "downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d" in namespace "downward-api-404" to be "Succeeded or Failed"
Nov 16 20:56:34.403: INFO: Pod "downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.665391ms
Nov 16 20:56:36.421: INFO: Pod "downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034269166s
Nov 16 20:56:38.443: INFO: Pod "downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056721299s
STEP: Saw pod success
Nov 16 20:56:38.443: INFO: Pod "downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d" satisfied condition "Succeeded or Failed"
Nov 16 20:56:38.459: INFO: Trying to get logs from node 10.193.87.27 pod downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d container dapi-container: <nil>
STEP: delete the pod
Nov 16 20:56:38.549: INFO: Waiting for pod downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d to disappear
Nov 16 20:56:38.564: INFO: Pod downward-api-7b796811-858a-4f2f-bb3c-adfeb53e119d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:38.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-404" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5027,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:38.609: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-3342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Nov 16 20:56:38.857: INFO: Major version: 1
STEP: Confirm minor version
Nov 16 20:56:38.857: INFO: cleanMinorVersion: 22
Nov 16 20:56:38.857: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:38.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3342" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":285,"skipped":5033,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:49.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-877" for this suite.

â€¢ [SLOW TEST:10.320 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":286,"skipped":5039,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:49.227: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Nov 16 20:56:50.248: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Nov 16 20:56:52.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693010, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693010, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693010, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693010, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Nov 16 20:56:55.357: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:55.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8981" for this suite.
STEP: Destroying namespace "webhook-8981-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.948 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":287,"skipped":5040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 16 20:56:56.464: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5887  7292ac5d-2956-4d34-997e-3b0e9b3de81f 47737 0 2021-11-16 20:56:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-11-16 20:56:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 20:56:56.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5887  7292ac5d-2956-4d34-997e-3b0e9b3de81f 47738 0 2021-11-16 20:56:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-11-16 20:56:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 16 20:56:56.524: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5887  7292ac5d-2956-4d34-997e-3b0e9b3de81f 47739 0 2021-11-16 20:56:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-11-16 20:56:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 20:56:56.525: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5887  7292ac5d-2956-4d34-997e-3b0e9b3de81f 47740 0 2021-11-16 20:56:56 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-11-16 20:56:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:56:56.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5887" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":288,"skipped":5062,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:56:56.571: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:56:56.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c" in namespace "downward-api-2041" to be "Succeeded or Failed"
Nov 16 20:56:56.860: INFO: Pod "downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.657075ms
Nov 16 20:56:58.880: INFO: Pod "downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037005203s
Nov 16 20:57:00.903: INFO: Pod "downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060600167s
STEP: Saw pod success
Nov 16 20:57:00.904: INFO: Pod "downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c" satisfied condition "Succeeded or Failed"
Nov 16 20:57:00.920: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c container client-container: <nil>
STEP: delete the pod
Nov 16 20:57:00.999: INFO: Waiting for pod downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c to disappear
Nov 16 20:57:01.014: INFO: Pod downwardapi-volume-44eb8071-461b-4a93-9021-78d5c2db329c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:57:01.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2041" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5069,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:57:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-ef35c9d8-433e-4b64-9ca3-ecc3d675c25c
STEP: Creating a pod to test consume secrets
Nov 16 20:57:01.360: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3" in namespace "projected-5548" to be "Succeeded or Failed"
Nov 16 20:57:01.378: INFO: Pod "pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.397507ms
Nov 16 20:57:03.400: INFO: Pod "pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.039988841s
Nov 16 20:57:05.423: INFO: Pod "pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062561894s
STEP: Saw pod success
Nov 16 20:57:05.423: INFO: Pod "pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3" satisfied condition "Succeeded or Failed"
Nov 16 20:57:05.439: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 20:57:05.513: INFO: Waiting for pod pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3 to disappear
Nov 16 20:57:05.527: INFO: Pod pod-projected-secrets-14cb642d-fb16-4ba1-b7bb-b61cd311c6c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:57:05.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5548" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:57:05.578: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Nov 16 20:57:05.868: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:57:07.890: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:57:09.890: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Nov 16 20:57:09.947: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:57:11.969: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:57:13.974: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 16 20:57:14.057: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:57:14.085: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 20:57:16.086: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:57:16.109: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 16 20:57:18.086: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 16 20:57:18.111: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:57:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1237" for this suite.

â€¢ [SLOW TEST:12.583 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:57:18.161: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:57:18.432: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16" in namespace "projected-1611" to be "Succeeded or Failed"
Nov 16 20:57:18.448: INFO: Pod "downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16": Phase="Pending", Reason="", readiness=false. Elapsed: 16.186041ms
Nov 16 20:57:20.477: INFO: Pod "downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04474286s
Nov 16 20:57:22.500: INFO: Pod "downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067301909s
STEP: Saw pod success
Nov 16 20:57:22.500: INFO: Pod "downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16" satisfied condition "Succeeded or Failed"
Nov 16 20:57:22.514: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16 container client-container: <nil>
STEP: delete the pod
Nov 16 20:57:22.579: INFO: Waiting for pod downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16 to disappear
Nov 16 20:57:22.595: INFO: Pod downwardapi-volume-cbd84e0e-d091-4c20-8849-1f0751e28e16 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:57:22.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1611" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:57:22.644: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-hzwn
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 20:57:22.965: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-hzwn" in namespace "subpath-1725" to be "Succeeded or Failed"
Nov 16 20:57:22.979: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Pending", Reason="", readiness=false. Elapsed: 14.341395ms
Nov 16 20:57:25.003: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037992334s
Nov 16 20:57:27.024: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 4.058865948s
Nov 16 20:57:29.047: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 6.081821825s
Nov 16 20:57:31.074: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 8.10893876s
Nov 16 20:57:33.098: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 10.13291453s
Nov 16 20:57:35.123: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 12.157953872s
Nov 16 20:57:37.142: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 14.177400902s
Nov 16 20:57:39.166: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 16.200730924s
Nov 16 20:57:41.191: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 18.226597141s
Nov 16 20:57:43.215: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 20.250003901s
Nov 16 20:57:45.238: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Running", Reason="", readiness=true. Elapsed: 22.273584723s
Nov 16 20:57:47.262: INFO: Pod "pod-subpath-test-downwardapi-hzwn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.297218845s
STEP: Saw pod success
Nov 16 20:57:47.262: INFO: Pod "pod-subpath-test-downwardapi-hzwn" satisfied condition "Succeeded or Failed"
Nov 16 20:57:47.278: INFO: Trying to get logs from node 10.193.87.27 pod pod-subpath-test-downwardapi-hzwn container test-container-subpath-downwardapi-hzwn: <nil>
STEP: delete the pod
Nov 16 20:57:47.362: INFO: Waiting for pod pod-subpath-test-downwardapi-hzwn to disappear
Nov 16 20:57:47.376: INFO: Pod pod-subpath-test-downwardapi-hzwn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-hzwn
Nov 16 20:57:47.376: INFO: Deleting pod "pod-subpath-test-downwardapi-hzwn" in namespace "subpath-1725"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:57:47.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1725" for this suite.

â€¢ [SLOW TEST:24.811 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":293,"skipped":5208,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:57:47.456: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-316
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-316
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-316
Nov 16 20:57:47.825: INFO: Found 0 stateful pods, waiting for 1
Nov 16 20:57:57.846: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 16 20:57:57.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 20:57:58.226: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 20:57:58.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 20:57:58.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 20:57:58.245: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 16 20:58:08.271: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 20:58:08.271: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:58:08.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999995873s
Nov 16 20:58:09.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.982095762s
Nov 16 20:58:10.426: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.961720842s
Nov 16 20:58:11.445: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.941868851s
Nov 16 20:58:12.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.923456618s
Nov 16 20:58:13.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.899936763s
Nov 16 20:58:14.510: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.878250832s
Nov 16 20:58:15.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.858627198s
Nov 16 20:58:16.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.838284717s
Nov 16 20:58:17.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 818.707681ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-316
Nov 16 20:58:18.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 20:58:18.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 20:58:18.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 20:58:18.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 20:58:18.908: INFO: Found 1 stateful pods, waiting for 3
Nov 16 20:58:28.934: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:58:28.934: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 16 20:58:28.934: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 16 20:58:28.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 20:58:29.315: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 20:58:29.315: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 20:58:29.315: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 20:58:29.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 20:58:29.737: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 20:58:29.737: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 20:58:29.737: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 20:58:29.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Nov 16 20:58:30.095: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Nov 16 20:58:30.095: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Nov 16 20:58:30.095: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Nov 16 20:58:30.095: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:58:30.110: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov 16 20:58:40.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 20:58:40.149: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 20:58:40.149: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 16 20:58:40.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996147s
Nov 16 20:58:41.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983511556s
Nov 16 20:58:42.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963696006s
Nov 16 20:58:43.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94017198s
Nov 16 20:58:44.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.920807034s
Nov 16 20:58:45.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.900577832s
Nov 16 20:58:46.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.87934253s
Nov 16 20:58:47.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.860115971s
Nov 16 20:58:48.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.838860851s
Nov 16 20:58:49.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 817.859174ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-316
Nov 16 20:58:50.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 20:58:50.742: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 20:58:50.742: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 20:58:50.742: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 20:58:50.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 20:58:51.066: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 20:58:51.067: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 20:58:51.067: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 20:58:51.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=statefulset-316 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Nov 16 20:58:51.408: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Nov 16 20:58:51.408: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Nov 16 20:58:51.408: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Nov 16 20:58:51.408: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Nov 16 20:59:01.501: INFO: Deleting all statefulset in ns statefulset-316
Nov 16 20:59:01.518: INFO: Scaling statefulset ss to 0
Nov 16 20:59:01.571: INFO: Waiting for statefulset status.replicas updated to 0
Nov 16 20:59:01.585: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:01.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-316" for this suite.

â€¢ [SLOW TEST:74.232 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":294,"skipped":5216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:01.692: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9612
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:59:01.965: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:02.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9612" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":295,"skipped":5242,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:02.780: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:59:03.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-5005 version'
Nov 16 20:59:03.167: INFO: stderr: ""
Nov 16 20:59:03.167: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.3\", GitCommit:\"c92036820499fedefec0f847e2054d824aea6cd1\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T18:41:28Z\", GoVersion:\"go1.16.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.3+IKS\", GitCommit:\"129625275ad573a344080313c18d5904ffb5c029\", GitTreeState:\"clean\", BuildDate:\"2021-10-29T06:19:34Z\", GoVersion:\"go1.16.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:03.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5005" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":296,"skipped":5242,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:03.216: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:59:03.499: INFO: Creating deployment "test-recreate-deployment"
Nov 16 20:59:03.518: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 16 20:59:03.549: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 16 20:59:05.581: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 16 20:59:05.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693143, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693143, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693143, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693143, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 16 20:59:07.622: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 16 20:59:07.761: INFO: Updating deployment test-recreate-deployment
Nov 16 20:59:07.761: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:59:07.900: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9770  e0b117e6-d70a-4598-8b08-f9508456de10 48553 2 2021-11-16 20:59:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007e84d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-11-16 20:59:07 +0000 UTC,LastTransitionTime:2021-11-16 20:59:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-11-16 20:59:07 +0000 UTC,LastTransitionTime:2021-11-16 20:59:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Nov 16 20:59:07.920: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-9770  445b322b-4cb4-41e0-ac98-cc5e7bf9b4ca 48549 1 2021-11-16 20:59:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e0b117e6-d70a-4598-8b08-f9508456de10 0xc0007e8bc0 0xc0007e8bc1}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0b117e6-d70a-4598-8b08-f9508456de10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007e8c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:59:07.920: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 16 20:59:07.921: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-9770  c50db5dd-53a2-4709-aed0-78c011cc30a3 48542 2 2021-11-16 20:59:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e0b117e6-d70a-4598-8b08-f9508456de10 0xc0007e89c7 0xc0007e89c8}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0b117e6-d70a-4598-8b08-f9508456de10\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007e8b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:59:07.940: INFO: Pod "test-recreate-deployment-85d47dcb4-b4vwv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-b4vwv test-recreate-deployment-85d47dcb4- deployment-9770  3c13ca42-6526-4df8-a500-e660d0aa4257 48552 0 2021-11-16 20:59:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 445b322b-4cb4-41e0-ac98-cc5e7bf9b4ca 0xc0007e93f0 0xc0007e93f1}] []  [{kube-controller-manager Update v1 2021-11-16 20:59:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"445b322b-4cb4-41e0-ac98-cc5e7bf9b4ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdb4w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdb4w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:59:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:07.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9770" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":297,"skipped":5247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:07.991: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 20:59:08.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369" in namespace "projected-8314" to be "Succeeded or Failed"
Nov 16 20:59:08.323: INFO: Pod "downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369": Phase="Pending", Reason="", readiness=false. Elapsed: 27.558363ms
Nov 16 20:59:10.345: INFO: Pod "downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049206445s
Nov 16 20:59:12.369: INFO: Pod "downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072892076s
STEP: Saw pod success
Nov 16 20:59:12.369: INFO: Pod "downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369" satisfied condition "Succeeded or Failed"
Nov 16 20:59:12.385: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369 container client-container: <nil>
STEP: delete the pod
Nov 16 20:59:12.463: INFO: Waiting for pod downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369 to disappear
Nov 16 20:59:12.479: INFO: Pod downwardapi-volume-de1d080a-7b37-4188-80ca-69e91b678369 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:12.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8314" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5275,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:12.531: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-15
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Nov 16 20:59:12.817: INFO: Creating simple deployment test-deployment-z66nq
Nov 16 20:59:12.864: INFO: deployment "test-deployment-z66nq" doesn't have the required revision set
Nov 16 20:59:14.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693152, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693152, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693152, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693152, loc:(*time.Location)(0xa09ece0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-z66nq-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Nov 16 20:59:16.969: INFO: Deployment test-deployment-z66nq has Conditions: [{Available True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z66nq-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Nov 16 20:59:17.007: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693155, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693155, loc:(*time.Location)(0xa09ece0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693155, loc:(*time.Location)(0xa09ece0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63772693152, loc:(*time.Location)(0xa09ece0)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-z66nq-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Nov 16 20:59:17.017: INFO: Observed &Deployment event: ADDED
Nov 16 20:59:17.017: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z66nq-794dd694d8"}
Nov 16 20:59:17.018: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.018: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z66nq-794dd694d8"}
Nov 16 20:59:17.018: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 20:59:17.019: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.019: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 20:59:17.019: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z66nq-794dd694d8" is progressing.}
Nov 16 20:59:17.019: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.019: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 20:59:17.019: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z66nq-794dd694d8" has successfully progressed.}
Nov 16 20:59:17.020: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.020: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 20:59:17.020: INFO: Observed Deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z66nq-794dd694d8" has successfully progressed.}
Nov 16 20:59:17.020: INFO: Found Deployment test-deployment-z66nq in namespace deployment-15 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 20:59:17.021: INFO: Deployment test-deployment-z66nq has an updated status
STEP: patching the Statefulset Status
Nov 16 20:59:17.021: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Nov 16 20:59:17.042: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Nov 16 20:59:17.051: INFO: Observed &Deployment event: ADDED
Nov 16 20:59:17.051: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z66nq-794dd694d8"}
Nov 16 20:59:17.051: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.051: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-z66nq-794dd694d8"}
Nov 16 20:59:17.051: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 20:59:17.052: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.052: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Nov 16 20:59:17.052: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:12 +0000 UTC 2021-11-16 20:59:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-z66nq-794dd694d8" is progressing.}
Nov 16 20:59:17.053: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.053: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 20:59:17.053: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z66nq-794dd694d8" has successfully progressed.}
Nov 16 20:59:17.054: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.054: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Nov 16 20:59:17.054: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2021-11-16 20:59:15 +0000 UTC 2021-11-16 20:59:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-z66nq-794dd694d8" has successfully progressed.}
Nov 16 20:59:17.054: INFO: Observed deployment test-deployment-z66nq in namespace deployment-15 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Nov 16 20:59:17.054: INFO: Observed &Deployment event: MODIFIED
Nov 16 20:59:17.055: INFO: Found deployment test-deployment-z66nq in namespace deployment-15 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Nov 16 20:59:17.055: INFO: Deployment test-deployment-z66nq has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Nov 16 20:59:17.071: INFO: Deployment "test-deployment-z66nq":
&Deployment{ObjectMeta:{test-deployment-z66nq  deployment-15  a9feb378-2f1e-43d4-b0df-d541c2a62642 48669 1 2021-11-16 20:59:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-11-16 20:59:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2021-11-16 20:59:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2021-11-16 20:59:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004206ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-z66nq-794dd694d8",LastUpdateTime:2021-11-16 20:59:17 +0000 UTC,LastTransitionTime:2021-11-16 20:59:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Nov 16 20:59:17.085: INFO: New ReplicaSet "test-deployment-z66nq-794dd694d8" of Deployment "test-deployment-z66nq":
&ReplicaSet{ObjectMeta:{test-deployment-z66nq-794dd694d8  deployment-15  68c81c39-3acc-43b4-8518-2e7cf96b576b 48662 1 2021-11-16 20:59:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-z66nq a9feb378-2f1e-43d4-b0df-d541c2a62642 0xc0042070f0 0xc0042070f1}] []  [{kube-controller-manager Update apps/v1 2021-11-16 20:59:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9feb378-2f1e-43d4-b0df-d541c2a62642\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2021-11-16 20:59:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004207198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Nov 16 20:59:17.101: INFO: Pod "test-deployment-z66nq-794dd694d8-f45qc" is available:
&Pod{ObjectMeta:{test-deployment-z66nq-794dd694d8-f45qc test-deployment-z66nq-794dd694d8- deployment-15  453fb87c-a27c-418d-a8c5-f7d40b3f0388 48661 0 2021-11-16 20:59:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:8e2c191db88b8a37d802e66d25b99b5c84833c8f1148048a597aee5c3f5e3af1 cni.projectcalico.org/podIP:172.30.9.47/32 cni.projectcalico.org/podIPs:172.30.9.47/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-z66nq-794dd694d8 68c81c39-3acc-43b4-8518-2e7cf96b576b 0xc0042075f0 0xc0042075f1}] []  [{kube-controller-manager Update v1 2021-11-16 20:59:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68c81c39-3acc-43b4-8518-2e7cf96b576b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2021-11-16 20:59:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2021-11-16 20:59:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.9.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fg8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fg8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.193.87.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:59:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:59:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:59:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-11-16 20:59:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.87.27,PodIP:172.30.9.47,StartTime:2021-11-16 20:59:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-11-16 20:59:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://8c83f27e50822e1381cb10e2d21944878584a16705969e122173a119216c6ee1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.9.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:17.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-15" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":299,"skipped":5282,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:59:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: creating the pod
STEP: submitting the pod to kubernetes
Nov 16 20:59:17.447: INFO: The status of Pod pod-logs-websocket-245b34ce-62d3-4130-b361-bf8be41f3aa8 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:59:19.473: INFO: The status of Pod pod-logs-websocket-245b34ce-62d3-4130-b361-bf8be41f3aa8 is Pending, waiting for it to be Running (with Ready = true)
Nov 16 20:59:21.470: INFO: The status of Pod pod-logs-websocket-245b34ce-62d3-4130-b361-bf8be41f3aa8 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:21.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-296" for this suite.
â€¢{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5284,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:21.584: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 16 20:59:21.995: INFO: Number of nodes with available pods: 0
Nov 16 20:59:21.995: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:59:23.036: INFO: Number of nodes with available pods: 0
Nov 16 20:59:23.036: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:59:24.051: INFO: Number of nodes with available pods: 0
Nov 16 20:59:24.051: INFO: Node 10.193.87.24 is running more than one daemon pod
Nov 16 20:59:25.036: INFO: Number of nodes with available pods: 3
Nov 16 20:59:25.036: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 16 20:59:25.126: INFO: Number of nodes with available pods: 2
Nov 16 20:59:25.126: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:26.163: INFO: Number of nodes with available pods: 2
Nov 16 20:59:26.163: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:27.167: INFO: Number of nodes with available pods: 2
Nov 16 20:59:27.167: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:28.164: INFO: Number of nodes with available pods: 2
Nov 16 20:59:28.164: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:29.166: INFO: Number of nodes with available pods: 2
Nov 16 20:59:29.166: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:30.168: INFO: Number of nodes with available pods: 2
Nov 16 20:59:30.168: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:31.164: INFO: Number of nodes with available pods: 2
Nov 16 20:59:31.164: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 20:59:32.168: INFO: Number of nodes with available pods: 3
Nov 16 20:59:32.168: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5035, will wait for the garbage collector to delete the pods
Nov 16 20:59:32.266: INFO: Deleting DaemonSet.extensions daemon-set took: 23.566889ms
Nov 16 20:59:32.367: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.616234ms
Nov 16 20:59:36.200: INFO: Number of nodes with available pods: 0
Nov 16 20:59:36.200: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 20:59:36.211: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48866"},"items":null}

Nov 16 20:59:36.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48866"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5035" for this suite.

â€¢ [SLOW TEST:14.759 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":301,"skipped":5300,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:36.343: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5960
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Nov 16 20:59:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 20:59:41.293: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 20:59:59.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5960" for this suite.

â€¢ [SLOW TEST:22.776 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":302,"skipped":5301,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 20:59:59.122: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1595
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 20:59:59.371: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Nov 16 21:00:04.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 create -f -'
Nov 16 21:00:05.274: INFO: stderr: ""
Nov 16 21:00:05.274: INFO: stdout: "e2e-test-crd-publish-openapi-9281-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 21:00:05.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 delete e2e-test-crd-publish-openapi-9281-crds test-foo'
Nov 16 21:00:05.442: INFO: stderr: ""
Nov 16 21:00:05.442: INFO: stdout: "e2e-test-crd-publish-openapi-9281-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Nov 16 21:00:05.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 apply -f -'
Nov 16 21:00:05.763: INFO: stderr: ""
Nov 16 21:00:05.763: INFO: stdout: "e2e-test-crd-publish-openapi-9281-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Nov 16 21:00:05.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 delete e2e-test-crd-publish-openapi-9281-crds test-foo'
Nov 16 21:00:05.901: INFO: stderr: ""
Nov 16 21:00:05.901: INFO: stdout: "e2e-test-crd-publish-openapi-9281-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Nov 16 21:00:05.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 create -f -'
Nov 16 21:00:06.152: INFO: rc: 1
Nov 16 21:00:06.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 apply -f -'
Nov 16 21:00:06.396: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Nov 16 21:00:06.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 create -f -'
Nov 16 21:00:06.684: INFO: rc: 1
Nov 16 21:00:06.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 --namespace=crd-publish-openapi-1595 apply -f -'
Nov 16 21:00:07.021: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Nov 16 21:00:07.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 explain e2e-test-crd-publish-openapi-9281-crds'
Nov 16 21:00:07.370: INFO: stderr: ""
Nov 16 21:00:07.370: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9281-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Nov 16 21:00:07.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 explain e2e-test-crd-publish-openapi-9281-crds.metadata'
Nov 16 21:00:07.730: INFO: stderr: ""
Nov 16 21:00:07.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9281-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Nov 16 21:00:07.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 explain e2e-test-crd-publish-openapi-9281-crds.spec'
Nov 16 21:00:07.988: INFO: stderr: ""
Nov 16 21:00:07.988: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9281-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Nov 16 21:00:07.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 explain e2e-test-crd-publish-openapi-9281-crds.spec.bars'
Nov 16 21:00:08.373: INFO: stderr: ""
Nov 16 21:00:08.373: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9281-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Nov 16 21:00:08.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=crd-publish-openapi-1595 explain e2e-test-crd-publish-openapi-9281-crds.spec.bars2'
Nov 16 21:00:08.690: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:13.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1595" for this suite.

â€¢ [SLOW TEST:14.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":303,"skipped":5317,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:13.468: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:13.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5936" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":304,"skipped":5322,"failed":0}
S
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:13.930: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Nov 16 21:00:14.245: INFO: observed Pod pod-test in namespace pods-2043 in phase Pending with labels: map[test-pod-static:true] & conditions []
Nov 16 21:00:14.255: INFO: observed Pod pod-test in namespace pods-2043 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  }]
Nov 16 21:00:14.299: INFO: observed Pod pod-test in namespace pods-2043 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  }]
Nov 16 21:00:15.231: INFO: observed Pod pod-test in namespace pods-2043 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  }]
Nov 16 21:00:16.371: INFO: Found Pod pod-test in namespace pods-2043 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-11-16 21:00:14 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Nov 16 21:00:16.407: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Nov 16 21:00:16.507: INFO: observed event type ADDED
Nov 16 21:00:16.507: INFO: observed event type MODIFIED
Nov 16 21:00:16.507: INFO: observed event type MODIFIED
Nov 16 21:00:16.507: INFO: observed event type MODIFIED
Nov 16 21:00:16.508: INFO: observed event type MODIFIED
Nov 16 21:00:16.508: INFO: observed event type MODIFIED
Nov 16 21:00:16.509: INFO: observed event type MODIFIED
Nov 16 21:00:16.509: INFO: observed event type MODIFIED
Nov 16 21:00:18.388: INFO: observed event type MODIFIED
Nov 16 21:00:18.852: INFO: observed event type MODIFIED
Nov 16 21:00:20.394: INFO: observed event type MODIFIED
Nov 16 21:00:20.425: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:20.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2043" for this suite.

â€¢ [SLOW TEST:6.560 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":305,"skipped":5323,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:20.491: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Nov 16 21:00:20.761: INFO: Waiting up to 5m0s for pod "var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033" in namespace "var-expansion-2008" to be "Succeeded or Failed"
Nov 16 21:00:20.777: INFO: Pod "var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033": Phase="Pending", Reason="", readiness=false. Elapsed: 15.533918ms
Nov 16 21:00:22.800: INFO: Pod "var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038310199s
Nov 16 21:00:24.820: INFO: Pod "var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058553621s
STEP: Saw pod success
Nov 16 21:00:24.820: INFO: Pod "var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033" satisfied condition "Succeeded or Failed"
Nov 16 21:00:24.835: INFO: Trying to get logs from node 10.193.87.27 pod var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:00:24.915: INFO: Waiting for pod var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033 to disappear
Nov 16 21:00:24.930: INFO: Pod var-expansion-9a77f95d-4309-4ce1-b17d-70ce9aa92033 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:24.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2008" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:24.977: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:25.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-800" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":307,"skipped":5378,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:25.392: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:00:25.671: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c" in namespace "downward-api-7528" to be "Succeeded or Failed"
Nov 16 21:00:25.685: INFO: Pod "downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.437469ms
Nov 16 21:00:27.708: INFO: Pod "downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037288577s
Nov 16 21:00:29.742: INFO: Pod "downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071563164s
STEP: Saw pod success
Nov 16 21:00:29.742: INFO: Pod "downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c" satisfied condition "Succeeded or Failed"
Nov 16 21:00:29.757: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c container client-container: <nil>
STEP: delete the pod
Nov 16 21:00:29.837: INFO: Waiting for pod downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c to disappear
Nov 16 21:00:29.854: INFO: Pod downwardapi-volume-7507d725-1dc9-4ea3-8bda-5ccd6a20a59c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:29.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7528" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:29.907: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Nov 16 21:00:34.768: INFO: Successfully updated pod "adopt-release--1-dgjfw"
STEP: Checking that the Job readopts the Pod
Nov 16 21:00:34.768: INFO: Waiting up to 15m0s for pod "adopt-release--1-dgjfw" in namespace "job-6834" to be "adopted"
Nov 16 21:00:34.784: INFO: Pod "adopt-release--1-dgjfw": Phase="Running", Reason="", readiness=true. Elapsed: 15.81906ms
Nov 16 21:00:34.784: INFO: Pod "adopt-release--1-dgjfw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Nov 16 21:00:35.324: INFO: Successfully updated pod "adopt-release--1-dgjfw"
STEP: Checking that the Job releases the Pod
Nov 16 21:00:35.324: INFO: Waiting up to 15m0s for pod "adopt-release--1-dgjfw" in namespace "job-6834" to be "released"
Nov 16 21:00:35.343: INFO: Pod "adopt-release--1-dgjfw": Phase="Running", Reason="", readiness=true. Elapsed: 18.159387ms
Nov 16 21:00:35.343: INFO: Pod "adopt-release--1-dgjfw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6834" for this suite.

â€¢ [SLOW TEST:5.480 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":309,"skipped":5416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:35.389: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:35.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5939" for this suite.
â€¢{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5460,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:35.762: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:00:36.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d" in namespace "projected-3453" to be "Succeeded or Failed"
Nov 16 21:00:36.048: INFO: Pod "downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.711323ms
Nov 16 21:00:38.070: INFO: Pod "downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037675572s
Nov 16 21:00:40.089: INFO: Pod "downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056863288s
STEP: Saw pod success
Nov 16 21:00:40.089: INFO: Pod "downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d" satisfied condition "Succeeded or Failed"
Nov 16 21:00:40.104: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d container client-container: <nil>
STEP: delete the pod
Nov 16 21:00:40.186: INFO: Waiting for pod downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d to disappear
Nov 16 21:00:40.200: INFO: Pod downwardapi-volume-3a954353-7e36-4b9d-bb54-cdff0ced565d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:40.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3453" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5463,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:40.254: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-863b5bb2-9a08-417f-bc53-520bfa618565
STEP: Creating a pod to test consume secrets
Nov 16 21:00:40.559: INFO: Waiting up to 5m0s for pod "pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b" in namespace "secrets-3885" to be "Succeeded or Failed"
Nov 16 21:00:40.574: INFO: Pod "pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.058141ms
Nov 16 21:00:42.595: INFO: Pod "pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036026896s
Nov 16 21:00:44.619: INFO: Pod "pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06026802s
STEP: Saw pod success
Nov 16 21:00:44.619: INFO: Pod "pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b" satisfied condition "Succeeded or Failed"
Nov 16 21:00:44.635: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:00:44.714: INFO: Waiting for pod pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b to disappear
Nov 16 21:00:44.729: INFO: Pod pod-secrets-739aa679-dafb-4de4-9fac-1f8936ba6a5b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:44.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3885" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5471,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:44.784: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:00:45.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f" in namespace "projected-9461" to be "Succeeded or Failed"
Nov 16 21:00:45.075: INFO: Pod "downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.726736ms
Nov 16 21:00:47.100: INFO: Pod "downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040572383s
Nov 16 21:00:49.118: INFO: Pod "downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05858563s
STEP: Saw pod success
Nov 16 21:00:49.118: INFO: Pod "downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f" satisfied condition "Succeeded or Failed"
Nov 16 21:00:49.190: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f container client-container: <nil>
STEP: delete the pod
Nov 16 21:00:49.266: INFO: Waiting for pod downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f to disappear
Nov 16 21:00:49.281: INFO: Pod downwardapi-volume-2bc9c59c-e72b-4d86-a13c-bdfb6d8ee61f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9461" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5482,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:49.332: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-35fc6812-2c00-4434-8ee3-9d972fc0d198
STEP: Creating a pod to test consume configMaps
Nov 16 21:00:49.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02" in namespace "projected-8215" to be "Succeeded or Failed"
Nov 16 21:00:49.654: INFO: Pod "pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02": Phase="Pending", Reason="", readiness=false. Elapsed: 25.072737ms
Nov 16 21:00:51.675: INFO: Pod "pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02": Phase="Running", Reason="", readiness=true. Elapsed: 2.046793945s
Nov 16 21:00:53.698: INFO: Pod "pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068947769s
STEP: Saw pod success
Nov 16 21:00:53.698: INFO: Pod "pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02" satisfied condition "Succeeded or Failed"
Nov 16 21:00:53.713: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02 container agnhost-container: <nil>
STEP: delete the pod
Nov 16 21:00:53.794: INFO: Waiting for pod pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02 to disappear
Nov 16 21:00:53.810: INFO: Pod pod-projected-configmaps-2b332590-1fb5-4905-9af2-b69864b36c02 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:53.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8215" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5492,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:53.858: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2239
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Nov 16 21:00:54.103: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-2239 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:54.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2239" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":315,"skipped":5506,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:54.248: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Nov 16 21:00:54.529: INFO: Pod name sample-pod: Found 0 pods out of 3
Nov 16 21:00:59.550: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Nov 16 21:00:59.562: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:00:59.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6996" for this suite.

â€¢ [SLOW TEST:5.439 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":316,"skipped":5511,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:00:59.688: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 21:01:00.022: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 16 21:01:00.057: INFO: Number of nodes with available pods: 0
Nov 16 21:01:00.057: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 16 21:01:00.133: INFO: Number of nodes with available pods: 0
Nov 16 21:01:00.133: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:01.151: INFO: Number of nodes with available pods: 0
Nov 16 21:01:01.151: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:02.156: INFO: Number of nodes with available pods: 0
Nov 16 21:01:02.156: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:03.416: INFO: Number of nodes with available pods: 1
Nov 16 21:01:03.416: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 16 21:01:03.519: INFO: Number of nodes with available pods: 0
Nov 16 21:01:03.519: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 16 21:01:03.553: INFO: Number of nodes with available pods: 0
Nov 16 21:01:03.553: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:04.577: INFO: Number of nodes with available pods: 0
Nov 16 21:01:04.577: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:05.572: INFO: Number of nodes with available pods: 0
Nov 16 21:01:05.572: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:06.571: INFO: Number of nodes with available pods: 0
Nov 16 21:01:06.571: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:07.572: INFO: Number of nodes with available pods: 0
Nov 16 21:01:07.572: INFO: Node 10.193.87.28 is running more than one daemon pod
Nov 16 21:01:08.578: INFO: Number of nodes with available pods: 1
Nov 16 21:01:08.579: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9660, will wait for the garbage collector to delete the pods
Nov 16 21:01:08.689: INFO: Deleting DaemonSet.extensions daemon-set took: 21.67941ms
Nov 16 21:01:08.789: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.592707ms
Nov 16 21:01:12.713: INFO: Number of nodes with available pods: 0
Nov 16 21:01:12.713: INFO: Number of running nodes: 0, number of available pods: 0
Nov 16 21:01:12.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49677"},"items":null}

Nov 16 21:01:12.740: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49677"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:01:12.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9660" for this suite.

â€¢ [SLOW TEST:13.201 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":317,"skipped":5513,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:01:12.889: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 16 21:01:13.160: INFO: Waiting up to 5m0s for pod "pod-3acfe605-9c03-4a92-9570-92e5a425702b" in namespace "emptydir-4352" to be "Succeeded or Failed"
Nov 16 21:01:13.176: INFO: Pod "pod-3acfe605-9c03-4a92-9570-92e5a425702b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.270902ms
Nov 16 21:01:15.197: INFO: Pod "pod-3acfe605-9c03-4a92-9570-92e5a425702b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037392002s
Nov 16 21:01:17.220: INFO: Pod "pod-3acfe605-9c03-4a92-9570-92e5a425702b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060341174s
STEP: Saw pod success
Nov 16 21:01:17.220: INFO: Pod "pod-3acfe605-9c03-4a92-9570-92e5a425702b" satisfied condition "Succeeded or Failed"
Nov 16 21:01:17.236: INFO: Trying to get logs from node 10.193.87.27 pod pod-3acfe605-9c03-4a92-9570-92e5a425702b container test-container: <nil>
STEP: delete the pod
Nov 16 21:01:17.319: INFO: Waiting for pod pod-3acfe605-9c03-4a92-9570-92e5a425702b to disappear
Nov 16 21:01:17.334: INFO: Pod pod-3acfe605-9c03-4a92-9570-92e5a425702b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:01:17.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4352" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":5515,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:01:17.384: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 16 21:01:17.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49727 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:01:17.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49727 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 16 21:01:27.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49777 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:01:27.732: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49777 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 16 21:01:37.790: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49793 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:01:37.791: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49793 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 16 21:01:47.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49808 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:01:47.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6406  2cc865bc-b8dc-41da-9875-1a98ca542f6b 49808 0 2021-11-16 21:01:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 16 21:01:57.885: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6406  f859edde-baeb-46e2-b915-bba725d2bc3d 49822 0 2021-11-16 21:01:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:01:57.885: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6406  f859edde-baeb-46e2-b915-bba725d2bc3d 49822 0 2021-11-16 21:01:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 16 21:02:07.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6406  f859edde-baeb-46e2-b915-bba725d2bc3d 49837 0 2021-11-16 21:01:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Nov 16 21:02:07.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6406  f859edde-baeb-46e2-b915-bba725d2bc3d 49837 0 2021-11-16 21:01:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-11-16 21:01:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:02:17.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6406" for this suite.

â€¢ [SLOW TEST:60.639 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":319,"skipped":5536,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:02:18.023: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-3c321d92-2561-40eb-a011-8da6cb2977af in namespace container-probe-9997
Nov 16 21:02:22.339: INFO: Started pod busybox-3c321d92-2561-40eb-a011-8da6cb2977af in namespace container-probe-9997
STEP: checking the pod's current state and verifying that restartCount is present
Nov 16 21:02:22.354: INFO: Initial restart count of pod busybox-3c321d92-2561-40eb-a011-8da6cb2977af is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:06:23.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9997" for this suite.

â€¢ [SLOW TEST:245.596 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":5544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:06:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-474feb29-c026-409a-8be1-c7f75bcfa93c
STEP: Creating a pod to test consume configMaps
Nov 16 21:06:23.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d" in namespace "configmap-4616" to be "Succeeded or Failed"
Nov 16 21:06:23.931: INFO: Pod "pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.648184ms
Nov 16 21:06:25.954: INFO: Pod "pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039282588s
Nov 16 21:06:27.974: INFO: Pod "pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059048409s
STEP: Saw pod success
Nov 16 21:06:27.974: INFO: Pod "pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d" satisfied condition "Succeeded or Failed"
Nov 16 21:06:27.989: INFO: Trying to get logs from node 10.193.87.27 pod pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d container agnhost-container: <nil>
STEP: delete the pod
Nov 16 21:06:28.129: INFO: Waiting for pod pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d to disappear
Nov 16 21:06:28.143: INFO: Pod pod-configmaps-b21f7de7-5180-4311-a333-4285b499cc4d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:06:28.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4616" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5587,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:06:28.190: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Nov 16 21:06:28.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149" in namespace "downward-api-7401" to be "Succeeded or Failed"
Nov 16 21:06:28.488: INFO: Pod "downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149": Phase="Pending", Reason="", readiness=false. Elapsed: 27.017652ms
Nov 16 21:06:30.507: INFO: Pod "downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046636354s
Nov 16 21:06:32.534: INFO: Pod "downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073099201s
STEP: Saw pod success
Nov 16 21:06:32.534: INFO: Pod "downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149" satisfied condition "Succeeded or Failed"
Nov 16 21:06:32.551: INFO: Trying to get logs from node 10.193.87.27 pod downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149 container client-container: <nil>
STEP: delete the pod
Nov 16 21:06:32.636: INFO: Waiting for pod downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149 to disappear
Nov 16 21:06:32.651: INFO: Pod downwardapi-volume-c2304773-6468-4341-a11d-b1aa7ee75149 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:06:32.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7401" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:06:32.703: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 16 21:06:33.016: INFO: Waiting up to 5m0s for pod "pod-a34da51a-3636-4869-9c64-1e375f043777" in namespace "emptydir-4471" to be "Succeeded or Failed"
Nov 16 21:06:33.036: INFO: Pod "pod-a34da51a-3636-4869-9c64-1e375f043777": Phase="Pending", Reason="", readiness=false. Elapsed: 19.457648ms
Nov 16 21:06:35.056: INFO: Pod "pod-a34da51a-3636-4869-9c64-1e375f043777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03926317s
Nov 16 21:06:37.078: INFO: Pod "pod-a34da51a-3636-4869-9c64-1e375f043777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061854472s
STEP: Saw pod success
Nov 16 21:06:37.078: INFO: Pod "pod-a34da51a-3636-4869-9c64-1e375f043777" satisfied condition "Succeeded or Failed"
Nov 16 21:06:37.093: INFO: Trying to get logs from node 10.193.87.27 pod pod-a34da51a-3636-4869-9c64-1e375f043777 container test-container: <nil>
STEP: delete the pod
Nov 16 21:06:37.187: INFO: Waiting for pod pod-a34da51a-3636-4869-9c64-1e375f043777 to disappear
Nov 16 21:06:37.204: INFO: Pod pod-a34da51a-3636-4869-9c64-1e375f043777 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:06:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4471" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":5624,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:06:37.260: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1714
Nov 16 21:06:37.545: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Nov 16 21:06:39.565: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Nov 16 21:06:39.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Nov 16 21:06:39.964: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Nov 16 21:06:39.964: INFO: stdout: "iptables"
Nov 16 21:06:39.964: INFO: proxyMode: iptables
Nov 16 21:06:40.013: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Nov 16 21:06:40.059: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1714
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1714
I1116 21:06:40.176883      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1714, replica count: 3
I1116 21:06:43.229300      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 16 21:06:43.263: INFO: Creating new exec pod
Nov 16 21:06:48.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec execpod-affinity84z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Nov 16 21:06:48.669: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Nov 16 21:06:48.669: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 21:06:48.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec execpod-affinity84z74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.74 80'
Nov 16 21:06:48.975: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.84.74 80\nConnection to 172.21.84.74 80 port [tcp/http] succeeded!\n"
Nov 16 21:06:48.975: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Nov 16 21:06:48.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec execpod-affinity84z74 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.84.74:80/ ; done'
Nov 16 21:06:49.492: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n"
Nov 16 21:06:49.492: INFO: stdout: "\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8\naffinity-clusterip-timeout-74pt8"
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Received response from host: affinity-clusterip-timeout-74pt8
Nov 16 21:06:49.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec execpod-affinity84z74 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.84.74:80/'
Nov 16 21:06:49.854: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n"
Nov 16 21:06:49.855: INFO: stdout: "affinity-clusterip-timeout-74pt8"
Nov 16 21:07:09.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-1714 exec execpod-affinity84z74 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.84.74:80/'
Nov 16 21:07:10.229: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.84.74:80/\n"
Nov 16 21:07:10.229: INFO: stdout: "affinity-clusterip-timeout-gfjtf"
Nov 16 21:07:10.229: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1714, will wait for the garbage collector to delete the pods
Nov 16 21:07:10.404: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 18.944555ms
Nov 16 21:07:10.504: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.188319ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:07:13.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1714" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:36.250 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":324,"skipped":5632,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:07:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-84
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:09:01.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-84" for this suite.

â€¢ [SLOW TEST:108.429 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":325,"skipped":5634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:09:01.948: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 21:09:02.237: INFO: Waiting up to 5m0s for pod "pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784" in namespace "emptydir-103" to be "Succeeded or Failed"
Nov 16 21:09:02.253: INFO: Pod "pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784": Phase="Pending", Reason="", readiness=false. Elapsed: 15.892965ms
Nov 16 21:09:04.271: INFO: Pod "pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033402252s
Nov 16 21:09:06.292: INFO: Pod "pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054049639s
STEP: Saw pod success
Nov 16 21:09:06.292: INFO: Pod "pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784" satisfied condition "Succeeded or Failed"
Nov 16 21:09:06.307: INFO: Trying to get logs from node 10.193.87.27 pod pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784 container test-container: <nil>
STEP: delete the pod
Nov 16 21:09:06.435: INFO: Waiting for pod pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784 to disappear
Nov 16 21:09:06.451: INFO: Pod pod-c59dc804-a3ef-4d8d-8b8f-930e1deca784 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:09:06.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-103" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":5690,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:09:06.501: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 16 21:09:06.770: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 16 21:09:11.801: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:09:11.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6192" for this suite.

â€¢ [SLOW TEST:5.413 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":327,"skipped":5700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:09:11.925: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Nov 16 21:09:12.158: INFO: Waiting up to 1m0s for all nodes to be ready
Nov 16 21:10:12.267: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 21:10:12.282: INFO: Starting informer...
STEP: Starting pods...
Nov 16 21:10:12.561: INFO: Pod1 is running on 10.193.87.27. Tainting Node
Nov 16 21:10:16.849: INFO: Pod2 is running on 10.193.87.27. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Nov 16 21:10:23.424: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Nov 16 21:10:43.280: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:10:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5931" for this suite.

â€¢ [SLOW TEST:91.483 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":328,"skipped":5750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:10:43.411: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217
Nov 16 21:10:43.685: INFO: Pod name my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217: Found 0 pods out of 1
Nov 16 21:10:48.710: INFO: Pod name my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217: Found 1 pods out of 1
Nov 16 21:10:48.711: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217" are running
Nov 16 21:10:48.729: INFO: Pod "my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217-sqgw2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 21:10:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 21:10:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 21:10:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-11-16 21:10:43 +0000 UTC Reason: Message:}])
Nov 16 21:10:48.729: INFO: Trying to dial the pod
Nov 16 21:10:53.826: INFO: Controller my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217: Got expected result from replica 1 [my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217-sqgw2]: "my-hostname-basic-e5c23c77-4f38-4d33-afda-68851bed9217-sqgw2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:10:53.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4761" for this suite.

â€¢ [SLOW TEST:10.467 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":329,"skipped":5813,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:10:53.878: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Nov 16 21:10:54.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8538 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Nov 16 21:10:54.413: INFO: stderr: ""
Nov 16 21:10:54.413: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Nov 16 21:10:54.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8538 delete pods e2e-test-httpd-pod'
Nov 16 21:10:58.657: INFO: stderr: ""
Nov 16 21:10:58.657: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:10:58.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8538" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":330,"skipped":5814,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:10:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 16 21:10:58.987: INFO: Waiting up to 5m0s for pod "pod-626c1107-7c83-459a-8cdf-8805427c2ea7" in namespace "emptydir-7379" to be "Succeeded or Failed"
Nov 16 21:10:59.002: INFO: Pod "pod-626c1107-7c83-459a-8cdf-8805427c2ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.539593ms
Nov 16 21:11:01.025: INFO: Pod "pod-626c1107-7c83-459a-8cdf-8805427c2ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037807687s
Nov 16 21:11:03.047: INFO: Pod "pod-626c1107-7c83-459a-8cdf-8805427c2ea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060236198s
STEP: Saw pod success
Nov 16 21:11:03.047: INFO: Pod "pod-626c1107-7c83-459a-8cdf-8805427c2ea7" satisfied condition "Succeeded or Failed"
Nov 16 21:11:03.062: INFO: Trying to get logs from node 10.193.87.27 pod pod-626c1107-7c83-459a-8cdf-8805427c2ea7 container test-container: <nil>
STEP: delete the pod
Nov 16 21:11:03.185: INFO: Waiting for pod pod-626c1107-7c83-459a-8cdf-8805427c2ea7 to disappear
Nov 16 21:11:03.201: INFO: Pod pod-626c1107-7c83-459a-8cdf-8805427c2ea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7379" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":5828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:03.248: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-b1289a29-265a-4ecd-8e7d-789da04db426
STEP: Creating a pod to test consume secrets
Nov 16 21:11:03.544: INFO: Waiting up to 5m0s for pod "pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df" in namespace "secrets-4409" to be "Succeeded or Failed"
Nov 16 21:11:03.562: INFO: Pod "pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df": Phase="Pending", Reason="", readiness=false. Elapsed: 18.248923ms
Nov 16 21:11:05.585: INFO: Pod "pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040654177s
Nov 16 21:11:07.608: INFO: Pod "pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064060085s
STEP: Saw pod success
Nov 16 21:11:07.608: INFO: Pod "pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df" satisfied condition "Succeeded or Failed"
Nov 16 21:11:07.623: INFO: Trying to get logs from node 10.193.87.27 pod pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df container secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:11:07.706: INFO: Waiting for pod pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df to disappear
Nov 16 21:11:07.723: INFO: Pod pod-secrets-c0e5ddb7-08d8-4653-86a1-ed78e5b1e1df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:07.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4409" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5864,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:07.771: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Nov 16 21:11:08.082: INFO: Waiting up to 5m0s for pod "var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901" in namespace "var-expansion-2250" to be "Succeeded or Failed"
Nov 16 21:11:08.099: INFO: Pod "var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901": Phase="Pending", Reason="", readiness=false. Elapsed: 16.77636ms
Nov 16 21:11:10.122: INFO: Pod "var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039972952s
Nov 16 21:11:12.143: INFO: Pod "var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061284159s
STEP: Saw pod success
Nov 16 21:11:12.143: INFO: Pod "var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901" satisfied condition "Succeeded or Failed"
Nov 16 21:11:12.160: INFO: Trying to get logs from node 10.193.87.27 pod var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:11:12.228: INFO: Waiting for pod var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901 to disappear
Nov 16 21:11:12.244: INFO: Pod var-expansion-68cc1b3c-8857-472d-8ed6-7d3c9dd36901 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:12.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2250" for this suite.
â€¢{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":333,"skipped":5866,"failed":0}

------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:12.288: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Nov 16 21:11:12.594: INFO: Waiting up to 5m0s for pod "client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae" in namespace "containers-7472" to be "Succeeded or Failed"
Nov 16 21:11:12.614: INFO: Pod "client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.215051ms
Nov 16 21:11:14.639: INFO: Pod "client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044933652s
Nov 16 21:11:16.659: INFO: Pod "client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064519562s
STEP: Saw pod success
Nov 16 21:11:16.659: INFO: Pod "client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae" satisfied condition "Succeeded or Failed"
Nov 16 21:11:16.674: INFO: Trying to get logs from node 10.193.87.27 pod client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae container agnhost-container: <nil>
STEP: delete the pod
Nov 16 21:11:16.753: INFO: Waiting for pod client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae to disappear
Nov 16 21:11:16.770: INFO: Pod client-containers-40205a16-71de-4b4b-a420-f2299ebda1ae no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:16.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7472" for this suite.
â€¢{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5866,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:16.818: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2943
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2943
STEP: creating replication controller externalsvc in namespace services-2943
I1116 21:11:17.224295      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2943, replica count: 2
I1116 21:11:20.279112      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Nov 16 21:11:20.428: INFO: Creating new exec pod
Nov 16 21:11:24.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=services-2943 exec execpod7jwcb -- /bin/sh -x -c nslookup nodeport-service.services-2943.svc.cluster.local'
Nov 16 21:11:24.894: INFO: stderr: "+ nslookup nodeport-service.services-2943.svc.cluster.local\n"
Nov 16 21:11:24.894: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2943.svc.cluster.local\tcanonical name = externalsvc.services-2943.svc.cluster.local.\nName:\texternalsvc.services-2943.svc.cluster.local\nAddress: 172.21.254.176\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2943, will wait for the garbage collector to delete the pods
Nov 16 21:11:24.999: INFO: Deleting ReplicationController externalsvc took: 22.735207ms
Nov 16 21:11:25.100: INFO: Terminating ReplicationController externalsvc pods took: 100.931731ms
Nov 16 21:11:27.669: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:27.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2943" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

â€¢ [SLOW TEST:10.948 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":335,"skipped":5877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:27.768: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9110" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":336,"skipped":5922,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:28.076: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Nov 16 21:11:28.351: INFO: namespace kubectl-9284
Nov 16 21:11:28.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9284 create -f -'
Nov 16 21:11:28.723: INFO: stderr: ""
Nov 16 21:11:28.723: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Nov 16 21:11:29.743: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 21:11:29.743: INFO: Found 0 / 1
Nov 16 21:11:30.742: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 21:11:30.742: INFO: Found 0 / 1
Nov 16 21:11:31.742: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 21:11:31.742: INFO: Found 1 / 1
Nov 16 21:11:31.742: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 16 21:11:31.760: INFO: Selector matched 1 pods for map[app:agnhost]
Nov 16 21:11:31.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 16 21:11:31.760: INFO: wait on agnhost-primary startup in kubectl-9284 
Nov 16 21:11:31.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9284 logs agnhost-primary-q7gdl agnhost-primary'
Nov 16 21:11:32.035: INFO: stderr: ""
Nov 16 21:11:32.035: INFO: stdout: "Paused\n"
STEP: exposing RC
Nov 16 21:11:32.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9284 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Nov 16 21:11:32.212: INFO: stderr: ""
Nov 16 21:11:32.212: INFO: stdout: "service/rm2 exposed\n"
Nov 16 21:11:32.227: INFO: Service rm2 in namespace kubectl-9284 found.
STEP: exposing service
Nov 16 21:11:34.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-9284 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Nov 16 21:11:34.476: INFO: stderr: ""
Nov 16 21:11:34.476: INFO: stdout: "service/rm3 exposed\n"
Nov 16 21:11:34.490: INFO: Service rm3 in namespace kubectl-9284 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:36.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9284" for this suite.

â€¢ [SLOW TEST:8.522 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":337,"skipped":5926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:36.599: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Nov 16 21:11:36.855: INFO: Got root ca configmap in namespace "svcaccounts-4228"
Nov 16 21:11:36.879: INFO: Deleted root ca configmap in namespace "svcaccounts-4228"
STEP: waiting for a new root ca configmap created
Nov 16 21:11:37.414: INFO: Recreated root ca configmap in namespace "svcaccounts-4228"
Nov 16 21:11:37.439: INFO: Updated root ca configmap in namespace "svcaccounts-4228"
STEP: waiting for the root ca configmap reconciled
Nov 16 21:11:37.956: INFO: Reconciled root ca configmap in namespace "svcaccounts-4228"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:11:37.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4228" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":338,"skipped":5954,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:11:38.007: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1696
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Nov 16 21:11:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
Nov 16 21:11:43.493: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:12:02.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1696" for this suite.

â€¢ [SLOW TEST:24.625 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":339,"skipped":5966,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:12:02.634: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-3526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Nov 16 21:12:02.924: INFO: Waiting up to 5m0s for pod "security-context-02442e48-4831-4c67-a1c2-0943df691664" in namespace "security-context-3526" to be "Succeeded or Failed"
Nov 16 21:12:02.942: INFO: Pod "security-context-02442e48-4831-4c67-a1c2-0943df691664": Phase="Pending", Reason="", readiness=false. Elapsed: 17.997559ms
Nov 16 21:12:04.966: INFO: Pod "security-context-02442e48-4831-4c67-a1c2-0943df691664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041610818s
Nov 16 21:12:06.984: INFO: Pod "security-context-02442e48-4831-4c67-a1c2-0943df691664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060401558s
STEP: Saw pod success
Nov 16 21:12:06.985: INFO: Pod "security-context-02442e48-4831-4c67-a1c2-0943df691664" satisfied condition "Succeeded or Failed"
Nov 16 21:12:07.003: INFO: Trying to get logs from node 10.193.87.27 pod security-context-02442e48-4831-4c67-a1c2-0943df691664 container test-container: <nil>
STEP: delete the pod
Nov 16 21:12:07.074: INFO: Waiting for pod security-context-02442e48-4831-4c67-a1c2-0943df691664 to disappear
Nov 16 21:12:07.090: INFO: Pod security-context-02442e48-4831-4c67-a1c2-0943df691664 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:12:07.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3526" for this suite.
â€¢{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":340,"skipped":5984,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:12:07.138: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-qjxj
STEP: Creating a pod to test atomic-volume-subpath
Nov 16 21:12:07.461: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qjxj" in namespace "subpath-6088" to be "Succeeded or Failed"
Nov 16 21:12:07.478: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Pending", Reason="", readiness=false. Elapsed: 17.144857ms
Nov 16 21:12:09.502: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040930721s
Nov 16 21:12:11.526: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.065036357s
Nov 16 21:12:13.550: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.08890777s
Nov 16 21:12:15.574: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.113017514s
Nov 16 21:12:17.596: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.135412308s
Nov 16 21:12:19.619: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.15824164s
Nov 16 21:12:21.646: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.185387173s
Nov 16 21:12:23.666: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.205515132s
Nov 16 21:12:25.701: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.240330965s
Nov 16 21:12:27.724: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.262718653s
Nov 16 21:12:29.747: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Running", Reason="", readiness=true. Elapsed: 22.285617434s
Nov 16 21:12:31.772: INFO: Pod "pod-subpath-test-secret-qjxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.311312033s
STEP: Saw pod success
Nov 16 21:12:31.773: INFO: Pod "pod-subpath-test-secret-qjxj" satisfied condition "Succeeded or Failed"
Nov 16 21:12:31.788: INFO: Trying to get logs from node 10.193.87.27 pod pod-subpath-test-secret-qjxj container test-container-subpath-secret-qjxj: <nil>
STEP: delete the pod
Nov 16 21:12:31.858: INFO: Waiting for pod pod-subpath-test-secret-qjxj to disappear
Nov 16 21:12:31.876: INFO: Pod pod-subpath-test-secret-qjxj no longer exists
STEP: Deleting pod pod-subpath-test-secret-qjxj
Nov 16 21:12:31.876: INFO: Deleting pod "pod-subpath-test-secret-qjxj" in namespace "subpath-6088"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:12:31.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6088" for this suite.

â€¢ [SLOW TEST:24.795 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":341,"skipped":5989,"failed":0}
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:12:31.935: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Nov 16 21:12:32.194: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Nov 16 21:12:32.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:32.524: INFO: stderr: ""
Nov 16 21:12:32.524: INFO: stdout: "service/agnhost-replica created\n"
Nov 16 21:12:32.524: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Nov 16 21:12:32.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:32.846: INFO: stderr: ""
Nov 16 21:12:32.846: INFO: stdout: "service/agnhost-primary created\n"
Nov 16 21:12:32.846: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 16 21:12:32.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:33.139: INFO: stderr: ""
Nov 16 21:12:33.139: INFO: stdout: "service/frontend created\n"
Nov 16 21:12:33.140: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Nov 16 21:12:33.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:33.434: INFO: stderr: ""
Nov 16 21:12:33.434: INFO: stdout: "deployment.apps/frontend created\n"
Nov 16 21:12:33.434: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 16 21:12:33.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:33.738: INFO: stderr: ""
Nov 16 21:12:33.738: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Nov 16 21:12:33.738: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 16 21:12:33.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 create -f -'
Nov 16 21:12:34.011: INFO: stderr: ""
Nov 16 21:12:34.011: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Nov 16 21:12:34.011: INFO: Waiting for all frontend pods to be Running.
Nov 16 21:12:39.062: INFO: Waiting for frontend to serve content.
Nov 16 21:12:39.129: INFO: Trying to add a new entry to the guestbook.
Nov 16 21:12:39.185: INFO: Verifying that added entry can be retrieved.
Nov 16 21:12:39.220: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Nov 16 21:12:44.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:44.432: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:44.432: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 21:12:44.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:44.625: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:44.625: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 21:12:44.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:44.831: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:44.831: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 21:12:44.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:44.966: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:44.966: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 21:12:44.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:45.113: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:45.114: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Nov 16 21:12:45.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790458781 --namespace=kubectl-8161 delete --grace-period=0 --force -f -'
Nov 16 21:12:45.226: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 16 21:12:45.226: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:12:45.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8161" for this suite.

â€¢ [SLOW TEST:13.349 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":342,"skipped":5989,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:12:45.284: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Nov 16 21:12:45.518: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 16 21:12:45.561: INFO: Waiting for terminating namespaces to be deleted...
Nov 16 21:12:45.577: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.24 before test
Nov 16 21:12:45.630: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-11-16 18:07:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Nov 16 21:12:45.630: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-tzl2b from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 21:12:45.630: INFO: calico-node-5cnhh from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:12:45.630: INFO: calico-typha-d5b48569-hcfb8 from kube-system started at 2021-11-16 18:05:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:12:45.630: INFO: coredns-b58d5f584-g92hx from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container coredns ready: true, restart count 0
Nov 16 21:12:45.630: INFO: coredns-b58d5f584-hv978 from kube-system started at 2021-11-16 21:10:17 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container coredns ready: true, restart count 0
Nov 16 21:12:45.630: INFO: ibm-keepalived-watcher-bl7zr from kube-system started at 2021-11-16 18:05:05 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:12:45.630: INFO: ibm-master-proxy-static-10.193.87.24 from kube-system started at 2021-11-16 18:05:01 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:12:45.630: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:12:45.630: INFO: konnectivity-agent-wtjm7 from kube-system started at 2021-11-16 18:13:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 21:12:45.630: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bzkx9 from kube-system started at 2021-11-16 18:08:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 21:12:45.630: INFO: agnhost-replica-6bcf79b489-z2vvj from kubectl-8161 started at 2021-11-16 21:12:34 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container replica ready: true, restart count 0
Nov 16 21:12:45.630: INFO: frontend-685fc574d5-8ftwx from kubectl-8161 started at 2021-11-16 21:12:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container guestbook-frontend ready: true, restart count 0
Nov 16 21:12:45.630: INFO: sonobuoy-e2e-job-54c8cae784204424 from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container e2e ready: true, restart count 0
Nov 16 21:12:45.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 21:12:45.630: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j6mjt from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 21:12:45.630: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 21:12:45.630: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.27 before test
Nov 16 21:12:45.659: INFO: calico-node-smn5q from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:12:45.660: INFO: calico-typha-d5b48569-n4z5b from kube-system started at 2021-11-16 21:10:54 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:12:45.660: INFO: ibm-keepalived-watcher-rb6gd from kube-system started at 2021-11-16 18:05:14 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:12:45.660: INFO: ibm-master-proxy-static-10.193.87.27 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:12:45.660: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:12:45.660: INFO: konnectivity-agent-ln55x from kube-system started at 2021-11-16 18:13:25 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 21:12:45.660: INFO: agnhost-primary-5db8ddd565-nk999 from kubectl-8161 started at 2021-11-16 21:12:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container primary ready: true, restart count 0
Nov 16 21:12:45.660: INFO: agnhost-replica-6bcf79b489-7zfgm from kubectl-8161 started at 2021-11-16 21:12:34 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container replica ready: true, restart count 0
Nov 16 21:12:45.660: INFO: frontend-685fc574d5-z8fsz from kubectl-8161 started at 2021-11-16 21:12:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container guestbook-frontend ready: true, restart count 0
Nov 16 21:12:45.660: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-j22wg from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 21:12:45.660: INFO: 	Container systemd-logs ready: true, restart count 0
Nov 16 21:12:45.660: INFO: 
Logging pods the apiserver thinks is on node 10.193.87.28 before test
Nov 16 21:12:45.705: INFO: catalog-operator-7489d5857-vlxrz from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.705: INFO: 	Container catalog-operator ready: true, restart count 0
Nov 16 21:12:45.705: INFO: ibm-cloud-provider-ip-165-192-87-50-578cd6b8cc-sf6d4 from ibm-system started at 2021-11-16 18:09:44 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.705: INFO: 	Container ibm-cloud-provider-ip-165-192-87-50 ready: true, restart count 0
Nov 16 21:12:45.705: INFO: olm-operator-7b6cd6c94c-5vgk4 from ibm-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.705: INFO: 	Container olm-operator ready: true, restart count 0
Nov 16 21:12:45.705: INFO: calico-kube-controllers-75488ccc5b-pf6m8 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.705: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 16 21:12:45.705: INFO: calico-node-sx675 from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.705: INFO: 	Container calico-node ready: true, restart count 0
Nov 16 21:12:45.706: INFO: calico-typha-d5b48569-s7xpk from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container calico-typha ready: true, restart count 0
Nov 16 21:12:45.706: INFO: coredns-autoscaler-689fb74d49-ww9hg from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container autoscaler ready: true, restart count 0
Nov 16 21:12:45.706: INFO: coredns-b58d5f584-ntqv9 from kube-system started at 2021-11-16 18:13:59 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container coredns ready: true, restart count 0
Nov 16 21:12:45.706: INFO: dashboard-metrics-scraper-6747f89c97-pzthq from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Nov 16 21:12:45.706: INFO: ibm-file-plugin-fd44cd466-zjcs4 from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Nov 16 21:12:45.706: INFO: ibm-keepalived-watcher-mfkdl from kube-system started at 2021-11-16 18:05:02 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container keepalived-watcher ready: true, restart count 0
Nov 16 21:12:45.706: INFO: ibm-master-proxy-static-10.193.87.28 from kube-system started at 2021-11-16 18:04:49 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Nov 16 21:12:45.706: INFO: 	Container pause ready: true, restart count 0
Nov 16 21:12:45.706: INFO: ibm-storage-watcher-765888f8c9-dffqn from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Nov 16 21:12:45.706: INFO: konnectivity-agent-g2pw7 from kube-system started at 2021-11-16 18:13:28 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container konnectivity-agent ready: true, restart count 0
Nov 16 21:12:45.706: INFO: kubernetes-dashboard-54c47dd995-czt2b from kube-system started at 2021-11-16 18:05:22 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 16 21:12:45.706: INFO: metrics-server-64bbdfc744-wxsz6 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container metrics-server ready: true, restart count 0
Nov 16 21:12:45.706: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Nov 16 21:12:45.706: INFO: public-crc69uu49t0k6l31sv41j0-alb1-6f79f8d789-bb4n2 from kube-system started at 2021-11-16 20:18:41 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container nginx-ingress ready: true, restart count 0
Nov 16 21:12:45.706: INFO: frontend-685fc574d5-6b4f4 from kubectl-8161 started at 2021-11-16 21:12:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container guestbook-frontend ready: true, restart count 0
Nov 16 21:12:45.706: INFO: sonobuoy from sonobuoy started at 2021-11-16 19:29:33 +0000 UTC (1 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Nov 16 21:12:45.706: INFO: sonobuoy-systemd-logs-daemon-set-8830d88ec5694f48-hpqln from sonobuoy started at 2021-11-16 19:29:40 +0000 UTC (2 container statuses recorded)
Nov 16 21:12:45.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Nov 16 21:12:45.706: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fe23af40-110d-4f24-8fce-33fd168080fb 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.193.87.27 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-fe23af40-110d-4f24-8fce-33fd168080fb off the node 10.193.87.27
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fe23af40-110d-4f24-8fce-33fd168080fb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:17:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1552" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:308.878 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":343,"skipped":6008,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:17:54.165: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 16 21:17:57.523: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:17:57.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-632" for this suite.
â€¢{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6011,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:17:57.629: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-e9783c52-233c-4c5c-9670-f6bcbecd4c24
STEP: Creating a pod to test consume secrets
Nov 16 21:17:57.928: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c" in namespace "projected-2547" to be "Succeeded or Failed"
Nov 16 21:17:57.945: INFO: Pod "pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.247412ms
Nov 16 21:17:59.972: INFO: Pod "pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044771611s
Nov 16 21:18:01.995: INFO: Pod "pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067128129s
STEP: Saw pod success
Nov 16 21:18:01.995: INFO: Pod "pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c" satisfied condition "Succeeded or Failed"
Nov 16 21:18:02.009: INFO: Trying to get logs from node 10.193.87.27 pod pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 16 21:18:02.160: INFO: Waiting for pod pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c to disappear
Nov 16 21:18:02.177: INFO: Pod pod-projected-secrets-6b2743f0-b269-4e7d-895f-3bdc2830886c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:18:02.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2547" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Nov 16 21:18:02.233: INFO: >>> kubeConfig: /tmp/kubeconfig-790458781
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Nov 16 21:18:02.508: INFO: Waiting up to 5m0s for pod "downward-api-20122680-3b39-4d70-8c8a-a57decb85034" in namespace "downward-api-3909" to be "Succeeded or Failed"
Nov 16 21:18:02.527: INFO: Pod "downward-api-20122680-3b39-4d70-8c8a-a57decb85034": Phase="Pending", Reason="", readiness=false. Elapsed: 17.924829ms
Nov 16 21:18:04.552: INFO: Pod "downward-api-20122680-3b39-4d70-8c8a-a57decb85034": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043190198s
Nov 16 21:18:06.575: INFO: Pod "downward-api-20122680-3b39-4d70-8c8a-a57decb85034": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066075085s
STEP: Saw pod success
Nov 16 21:18:06.575: INFO: Pod "downward-api-20122680-3b39-4d70-8c8a-a57decb85034" satisfied condition "Succeeded or Failed"
Nov 16 21:18:06.590: INFO: Trying to get logs from node 10.193.87.27 pod downward-api-20122680-3b39-4d70-8c8a-a57decb85034 container dapi-container: <nil>
STEP: delete the pod
Nov 16 21:18:06.672: INFO: Waiting for pod downward-api-20122680-3b39-4d70-8c8a-a57decb85034 to disappear
Nov 16 21:18:06.686: INFO: Pod downward-api-20122680-3b39-4d70-8c8a-a57decb85034 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Nov 16 21:18:06.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3909" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6076,"failed":0}
SSSSSSSSSSNov 16 21:18:06.743: INFO: Running AfterSuite actions on all nodes
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Nov 16 21:18:06.743: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Nov 16 21:18:06.743: INFO: Running AfterSuite actions on node 1
Nov 16 21:18:06.743: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6086,"failed":0}

Ran 346 of 6432 Specs in 6487.112 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6086 Skipped
PASS

Ginkgo ran 1 suite in 1h48m9.27730399s
Test Suite Passed
