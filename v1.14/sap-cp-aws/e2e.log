Conformance test: not doing test setup.
I0409 06:41:13.617881    6827 e2e.go:240] Starting e2e run "773aa641-5a92-11e9-8c35-36e6c88ddb32" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1554792072 - Will randomize all specs
Will run 204 of 3584 specs

Apr  9 06:41:13.875: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 06:41:13.877: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr  9 06:41:14.058: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr  9 06:41:14.226: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr  9 06:41:14.226: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Apr  9 06:41:14.226: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr  9 06:41:14.265: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'addons-kube2iam' (0 seconds elapsed)
Apr  9 06:41:14.265: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr  9 06:41:14.265: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr  9 06:41:14.265: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Apr  9 06:41:14.265: INFO: e2e test version: v1.14.0
Apr  9 06:41:14.290: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:41:14.290: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
Apr  9 06:41:14.423: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr  9 06:41:14.502: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-78a0c507-5a92-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 06:41:14.755: INFO: Waiting up to 5m0s for pod "pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32" in namespace "configmap-1683" to be "success or failure"
Apr  9 06:41:14.780: INFO: Pod "pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 24.774744ms
Apr  9 06:41:16.806: INFO: Pod "pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050293256s
Apr  9 06:41:18.831: INFO: Pod "pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075856968s
STEP: Saw pod success
Apr  9 06:41:18.831: INFO: Pod "pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:41:18.856: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 06:41:19.051: INFO: Waiting for pod pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:41:19.076: INFO: Pod pod-configmaps-78a490ae-5a92-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:41:19.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1683" for this suite.
Apr  9 06:41:25.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:41:26.213: INFO: namespace configmap-1683 deletion completed in 7.111393071s
•SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:41:26.213: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:41:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2685" for this suite.
Apr  9 06:42:14.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:42:15.680: INFO: namespace kubelet-test-2685 deletion completed in 44.9955232s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:42:15.680: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 06:42:15.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32" in namespace "projected-6882" to be "success or failure"
Apr  9 06:42:16.022: INFO: Pod "downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.067801ms
Apr  9 06:42:18.047: INFO: Pod "downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050604557s
STEP: Saw pod success
Apr  9 06:42:18.047: INFO: Pod "downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:42:18.073: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 06:42:18.133: INFO: Waiting for pod downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:42:18.158: INFO: Pod downwardapi-volume-9d253228-5a92-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:42:18.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6882" for this suite.
Apr  9 06:42:24.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:42:25.235: INFO: namespace projected-6882 deletion completed in 7.05114144s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:42:25.235: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4529
I0409 06:42:25.511286    6827 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4529, replica count: 1
I0409 06:42:26.561741    6827 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 06:42:27.561914    6827 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 06:42:28.562114    6827 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  9 06:42:28.691: INFO: Created: latency-svc-pjt8x
Apr  9 06:42:28.693: INFO: Got endpoints: latency-svc-pjt8x [30.851511ms]
Apr  9 06:42:28.722: INFO: Created: latency-svc-blqtf
Apr  9 06:42:28.725: INFO: Got endpoints: latency-svc-blqtf [32.558952ms]
Apr  9 06:42:28.725: INFO: Created: latency-svc-kszdn
Apr  9 06:42:28.726: INFO: Got endpoints: latency-svc-kszdn [33.715769ms]
Apr  9 06:42:28.744: INFO: Created: latency-svc-d4mcd
Apr  9 06:42:28.745: INFO: Got endpoints: latency-svc-d4mcd [52.616707ms]
Apr  9 06:42:28.748: INFO: Created: latency-svc-w6fcj
Apr  9 06:42:28.753: INFO: Created: latency-svc-sfpkv
Apr  9 06:42:28.753: INFO: Got endpoints: latency-svc-w6fcj [59.926318ms]
Apr  9 06:42:28.754: INFO: Got endpoints: latency-svc-sfpkv [61.371148ms]
Apr  9 06:42:28.757: INFO: Created: latency-svc-kqx46
Apr  9 06:42:28.758: INFO: Got endpoints: latency-svc-kqx46 [32.366505ms]
Apr  9 06:42:28.762: INFO: Created: latency-svc-qgdpc
Apr  9 06:42:28.763: INFO: Got endpoints: latency-svc-qgdpc [70.505056ms]
Apr  9 06:42:28.765: INFO: Created: latency-svc-nnrnd
Apr  9 06:42:28.770: INFO: Got endpoints: latency-svc-nnrnd [76.873819ms]
Apr  9 06:42:28.770: INFO: Created: latency-svc-t8t7n
Apr  9 06:42:28.772: INFO: Got endpoints: latency-svc-t8t7n [78.68333ms]
Apr  9 06:42:28.776: INFO: Created: latency-svc-2p4rx
Apr  9 06:42:28.777: INFO: Got endpoints: latency-svc-2p4rx [83.55201ms]
Apr  9 06:42:28.781: INFO: Created: latency-svc-7s4tq
Apr  9 06:42:28.785: INFO: Created: latency-svc-6w4ps
Apr  9 06:42:28.785: INFO: Got endpoints: latency-svc-7s4tq [92.041551ms]
Apr  9 06:42:28.786: INFO: Got endpoints: latency-svc-6w4ps [92.837411ms]
Apr  9 06:42:28.789: INFO: Created: latency-svc-9jfd5
Apr  9 06:42:28.794: INFO: Created: latency-svc-qvtnf
Apr  9 06:42:28.794: INFO: Got endpoints: latency-svc-9jfd5 [101.048707ms]
Apr  9 06:42:28.796: INFO: Got endpoints: latency-svc-qvtnf [102.679141ms]
Apr  9 06:42:28.799: INFO: Created: latency-svc-6xfcc
Apr  9 06:42:28.800: INFO: Got endpoints: latency-svc-6xfcc [106.61529ms]
Apr  9 06:42:28.804: INFO: Created: latency-svc-s6hz4
Apr  9 06:42:28.815: INFO: Created: latency-svc-8k78g
Apr  9 06:42:28.815: INFO: Got endpoints: latency-svc-s6hz4 [121.927621ms]
Apr  9 06:42:28.816: INFO: Got endpoints: latency-svc-8k78g [89.393329ms]
Apr  9 06:42:28.820: INFO: Created: latency-svc-96547
Apr  9 06:42:28.821: INFO: Got endpoints: latency-svc-96547 [75.323109ms]
Apr  9 06:42:28.825: INFO: Created: latency-svc-p6cxf
Apr  9 06:42:28.827: INFO: Got endpoints: latency-svc-p6cxf [73.691068ms]
Apr  9 06:42:28.829: INFO: Created: latency-svc-6sp5x
Apr  9 06:42:28.830: INFO: Got endpoints: latency-svc-6sp5x [75.822111ms]
Apr  9 06:42:28.834: INFO: Created: latency-svc-ss8cg
Apr  9 06:42:28.835: INFO: Got endpoints: latency-svc-ss8cg [77.386267ms]
Apr  9 06:42:28.839: INFO: Created: latency-svc-dlptd
Apr  9 06:42:28.840: INFO: Got endpoints: latency-svc-dlptd [76.681652ms]
Apr  9 06:42:28.843: INFO: Created: latency-svc-m74f7
Apr  9 06:42:28.844: INFO: Got endpoints: latency-svc-m74f7 [74.158064ms]
Apr  9 06:42:28.847: INFO: Created: latency-svc-5z4rh
Apr  9 06:42:28.850: INFO: Got endpoints: latency-svc-5z4rh [78.697172ms]
Apr  9 06:42:28.857: INFO: Created: latency-svc-zzqgz
Apr  9 06:42:28.858: INFO: Got endpoints: latency-svc-zzqgz [81.039621ms]
Apr  9 06:42:28.862: INFO: Created: latency-svc-rw2tv
Apr  9 06:42:28.863: INFO: Got endpoints: latency-svc-rw2tv [77.778108ms]
Apr  9 06:42:28.866: INFO: Created: latency-svc-98hbd
Apr  9 06:42:28.867: INFO: Got endpoints: latency-svc-98hbd [81.040442ms]
Apr  9 06:42:28.870: INFO: Created: latency-svc-vhlq5
Apr  9 06:42:28.871: INFO: Got endpoints: latency-svc-vhlq5 [77.111872ms]
Apr  9 06:42:28.874: INFO: Created: latency-svc-nbp44
Apr  9 06:42:28.876: INFO: Got endpoints: latency-svc-nbp44 [80.045626ms]
Apr  9 06:42:28.879: INFO: Created: latency-svc-7jblx
Apr  9 06:42:28.880: INFO: Got endpoints: latency-svc-7jblx [80.141428ms]
Apr  9 06:42:28.895: INFO: Created: latency-svc-2n2z9
Apr  9 06:42:28.898: INFO: Got endpoints: latency-svc-2n2z9 [83.363278ms]
Apr  9 06:42:28.899: INFO: Created: latency-svc-bv94s
Apr  9 06:42:28.902: INFO: Got endpoints: latency-svc-bv94s [86.142616ms]
Apr  9 06:42:28.902: INFO: Created: latency-svc-vmpkd
Apr  9 06:42:28.903: INFO: Got endpoints: latency-svc-vmpkd [82.64304ms]
Apr  9 06:42:28.906: INFO: Created: latency-svc-fsdp2
Apr  9 06:42:28.911: INFO: Created: latency-svc-96l66
Apr  9 06:42:28.924: INFO: Created: latency-svc-md4vq
Apr  9 06:42:28.927: INFO: Created: latency-svc-tm58r
Apr  9 06:42:28.933: INFO: Created: latency-svc-2xzsb
Apr  9 06:42:28.937: INFO: Created: latency-svc-bzsxj
Apr  9 06:42:28.941: INFO: Created: latency-svc-hqnwf
Apr  9 06:42:28.942: INFO: Got endpoints: latency-svc-fsdp2 [115.104262ms]
Apr  9 06:42:28.947: INFO: Created: latency-svc-frrcv
Apr  9 06:42:28.951: INFO: Created: latency-svc-5hfnb
Apr  9 06:42:28.955: INFO: Created: latency-svc-qjncz
Apr  9 06:42:28.959: INFO: Created: latency-svc-6gk55
Apr  9 06:42:28.963: INFO: Created: latency-svc-psqxm
Apr  9 06:42:28.967: INFO: Created: latency-svc-svl5s
Apr  9 06:42:28.972: INFO: Created: latency-svc-qvf55
Apr  9 06:42:28.976: INFO: Created: latency-svc-hwk4x
Apr  9 06:42:28.980: INFO: Created: latency-svc-lwd8p
Apr  9 06:42:28.993: INFO: Got endpoints: latency-svc-96l66 [162.372734ms]
Apr  9 06:42:29.022: INFO: Created: latency-svc-nhq2d
Apr  9 06:42:29.168: INFO: Got endpoints: latency-svc-2xzsb [324.404231ms]
Apr  9 06:42:29.169: INFO: Got endpoints: latency-svc-tm58r [328.38819ms]
Apr  9 06:42:29.169: INFO: Got endpoints: latency-svc-md4vq [333.339992ms]
Apr  9 06:42:29.193: INFO: Got endpoints: latency-svc-bzsxj [342.313942ms]
Apr  9 06:42:29.199: INFO: Created: latency-svc-w8cmc
Apr  9 06:42:29.203: INFO: Created: latency-svc-9xvvb
Apr  9 06:42:29.206: INFO: Created: latency-svc-v65mv
Apr  9 06:42:29.222: INFO: Created: latency-svc-2llpk
Apr  9 06:42:29.242: INFO: Got endpoints: latency-svc-hqnwf [384.408371ms]
Apr  9 06:42:29.274: INFO: Created: latency-svc-lgs7g
Apr  9 06:42:29.292: INFO: Got endpoints: latency-svc-5hfnb [425.361216ms]
Apr  9 06:42:29.322: INFO: Created: latency-svc-7gkqc
Apr  9 06:42:29.342: INFO: Got endpoints: latency-svc-frrcv [479.225307ms]
Apr  9 06:42:29.400: INFO: Got endpoints: latency-svc-qjncz [528.664962ms]
Apr  9 06:42:29.401: INFO: Created: latency-svc-sklpn
Apr  9 06:42:29.431: INFO: Created: latency-svc-sv7v5
Apr  9 06:42:29.442: INFO: Got endpoints: latency-svc-6gk55 [566.313859ms]
Apr  9 06:42:29.475: INFO: Created: latency-svc-wgxvt
Apr  9 06:42:29.492: INFO: Got endpoints: latency-svc-psqxm [612.56494ms]
Apr  9 06:42:29.522: INFO: Created: latency-svc-gwgzm
Apr  9 06:42:29.542: INFO: Got endpoints: latency-svc-svl5s [643.859022ms]
Apr  9 06:42:29.571: INFO: Created: latency-svc-xp2zz
Apr  9 06:42:29.592: INFO: Got endpoints: latency-svc-qvf55 [690.13618ms]
Apr  9 06:42:29.624: INFO: Created: latency-svc-rzwbc
Apr  9 06:42:29.642: INFO: Got endpoints: latency-svc-hwk4x [738.710515ms]
Apr  9 06:42:29.672: INFO: Created: latency-svc-ww5rv
Apr  9 06:42:29.695: INFO: Got endpoints: latency-svc-lwd8p [752.809173ms]
Apr  9 06:42:29.723: INFO: Created: latency-svc-phdcb
Apr  9 06:42:29.742: INFO: Got endpoints: latency-svc-nhq2d [749.392732ms]
Apr  9 06:42:29.771: INFO: Created: latency-svc-8ghj6
Apr  9 06:42:29.792: INFO: Got endpoints: latency-svc-w8cmc [623.816674ms]
Apr  9 06:42:29.821: INFO: Created: latency-svc-9tj22
Apr  9 06:42:29.842: INFO: Got endpoints: latency-svc-9xvvb [673.280327ms]
Apr  9 06:42:29.871: INFO: Created: latency-svc-rfnbc
Apr  9 06:42:29.892: INFO: Got endpoints: latency-svc-v65mv [723.634362ms]
Apr  9 06:42:29.923: INFO: Created: latency-svc-s5w74
Apr  9 06:42:29.942: INFO: Got endpoints: latency-svc-2llpk [749.442324ms]
Apr  9 06:42:29.971: INFO: Created: latency-svc-2k98t
Apr  9 06:42:29.993: INFO: Got endpoints: latency-svc-lgs7g [750.511272ms]
Apr  9 06:42:30.022: INFO: Created: latency-svc-sv4tp
Apr  9 06:42:30.044: INFO: Got endpoints: latency-svc-7gkqc [751.850018ms]
Apr  9 06:42:30.073: INFO: Created: latency-svc-djq47
Apr  9 06:42:30.092: INFO: Got endpoints: latency-svc-sklpn [749.958807ms]
Apr  9 06:42:30.121: INFO: Created: latency-svc-6zkc8
Apr  9 06:42:30.142: INFO: Got endpoints: latency-svc-sv7v5 [741.911567ms]
Apr  9 06:42:30.170: INFO: Created: latency-svc-k8xk7
Apr  9 06:42:30.192: INFO: Got endpoints: latency-svc-wgxvt [750.140943ms]
Apr  9 06:42:30.221: INFO: Created: latency-svc-75b4s
Apr  9 06:42:30.242: INFO: Got endpoints: latency-svc-gwgzm [749.34079ms]
Apr  9 06:42:30.271: INFO: Created: latency-svc-f9dwb
Apr  9 06:42:30.292: INFO: Got endpoints: latency-svc-xp2zz [749.539127ms]
Apr  9 06:42:30.325: INFO: Created: latency-svc-4zmzh
Apr  9 06:42:30.342: INFO: Got endpoints: latency-svc-rzwbc [749.795389ms]
Apr  9 06:42:30.372: INFO: Created: latency-svc-dctjl
Apr  9 06:42:30.392: INFO: Got endpoints: latency-svc-ww5rv [749.915726ms]
Apr  9 06:42:30.423: INFO: Created: latency-svc-v7rdn
Apr  9 06:42:30.442: INFO: Got endpoints: latency-svc-phdcb [747.203727ms]
Apr  9 06:42:30.472: INFO: Created: latency-svc-h4g4p
Apr  9 06:42:30.492: INFO: Got endpoints: latency-svc-8ghj6 [750.227707ms]
Apr  9 06:42:30.522: INFO: Created: latency-svc-qk2sz
Apr  9 06:42:30.542: INFO: Got endpoints: latency-svc-9tj22 [750.080636ms]
Apr  9 06:42:30.572: INFO: Created: latency-svc-25bdf
Apr  9 06:42:30.593: INFO: Got endpoints: latency-svc-rfnbc [750.671388ms]
Apr  9 06:42:30.622: INFO: Created: latency-svc-5lh4z
Apr  9 06:42:30.642: INFO: Got endpoints: latency-svc-s5w74 [749.867916ms]
Apr  9 06:42:30.671: INFO: Created: latency-svc-x6p7s
Apr  9 06:42:30.692: INFO: Got endpoints: latency-svc-2k98t [750.139322ms]
Apr  9 06:42:30.722: INFO: Created: latency-svc-r6xdd
Apr  9 06:42:30.742: INFO: Got endpoints: latency-svc-sv4tp [749.379299ms]
Apr  9 06:42:30.771: INFO: Created: latency-svc-wpzdv
Apr  9 06:42:30.792: INFO: Got endpoints: latency-svc-djq47 [747.781238ms]
Apr  9 06:42:30.822: INFO: Created: latency-svc-s7f8p
Apr  9 06:42:30.843: INFO: Got endpoints: latency-svc-6zkc8 [751.06909ms]
Apr  9 06:42:30.874: INFO: Created: latency-svc-cxfpf
Apr  9 06:42:30.894: INFO: Got endpoints: latency-svc-k8xk7 [751.732671ms]
Apr  9 06:42:30.923: INFO: Created: latency-svc-g6pm6
Apr  9 06:42:30.946: INFO: Got endpoints: latency-svc-75b4s [753.974341ms]
Apr  9 06:42:30.976: INFO: Created: latency-svc-nhmnf
Apr  9 06:42:30.992: INFO: Got endpoints: latency-svc-f9dwb [750.326461ms]
Apr  9 06:42:31.022: INFO: Created: latency-svc-g8ph2
Apr  9 06:42:31.042: INFO: Got endpoints: latency-svc-4zmzh [750.187027ms]
Apr  9 06:42:31.072: INFO: Created: latency-svc-hmtlg
Apr  9 06:42:31.092: INFO: Got endpoints: latency-svc-dctjl [749.817542ms]
Apr  9 06:42:31.122: INFO: Created: latency-svc-5g6q9
Apr  9 06:42:31.142: INFO: Got endpoints: latency-svc-v7rdn [749.916161ms]
Apr  9 06:42:31.171: INFO: Created: latency-svc-v5rch
Apr  9 06:42:31.192: INFO: Got endpoints: latency-svc-h4g4p [750.178385ms]
Apr  9 06:42:31.221: INFO: Created: latency-svc-lx9jb
Apr  9 06:42:31.242: INFO: Got endpoints: latency-svc-qk2sz [749.436919ms]
Apr  9 06:42:31.270: INFO: Created: latency-svc-gft2v
Apr  9 06:42:31.292: INFO: Got endpoints: latency-svc-25bdf [749.529524ms]
Apr  9 06:42:31.321: INFO: Created: latency-svc-ldb6v
Apr  9 06:42:31.342: INFO: Got endpoints: latency-svc-5lh4z [749.493366ms]
Apr  9 06:42:31.371: INFO: Created: latency-svc-xhndx
Apr  9 06:42:31.392: INFO: Got endpoints: latency-svc-x6p7s [749.73251ms]
Apr  9 06:42:31.420: INFO: Created: latency-svc-tbwvl
Apr  9 06:42:31.442: INFO: Got endpoints: latency-svc-r6xdd [749.348248ms]
Apr  9 06:42:31.471: INFO: Created: latency-svc-8sv5z
Apr  9 06:42:31.492: INFO: Got endpoints: latency-svc-wpzdv [749.874881ms]
Apr  9 06:42:31.521: INFO: Created: latency-svc-w2klh
Apr  9 06:42:31.542: INFO: Got endpoints: latency-svc-s7f8p [750.06011ms]
Apr  9 06:42:31.572: INFO: Created: latency-svc-4p44k
Apr  9 06:42:31.592: INFO: Got endpoints: latency-svc-cxfpf [748.708255ms]
Apr  9 06:42:31.621: INFO: Created: latency-svc-5tqzl
Apr  9 06:42:31.642: INFO: Got endpoints: latency-svc-g6pm6 [748.255636ms]
Apr  9 06:42:31.671: INFO: Created: latency-svc-49wms
Apr  9 06:42:31.692: INFO: Got endpoints: latency-svc-nhmnf [745.889963ms]
Apr  9 06:42:31.721: INFO: Created: latency-svc-jb95f
Apr  9 06:42:31.742: INFO: Got endpoints: latency-svc-g8ph2 [749.638273ms]
Apr  9 06:42:31.770: INFO: Created: latency-svc-p8n8t
Apr  9 06:42:31.792: INFO: Got endpoints: latency-svc-hmtlg [749.74955ms]
Apr  9 06:42:31.821: INFO: Created: latency-svc-76ml8
Apr  9 06:42:31.842: INFO: Got endpoints: latency-svc-5g6q9 [749.551873ms]
Apr  9 06:42:31.871: INFO: Created: latency-svc-9kqfh
Apr  9 06:42:31.893: INFO: Got endpoints: latency-svc-v5rch [750.285534ms]
Apr  9 06:42:31.922: INFO: Created: latency-svc-m748t
Apr  9 06:42:31.942: INFO: Got endpoints: latency-svc-lx9jb [749.777349ms]
Apr  9 06:42:31.971: INFO: Created: latency-svc-r5f2h
Apr  9 06:42:31.993: INFO: Got endpoints: latency-svc-gft2v [751.394559ms]
Apr  9 06:42:32.023: INFO: Created: latency-svc-2w9tq
Apr  9 06:42:32.042: INFO: Got endpoints: latency-svc-ldb6v [750.225148ms]
Apr  9 06:42:32.072: INFO: Created: latency-svc-gg6rm
Apr  9 06:42:32.093: INFO: Got endpoints: latency-svc-xhndx [750.479318ms]
Apr  9 06:42:32.122: INFO: Created: latency-svc-m68c5
Apr  9 06:42:32.142: INFO: Got endpoints: latency-svc-tbwvl [749.8247ms]
Apr  9 06:42:32.170: INFO: Created: latency-svc-jldr6
Apr  9 06:42:32.192: INFO: Got endpoints: latency-svc-8sv5z [750.260306ms]
Apr  9 06:42:32.221: INFO: Created: latency-svc-2wxq5
Apr  9 06:42:32.242: INFO: Got endpoints: latency-svc-w2klh [749.793049ms]
Apr  9 06:42:32.271: INFO: Created: latency-svc-7tm65
Apr  9 06:42:32.292: INFO: Got endpoints: latency-svc-4p44k [749.868068ms]
Apr  9 06:42:32.321: INFO: Created: latency-svc-8v8bz
Apr  9 06:42:32.342: INFO: Got endpoints: latency-svc-5tqzl [749.702803ms]
Apr  9 06:42:32.371: INFO: Created: latency-svc-4d69s
Apr  9 06:42:32.392: INFO: Got endpoints: latency-svc-49wms [750.041813ms]
Apr  9 06:42:32.422: INFO: Created: latency-svc-rljkd
Apr  9 06:42:32.442: INFO: Got endpoints: latency-svc-jb95f [749.611568ms]
Apr  9 06:42:32.470: INFO: Created: latency-svc-gvwzp
Apr  9 06:42:32.492: INFO: Got endpoints: latency-svc-p8n8t [749.867367ms]
Apr  9 06:42:32.521: INFO: Created: latency-svc-k7qht
Apr  9 06:42:32.542: INFO: Got endpoints: latency-svc-76ml8 [749.757706ms]
Apr  9 06:42:32.571: INFO: Created: latency-svc-mcz4p
Apr  9 06:42:32.593: INFO: Got endpoints: latency-svc-9kqfh [750.57278ms]
Apr  9 06:42:32.622: INFO: Created: latency-svc-2gtqc
Apr  9 06:42:32.642: INFO: Got endpoints: latency-svc-m748t [749.252952ms]
Apr  9 06:42:32.671: INFO: Created: latency-svc-txgwg
Apr  9 06:42:32.692: INFO: Got endpoints: latency-svc-r5f2h [749.970724ms]
Apr  9 06:42:32.726: INFO: Created: latency-svc-n7m6s
Apr  9 06:42:32.742: INFO: Got endpoints: latency-svc-2w9tq [748.551537ms]
Apr  9 06:42:32.775: INFO: Created: latency-svc-v6s4b
Apr  9 06:42:32.795: INFO: Got endpoints: latency-svc-gg6rm [752.31238ms]
Apr  9 06:42:32.826: INFO: Created: latency-svc-hwfzj
Apr  9 06:42:32.842: INFO: Got endpoints: latency-svc-m68c5 [749.476902ms]
Apr  9 06:42:32.872: INFO: Created: latency-svc-jlqzk
Apr  9 06:42:32.892: INFO: Got endpoints: latency-svc-jldr6 [750.144277ms]
Apr  9 06:42:32.923: INFO: Created: latency-svc-xqx9p
Apr  9 06:42:32.942: INFO: Got endpoints: latency-svc-2wxq5 [750.007108ms]
Apr  9 06:42:32.974: INFO: Created: latency-svc-nvs65
Apr  9 06:42:32.992: INFO: Got endpoints: latency-svc-7tm65 [750.280429ms]
Apr  9 06:42:33.021: INFO: Created: latency-svc-5xjqp
Apr  9 06:42:33.042: INFO: Got endpoints: latency-svc-8v8bz [749.956617ms]
Apr  9 06:42:33.071: INFO: Created: latency-svc-75tff
Apr  9 06:42:33.092: INFO: Got endpoints: latency-svc-4d69s [750.242312ms]
Apr  9 06:42:33.121: INFO: Created: latency-svc-2zm69
Apr  9 06:42:33.142: INFO: Got endpoints: latency-svc-rljkd [750.092399ms]
Apr  9 06:42:33.192: INFO: Got endpoints: latency-svc-gvwzp [750.367013ms]
Apr  9 06:42:33.217: INFO: Created: latency-svc-8dlzp
Apr  9 06:42:33.222: INFO: Created: latency-svc-l8w4h
Apr  9 06:42:33.243: INFO: Got endpoints: latency-svc-k7qht [751.078172ms]
Apr  9 06:42:33.273: INFO: Created: latency-svc-x8l78
Apr  9 06:42:33.292: INFO: Got endpoints: latency-svc-mcz4p [750.620041ms]
Apr  9 06:42:33.322: INFO: Created: latency-svc-k59w9
Apr  9 06:42:33.342: INFO: Got endpoints: latency-svc-2gtqc [749.33079ms]
Apr  9 06:42:33.373: INFO: Created: latency-svc-rhr79
Apr  9 06:42:33.392: INFO: Got endpoints: latency-svc-txgwg [750.275751ms]
Apr  9 06:42:33.423: INFO: Created: latency-svc-7b2rt
Apr  9 06:42:33.442: INFO: Got endpoints: latency-svc-n7m6s [749.867721ms]
Apr  9 06:42:33.473: INFO: Created: latency-svc-g8vj4
Apr  9 06:42:33.492: INFO: Got endpoints: latency-svc-v6s4b [750.319454ms]
Apr  9 06:42:33.523: INFO: Created: latency-svc-hkt9w
Apr  9 06:42:33.542: INFO: Got endpoints: latency-svc-hwfzj [747.253422ms]
Apr  9 06:42:33.574: INFO: Created: latency-svc-qs8f6
Apr  9 06:42:33.592: INFO: Got endpoints: latency-svc-jlqzk [749.767983ms]
Apr  9 06:42:33.623: INFO: Created: latency-svc-tbb8g
Apr  9 06:42:33.642: INFO: Got endpoints: latency-svc-xqx9p [749.533047ms]
Apr  9 06:42:33.766: INFO: Got endpoints: latency-svc-5xjqp [774.127173ms]
Apr  9 06:42:33.766: INFO: Got endpoints: latency-svc-nvs65 [824.052575ms]
Apr  9 06:42:33.771: INFO: Created: latency-svc-vdr8m
Apr  9 06:42:33.792: INFO: Got endpoints: latency-svc-75tff [750.081276ms]
Apr  9 06:42:33.891: INFO: Created: latency-svc-p47jg
Apr  9 06:42:33.895: INFO: Got endpoints: latency-svc-8dlzp [752.824964ms]
Apr  9 06:42:33.895: INFO: Got endpoints: latency-svc-2zm69 [803.271396ms]
Apr  9 06:42:33.898: INFO: Created: latency-svc-smpvn
Apr  9 06:42:33.902: INFO: Created: latency-svc-cqlbj
Apr  9 06:42:33.926: INFO: Created: latency-svc-wmhgd
Apr  9 06:42:33.929: INFO: Created: latency-svc-fhkgd
Apr  9 06:42:33.942: INFO: Got endpoints: latency-svc-l8w4h [749.450681ms]
Apr  9 06:42:33.995: INFO: Created: latency-svc-9wn92
Apr  9 06:42:33.995: INFO: Got endpoints: latency-svc-x8l78 [751.810212ms]
Apr  9 06:42:34.025: INFO: Created: latency-svc-pdtsg
Apr  9 06:42:34.042: INFO: Got endpoints: latency-svc-k59w9 [749.670844ms]
Apr  9 06:42:34.073: INFO: Created: latency-svc-s44s9
Apr  9 06:42:34.092: INFO: Got endpoints: latency-svc-rhr79 [749.856897ms]
Apr  9 06:42:34.123: INFO: Created: latency-svc-cmg7s
Apr  9 06:42:34.142: INFO: Got endpoints: latency-svc-7b2rt [749.515642ms]
Apr  9 06:42:34.174: INFO: Created: latency-svc-dl5cq
Apr  9 06:42:34.192: INFO: Got endpoints: latency-svc-g8vj4 [750.384866ms]
Apr  9 06:42:34.224: INFO: Created: latency-svc-2vds7
Apr  9 06:42:34.242: INFO: Got endpoints: latency-svc-hkt9w [750.036816ms]
Apr  9 06:42:34.273: INFO: Created: latency-svc-hw8j8
Apr  9 06:42:34.292: INFO: Got endpoints: latency-svc-qs8f6 [750.023498ms]
Apr  9 06:42:34.323: INFO: Created: latency-svc-6p7gh
Apr  9 06:42:34.342: INFO: Got endpoints: latency-svc-tbb8g [750.063655ms]
Apr  9 06:42:34.374: INFO: Created: latency-svc-w9xvh
Apr  9 06:42:34.392: INFO: Got endpoints: latency-svc-vdr8m [750.396777ms]
Apr  9 06:42:34.424: INFO: Created: latency-svc-nqntg
Apr  9 06:42:34.442: INFO: Got endpoints: latency-svc-p47jg [675.200043ms]
Apr  9 06:42:34.473: INFO: Created: latency-svc-sjvgp
Apr  9 06:42:34.495: INFO: Got endpoints: latency-svc-smpvn [728.297283ms]
Apr  9 06:42:34.526: INFO: Created: latency-svc-9r786
Apr  9 06:42:34.542: INFO: Got endpoints: latency-svc-cqlbj [749.392664ms]
Apr  9 06:42:34.572: INFO: Created: latency-svc-5rzm8
Apr  9 06:42:34.592: INFO: Got endpoints: latency-svc-wmhgd [696.776433ms]
Apr  9 06:42:34.623: INFO: Created: latency-svc-xb4hb
Apr  9 06:42:34.642: INFO: Got endpoints: latency-svc-fhkgd [746.445922ms]
Apr  9 06:42:34.672: INFO: Created: latency-svc-qrrxl
Apr  9 06:42:34.692: INFO: Got endpoints: latency-svc-9wn92 [750.133949ms]
Apr  9 06:42:34.723: INFO: Created: latency-svc-767xs
Apr  9 06:42:34.742: INFO: Got endpoints: latency-svc-pdtsg [746.911678ms]
Apr  9 06:42:34.773: INFO: Created: latency-svc-lr25w
Apr  9 06:42:34.792: INFO: Got endpoints: latency-svc-s44s9 [749.593772ms]
Apr  9 06:42:34.822: INFO: Created: latency-svc-vjn6p
Apr  9 06:42:34.842: INFO: Got endpoints: latency-svc-cmg7s [749.794397ms]
Apr  9 06:42:34.873: INFO: Created: latency-svc-kxh7q
Apr  9 06:42:34.892: INFO: Got endpoints: latency-svc-dl5cq [750.09835ms]
Apr  9 06:42:34.923: INFO: Created: latency-svc-rw67l
Apr  9 06:42:34.942: INFO: Got endpoints: latency-svc-2vds7 [749.587386ms]
Apr  9 06:42:34.972: INFO: Created: latency-svc-dhhxd
Apr  9 06:42:34.992: INFO: Got endpoints: latency-svc-hw8j8 [749.525702ms]
Apr  9 06:42:35.023: INFO: Created: latency-svc-4f9q7
Apr  9 06:42:35.042: INFO: Got endpoints: latency-svc-6p7gh [749.588852ms]
Apr  9 06:42:35.073: INFO: Created: latency-svc-rznp2
Apr  9 06:42:35.092: INFO: Got endpoints: latency-svc-w9xvh [749.685788ms]
Apr  9 06:42:35.124: INFO: Created: latency-svc-sdxbf
Apr  9 06:42:35.142: INFO: Got endpoints: latency-svc-nqntg [749.769023ms]
Apr  9 06:42:35.174: INFO: Created: latency-svc-gkh2p
Apr  9 06:42:35.192: INFO: Got endpoints: latency-svc-sjvgp [749.949386ms]
Apr  9 06:42:35.226: INFO: Created: latency-svc-9f86p
Apr  9 06:42:35.242: INFO: Got endpoints: latency-svc-9r786 [746.973434ms]
Apr  9 06:42:35.274: INFO: Created: latency-svc-hzzkr
Apr  9 06:42:35.293: INFO: Got endpoints: latency-svc-5rzm8 [750.78596ms]
Apr  9 06:42:35.325: INFO: Created: latency-svc-whtgw
Apr  9 06:42:35.342: INFO: Got endpoints: latency-svc-xb4hb [749.89106ms]
Apr  9 06:42:35.373: INFO: Created: latency-svc-7k8fj
Apr  9 06:42:35.392: INFO: Got endpoints: latency-svc-qrrxl [750.046439ms]
Apr  9 06:42:35.427: INFO: Created: latency-svc-5bgj4
Apr  9 06:42:35.442: INFO: Got endpoints: latency-svc-767xs [749.629881ms]
Apr  9 06:42:35.475: INFO: Created: latency-svc-nbvnx
Apr  9 06:42:35.492: INFO: Got endpoints: latency-svc-lr25w [750.119865ms]
Apr  9 06:42:35.527: INFO: Created: latency-svc-tjpkk
Apr  9 06:42:35.542: INFO: Got endpoints: latency-svc-vjn6p [750.102273ms]
Apr  9 06:42:35.580: INFO: Created: latency-svc-8dqxb
Apr  9 06:42:35.592: INFO: Got endpoints: latency-svc-kxh7q [750.157922ms]
Apr  9 06:42:35.623: INFO: Created: latency-svc-229bp
Apr  9 06:42:35.643: INFO: Got endpoints: latency-svc-rw67l [751.283847ms]
Apr  9 06:42:35.674: INFO: Created: latency-svc-45rzc
Apr  9 06:42:35.692: INFO: Got endpoints: latency-svc-dhhxd [750.173725ms]
Apr  9 06:42:35.723: INFO: Created: latency-svc-mw7bg
Apr  9 06:42:35.742: INFO: Got endpoints: latency-svc-4f9q7 [750.340664ms]
Apr  9 06:42:35.774: INFO: Created: latency-svc-ghfzd
Apr  9 06:42:35.792: INFO: Got endpoints: latency-svc-rznp2 [750.07781ms]
Apr  9 06:42:35.824: INFO: Created: latency-svc-kv649
Apr  9 06:42:35.842: INFO: Got endpoints: latency-svc-sdxbf [750.098972ms]
Apr  9 06:42:35.874: INFO: Created: latency-svc-l6r76
Apr  9 06:42:35.893: INFO: Got endpoints: latency-svc-gkh2p [750.871574ms]
Apr  9 06:42:35.926: INFO: Created: latency-svc-jl9t5
Apr  9 06:42:35.942: INFO: Got endpoints: latency-svc-9f86p [750.079367ms]
Apr  9 06:42:35.973: INFO: Created: latency-svc-5vv69
Apr  9 06:42:35.992: INFO: Got endpoints: latency-svc-hzzkr [750.473464ms]
Apr  9 06:42:36.024: INFO: Created: latency-svc-h4ksp
Apr  9 06:42:36.042: INFO: Got endpoints: latency-svc-whtgw [749.305695ms]
Apr  9 06:42:36.073: INFO: Created: latency-svc-drb4l
Apr  9 06:42:36.092: INFO: Got endpoints: latency-svc-7k8fj [750.387667ms]
Apr  9 06:42:36.124: INFO: Created: latency-svc-h6rcp
Apr  9 06:42:36.142: INFO: Got endpoints: latency-svc-5bgj4 [749.806099ms]
Apr  9 06:42:36.173: INFO: Created: latency-svc-8ws7k
Apr  9 06:42:36.192: INFO: Got endpoints: latency-svc-nbvnx [750.227343ms]
Apr  9 06:42:36.223: INFO: Created: latency-svc-9d25n
Apr  9 06:42:36.242: INFO: Got endpoints: latency-svc-tjpkk [750.310543ms]
Apr  9 06:42:36.273: INFO: Created: latency-svc-tl4qr
Apr  9 06:42:36.292: INFO: Got endpoints: latency-svc-8dqxb [750.138763ms]
Apr  9 06:42:36.324: INFO: Created: latency-svc-8rj4g
Apr  9 06:42:36.342: INFO: Got endpoints: latency-svc-229bp [749.915204ms]
Apr  9 06:42:36.374: INFO: Created: latency-svc-c49t4
Apr  9 06:42:36.392: INFO: Got endpoints: latency-svc-45rzc [748.616789ms]
Apr  9 06:42:36.424: INFO: Created: latency-svc-xh9mj
Apr  9 06:42:36.442: INFO: Got endpoints: latency-svc-mw7bg [749.962118ms]
Apr  9 06:42:36.477: INFO: Created: latency-svc-mkc8c
Apr  9 06:42:36.492: INFO: Got endpoints: latency-svc-ghfzd [749.950182ms]
Apr  9 06:42:36.523: INFO: Created: latency-svc-q9gc7
Apr  9 06:42:36.542: INFO: Got endpoints: latency-svc-kv649 [750.042188ms]
Apr  9 06:42:36.592: INFO: Got endpoints: latency-svc-l6r76 [749.77729ms]
Apr  9 06:42:36.642: INFO: Got endpoints: latency-svc-jl9t5 [749.236656ms]
Apr  9 06:42:36.692: INFO: Got endpoints: latency-svc-5vv69 [750.259479ms]
Apr  9 06:42:36.742: INFO: Got endpoints: latency-svc-h4ksp [749.680399ms]
Apr  9 06:42:36.792: INFO: Got endpoints: latency-svc-drb4l [750.223609ms]
Apr  9 06:42:36.843: INFO: Got endpoints: latency-svc-h6rcp [750.277699ms]
Apr  9 06:42:36.893: INFO: Got endpoints: latency-svc-8ws7k [750.943593ms]
Apr  9 06:42:36.943: INFO: Got endpoints: latency-svc-9d25n [750.330671ms]
Apr  9 06:42:36.993: INFO: Got endpoints: latency-svc-tl4qr [750.202874ms]
Apr  9 06:42:37.043: INFO: Got endpoints: latency-svc-8rj4g [750.413946ms]
Apr  9 06:42:37.093: INFO: Got endpoints: latency-svc-c49t4 [751.249342ms]
Apr  9 06:42:37.143: INFO: Got endpoints: latency-svc-xh9mj [751.089729ms]
Apr  9 06:42:37.195: INFO: Got endpoints: latency-svc-mkc8c [752.672493ms]
Apr  9 06:42:37.242: INFO: Got endpoints: latency-svc-q9gc7 [749.653656ms]
Apr  9 06:42:37.242: INFO: Latencies: [32.366505ms 32.558952ms 33.715769ms 52.616707ms 59.926318ms 61.371148ms 70.505056ms 73.691068ms 74.158064ms 75.323109ms 75.822111ms 76.681652ms 76.873819ms 77.111872ms 77.386267ms 77.778108ms 78.68333ms 78.697172ms 80.045626ms 80.141428ms 81.039621ms 81.040442ms 82.64304ms 83.363278ms 83.55201ms 86.142616ms 89.393329ms 92.041551ms 92.837411ms 101.048707ms 102.679141ms 106.61529ms 115.104262ms 121.927621ms 162.372734ms 324.404231ms 328.38819ms 333.339992ms 342.313942ms 384.408371ms 425.361216ms 479.225307ms 528.664962ms 566.313859ms 612.56494ms 623.816674ms 643.859022ms 673.280327ms 675.200043ms 690.13618ms 696.776433ms 723.634362ms 728.297283ms 738.710515ms 741.911567ms 745.889963ms 746.445922ms 746.911678ms 746.973434ms 747.203727ms 747.253422ms 747.781238ms 748.255636ms 748.551537ms 748.616789ms 748.708255ms 749.236656ms 749.252952ms 749.305695ms 749.33079ms 749.34079ms 749.348248ms 749.379299ms 749.392664ms 749.392732ms 749.436919ms 749.442324ms 749.450681ms 749.476902ms 749.493366ms 749.515642ms 749.525702ms 749.529524ms 749.533047ms 749.539127ms 749.551873ms 749.587386ms 749.588852ms 749.593772ms 749.611568ms 749.629881ms 749.638273ms 749.653656ms 749.670844ms 749.680399ms 749.685788ms 749.702803ms 749.73251ms 749.74955ms 749.757706ms 749.767983ms 749.769023ms 749.77729ms 749.777349ms 749.793049ms 749.794397ms 749.795389ms 749.806099ms 749.817542ms 749.8247ms 749.856897ms 749.867367ms 749.867721ms 749.867916ms 749.868068ms 749.874881ms 749.89106ms 749.915204ms 749.915726ms 749.916161ms 749.949386ms 749.950182ms 749.956617ms 749.958807ms 749.962118ms 749.970724ms 750.007108ms 750.023498ms 750.036816ms 750.041813ms 750.042188ms 750.046439ms 750.06011ms 750.063655ms 750.07781ms 750.079367ms 750.080636ms 750.081276ms 750.092399ms 750.09835ms 750.098972ms 750.102273ms 750.119865ms 750.133949ms 750.138763ms 750.139322ms 750.140943ms 750.144277ms 750.157922ms 750.173725ms 750.178385ms 750.187027ms 750.202874ms 750.223609ms 750.225148ms 750.227343ms 750.227707ms 750.242312ms 750.259479ms 750.260306ms 750.275751ms 750.277699ms 750.280429ms 750.285534ms 750.310543ms 750.319454ms 750.326461ms 750.330671ms 750.340664ms 750.367013ms 750.384866ms 750.387667ms 750.396777ms 750.413946ms 750.473464ms 750.479318ms 750.511272ms 750.57278ms 750.620041ms 750.671388ms 750.78596ms 750.871574ms 750.943593ms 751.06909ms 751.078172ms 751.089729ms 751.249342ms 751.283847ms 751.394559ms 751.732671ms 751.810212ms 751.850018ms 752.31238ms 752.672493ms 752.809173ms 752.824964ms 753.974341ms 774.127173ms 803.271396ms 824.052575ms]
Apr  9 06:42:37.242: INFO: 50 %ile: 749.767983ms
Apr  9 06:42:37.242: INFO: 90 %ile: 750.78596ms
Apr  9 06:42:37.242: INFO: 99 %ile: 803.271396ms
Apr  9 06:42:37.242: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:42:37.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4529" for this suite.
Apr  9 06:42:47.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:42:48.399: INFO: namespace svc-latency-4529 deletion completed in 11.128512876s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:42:48.400: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr  9 06:42:48.734: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:43:01.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6552" for this suite.
Apr  9 06:43:07.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:43:09.000: INFO: namespace pods-6552 deletion completed in 7.102803554s
•SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:43:09.000: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 06:43:09.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32" in namespace "downward-api-1356" to be "success or failure"
Apr  9 06:43:09.337: INFO: Pod "downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.047241ms
Apr  9 06:43:11.365: INFO: Pod "downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054804567s
STEP: Saw pod success
Apr  9 06:43:11.365: INFO: Pod "downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:43:11.393: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 06:43:11.456: INFO: Waiting for pod downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:43:11.483: INFO: Pod downwardapi-volume-bcebc5f0-5a92-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:43:11.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1356" for this suite.
Apr  9 06:43:17.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:43:18.594: INFO: namespace downward-api-1356 deletion completed in 7.083779646s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:43:18.595: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 06:43:18.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32" in namespace "projected-9843" to be "success or failure"
Apr  9 06:43:18.934: INFO: Pod "downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.019071ms
Apr  9 06:43:20.963: INFO: Pod "downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055336224s
STEP: Saw pod success
Apr  9 06:43:20.963: INFO: Pod "downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:43:20.990: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 06:43:21.054: INFO: Waiting for pod downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:43:21.081: INFO: Pod downwardapi-volume-c2a45ad4-5a92-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:43:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9843" for this suite.
Apr  9 06:43:27.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:43:28.232: INFO: namespace projected-9843 deletion completed in 7.122994289s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:43:28.232: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr  9 06:43:30.689: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:43:30.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3013" for this suite.
Apr  9 06:43:52.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:43:53.885: INFO: namespace replicaset-3013 deletion completed in 23.083541387s
•S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:43:53.885: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1812
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-1812
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1812
STEP: Deleting pre-stop pod
Apr  9 06:44:07.521: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:44:07.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1812" for this suite.
Apr  9 06:44:45.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:44:46.676: INFO: namespace prestop-1812 deletion completed in 39.099186283s
•SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:44:46.677: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr  9 06:44:46.972: INFO: Waiting up to 5m0s for pod "client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32" in namespace "containers-4912" to be "success or failure"
Apr  9 06:44:47.000: INFO: Pod "client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.184493ms
Apr  9 06:44:49.027: INFO: Pod "client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055106639s
Apr  9 06:44:51.055: INFO: Pod "client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083131737s
STEP: Saw pod success
Apr  9 06:44:51.056: INFO: Pod "client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:44:51.083: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 06:44:51.148: INFO: Waiting for pod client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:44:51.177: INFO: Pod client-containers-f721f215-5a92-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:44:51.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4912" for this suite.
Apr  9 06:44:57.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:44:58.403: INFO: namespace containers-4912 deletion completed in 7.197292667s
•SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:44:58.403: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 06:44:58.719: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config version --client'
Apr  9 06:44:58.801: INFO: stderr: ""
Apr  9 06:44:58.801: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.3\", GitCommit:\"435f92c719f279a3a67808c80521ea17d5715c66\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T12:57:14Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr  9 06:44:58.828: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-5529'
Apr  9 06:44:59.516: INFO: stderr: ""
Apr  9 06:44:59.516: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr  9 06:44:59.516: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-5529'
Apr  9 06:44:59.914: INFO: stderr: ""
Apr  9 06:44:59.914: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 06:45:00.942: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:00.942: INFO: Found 0 / 1
Apr  9 06:45:01.944: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:01.944: INFO: Found 0 / 1
Apr  9 06:45:02.942: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:02.942: INFO: Found 1 / 1
Apr  9 06:45:02.942: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 06:45:02.970: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:02.970: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 06:45:02.970: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config describe pod redis-master-bzppp --namespace=kubectl-5529'
Apr  9 06:45:03.281: INFO: stderr: ""
Apr  9 06:45:03.281: INFO: stdout: "Name:               redis-master-bzppp\nNamespace:          kubectl-5529\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-250-30-1.eu-west-1.compute.internal/10.250.30.1\nStart Time:         Tue, 09 Apr 2019 06:44:59 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.14/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.14\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f387b1cb192de41a7bde7a9d0ab6902631afb94e1da8233841865769362c8ace\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 Apr 2019 06:45:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-kgwlh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-kgwlh:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-kgwlh\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                Message\n  ----    ------     ----  ----                                                -------\n  Normal  Scheduled  4s    default-scheduler                                   Successfully assigned kubectl-5529/redis-master-bzppp to ip-10-250-30-1.eu-west-1.compute.internal\n  Normal  Pulling    3s    kubelet, ip-10-250-30-1.eu-west-1.compute.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     1s    kubelet, ip-10-250-30-1.eu-west-1.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    1s    kubelet, ip-10-250-30-1.eu-west-1.compute.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-250-30-1.eu-west-1.compute.internal  Started container redis-master\n"
Apr  9 06:45:03.281: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config describe rc redis-master --namespace=kubectl-5529'
Apr  9 06:45:03.609: INFO: stderr: ""
Apr  9 06:45:03.609: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5529\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-bzppp\n"
Apr  9 06:45:03.610: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config describe service redis-master --namespace=kubectl-5529'
Apr  9 06:45:03.929: INFO: stderr: ""
Apr  9 06:45:03.929: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5529\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.68.17.42\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.14:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr  9 06:45:03.956: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config describe node ip-10-250-14-41.eu-west-1.compute.internal'
Apr  9 06:45:04.317: INFO: stderr: ""
Apr  9 06:45:04.317: INFO: stdout: "Name:               ip-10-250-14-41.eu-west-1.compute.internal\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-14-41.eu-west-1.compute.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.14.41/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 Apr 2019 06:35:19 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 09 Apr 2019 06:45:00 +0000   Tue, 09 Apr 2019 06:35:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 09 Apr 2019 06:45:00 +0000   Tue, 09 Apr 2019 06:35:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 09 Apr 2019 06:45:00 +0000   Tue, 09 Apr 2019 06:35:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 09 Apr 2019 06:45:00 +0000   Tue, 09 Apr 2019 06:35:39 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.14.41\n  InternalDNS:  ip-10-250-14-41.eu-west-1.compute.internal\n  Hostname:     ip-10-250-14-41.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           17897500Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      8167868Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         1920m\n ephemeral-storage:           17410687987\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6871960161\n pods:                        110\nSystem Info:\n Machine ID:                 e9456e9b63b0463fabe3f6763d403af2\n System UUID:                ec2dfcf6-a86b-9bd8-1c03-9e4079f08865\n Boot ID:                    0e1a8d32-e639-47a9-87f0-654a285d00cf\n Kernel Version:             4.19.25-coreos\n OS Image:                   Container Linux by CoreOS 2023.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     100.96.0.0/24\nProviderID:                  aws:///eu-west-1b/i-0a170e400c8a3502d\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube2iam-dkzn8                                              10m (0%)      80m (4%)    16Mi (0%)        128Mi (1%)\n  kube-system                addons-kubernetes-dashboard-665df4b66d-8rnhz                       50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)\n  kube-system                addons-nginx-ingress-controller-f88658d78-sg5bg                    100m (5%)     2 (104%)    100Mi (1%)       800Mi (12%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-t75n5    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-6dc58dcffc-4zzfb                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)\n  kube-system                calico-node-ds2qx                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)\n  kube-system                coredns-7f7f7978c8-96z5b                                           50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)\n  kube-system                coredns-7f7f7978c8-fg8rc                                           50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)\n  kube-system                kube-proxy-m5x28                                                   20m (1%)      0 (0%)      64Mi (0%)        0 (0%)\n  kube-system                metrics-server-845bbc9978-plq7d                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (6%)\n  kube-system                node-exporter-zntwd                                                5m (0%)       15m (0%)    10Mi (0%)        100Mi (1%)\n  kube-system                vpn-shoot-77d7f4479f-6ss5n                                         50m (2%)      100m (5%)   50Mi (0%)        100Mi (1%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         460m (23%)  3085m (160%)\n  memory                      525Mi (8%)  2719Mi (41%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age    From                                                    Message\n  ----    ------                   ----   ----                                                    -------\n  Normal  Starting                 9m45s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Starting kubelet.\n  Normal  NodeHasSufficientMemory  9m45s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Node ip-10-250-14-41.eu-west-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    9m45s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Node ip-10-250-14-41.eu-west-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     9m45s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Node ip-10-250-14-41.eu-west-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  9m45s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Updated Node Allocatable limit across pods\n  Normal  Starting                 9m43s  kube-proxy, ip-10-250-14-41.eu-west-1.compute.internal  Starting kube-proxy.\n  Normal  NodeReady                9m25s  kubelet, ip-10-250-14-41.eu-west-1.compute.internal     Node ip-10-250-14-41.eu-west-1.compute.internal status is now: NodeReady\n"
Apr  9 06:45:04.317: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config describe namespace kubectl-5529'
Apr  9 06:45:04.601: INFO: stderr: ""
Apr  9 06:45:04.601: INFO: stdout: "Name:         kubectl-5529\nLabels:       e2e-framework=kubectl\n              e2e-run=773aa641-5a92-11e9-8c35-36e6c88ddb32\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:45:04.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5529" for this suite.
Apr  9 06:45:26.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:45:27.722: INFO: namespace kubectl-5529 deletion completed in 23.093124667s
•SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:45:27.722: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr  9 06:45:27.986: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-7039'
Apr  9 06:45:28.409: INFO: stderr: ""
Apr  9 06:45:28.409: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr  9 06:45:29.437: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:29.437: INFO: Found 0 / 1
Apr  9 06:45:30.437: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:30.437: INFO: Found 1 / 1
Apr  9 06:45:30.438: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 06:45:30.465: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 06:45:30.466: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr  9 06:45:30.466: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config logs redis-master-djbzk redis-master --namespace=kubectl-7039'
Apr  9 06:45:30.729: INFO: stderr: ""
Apr  9 06:45:30.729: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 06:45:29.242 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 06:45:29.242 # Server started, Redis version 3.2.12\n1:M 09 Apr 06:45:29.242 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 06:45:29.242 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr  9 06:45:30.729: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config log redis-master-djbzk redis-master --namespace=kubectl-7039 --tail=1'
Apr  9 06:45:31.020: INFO: stderr: ""
Apr  9 06:45:31.020: INFO: stdout: "1:M 09 Apr 06:45:29.242 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr  9 06:45:31.020: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config log redis-master-djbzk redis-master --namespace=kubectl-7039 --limit-bytes=1'
Apr  9 06:45:31.297: INFO: stderr: ""
Apr  9 06:45:31.297: INFO: stdout: " "
STEP: exposing timestamps
Apr  9 06:45:31.297: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config log redis-master-djbzk redis-master --namespace=kubectl-7039 --tail=1 --timestamps'
Apr  9 06:45:31.573: INFO: stderr: ""
Apr  9 06:45:31.573: INFO: stdout: "2019-04-09T06:45:29.242509029Z 1:M 09 Apr 06:45:29.242 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr  9 06:45:34.074: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config log redis-master-djbzk redis-master --namespace=kubectl-7039 --since=1s'
Apr  9 06:45:34.337: INFO: stderr: ""
Apr  9 06:45:34.337: INFO: stdout: ""
Apr  9 06:45:34.337: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config log redis-master-djbzk redis-master --namespace=kubectl-7039 --since=24h'
Apr  9 06:45:34.583: INFO: stderr: ""
Apr  9 06:45:34.583: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 06:45:29.242 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 06:45:29.242 # Server started, Redis version 3.2.12\n1:M 09 Apr 06:45:29.242 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 06:45:29.242 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr  9 06:45:34.583: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7039'
Apr  9 06:45:34.812: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 06:45:34.812: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr  9 06:45:34.812: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-7039'
Apr  9 06:45:35.046: INFO: stderr: "No resources found.\n"
Apr  9 06:45:35.046: INFO: stdout: ""
Apr  9 06:45:35.046: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -l name=nginx --namespace=kubectl-7039 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 06:45:35.279: INFO: stderr: ""
Apr  9 06:45:35.279: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:45:35.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7039" for this suite.
Apr  9 06:45:57.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:45:58.396: INFO: namespace kubectl-7039 deletion completed in 23.089023024s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:45:58.397: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr  9 06:45:58.708: INFO: Waiting up to 5m0s for pod "var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32" in namespace "var-expansion-8471" to be "success or failure"
Apr  9 06:45:58.735: INFO: Pod "var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.154169ms
Apr  9 06:46:00.763: INFO: Pod "var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054863332s
STEP: Saw pod success
Apr  9 06:46:00.763: INFO: Pod "var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:46:00.790: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 06:46:00.854: INFO: Waiting for pod var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:46:00.881: INFO: Pod var-expansion-21e3e12c-5a93-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:46:00.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8471" for this suite.
Apr  9 06:46:06.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:46:08.017: INFO: namespace var-expansion-8471 deletion completed in 7.108227877s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:46:08.018: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 06:46:08.487: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8659'
Apr  9 06:46:08.822: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Apr  9 06:46:08.822: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr  9 06:46:08.849: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-8659'
Apr  9 06:46:09.109: INFO: stderr: ""
Apr  9 06:46:09.109: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:46:09.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8659" for this suite.
Apr  9 06:46:15.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:46:16.225: INFO: namespace kubectl-8659 deletion completed in 7.088222991s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:46:16.225: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  9 06:46:16.519: INFO: Waiting up to 5m0s for pod "pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32" in namespace "emptydir-1516" to be "success or failure"
Apr  9 06:46:16.546: INFO: Pod "pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.928075ms
Apr  9 06:46:18.573: INFO: Pod "pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054551069s
Apr  9 06:46:20.601: INFO: Pod "pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082465706s
STEP: Saw pod success
Apr  9 06:46:20.601: INFO: Pod "pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:46:20.629: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 06:46:20.695: INFO: Waiting for pod pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:46:20.722: INFO: Pod pod-2c81c7ce-5a93-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:46:20.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1516" for this suite.
Apr  9 06:46:26.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:46:27.974: INFO: namespace emptydir-1516 deletion completed in 7.224923448s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:46:27.975: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr  9 06:46:28.284: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-6763'
Apr  9 06:46:28.672: INFO: stderr: ""
Apr  9 06:46:28.672: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 06:46:28.673: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:28.909: INFO: stderr: ""
Apr  9 06:46:28.909: INFO: stdout: "update-demo-nautilus-g56gr update-demo-nautilus-rmnqn "
Apr  9 06:46:28.909: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-g56gr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:29.120: INFO: stderr: ""
Apr  9 06:46:29.120: INFO: stdout: ""
Apr  9 06:46:29.120: INFO: update-demo-nautilus-g56gr is created but not running
Apr  9 06:46:34.120: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:34.343: INFO: stderr: ""
Apr  9 06:46:34.343: INFO: stdout: "update-demo-nautilus-g56gr update-demo-nautilus-rmnqn "
Apr  9 06:46:34.343: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-g56gr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:34.601: INFO: stderr: ""
Apr  9 06:46:34.601: INFO: stdout: "true"
Apr  9 06:46:34.601: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-g56gr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:34.808: INFO: stderr: ""
Apr  9 06:46:34.808: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:34.808: INFO: validating pod update-demo-nautilus-g56gr
Apr  9 06:46:34.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:34.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:34.928: INFO: update-demo-nautilus-g56gr is verified up and running
Apr  9 06:46:34.928: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:35.168: INFO: stderr: ""
Apr  9 06:46:35.168: INFO: stdout: "true"
Apr  9 06:46:35.168: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:35.374: INFO: stderr: ""
Apr  9 06:46:35.374: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:35.374: INFO: validating pod update-demo-nautilus-rmnqn
Apr  9 06:46:35.495: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:35.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:35.495: INFO: update-demo-nautilus-rmnqn is verified up and running
STEP: scaling down the replication controller
Apr  9 06:46:35.501: INFO: scanned /root for discovery docs: <nil>
Apr  9 06:46:35.501: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6763'
Apr  9 06:46:35.824: INFO: stderr: ""
Apr  9 06:46:35.824: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 06:46:35.824: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:36.030: INFO: stderr: ""
Apr  9 06:46:36.030: INFO: stdout: "update-demo-nautilus-g56gr update-demo-nautilus-rmnqn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  9 06:46:41.030: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:41.238: INFO: stderr: ""
Apr  9 06:46:41.238: INFO: stdout: "update-demo-nautilus-g56gr update-demo-nautilus-rmnqn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  9 06:46:46.238: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:46.458: INFO: stderr: ""
Apr  9 06:46:46.458: INFO: stdout: "update-demo-nautilus-rmnqn "
Apr  9 06:46:46.458: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:46.673: INFO: stderr: ""
Apr  9 06:46:46.673: INFO: stdout: "true"
Apr  9 06:46:46.673: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:46.878: INFO: stderr: ""
Apr  9 06:46:46.878: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:46.878: INFO: validating pod update-demo-nautilus-rmnqn
Apr  9 06:46:46.909: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:46.909: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:46.909: INFO: update-demo-nautilus-rmnqn is verified up and running
STEP: scaling up the replication controller
Apr  9 06:46:46.914: INFO: scanned /root for discovery docs: <nil>
Apr  9 06:46:46.914: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6763'
Apr  9 06:46:47.219: INFO: stderr: ""
Apr  9 06:46:47.219: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 06:46:47.220: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:47.453: INFO: stderr: ""
Apr  9 06:46:47.453: INFO: stdout: "update-demo-nautilus-rmnqn update-demo-nautilus-ts6gl "
Apr  9 06:46:47.453: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:47.654: INFO: stderr: ""
Apr  9 06:46:47.654: INFO: stdout: "true"
Apr  9 06:46:47.654: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:47.875: INFO: stderr: ""
Apr  9 06:46:47.875: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:47.875: INFO: validating pod update-demo-nautilus-rmnqn
Apr  9 06:46:47.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:47.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:47.905: INFO: update-demo-nautilus-rmnqn is verified up and running
Apr  9 06:46:47.905: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-ts6gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:48.111: INFO: stderr: ""
Apr  9 06:46:48.111: INFO: stdout: ""
Apr  9 06:46:48.111: INFO: update-demo-nautilus-ts6gl is created but not running
Apr  9 06:46:53.111: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6763'
Apr  9 06:46:53.335: INFO: stderr: ""
Apr  9 06:46:53.335: INFO: stdout: "update-demo-nautilus-rmnqn update-demo-nautilus-ts6gl "
Apr  9 06:46:53.335: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:53.553: INFO: stderr: ""
Apr  9 06:46:53.553: INFO: stdout: "true"
Apr  9 06:46:53.553: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-rmnqn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:53.766: INFO: stderr: ""
Apr  9 06:46:53.766: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:53.766: INFO: validating pod update-demo-nautilus-rmnqn
Apr  9 06:46:53.795: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:53.795: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:53.795: INFO: update-demo-nautilus-rmnqn is verified up and running
Apr  9 06:46:53.795: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-ts6gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:54.000: INFO: stderr: ""
Apr  9 06:46:54.000: INFO: stdout: "true"
Apr  9 06:46:54.000: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-ts6gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6763'
Apr  9 06:46:54.209: INFO: stderr: ""
Apr  9 06:46:54.209: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 06:46:54.209: INFO: validating pod update-demo-nautilus-ts6gl
Apr  9 06:46:54.327: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 06:46:54.327: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 06:46:54.327: INFO: update-demo-nautilus-ts6gl is verified up and running
STEP: using delete to clean up resources
Apr  9 06:46:54.327: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6763'
Apr  9 06:46:54.577: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 06:46:54.577: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  9 06:46:54.577: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6763'
Apr  9 06:46:54.815: INFO: stderr: "No resources found.\n"
Apr  9 06:46:54.815: INFO: stdout: ""
Apr  9 06:46:54.815: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -l name=update-demo --namespace=kubectl-6763 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 06:46:55.025: INFO: stderr: ""
Apr  9 06:46:55.025: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:46:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6763" for this suite.
Apr  9 06:47:17.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:47:18.152: INFO: namespace kubectl-6763 deletion completed in 23.098578667s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:47:18.153: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3832
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3832
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3832
Apr  9 06:47:18.587: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr  9 06:47:28.615: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr  9 06:47:28.643: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 06:47:29.487: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 06:47:29.487: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 06:47:29.487: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 06:47:29.515: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  9 06:47:39.543: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 06:47:39.543: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 06:47:39.653: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999606s
Apr  9 06:47:40.681: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.972818659s
Apr  9 06:47:41.709: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.944824667s
Apr  9 06:47:42.737: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.917217159s
Apr  9 06:47:43.765: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.889305349s
Apr  9 06:47:44.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.861475415s
Apr  9 06:47:45.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.833406989s
Apr  9 06:47:46.849: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.805356846s
Apr  9 06:47:47.877: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.777120522s
Apr  9 06:47:48.905: INFO: Verifying statefulset ss doesn't scale past 1 for another 749.065445ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3832
Apr  9 06:47:49.933: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:47:50.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 06:47:50.684: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 06:47:50.684: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 06:47:50.712: INFO: Found 1 stateful pods, waiting for 3
Apr  9 06:48:00.743: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 06:48:00.744: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 06:48:00.744: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr  9 06:48:00.798: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 06:48:01.751: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 06:48:01.751: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 06:48:01.751: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 06:48:01.751: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 06:48:02.580: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 06:48:02.580: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 06:48:02.580: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 06:48:02.580: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 06:48:03.363: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 06:48:03.363: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 06:48:03.363: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 06:48:03.363: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 06:48:03.393: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr  9 06:48:13.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 06:48:13.449: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 06:48:13.449: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 06:48:13.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999483s
Apr  9 06:48:14.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.971891715s
Apr  9 06:48:15.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.944012153s
Apr  9 06:48:16.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.916032649s
Apr  9 06:48:17.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.888198424s
Apr  9 06:48:18.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.860256303s
Apr  9 06:48:19.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.83248279s
Apr  9 06:48:20.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.804431718s
Apr  9 06:48:21.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.776380041s
Apr  9 06:48:22.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 748.801115ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3832
Apr  9 06:48:23.810: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:24.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 06:48:24.579: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 06:48:24.579: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 06:48:24.579: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:25.355: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 06:48:25.355: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 06:48:25.355: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 06:48:25.355: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:26.127: INFO: rc: 1
Apr  9 06:48:26.127: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (62abb898775795138ab3027b3bffcc4ea51309f5cc6e0ec6c650e73372527dd6)
 [] <nil> 0xc000f3c870 exit status 1 <nil> <nil> true [0xc002708c18 0xc002708c40 0xc002708c58] [0xc002708c18 0xc002708c40 0xc002708c58] [0xc002708c38 0xc002708c50] [0x9bf9f0 0x9bf9f0] 0xc002831f20 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (62abb898775795138ab3027b3bffcc4ea51309f5cc6e0ec6c650e73372527dd6)

error:
exit status 1

Apr  9 06:48:36.127: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:36.363: INFO: rc: 1
Apr  9 06:48:36.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00278cc60 exit status 1 <nil> <nil> true [0xc000c7f1b8 0xc000c7f1f8 0xc000c7f238] [0xc000c7f1b8 0xc000c7f1f8 0xc000c7f238] [0xc000c7f1e0 0xc000c7f228] [0x9bf9f0 0x9bf9f0] 0xc0029a8f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:48:46.363: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:46.558: INFO: rc: 1
Apr  9 06:48:46.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f3cae0 exit status 1 <nil> <nil> true [0xc002708c70 0xc002708cc8 0xc002708ce0] [0xc002708c70 0xc002708cc8 0xc002708ce0] [0xc002708cb0 0xc002708cd8] [0x9bf9f0 0x9bf9f0] 0xc000f20240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:48:56.558: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:48:56.771: INFO: rc: 1
Apr  9 06:48:56.771: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f3cd20 exit status 1 <nil> <nil> true [0xc002708ce8 0xc002708d00 0xc002708d18] [0xc002708ce8 0xc002708d00 0xc002708d18] [0xc002708cf8 0xc002708d10] [0x9bf9f0 0x9bf9f0] 0xc000896540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:06.771: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:06.975: INFO: rc: 1
Apr  9 06:49:06.975: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000f3cf30 exit status 1 <nil> <nil> true [0xc002708d28 0xc002708d70 0xc002708db0] [0xc002708d28 0xc002708d70 0xc002708db0] [0xc002708d50 0xc002708da8] [0x9bf9f0 0x9bf9f0] 0xc001787c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:16.975: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:17.219: INFO: rc: 1
Apr  9 06:49:17.219: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c61e0 exit status 1 <nil> <nil> true [0xc0001b4238 0xc0001b4328 0xc0001b4468] [0xc0001b4238 0xc0001b4328 0xc0001b4468] [0xc0001b4310 0xc0001b4420] [0x9bf9f0 0x9bf9f0] 0xc00137e900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:27.220: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:27.446: INFO: rc: 1
Apr  9 06:49:27.446: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c63f0 exit status 1 <nil> <nil> true [0xc0001b44a8 0xc0001b44f0 0xc0001b4598] [0xc0001b44a8 0xc0001b44f0 0xc0001b4598] [0xc0001b44e0 0xc0001b4560] [0x9bf9f0 0x9bf9f0] 0xc000f20240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:37.446: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:37.717: INFO: rc: 1
Apr  9 06:49:37.717: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84120 exit status 1 <nil> <nil> true [0xc002708018 0xc002708060 0xc002708088] [0xc002708018 0xc002708060 0xc002708088] [0xc002708048 0xc002708070] [0x9bf9f0 0x9bf9f0] 0xc001852120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:47.717: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:47.915: INFO: rc: 1
Apr  9 06:49:47.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84330 exit status 1 <nil> <nil> true [0xc0027080a8 0xc0027080d0 0xc002708118] [0xc0027080a8 0xc0027080d0 0xc002708118] [0xc0027080c8 0xc0027080f8] [0x9bf9f0 0x9bf9f0] 0xc001853140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:49:57.916: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:49:58.121: INFO: rc: 1
Apr  9 06:49:58.121: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84540 exit status 1 <nil> <nil> true [0xc002708138 0xc002708160 0xc002708178] [0xc002708138 0xc002708160 0xc002708178] [0xc002708158 0xc002708170] [0x9bf9f0 0x9bf9f0] 0xc001d20120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:08.122: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:08.316: INFO: rc: 1
Apr  9 06:50:08.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b90b0 exit status 1 <nil> <nil> true [0xc002f1a000 0xc002f1a018 0xc002f1a030] [0xc002f1a000 0xc002f1a018 0xc002f1a030] [0xc002f1a010 0xc002f1a028] [0x9bf9f0 0x9bf9f0] 0xc0021d6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:18.317: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:18.548: INFO: rc: 1
Apr  9 06:50:18.548: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b9320 exit status 1 <nil> <nil> true [0xc002f1a038 0xc002f1a050 0xc002f1a068] [0xc002f1a038 0xc002f1a050 0xc002f1a068] [0xc002f1a048 0xc002f1a060] [0x9bf9f0 0x9bf9f0] 0xc0023e4900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:28.548: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:28.758: INFO: rc: 1
Apr  9 06:50:28.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84780 exit status 1 <nil> <nil> true [0xc002708190 0xc0027081c0 0xc002708220] [0xc002708190 0xc0027081c0 0xc002708220] [0xc0027081b8 0xc002708200] [0x9bf9f0 0x9bf9f0] 0xc001d212c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:38.759: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:39.001: INFO: rc: 1
Apr  9 06:50:39.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c67b0 exit status 1 <nil> <nil> true [0xc0001b4658 0xc0001b46e8 0xc0001b4768] [0xc0001b4658 0xc0001b46e8 0xc0001b4768] [0xc0001b46b0 0xc0001b4728] [0x9bf9f0 0x9bf9f0] 0xc0024be2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:49.001: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:49.200: INFO: rc: 1
Apr  9 06:50:49.200: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c69c0 exit status 1 <nil> <nil> true [0xc0001b4798 0xc0001b4ca0 0xc0001b4d00] [0xc0001b4798 0xc0001b4ca0 0xc0001b4d00] [0xc0001b48d0 0xc0001b4ce8] [0x9bf9f0 0x9bf9f0] 0xc0024be600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:50:59.200: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:50:59.477: INFO: rc: 1
Apr  9 06:50:59.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f46240 exit status 1 <nil> <nil> true [0xc0024f2000 0xc0024f2018 0xc0024f2030] [0xc0024f2000 0xc0024f2018 0xc0024f2030] [0xc0024f2010 0xc0024f2028] [0x9bf9f0 0x9bf9f0] 0xc0028302a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:51:09.477: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:51:09.693: INFO: rc: 1
Apr  9 06:51:09.693: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f46480 exit status 1 <nil> <nil> true [0xc0024f2038 0xc0024f2050 0xc0024f2068] [0xc0024f2038 0xc0024f2050 0xc0024f2068] [0xc0024f2048 0xc0024f2060] [0x9bf9f0 0x9bf9f0] 0xc002830720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:51:19.693: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:51:19.912: INFO: rc: 1
Apr  9 06:51:19.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84150 exit status 1 <nil> <nil> true [0xc0001b4170 0xc0001b4310 0xc0001b4420] [0xc0001b4170 0xc0001b4310 0xc0001b4420] [0xc0001b4298 0xc0001b43f0] [0x9bf9f0 0x9bf9f0] 0xc0021d6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:51:29.913: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:51:30.147: INFO: rc: 1
Apr  9 06:51:30.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f461e0 exit status 1 <nil> <nil> true [0xc0024f2000 0xc0024f2018 0xc0024f2030] [0xc0024f2000 0xc0024f2018 0xc0024f2030] [0xc0024f2010 0xc0024f2028] [0x9bf9f0 0x9bf9f0] 0xc001852ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:51:40.148: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:51:40.390: INFO: rc: 1
Apr  9 06:51:40.390: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c6240 exit status 1 <nil> <nil> true [0xc002f1a000 0xc002f1a018 0xc002f1a030] [0xc002f1a000 0xc002f1a018 0xc002f1a030] [0xc002f1a010 0xc002f1a028] [0x9bf9f0 0x9bf9f0] 0xc000f20240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:51:50.390: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:51:50.632: INFO: rc: 1
Apr  9 06:51:50.632: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c6480 exit status 1 <nil> <nil> true [0xc002f1a038 0xc002f1a050 0xc002f1a068] [0xc002f1a038 0xc002f1a050 0xc002f1a068] [0xc002f1a048 0xc002f1a060] [0x9bf9f0 0x9bf9f0] 0xc000896540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:00.632: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:00.822: INFO: rc: 1
Apr  9 06:52:00.823: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c6810 exit status 1 <nil> <nil> true [0xc002f1a070 0xc002f1a088 0xc002f1a0a0] [0xc002f1a070 0xc002f1a088 0xc002f1a0a0] [0xc002f1a080 0xc002f1a098] [0x9bf9f0 0x9bf9f0] 0xc001787c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:10.823: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:11.028: INFO: rc: 1
Apr  9 06:52:11.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b9050 exit status 1 <nil> <nil> true [0xc002708030 0xc002708068 0xc0027080a8] [0xc002708030 0xc002708068 0xc0027080a8] [0xc002708060 0xc002708088] [0x9bf9f0 0x9bf9f0] 0xc0024be300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:21.029: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:21.264: INFO: rc: 1
Apr  9 06:52:21.265: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c6a80 exit status 1 <nil> <nil> true [0xc002f1a0a8 0xc002f1a0c0 0xc002f1a0d8] [0xc002f1a0a8 0xc002f1a0c0 0xc002f1a0d8] [0xc002f1a0b8 0xc002f1a0d0] [0x9bf9f0 0x9bf9f0] 0xc001d21140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:31.266: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:31.457: INFO: rc: 1
Apr  9 06:52:31.457: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b92c0 exit status 1 <nil> <nil> true [0xc0027080c0 0xc0027080d8 0xc002708138] [0xc0027080c0 0xc0027080d8 0xc002708138] [0xc0027080d0 0xc002708118] [0x9bf9f0 0x9bf9f0] 0xc0024be660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:41.457: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:41.671: INFO: rc: 1
Apr  9 06:52:41.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f843c0 exit status 1 <nil> <nil> true [0xc0001b4468 0xc0001b44e0 0xc0001b4560] [0xc0001b4468 0xc0001b44e0 0xc0001b4560] [0xc0001b44c8 0xc0001b4500] [0x9bf9f0 0x9bf9f0] 0xc0028300c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:52:51.671: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:52:51.900: INFO: rc: 1
Apr  9 06:52:51.901: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b9500 exit status 1 <nil> <nil> true [0xc002708150 0xc002708168 0xc002708190] [0xc002708150 0xc002708168 0xc002708190] [0xc002708160 0xc002708178] [0x9bf9f0 0x9bf9f0] 0xc0024be960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:53:01.901: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:53:02.107: INFO: rc: 1
Apr  9 06:53:02.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84630 exit status 1 <nil> <nil> true [0xc0001b4598 0xc0001b46b0 0xc0001b4728] [0xc0001b4598 0xc0001b46b0 0xc0001b4728] [0xc0001b4678 0xc0001b4710] [0x9bf9f0 0x9bf9f0] 0xc002830540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:53:12.108: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:53:12.331: INFO: rc: 1
Apr  9 06:53:12.331: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023b9740 exit status 1 <nil> <nil> true [0xc0027081a8 0xc0027081e0 0xc002708238] [0xc0027081a8 0xc0027081e0 0xc002708238] [0xc0027081c0 0xc002708220] [0x9bf9f0 0x9bf9f0] 0xc0024bed20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:53:22.332: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:53:22.557: INFO: rc: 1
Apr  9 06:53:22.557: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl [kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001f84120 exit status 1 <nil> <nil> true [0xc0001b4238 0xc0001b4328 0xc0001b4468] [0xc0001b4238 0xc0001b4328 0xc0001b4468] [0xc0001b4310 0xc0001b4420] [0x9bf9f0 0x9bf9f0] 0xc00137e900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr  9 06:53:32.558: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3832 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 06:53:32.755: INFO: rc: 1
Apr  9 06:53:32.756: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Apr  9 06:53:32.756: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 06:53:32.839: INFO: Deleting all statefulset in ns statefulset-3832
Apr  9 06:53:32.866: INFO: Scaling statefulset ss to 0
Apr  9 06:53:32.947: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 06:53:32.974: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:53:33.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3832" for this suite.
Apr  9 06:53:39.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:53:40.179: INFO: namespace statefulset-3832 deletion completed in 7.095508763s

• [SLOW TEST:382.027 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:53:40.180: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-616
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-616 to expose endpoints map[]
Apr  9 06:53:40.536: INFO: successfully validated that service endpoint-test2 in namespace services-616 exposes endpoints map[] (27.014653ms elapsed)
STEP: Creating pod pod1 in namespace services-616
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-616 to expose endpoints map[pod1:[80]]
Apr  9 06:53:41.675: INFO: successfully validated that service endpoint-test2 in namespace services-616 exposes endpoints map[pod1:[80]] (1.108898545s elapsed)
STEP: Creating pod pod2 in namespace services-616
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-616 to expose endpoints map[pod1:[80] pod2:[80]]
Apr  9 06:53:42.867: INFO: successfully validated that service endpoint-test2 in namespace services-616 exposes endpoints map[pod1:[80] pod2:[80]] (1.163309091s elapsed)
STEP: Deleting pod pod1 in namespace services-616
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-616 to expose endpoints map[pod2:[80]]
Apr  9 06:53:42.949: INFO: successfully validated that service endpoint-test2 in namespace services-616 exposes endpoints map[pod2:[80]] (54.311504ms elapsed)
STEP: Deleting pod pod2 in namespace services-616
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-616 to expose endpoints map[]
Apr  9 06:53:43.005: INFO: successfully validated that service endpoint-test2 in namespace services-616 exposes endpoints map[] (27.144293ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:53:43.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-616" for this suite.
Apr  9 06:53:55.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:53:56.150: INFO: namespace services-616 deletion completed in 13.085647564s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:53:56.150: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr  9 06:53:56.532: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:53:56.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1985" for this suite.
Apr  9 06:54:02.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:54:03.729: INFO: namespace replication-controller-1985 deletion completed in 7.086992599s
•S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:54:03.729: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8735
Apr  9 06:54:08.194: INFO: Started pod liveness-http in namespace container-probe-8735
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 06:54:08.221: INFO: Initial restart count of pod liveness-http is 0
Apr  9 06:54:20.416: INFO: Restart count of pod container-probe-8735/liveness-http is now 1 (12.194795965s elapsed)
Apr  9 06:54:40.694: INFO: Restart count of pod container-probe-8735/liveness-http is now 2 (32.47310679s elapsed)
Apr  9 06:55:00.984: INFO: Restart count of pod container-probe-8735/liveness-http is now 3 (52.763143479s elapsed)
Apr  9 06:55:21.299: INFO: Restart count of pod container-probe-8735/liveness-http is now 4 (1m13.077386226s elapsed)
Apr  9 06:56:26.213: INFO: Restart count of pod container-probe-8735/liveness-http is now 5 (2m17.991732405s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:56:26.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8735" for this suite.
Apr  9 06:56:32.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:56:33.360: INFO: namespace container-probe-8735 deletion completed in 7.088857931s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:56:33.360: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-9c61515c-5a94-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 06:56:33.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32" in namespace "configmap-3942" to be "success or failure"
Apr  9 06:56:33.763: INFO: Pod "pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.875235ms
Apr  9 06:56:35.791: INFO: Pod "pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054942226s
STEP: Saw pod success
Apr  9 06:56:35.791: INFO: Pod "pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:56:35.818: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 06:56:35.885: INFO: Waiting for pod pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:56:35.912: INFO: Pod pod-configmaps-9c657d0f-5a94-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:56:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3942" for this suite.
Apr  9 06:56:42.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:56:43.040: INFO: namespace configmap-3942 deletion completed in 7.100599385s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:56:43.041: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 06:56:47.548: INFO: Waiting up to 5m0s for pod "client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32" in namespace "pods-7939" to be "success or failure"
Apr  9 06:56:47.575: INFO: Pod "client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.130043ms
Apr  9 06:56:49.603: INFO: Pod "client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05507766s
STEP: Saw pod success
Apr  9 06:56:49.603: INFO: Pod "client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:56:49.630: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32 container env3cont: <nil>
STEP: delete the pod
Apr  9 06:56:49.696: INFO: Waiting for pod client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:56:49.723: INFO: Pod client-envvars-a4a1463b-5a94-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:56:49.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7939" for this suite.
Apr  9 06:57:27.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:57:28.994: INFO: namespace pods-7939 deletion completed in 39.243620706s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:57:28.994: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  9 06:57:29.315: INFO: Waiting up to 5m0s for pod "pod-bd863374-5a94-11e9-8c35-36e6c88ddb32" in namespace "emptydir-6224" to be "success or failure"
Apr  9 06:57:29.342: INFO: Pod "pod-bd863374-5a94-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.146174ms
Apr  9 06:57:31.370: INFO: Pod "pod-bd863374-5a94-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054997514s
STEP: Saw pod success
Apr  9 06:57:31.370: INFO: Pod "pod-bd863374-5a94-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:57:31.398: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-bd863374-5a94-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 06:57:31.463: INFO: Waiting for pod pod-bd863374-5a94-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:57:31.490: INFO: Pod pod-bd863374-5a94-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:57:31.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6224" for this suite.
Apr  9 06:57:37.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:57:38.612: INFO: namespace emptydir-6224 deletion completed in 7.09493479s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:57:38.613: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:58:38.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5257" for this suite.
Apr  9 06:58:55.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:58:56.053: INFO: namespace container-probe-5257 deletion completed in 17.084461673s
•S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:58:56.054: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8188
STEP: Creating secret with name secret-test-f16ff3c9-5a94-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 06:58:56.735: INFO: Waiting up to 5m0s for pod "pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32" in namespace "secrets-7446" to be "success or failure"
Apr  9 06:58:56.762: INFO: Pod "pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.011111ms
Apr  9 06:58:58.790: INFO: Pod "pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054889666s
STEP: Saw pod success
Apr  9 06:58:58.790: INFO: Pod "pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:58:58.818: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 06:58:58.882: INFO: Waiting for pod pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:58:58.909: INFO: Pod pod-secrets-f1a17fe8-5a94-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:58:58.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7446" for this suite.
Apr  9 06:59:05.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:59:06.029: INFO: namespace secrets-7446 deletion completed in 7.092075942s
STEP: Destroying namespace "secret-namespace-8188" for this suite.
Apr  9 06:59:12.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:59:13.119: INFO: namespace secret-namespace-8188 deletion completed in 7.090151782s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:59:13.119: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 06:59:13.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32" in namespace "downward-api-8082" to be "success or failure"
Apr  9 06:59:13.442: INFO: Pod "downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.073983ms
Apr  9 06:59:15.470: INFO: Pod "downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054528398s
STEP: Saw pod success
Apr  9 06:59:15.470: INFO: Pod "downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 06:59:15.497: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 06:59:15.561: INFO: Waiting for pod downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 06:59:15.588: INFO: Pod downwardapi-volume-fb92a65a-5a94-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:59:15.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8082" for this suite.
Apr  9 06:59:21.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:59:22.705: INFO: namespace downward-api-8082 deletion completed in 7.089458845s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:59:22.705: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-668
Apr  9 06:59:25.064: INFO: Started pod liveness-http in namespace container-probe-668
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 06:59:25.092: INFO: Initial restart count of pod liveness-http is 0
Apr  9 06:59:45.400: INFO: Restart count of pod container-probe-668/liveness-http is now 1 (20.308178716s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 06:59:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-668" for this suite.
Apr  9 06:59:51.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 06:59:52.554: INFO: namespace container-probe-668 deletion completed in 7.096939986s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 06:59:52.555: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0409 07:00:23.100231    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 07:00:23.100: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:00:23.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2896" for this suite.
Apr  9 07:00:29.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:00:30.239: INFO: namespace gc-2896 deletion completed in 7.112288323s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:00:30.240: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2995c7ff-5a95-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:00:30.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32" in namespace "projected-3150" to be "success or failure"
Apr  9 07:00:30.666: INFO: Pod "pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.252972ms
Apr  9 07:00:32.695: INFO: Pod "pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056028087s
STEP: Saw pod success
Apr  9 07:00:32.695: INFO: Pod "pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:00:32.722: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:00:32.788: INFO: Waiting for pod pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:00:32.815: INFO: Pod pod-projected-configmaps-2999ed9d-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:00:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3150" for this suite.
Apr  9 07:00:38.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:00:39.938: INFO: namespace projected-3150 deletion completed in 7.090927152s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:00:39.939: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 07:00:42.987: INFO: Successfully updated pod "labelsupdate2f5e1f7d-5a95-11e9-8c35-36e6c88ddb32"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:00:47.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9501" for this suite.
Apr  9 07:01:09.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:01:10.212: INFO: namespace projected-9501 deletion completed in 23.094467337s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:01:10.212: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  9 07:01:10.513: INFO: Waiting up to 5m0s for pod "pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32" in namespace "emptydir-1649" to be "success or failure"
Apr  9 07:01:10.540: INFO: Pod "pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.688228ms
Apr  9 07:01:12.568: INFO: Pod "pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055737304s
STEP: Saw pod success
Apr  9 07:01:12.568: INFO: Pod "pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:01:12.596: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:01:12.661: INFO: Waiting for pod pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:01:12.688: INFO: Pod pod-415e0c55-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:01:12.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1649" for this suite.
Apr  9 07:01:18.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:01:19.803: INFO: namespace emptydir-1649 deletion completed in 7.086569584s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:01:19.803: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 07:01:36.905: INFO: DNS probes using dns-9573/dns-test-471615c8-5a95-11e9-8c35-36e6c88ddb32 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:01:36.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9573" for this suite.
Apr  9 07:01:43.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:01:44.098: INFO: namespace dns-9573 deletion completed in 7.133705655s
•S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:01:44.098: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:01:44.378: INFO: Creating deployment "nginx-deployment"
Apr  9 07:01:44.406: INFO: Waiting for observed generation 1
Apr  9 07:01:44.433: INFO: Waiting for all required pods to come up
Apr  9 07:01:44.461: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr  9 07:01:48.525: INFO: Waiting for deployment "nginx-deployment" to complete
Apr  9 07:01:48.579: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr  9 07:01:48.635: INFO: Updating deployment nginx-deployment
Apr  9 07:01:48.635: INFO: Waiting for observed generation 2
Apr  9 07:01:48.662: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr  9 07:01:50.717: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr  9 07:01:50.744: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr  9 07:01:50.826: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr  9 07:01:50.826: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr  9 07:01:50.854: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr  9 07:01:50.910: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr  9 07:01:50.910: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr  9 07:01:50.966: INFO: Updating deployment nginx-deployment
Apr  9 07:01:50.966: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr  9 07:01:51.021: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr  9 07:01:51.048: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 07:01:51.103: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1336,SelfLink:/apis/apps/v1/namespaces/deployment-1336/deployments/nginx-deployment,UID:5593be72-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6416,Generation:3,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-04-09 07:01:50 +0000 UTC 2019-04-09 07:01:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-09 07:01:50 +0000 UTC 2019-04-09 07:01:44 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr  9 07:01:51.131: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1336,SelfLink:/apis/apps/v1/namespaces/deployment-1336/replicasets/nginx-deployment-5f9595f595,UID:581969b0-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6413,Generation:3,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5593be72-5a95-11e9-afbf-1e9604a4a829 0xc00285cb47 0xc00285cb48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 07:01:51.131: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr  9 07:01:51.131: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1336,SelfLink:/apis/apps/v1/namespaces/deployment-1336/replicasets/nginx-deployment-6f478d8d8,UID:559431fe-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6412,Generation:3,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5593be72-5a95-11e9-afbf-1e9604a4a829 0xc00285cc17 0xc00285cc18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr  9 07:01:51.161: INFO: Pod "nginx-deployment-5f9595f595-656mz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-656mz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-656mz,UID:598122ad-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6411,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024ca540 0xc0024ca541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ca5b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ca5d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.161: INFO: Pod "nginx-deployment-5f9595f595-84g9x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-84g9x,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-84g9x,UID:5819b67c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6359,Generation:0,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024ca660 0xc0024ca661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ca6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ca6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.161: INFO: Pod "nginx-deployment-5f9595f595-9knlz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9knlz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-9knlz,UID:597f1637-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6422,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024ca7f0 0xc0024ca7f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ca860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ca880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-c2gm8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-c2gm8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-c2gm8,UID:5822015b-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6361,Generation:0,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.49/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024ca960 0xc0024ca961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ca9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ca9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-fv78h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-fv78h,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-fv78h,UID:597e5db1-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6417,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024caad0 0xc0024caad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cab40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cab60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-gtbb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gtbb5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-gtbb5,UID:597f3dd4-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6430,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cac30 0xc0024cac31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024caca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cacc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-jbnc5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jbnc5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-jbnc5,UID:597de5c9-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6403,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cad90 0xc0024cad91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cae00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cae20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-jtmwg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jtmwg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-jtmwg,UID:5822826c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6360,Generation:0,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.48/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024caf00 0xc0024caf01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024caf70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024caf90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-k6qtb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k6qtb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-k6qtb,UID:581a1b2e-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6363,Generation:0,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.50/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cb070 0xc0024cb071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb0e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.162: INFO: Pod "nginx-deployment-5f9595f595-rkk2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rkk2r,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-rkk2r,UID:597f359a-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6428,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cb1d0 0xc0024cb1d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-5f9595f595-x49f8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-x49f8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-x49f8,UID:597f25f6-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6426,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cb330 0xc0024cb331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb3d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-5f9595f595-xsmjl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xsmjl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-xsmjl,UID:581a16bb-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6362,Generation:0,CreationTimestamp:2019-04-09 07:01:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cb4b0 0xc0024cb4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:100.96.0.17,StartTime:2019-04-09 07:01:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-5f9595f595-z5xpq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-z5xpq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-5f9595f595-z5xpq,UID:597e6350-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6423,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 581969b0-5a95-11e9-afbf-1e9604a4a829 0xc0024cb630 0xc0024cb631}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb6a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb6c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-6f478d8d8-2bpcm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2bpcm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-2bpcm,UID:55951ff0-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6295,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cb7a0 0xc0024cb7a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:100.96.0.13,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://34fb2682b229a4b02c1c5c68c22922ed56efa663fd76fe0ebdd2f8291b477852}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-6f478d8d8-5wkdm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5wkdm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-5wkdm,UID:5981174c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6407,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cb8f0 0xc0024cb8f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cb950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cb970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.163: INFO: Pod "nginx-deployment-6f478d8d8-847xs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-847xs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-847xs,UID:55952594-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6306,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.45/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cba00 0xc0024cba01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cba60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cba80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.45,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://e62c7d930ebeee14b7417fe69698887f746fbb2e6574d2f1bba029f0ee5de6d2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-8czxh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8czxh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-8czxh,UID:5595c10c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6292,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cbb60 0xc0024cbb61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cbbc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cbbe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:100.96.0.15,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://3061e5f51ca69a7f6923a760a527c5015d8bfea529da93afa983e226105d1361}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-8p44m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8p44m,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-8p44m,UID:5595c1a7-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6300,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.46/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cbcc0 0xc0024cbcc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cbd20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cbd40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.46,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://229e403949182f0edff227cd117e37570de6057b618cd45a5cfde01475c8eae8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-8zsl2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8zsl2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-8zsl2,UID:5981240c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6429,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cbe10 0xc0024cbe11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cbe70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cbe90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-bfxwx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bfxwx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-bfxwx,UID:597ef729-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6427,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc0024cbf50 0xc0024cbf51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024cbfb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024cbfd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-d6wft" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d6wft,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-d6wft,UID:597e3b32-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6419,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a340e0 0xc002a340e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a341a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-dbvjf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dbvjf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-dbvjf,UID:5595c3fa-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6315,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a34430 0xc002a34431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a34580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.43,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://a2cd3c768e50241ca74001bd61c00aa5c45f81d452ab7a93226611379f315172}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.164: INFO: Pod "nginx-deployment-6f478d8d8-f82bv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-f82bv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-f82bv,UID:597efd1e-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6424,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a34670 0xc002a34671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a346d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a346f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-ftdqr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ftdqr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-ftdqr,UID:597ee58c-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6420,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a34820 0xc002a34821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a348a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-jzk8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jzk8n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-jzk8n,UID:597d729b-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6415,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a349b0 0xc002a349b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a34a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-km89n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-km89n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-km89n,UID:597e2769-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6418,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a34bf0 0xc002a34bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a34cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-kqdnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kqdnl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-kqdnl,UID:5981298f-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6410,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a34e60 0xc002a34e61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a34ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a34f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-pvr7q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pvr7q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-pvr7q,UID:55966a1f-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6289,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.16/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a35040 0xc002a35041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a350f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a35110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:100.96.0.16,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://d14cf1a0f2727cf9ed0f5e54993543d35dafd3aed04431b6dbae0df5912834dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-q5mx5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q5mx5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-q5mx5,UID:5980f501-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6408,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a35280 0xc002a35281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a35340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a35360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-rcm9s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rcm9s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-rcm9s,UID:55966a67-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6303,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a35440 0xc002a35441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a354f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a35510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.44,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://abfc1758127e66c099f0fd47b3bb049e6a7b53e3d0a820e70bc0bf9e976f57ec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.165: INFO: Pod "nginx-deployment-6f478d8d8-s8kmm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s8kmm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-s8kmm,UID:59812038-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6431,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a356a0 0xc002a356a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a35760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a35780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.166: INFO: Pod "nginx-deployment-6f478d8d8-zb8pq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zb8pq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-zb8pq,UID:55965ac9-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6286,Generation:0,CreationTimestamp:2019-04-09 07:01:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a358d0 0xc002a358d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a35980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a359b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:100.96.0.14,StartTime:2019-04-09 07:01:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 07:01:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://a4b100c81d42835694269ceddac0470055bdc662c121de0c2c5091d2b088bbf9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 07:01:51.166: INFO: Pod "nginx-deployment-6f478d8d8-zvt5h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zvt5h,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1336,SelfLink:/api/v1/namespaces/deployment-1336/pods/nginx-deployment-6f478d8d8-zvt5h,UID:597ef29a-5a95-11e9-afbf-1e9604a4a829,ResourceVersion:6425,Generation:0,CreationTimestamp:2019-04-09 07:01:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 559431fe-5a95-11e9-afbf-1e9604a4a829 0xc002a35af0 0xc002a35af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx74 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx74,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vfx74 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-14-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a35b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a35bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:01:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.14.41,PodIP:,StartTime:2019-04-09 07:01:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:01:51.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1336" for this suite.
Apr  9 07:01:59.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:02:00.291: INFO: namespace deployment-1336 deletion completed in 9.097909436s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:02:00.291: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 07:02:05.325: INFO: Successfully updated pod "annotationupdate5f3babbc-5a95-11e9-8c35-36e6c88ddb32"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:02:07.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1119" for this suite.
Apr  9 07:02:29.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:02:30.523: INFO: namespace downward-api-1119 deletion completed in 23.103016057s
•SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:02:30.523: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:02:34.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9865" for this suite.
Apr  9 07:02:40.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:02:41.998: INFO: namespace kubelet-test-9865 deletion completed in 7.087934174s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:02:41.999: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-bhkv
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 07:02:42.376: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bhkv" in namespace "subpath-9611" to be "success or failure"
Apr  9 07:02:42.403: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Pending", Reason="", readiness=false. Elapsed: 27.178694ms
Apr  9 07:02:44.431: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 2.055522202s
Apr  9 07:02:46.459: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.083505169s
Apr  9 07:02:48.487: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 6.111374947s
Apr  9 07:02:50.515: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 8.139443635s
Apr  9 07:02:52.543: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 10.167566117s
Apr  9 07:02:54.572: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 12.195803661s
Apr  9 07:02:56.600: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 14.223715557s
Apr  9 07:02:58.628: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.251868504s
Apr  9 07:03:00.656: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 18.279934042s
Apr  9 07:03:02.684: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Running", Reason="", readiness=true. Elapsed: 20.308339427s
Apr  9 07:03:04.713: INFO: Pod "pod-subpath-test-configmap-bhkv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.336806679s
STEP: Saw pod success
Apr  9 07:03:04.713: INFO: Pod "pod-subpath-test-configmap-bhkv" satisfied condition "success or failure"
Apr  9 07:03:04.740: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-subpath-test-configmap-bhkv container test-container-subpath-configmap-bhkv: <nil>
STEP: delete the pod
Apr  9 07:03:04.805: INFO: Waiting for pod pod-subpath-test-configmap-bhkv to disappear
Apr  9 07:03:04.831: INFO: Pod pod-subpath-test-configmap-bhkv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bhkv
Apr  9 07:03:04.832: INFO: Deleting pod "pod-subpath-test-configmap-bhkv" in namespace "subpath-9611"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:04.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9611" for this suite.
Apr  9 07:03:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:03:11.971: INFO: namespace subpath-9611 deletion completed in 7.083901818s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:03:11.971: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9151
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr  9 07:03:12.313: INFO: Waiting up to 5m0s for pod "pod-89f78168-5a95-11e9-8c35-36e6c88ddb32" in namespace "emptydir-9151" to be "success or failure"
Apr  9 07:03:12.340: INFO: Pod "pod-89f78168-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.005188ms
Apr  9 07:03:14.368: INFO: Pod "pod-89f78168-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055430495s
STEP: Saw pod success
Apr  9 07:03:14.368: INFO: Pod "pod-89f78168-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:03:14.396: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-89f78168-5a95-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:03:14.460: INFO: Waiting for pod pod-89f78168-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:03:14.487: INFO: Pod pod-89f78168-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:14.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9151" for this suite.
Apr  9 07:03:20.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:03:21.596: INFO: namespace emptydir-9151 deletion completed in 7.081217033s
•
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:03:21.596: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 07:03:21.933: INFO: Waiting up to 5m0s for pod "downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32" in namespace "downward-api-1725" to be "success or failure"
Apr  9 07:03:21.960: INFO: Pod "downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.896822ms
Apr  9 07:03:23.988: INFO: Pod "downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055211441s
STEP: Saw pod success
Apr  9 07:03:23.988: INFO: Pod "downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:03:24.015: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:03:24.101: INFO: Waiting for pod downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:03:24.129: INFO: Pod downward-api-8fb357bf-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1725" for this suite.
Apr  9 07:03:30.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:03:31.243: INFO: namespace downward-api-1725 deletion completed in 7.086545543s
•SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:03:31.243: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:03:31.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32" in namespace "projected-2319" to be "success or failure"
Apr  9 07:03:31.635: INFO: Pod "downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.177658ms
Apr  9 07:03:33.663: INFO: Pod "downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055117311s
STEP: Saw pod success
Apr  9 07:03:33.663: INFO: Pod "downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:03:33.690: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:03:33.754: INFO: Waiting for pod downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:03:33.781: INFO: Pod downwardapi-volume-9577a2d9-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:33.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2319" for this suite.
Apr  9 07:03:39.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:03:40.895: INFO: namespace projected-2319 deletion completed in 7.086165705s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:03:40.895: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr  9 07:03:41.209: INFO: Waiting up to 5m0s for pod "client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32" in namespace "containers-3644" to be "success or failure"
Apr  9 07:03:41.236: INFO: Pod "client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.211602ms
Apr  9 07:03:43.264: INFO: Pod "client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055215034s
STEP: Saw pod success
Apr  9 07:03:43.264: INFO: Pod "client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:03:43.297: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:03:43.362: INFO: Waiting for pod client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:03:43.389: INFO: Pod client-containers-9b30bb27-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:43.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3644" for this suite.
Apr  9 07:03:49.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:03:50.511: INFO: namespace containers-3644 deletion completed in 7.094135843s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:03:50.511: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  9 07:03:55.479: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a0e9ded4-5a95-11e9-8c35-36e6c88ddb32"
Apr  9 07:03:55.479: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a0e9ded4-5a95-11e9-8c35-36e6c88ddb32" in namespace "pods-8546" to be "terminated due to deadline exceeded"
Apr  9 07:03:55.506: INFO: Pod "pod-update-activedeadlineseconds-a0e9ded4-5a95-11e9-8c35-36e6c88ddb32": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 27.049986ms
Apr  9 07:03:55.506: INFO: Pod "pod-update-activedeadlineseconds-a0e9ded4-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:03:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8546" for this suite.
Apr  9 07:04:01.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:04:02.625: INFO: namespace pods-8546 deletion completed in 7.09152769s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:04:02.625: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr  9 07:04:02.891: INFO: namespace kubectl-1224
Apr  9 07:04:02.891: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-1224'
Apr  9 07:04:05.135: INFO: stderr: ""
Apr  9 07:04:05.135: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 07:04:06.163: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 07:04:06.163: INFO: Found 1 / 1
Apr  9 07:04:06.163: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 07:04:06.190: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 07:04:06.190: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 07:04:06.190: INFO: wait on redis-master startup in kubectl-1224 
Apr  9 07:04:06.190: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config logs redis-master-ph7wb redis-master --namespace=kubectl-1224'
Apr  9 07:04:06.454: INFO: stderr: ""
Apr  9 07:04:06.454: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 07:04:05.943 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 07:04:05.943 # Server started, Redis version 3.2.12\n1:M 09 Apr 07:04:05.943 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 07:04:05.943 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr  9 07:04:06.454: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1224'
Apr  9 07:04:06.709: INFO: stderr: ""
Apr  9 07:04:06.709: INFO: stdout: "service/rm2 exposed\n"
Apr  9 07:04:06.737: INFO: Service rm2 in namespace kubectl-1224 found.
STEP: exposing service
Apr  9 07:04:08.791: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1224'
Apr  9 07:04:09.090: INFO: stderr: ""
Apr  9 07:04:09.090: INFO: stdout: "service/rm3 exposed\n"
Apr  9 07:04:09.118: INFO: Service rm3 in namespace kubectl-1224 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:04:11.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1224" for this suite.
Apr  9 07:04:33.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:04:34.284: INFO: namespace kubectl-1224 deletion completed in 23.082168909s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:04:34.284: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 07:04:34.580: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:04:38.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9937" for this suite.
Apr  9 07:05:00.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:05:01.525: INFO: namespace init-container-9937 deletion completed in 23.092628343s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:05:01.525: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr  9 07:05:03.958: INFO: Pod pod-hostip-cb411017-5a95-11e9-8c35-36e6c88ddb32 has hostIP: 10.250.30.1
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:05:03.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7069" for this suite.
Apr  9 07:05:26.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:05:27.076: INFO: namespace pods-7069 deletion completed in 23.0892626s
•SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:05:27.076: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-da7d5962-5a95-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:05:27.435: INFO: Waiting up to 5m0s for pod "pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32" in namespace "configmap-2977" to be "success or failure"
Apr  9 07:05:27.462: INFO: Pod "pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.134034ms
Apr  9 07:05:29.490: INFO: Pod "pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05528571s
STEP: Saw pod success
Apr  9 07:05:29.490: INFO: Pod "pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:05:29.518: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:05:29.582: INFO: Waiting for pod pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:05:29.609: INFO: Pod pod-configmaps-da8186d3-5a95-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:05:29.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2977" for this suite.
Apr  9 07:05:35.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:05:36.744: INFO: namespace configmap-2977 deletion completed in 7.107360954s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:05:36.744: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9715
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3827
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:05:43.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-460" for this suite.
Apr  9 07:05:49.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:05:50.896: INFO: namespace namespaces-460 deletion completed in 7.090516639s
STEP: Destroying namespace "nsdeletetest-9715" for this suite.
Apr  9 07:05:50.924: INFO: Namespace nsdeletetest-9715 was already deleted
STEP: Destroying namespace "nsdeletetest-3827" for this suite.
Apr  9 07:05:57.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:05:58.008: INFO: namespace nsdeletetest-3827 deletion completed in 7.083575996s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:05:58.008: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0409 07:06:04.475088    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 07:06:04.475: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:06:04.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6653" for this suite.
Apr  9 07:06:10.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:06:11.590: INFO: namespace gc-6653 deletion completed in 7.087874234s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:06:11.591: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:06:34.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2335" for this suite.
Apr  9 07:06:40.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:06:41.264: INFO: namespace container-runtime-2335 deletion completed in 7.096488324s
•SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:06:41.265: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-06c6a13b-5a96-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:06:41.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32" in namespace "projected-4920" to be "success or failure"
Apr  9 07:06:41.761: INFO: Pod "pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.90775ms
Apr  9 07:06:43.789: INFO: Pod "pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054884606s
STEP: Saw pod success
Apr  9 07:06:43.790: INFO: Pod "pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:06:43.817: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:06:43.881: INFO: Waiting for pod pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:06:43.908: INFO: Pod pod-projected-configmaps-06cacb12-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:06:43.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4920" for this suite.
Apr  9 07:06:50.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:06:51.021: INFO: namespace projected-4920 deletion completed in 7.084771342s
•SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:06:51.021: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 07:06:51.287: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3275'
Apr  9 07:06:51.548: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Apr  9 07:06:51.548: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Apr  9 07:06:51.611: INFO: scanned /root for discovery docs: <nil>
Apr  9 07:06:51.611: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3275'
Apr  9 07:07:01.940: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr  9 07:07:01.940: INFO: stdout: "Created e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1\nScaling up e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr  9 07:07:01.940: INFO: stdout: "Created e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1\nScaling up e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr  9 07:07:01.940: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3275'
Apr  9 07:07:02.165: INFO: stderr: ""
Apr  9 07:07:02.165: INFO: stdout: "e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1-6hrtq "
Apr  9 07:07:02.165: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1-6hrtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3275'
Apr  9 07:07:02.369: INFO: stderr: ""
Apr  9 07:07:02.370: INFO: stdout: "true"
Apr  9 07:07:02.370: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1-6hrtq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3275'
Apr  9 07:07:02.576: INFO: stderr: ""
Apr  9 07:07:02.576: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr  9 07:07:02.576: INFO: e2e-test-nginx-rc-3c1b6d2c77b1804a7aa14cdfd4dbeff1-6hrtq is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr  9 07:07:02.576: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-3275'
Apr  9 07:07:02.852: INFO: stderr: ""
Apr  9 07:07:02.852: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:07:02.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3275" for this suite.
Apr  9 07:07:08.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:07:09.974: INFO: namespace kubectl-3275 deletion completed in 7.094916437s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:07:09.975: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-42
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-42
Apr  9 07:07:12.367: INFO: Started pod liveness-exec in namespace container-probe-42
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 07:07:12.395: INFO: Initial restart count of pod liveness-exec is 0
Apr  9 07:08:01.098: INFO: Restart count of pod container-probe-42/liveness-exec is now 1 (48.703274079s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:01.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-42" for this suite.
Apr  9 07:08:07.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:08:08.253: INFO: namespace container-probe-42 deletion completed in 7.094327621s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:08:08.253: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-2527/secret-test-3a92ac4a-5a96-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:08:08.637: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32" in namespace "secrets-2527" to be "success or failure"
Apr  9 07:08:08.664: INFO: Pod "pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.32995ms
Apr  9 07:08:10.692: INFO: Pod "pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055475083s
STEP: Saw pod success
Apr  9 07:08:10.692: INFO: Pod "pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:08:10.720: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32 container env-test: <nil>
STEP: delete the pod
Apr  9 07:08:10.787: INFO: Waiting for pod pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:08:10.814: INFO: Pod pod-configmaps-3a96e05b-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:10.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2527" for this suite.
Apr  9 07:08:16.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:08:17.937: INFO: namespace secrets-2527 deletion completed in 7.094906579s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:08:17.938: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:08:18.344: INFO: (0) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 32.81014ms)
Apr  9 07:08:18.389: INFO: (1) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 45.651467ms)
Apr  9 07:08:18.419: INFO: (2) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.603515ms)
Apr  9 07:08:18.448: INFO: (3) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.817549ms)
Apr  9 07:08:18.477: INFO: (4) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.085146ms)
Apr  9 07:08:18.507: INFO: (5) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.465938ms)
Apr  9 07:08:18.537: INFO: (6) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.920274ms)
Apr  9 07:08:18.567: INFO: (7) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 30.099939ms)
Apr  9 07:08:18.596: INFO: (8) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.319412ms)
Apr  9 07:08:18.626: INFO: (9) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.438874ms)
Apr  9 07:08:18.655: INFO: (10) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.378082ms)
Apr  9 07:08:18.685: INFO: (11) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.760708ms)
Apr  9 07:08:18.715: INFO: (12) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.566669ms)
Apr  9 07:08:18.744: INFO: (13) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.020073ms)
Apr  9 07:08:18.773: INFO: (14) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.984207ms)
Apr  9 07:08:18.802: INFO: (15) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.371589ms)
Apr  9 07:08:18.832: INFO: (16) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.741287ms)
Apr  9 07:08:18.861: INFO: (17) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.145827ms)
Apr  9 07:08:18.890: INFO: (18) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.752248ms)
Apr  9 07:08:18.919: INFO: (19) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.294908ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:18.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4881" for this suite.
Apr  9 07:08:25.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:08:26.041: INFO: namespace proxy-4881 deletion completed in 7.09324494s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:08:26.041: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  9 07:08:26.403: INFO: Waiting up to 5m0s for pod "pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32" in namespace "emptydir-3186" to be "success or failure"
Apr  9 07:08:26.431: INFO: Pod "pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.258382ms
Apr  9 07:08:28.459: INFO: Pod "pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055222373s
STEP: Saw pod success
Apr  9 07:08:28.459: INFO: Pod "pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:08:28.486: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:08:28.551: INFO: Waiting for pod pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:08:28.578: INFO: Pod pod-452df3cd-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:28.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3186" for this suite.
Apr  9 07:08:34.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:08:35.689: INFO: namespace emptydir-3186 deletion completed in 7.082178275s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:08:35.689: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 07:08:35.978: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8367'
Apr  9 07:08:36.214: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Apr  9 07:08:36.214: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr  9 07:08:38.269: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-8367'
Apr  9 07:08:38.536: INFO: stderr: ""
Apr  9 07:08:38.536: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8367" for this suite.
Apr  9 07:08:54.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:08:55.658: INFO: namespace kubectl-8367 deletion completed in 17.094313217s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:08:55.659: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-56d30e05-5a96-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:08:56.034: INFO: Waiting up to 5m0s for pod "pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32" in namespace "configmap-9382" to be "success or failure"
Apr  9 07:08:56.061: INFO: Pod "pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.239179ms
Apr  9 07:08:58.089: INFO: Pod "pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054934794s
STEP: Saw pod success
Apr  9 07:08:58.089: INFO: Pod "pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:08:58.117: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:08:58.181: INFO: Waiting for pod pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:08:58.208: INFO: Pod pod-configmaps-56d73a76-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:08:58.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9382" for this suite.
Apr  9 07:09:04.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:09:05.335: INFO: namespace configmap-9382 deletion completed in 7.0980129s
•S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:09:05.336: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32
Apr  9 07:09:05.733: INFO: Pod name my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32: Found 1 pods out of 1
Apr  9 07:09:05.733: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32" are running
Apr  9 07:09:07.788: INFO: Pod "my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32-gk6rv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:09:05 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:09:05 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:09:05 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:09:05 +0000 UTC Reason: Message:}])
Apr  9 07:09:07.788: INFO: Trying to dial the pod
Apr  9 07:09:12.961: INFO: Controller my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32: Got expected result from replica 1 [my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32-gk6rv]: "my-hostname-basic-5c9b2803-5a96-11e9-8c35-36e6c88ddb32-gk6rv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:09:12.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1803" for this suite.
Apr  9 07:09:19.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:09:20.082: INFO: namespace replication-controller-1803 deletion completed in 7.093020541s
•S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:09:20.082: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4547
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr  9 07:09:20.462: INFO: Found 1 stateful pods, waiting for 3
Apr  9 07:09:30.491: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:09:30.491: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:09:30.491: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr  9 07:09:30.640: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr  9 07:09:30.761: INFO: Updating stateful set ss2
Apr  9 07:09:30.816: INFO: Waiting for Pod statefulset-4547/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr  9 07:09:40.962: INFO: Found 2 stateful pods, waiting for 3
Apr  9 07:09:50.990: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:09:50.991: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:09:50.991: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr  9 07:09:51.111: INFO: Updating stateful set ss2
Apr  9 07:09:51.166: INFO: Waiting for Pod statefulset-4547/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:10:01.228: INFO: Waiting for Pod statefulset-4547/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:10:11.288: INFO: Updating stateful set ss2
Apr  9 07:10:11.344: INFO: Waiting for StatefulSet statefulset-4547/ss2 to complete update
Apr  9 07:10:11.345: INFO: Waiting for Pod statefulset-4547/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:10:21.400: INFO: Waiting for StatefulSet statefulset-4547/ss2 to complete update
Apr  9 07:10:21.401: INFO: Waiting for Pod statefulset-4547/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 07:10:31.400: INFO: Deleting all statefulset in ns statefulset-4547
Apr  9 07:10:31.427: INFO: Scaling statefulset ss2 to 0
Apr  9 07:11:01.539: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:11:01.566: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:11:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4547" for this suite.
Apr  9 07:11:07.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:11:08.915: INFO: namespace statefulset-4547 deletion completed in 7.237376124s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:11:08.915: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-94
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr  9 07:11:09.223: INFO: Waiting up to 5m0s for pod "var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32" in namespace "var-expansion-94" to be "success or failure"
Apr  9 07:11:09.250: INFO: Pod "var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.200878ms
Apr  9 07:11:11.278: INFO: Pod "var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055099187s
STEP: Saw pod success
Apr  9 07:11:11.278: INFO: Pod "var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:11:11.306: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:11:11.369: INFO: Waiting for pod var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:11:11.396: INFO: Pod var-expansion-a63a011d-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:11:11.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-94" for this suite.
Apr  9 07:11:17.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:11:18.520: INFO: namespace var-expansion-94 deletion completed in 7.095967532s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:11:18.520: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:11:18.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32" in namespace "downward-api-7033" to be "success or failure"
Apr  9 07:11:18.844: INFO: Pod "downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.573658ms
Apr  9 07:11:20.872: INFO: Pod "downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055882891s
STEP: Saw pod success
Apr  9 07:11:20.872: INFO: Pod "downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:11:20.900: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:11:20.964: INFO: Waiting for pod downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:11:20.991: INFO: Pod downwardapi-volume-abf1f137-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:11:20.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7033" for this suite.
Apr  9 07:11:27.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:11:28.107: INFO: namespace downward-api-7033 deletion completed in 7.088054886s
•SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:11:28.107: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr  9 07:11:29.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:11:31.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:11:33.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:11:35.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:11:37.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690390688, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:11:40.279: INFO: Waited 1.121521328s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:11:41.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-714" for this suite.
Apr  9 07:11:47.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:11:48.968: INFO: namespace aggregator-714 deletion completed in 7.092452796s
•SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:11:48.969: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-be1ecb53-5a96-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:11:49.336: INFO: Waiting up to 5m0s for pod "pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32" in namespace "secrets-9284" to be "success or failure"
Apr  9 07:11:49.363: INFO: Pod "pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.604202ms
Apr  9 07:11:51.392: INFO: Pod "pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056444367s
STEP: Saw pod success
Apr  9 07:11:51.392: INFO: Pod "pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:11:51.420: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:11:51.487: INFO: Waiting for pod pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:11:51.514: INFO: Pod pod-secrets-be22f613-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:11:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9284" for this suite.
Apr  9 07:11:57.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:11:58.635: INFO: namespace secrets-9284 deletion completed in 7.092978441s
•SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:11:58.635: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-c3e688f4-5a96-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:11:59.033: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32" in namespace "projected-4292" to be "success or failure"
Apr  9 07:11:59.060: INFO: Pod "pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.303985ms
Apr  9 07:12:01.088: INFO: Pod "pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055213353s
STEP: Saw pod success
Apr  9 07:12:01.088: INFO: Pod "pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:12:01.115: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:12:01.181: INFO: Waiting for pod pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:12:01.208: INFO: Pod pod-projected-configmaps-c3eaaf43-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:12:01.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4292" for this suite.
Apr  9 07:12:07.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:12:08.332: INFO: namespace projected-4292 deletion completed in 7.095754658s
•S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:12:08.332: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr  9 07:12:08.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8806,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 07:12:08.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8807,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr  9 07:12:08.847: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8809,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr  9 07:12:19.041: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8832,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 07:12:19.042: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8834,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr  9 07:12:19.042: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8117,SelfLink:/api/v1/namespaces/watch-8117/configmaps/e2e-watch-test-label-changed,UID:c9b5167e-5a96-11e9-afbf-1e9604a4a829,ResourceVersion:8835,Generation:0,CreationTimestamp:2019-04-09 07:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:12:19.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8117" for this suite.
Apr  9 07:12:25.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:12:26.159: INFO: namespace watch-8117 deletion completed in 7.089792257s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:12:26.160: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr  9 07:12:26.482: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2338'
Apr  9 07:12:27.016: INFO: stderr: ""
Apr  9 07:12:27.016: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 07:12:27.016: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2338'
Apr  9 07:12:27.251: INFO: stderr: ""
Apr  9 07:12:27.251: INFO: stdout: "update-demo-nautilus-xkffb update-demo-nautilus-zhdpl "
Apr  9 07:12:27.251: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-xkffb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:27.453: INFO: stderr: ""
Apr  9 07:12:27.453: INFO: stdout: ""
Apr  9 07:12:27.453: INFO: update-demo-nautilus-xkffb is created but not running
Apr  9 07:12:32.454: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2338'
Apr  9 07:12:32.663: INFO: stderr: ""
Apr  9 07:12:32.663: INFO: stdout: "update-demo-nautilus-xkffb update-demo-nautilus-zhdpl "
Apr  9 07:12:32.663: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-xkffb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:32.863: INFO: stderr: ""
Apr  9 07:12:32.863: INFO: stdout: "true"
Apr  9 07:12:32.863: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-xkffb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:33.055: INFO: stderr: ""
Apr  9 07:12:33.055: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 07:12:33.055: INFO: validating pod update-demo-nautilus-xkffb
Apr  9 07:12:33.170: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 07:12:33.170: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 07:12:33.170: INFO: update-demo-nautilus-xkffb is verified up and running
Apr  9 07:12:33.170: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-zhdpl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:33.389: INFO: stderr: ""
Apr  9 07:12:33.390: INFO: stdout: "true"
Apr  9 07:12:33.390: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-zhdpl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:33.611: INFO: stderr: ""
Apr  9 07:12:33.611: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 07:12:33.611: INFO: validating pod update-demo-nautilus-zhdpl
Apr  9 07:12:33.726: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 07:12:33.726: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 07:12:33.726: INFO: update-demo-nautilus-zhdpl is verified up and running
STEP: rolling-update to new replication controller
Apr  9 07:12:33.731: INFO: scanned /root for discovery docs: <nil>
Apr  9 07:12:33.731: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2338'
Apr  9 07:12:49.365: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr  9 07:12:49.365: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 07:12:49.366: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2338'
Apr  9 07:12:49.574: INFO: stderr: ""
Apr  9 07:12:49.574: INFO: stdout: "update-demo-kitten-48ccg update-demo-kitten-w9p2l update-demo-nautilus-zhdpl "
STEP: Replicas for name=update-demo: expected=2 actual=3
Apr  9 07:12:54.574: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2338'
Apr  9 07:12:54.813: INFO: stderr: ""
Apr  9 07:12:54.814: INFO: stdout: "update-demo-kitten-48ccg update-demo-kitten-w9p2l "
Apr  9 07:12:54.814: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-kitten-48ccg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:55.026: INFO: stderr: ""
Apr  9 07:12:55.026: INFO: stdout: "true"
Apr  9 07:12:55.027: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-kitten-48ccg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:12:55.246: INFO: stderr: ""
Apr  9 07:12:55.246: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr  9 07:12:55.246: INFO: validating pod update-demo-kitten-48ccg
Apr  9 07:12:55.362: INFO: got data: {
  "image": "kitten.jpg"
}

Apr  9 07:12:55.362: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr  9 07:12:55.362: INFO: update-demo-kitten-48ccg is verified up and running
Apr  9 07:12:55.362: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-kitten-w9p2l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:13:05.577: INFO: stderr: ""
Apr  9 07:13:05.577: INFO: stdout: "true"
Apr  9 07:13:05.577: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-kitten-w9p2l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2338'
Apr  9 07:13:05.768: INFO: stderr: ""
Apr  9 07:13:05.768: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr  9 07:13:05.768: INFO: validating pod update-demo-kitten-w9p2l
Apr  9 07:13:05.886: INFO: got data: {
  "image": "kitten.jpg"
}

Apr  9 07:13:05.886: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr  9 07:13:05.886: INFO: update-demo-kitten-w9p2l is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:13:05.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2338" for this suite.
Apr  9 07:13:27.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:13:29.000: INFO: namespace kubectl-2338 deletion completed in 23.086297416s
•SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:13:29.000: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:13:29.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32" in namespace "projected-152" to be "success or failure"
Apr  9 07:13:29.341: INFO: Pod "downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.237825ms
Apr  9 07:13:31.369: INFO: Pod "downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055331432s
STEP: Saw pod success
Apr  9 07:13:31.369: INFO: Pod "downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:13:31.396: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:13:31.463: INFO: Waiting for pod downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:13:31.490: INFO: Pod downwardapi-volume-f9ba435e-5a96-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:13:31.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-152" for this suite.
Apr  9 07:13:37.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:13:38.602: INFO: namespace projected-152 deletion completed in 7.084548978s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:13:38.603: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3934
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr  9 07:13:38.967: INFO: Found 1 stateful pods, waiting for 3
Apr  9 07:13:48.996: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:13:48.996: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:13:48.996: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:13:49.081: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3934 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:13:49.895: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:13:49.895: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:13:49.895: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr  9 07:14:00.071: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr  9 07:14:00.154: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3934 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 07:14:00.937: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 07:14:00.937: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 07:14:00.937: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 07:14:11.105: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:14:11.105: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:14:11.105: INFO: Waiting for Pod statefulset-3934/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:14:21.160: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:14:21.160: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:14:21.160: INFO: Waiting for Pod statefulset-3934/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 07:14:31.160: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:14:31.160: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr  9 07:14:41.160: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3934 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:14:42.012: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:14:42.012: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:14:42.012: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 07:14:42.133: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr  9 07:14:42.215: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-3934 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 07:14:43.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 07:14:43.006: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 07:14:43.006: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 07:14:43.249: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:14:43.249: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:14:43.249: INFO: Waiting for Pod statefulset-3934/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:14:43.249: INFO: Waiting for Pod statefulset-3934/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:14:53.308: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:14:53.308: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:14:53.308: INFO: Waiting for Pod statefulset-3934/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:15:03.304: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
Apr  9 07:15:03.305: INFO: Waiting for Pod statefulset-3934/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 07:15:13.304: INFO: Waiting for StatefulSet statefulset-3934/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 07:15:23.304: INFO: Deleting all statefulset in ns statefulset-3934
Apr  9 07:15:23.332: INFO: Scaling statefulset ss2 to 0
Apr  9 07:15:43.442: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:15:43.469: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:15:43.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3934" for this suite.
Apr  9 07:15:49.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:15:50.671: INFO: namespace statefulset-3934 deletion completed in 7.091013945s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:15:50.671: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  9 07:15:51.006: INFO: Waiting up to 5m0s for pod "pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32" in namespace "emptydir-2930" to be "success or failure"
Apr  9 07:15:51.034: INFO: Pod "pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.424779ms
Apr  9 07:15:53.062: INFO: Pod "pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055597726s
STEP: Saw pod success
Apr  9 07:15:53.062: INFO: Pod "pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:15:53.089: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:15:53.154: INFO: Waiting for pod pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:15:53.182: INFO: Pod pod-4e2ef5af-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:15:53.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2930" for this suite.
Apr  9 07:15:59.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:16:00.297: INFO: namespace emptydir-2930 deletion completed in 7.086770829s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:16:00.297: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr  9 07:16:00.606: INFO: Waiting up to 5m0s for pod "var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32" in namespace "var-expansion-1218" to be "success or failure"
Apr  9 07:16:00.633: INFO: Pod "var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.21419ms
Apr  9 07:16:02.661: INFO: Pod "var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054993867s
STEP: Saw pod success
Apr  9 07:16:02.661: INFO: Pod "var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:16:02.688: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:16:02.752: INFO: Waiting for pod var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:16:02.779: INFO: Pod var-expansion-53e7bf46-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:16:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1218" for this suite.
Apr  9 07:16:08.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:16:09.901: INFO: namespace var-expansion-1218 deletion completed in 7.093801411s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:16:09.902: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-166
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-59a332ce-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating configMap with name cm-test-opt-upd-59a3331b-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-59a332ce-5a97-11e9-8c35-36e6c88ddb32
STEP: Updating configmap cm-test-opt-upd-59a3331b-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating configMap with name cm-test-opt-create-59a33330-5a97-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:16:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-166" for this suite.
Apr  9 07:16:39.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:16:40.093: INFO: namespace configmap-166 deletion completed in 23.132479589s
•SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:16:40.093: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6ba0d523-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:16:40.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32" in namespace "projected-6219" to be "success or failure"
Apr  9 07:16:40.461: INFO: Pod "pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.22154ms
Apr  9 07:16:42.489: INFO: Pod "pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055226952s
STEP: Saw pod success
Apr  9 07:16:42.489: INFO: Pod "pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:16:42.517: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:16:42.581: INFO: Waiting for pod pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:16:42.608: INFO: Pod pod-projected-configmaps-6ba4ff89-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:16:42.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6219" for this suite.
Apr  9 07:16:48.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:16:49.722: INFO: namespace projected-6219 deletion completed in 7.0858926s
•SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:16:49.722: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:17:16.098: INFO: Container started at 2019-04-09 07:16:50 +0000 UTC, pod became ready at 2019-04-09 07:17:14 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:17:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-175" for this suite.
Apr  9 07:17:38.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:17:39.222: INFO: namespace container-probe-175 deletion completed in 23.095914883s
•SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:17:39.222: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0409 07:17:40.266361    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 07:17:40.266: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:17:40.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5641" for this suite.
Apr  9 07:17:46.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:17:47.391: INFO: namespace gc-5641 deletion completed in 7.096767853s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:17:47.391: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-822 to expose endpoints map[]
Apr  9 07:17:47.737: INFO: successfully validated that service multi-endpoint-test in namespace services-822 exposes endpoints map[] (27.495245ms elapsed)
STEP: Creating pod pod1 in namespace services-822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-822 to expose endpoints map[pod1:[100]]
Apr  9 07:17:49.930: INFO: successfully validated that service multi-endpoint-test in namespace services-822 exposes endpoints map[pod1:[100]] (2.163433989s elapsed)
STEP: Creating pod pod2 in namespace services-822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-822 to expose endpoints map[pod1:[100] pod2:[101]]
Apr  9 07:17:52.206: INFO: successfully validated that service multi-endpoint-test in namespace services-822 exposes endpoints map[pod1:[100] pod2:[101]] (2.247343511s elapsed)
STEP: Deleting pod pod1 in namespace services-822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-822 to expose endpoints map[pod2:[101]]
Apr  9 07:17:52.289: INFO: successfully validated that service multi-endpoint-test in namespace services-822 exposes endpoints map[pod2:[101]] (54.365195ms elapsed)
STEP: Deleting pod pod2 in namespace services-822
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-822 to expose endpoints map[]
Apr  9 07:17:52.345: INFO: successfully validated that service multi-endpoint-test in namespace services-822 exposes endpoints map[] (27.102248ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:17:52.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-822" for this suite.
Apr  9 07:18:14.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:18:15.496: INFO: namespace services-822 deletion completed in 23.089857476s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:18:15.496: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr  9 07:18:15.783: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-8560'
Apr  9 07:18:18.035: INFO: stderr: ""
Apr  9 07:18:18.036: INFO: stdout: "pod/pause created\n"
Apr  9 07:18:18.036: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr  9 07:18:18.036: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8560" to be "running and ready"
Apr  9 07:18:18.063: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 27.289845ms
Apr  9 07:18:20.091: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.055195761s
Apr  9 07:18:20.091: INFO: Pod "pause" satisfied condition "running and ready"
Apr  9 07:18:20.091: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr  9 07:18:20.091: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-8560'
Apr  9 07:18:20.324: INFO: stderr: ""
Apr  9 07:18:20.324: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr  9 07:18:20.324: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pod pause -L testing-label --namespace=kubectl-8560'
Apr  9 07:18:20.547: INFO: stderr: ""
Apr  9 07:18:20.547: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr  9 07:18:20.547: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config label pods pause testing-label- --namespace=kubectl-8560'
Apr  9 07:18:20.780: INFO: stderr: ""
Apr  9 07:18:20.780: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr  9 07:18:20.780: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pod pause -L testing-label --namespace=kubectl-8560'
Apr  9 07:18:20.991: INFO: stderr: ""
Apr  9 07:18:20.991: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr  9 07:18:20.992: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8560'
Apr  9 07:18:21.221: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:18:21.221: INFO: stdout: "pod \"pause\" force deleted\n"
Apr  9 07:18:21.221: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-8560'
Apr  9 07:18:21.445: INFO: stderr: "No resources found.\n"
Apr  9 07:18:21.445: INFO: stdout: ""
Apr  9 07:18:21.445: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -l name=pause --namespace=kubectl-8560 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 07:18:21.630: INFO: stderr: ""
Apr  9 07:18:21.631: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:18:21.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8560" for this suite.
Apr  9 07:18:27.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:18:28.762: INFO: namespace kubectl-8560 deletion completed in 7.100715268s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:18:28.762: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-ac63b363-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:18:29.085: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32" in namespace "projected-9009" to be "success or failure"
Apr  9 07:18:29.113: INFO: Pod "pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.473345ms
Apr  9 07:18:31.141: INFO: Pod "pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056079145s
STEP: Saw pod success
Apr  9 07:18:31.141: INFO: Pod "pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:18:31.169: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:18:31.233: INFO: Waiting for pod pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:18:31.260: INFO: Pod pod-projected-secrets-ac67dfe2-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:18:31.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9009" for this suite.
Apr  9 07:18:37.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:18:38.381: INFO: namespace projected-9009 deletion completed in 7.093336157s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:18:38.381: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  9 07:18:38.709: INFO: Waiting up to 5m0s for pod "pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32" in namespace "emptydir-7878" to be "success or failure"
Apr  9 07:18:38.736: INFO: Pod "pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.193945ms
Apr  9 07:18:40.764: INFO: Pod "pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05524523s
STEP: Saw pod success
Apr  9 07:18:40.764: INFO: Pod "pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:18:40.792: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:18:40.860: INFO: Waiting for pod pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:18:40.887: INFO: Pod pod-b2243bb3-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:18:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7878" for this suite.
Apr  9 07:18:46.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:18:48.004: INFO: namespace emptydir-7878 deletion completed in 7.08869611s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:18:48.004: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  9 07:18:48.308: INFO: Waiting up to 5m0s for pod "pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32" in namespace "emptydir-556" to be "success or failure"
Apr  9 07:18:48.335: INFO: Pod "pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.097303ms
Apr  9 07:18:50.363: INFO: Pod "pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054929104s
STEP: Saw pod success
Apr  9 07:18:50.363: INFO: Pod "pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:18:50.390: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:18:50.454: INFO: Waiting for pod pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:18:50.481: INFO: Pod pod-b7dd1270-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:18:50.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-556" for this suite.
Apr  9 07:18:56.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:18:57.595: INFO: namespace emptydir-556 deletion completed in 7.085776383s
•SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:18:57.596: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 07:18:57.879: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 07:18:57.933: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 07:18:57.960: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-41.eu-west-1.compute.internal before test
Apr  9 07:18:58.005: INFO: calico-node-ds2qx from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:18:58.005: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-t75n5 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr  9 07:18:58.005: INFO: addons-nginx-ingress-controller-f88658d78-sg5bg from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr  9 07:18:58.005: INFO: coredns-7f7f7978c8-96z5b from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:18:58.005: INFO: addons-kube2iam-dkzn8 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:18:58.005: INFO: vpn-shoot-77d7f4479f-6ss5n from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr  9 07:18:58.005: INFO: addons-kubernetes-dashboard-665df4b66d-8rnhz from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  9 07:18:58.005: INFO: metrics-server-845bbc9978-plq7d from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container metrics-server ready: true, restart count 0
Apr  9 07:18:58.005: INFO: coredns-7f7f7978c8-fg8rc from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:18:58.005: INFO: blackbox-exporter-6dc58dcffc-4zzfb from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr  9 07:18:58.005: INFO: kube-proxy-m5x28 from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  9 07:18:58.005: INFO: node-exporter-zntwd from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.005: INFO: 	Container node-exporter ready: true, restart count 0
Apr  9 07:18:58.005: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-30-1.eu-west-1.compute.internal before test
Apr  9 07:18:58.058: INFO: calico-node-8wb77 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.058: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:18:58.058: INFO: addons-kube2iam-tdsnm from kube-system started at 2019-04-09 06:35:41 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.058: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:18:58.058: INFO: kube-proxy-g7h72 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.058: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  9 07:18:58.058: INFO: node-exporter-m5mnx from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:18:58.058: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1593bd3680971fd7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:18:59.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5889" for this suite.
Apr  9 07:19:05.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:19:06.317: INFO: namespace sched-pred-5889 deletion completed in 7.089447458s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:19:06.318: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:19:06.693: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c2cff94c-5a97-11e9-afbf-1e9604a4a829", Controller:(*bool)(0xc0032587fa), BlockOwnerDeletion:(*bool)(0xc0032587fb)}}
Apr  9 07:19:06.722: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c2c772dc-5a97-11e9-afbf-1e9604a4a829", Controller:(*bool)(0xc002b4d846), BlockOwnerDeletion:(*bool)(0xc002b4d847)}}
Apr  9 07:19:06.750: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c2cbafe1-5a97-11e9-afbf-1e9604a4a829", Controller:(*bool)(0xc002b7b306), BlockOwnerDeletion:(*bool)(0xc002b7b307)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:19:11.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9586" for this suite.
Apr  9 07:19:18.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:19:19.064: INFO: namespace gc-9586 deletion completed in 7.167305065s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:19:19.064: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:19:19.378: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config version'
Apr  9 07:19:19.614: INFO: stderr: ""
Apr  9 07:19:19.614: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.3\", GitCommit:\"435f92c719f279a3a67808c80521ea17d5715c66\", GitTreeState:\"clean\", BuildDate:\"2018-11-26T12:57:14Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:19:19.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9911" for this suite.
Apr  9 07:19:25.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:19:26.733: INFO: namespace kubectl-9911 deletion completed in 7.091395422s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:19:26.733: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:19:27.054: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 07:19:29.110: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr  9 07:19:31.138: INFO: Creating deployment "test-rollover-deployment"
Apr  9 07:19:31.193: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr  9 07:19:31.220: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr  9 07:19:31.274: INFO: Ensure that both replica sets have 1 created replica
Apr  9 07:19:31.330: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr  9 07:19:31.385: INFO: Updating deployment test-rollover-deployment
Apr  9 07:19:31.385: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr  9 07:19:31.413: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr  9 07:19:31.467: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr  9 07:19:31.525: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:31.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:33.581: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:33.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391172, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:35.581: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:35.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391172, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:37.581: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:37.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391172, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:39.584: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:39.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391172, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:41.581: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 07:19:41.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391172, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690391171, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 07:19:43.581: INFO: 
Apr  9 07:19:43.581: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 07:19:43.664: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/deployments/test-rollover-deployment,UID:d16a6f6f-5a97-11e9-afbf-1e9604a4a829,ResourceVersion:10424,Generation:2,CreationTimestamp:2019-04-09 07:19:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-09 07:19:31 +0000 UTC 2019-04-09 07:19:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-09 07:19:42 +0000 UTC 2019-04-09 07:19:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr  9 07:19:43.691: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/replicasets/test-rollover-deployment-766b4d6c9d,UID:d18c1ee3-5a97-11e9-afbf-1e9604a4a829,ResourceVersion:10417,Generation:2,CreationTimestamp:2019-04-09 07:19:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d16a6f6f-5a97-11e9-afbf-1e9604a4a829 0xc0026e7717 0xc0026e7718}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 07:19:43.692: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr  9 07:19:43.692: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/replicasets/test-rollover-controller,UID:cef2b867-5a97-11e9-afbf-1e9604a4a829,ResourceVersion:10423,Generation:2,CreationTimestamp:2019-04-09 07:19:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d16a6f6f-5a97-11e9-afbf-1e9604a4a829 0xc0026e7567 0xc0026e7568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 07:19:43.692: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/replicasets/test-rollover-deployment-6455657675,UID:d16b7ab4-5a97-11e9-afbf-1e9604a4a829,ResourceVersion:10385,Generation:2,CreationTimestamp:2019-04-09 07:19:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d16a6f6f-5a97-11e9-afbf-1e9604a4a829 0xc0026e7637 0xc0026e7638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 07:19:43.719: INFO: Pod "test-rollover-deployment-766b4d6c9d-m2nm5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-m2nm5,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-6517,SelfLink:/api/v1/namespaces/deployment-6517/pods/test-rollover-deployment-766b4d6c9d-m2nm5,UID:d18d75aa-5a97-11e9-afbf-1e9604a4a829,ResourceVersion:10394,Generation:0,CreationTimestamp:2019-04-09 07:19:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.122/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d d18c1ee3-5a97-11e9-afbf-1e9604a4a829 0xc0029d4237 0xc0029d4238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qlhj9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qlhj9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qlhj9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d42a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d42c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:19:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:19:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:19:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:19:31 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.122,StartTime:2019-04-09 07:19:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-09 07:19:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://5b35a09fecb67956029c6c4b07c6ac597e2e9632e9ca4a36954fb02bdd3f2c65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:19:43.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6517" for this suite.
Apr  9 07:19:49.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:19:50.833: INFO: namespace deployment-6517 deletion completed in 7.08637636s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:19:50.833: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 07:19:53.907: INFO: Successfully updated pod "annotationupdatedd5a9d56-5a97-11e9-8c35-36e6c88ddb32"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:19:55.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-326" for this suite.
Apr  9 07:20:18.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:20:19.100: INFO: namespace projected-326 deletion completed in 23.095103146s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:20:19.101: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ee29aca1-5a97-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:20:19.435: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32" in namespace "projected-7861" to be "success or failure"
Apr  9 07:20:19.463: INFO: Pod "pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.357519ms
Apr  9 07:20:21.491: INFO: Pod "pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055644064s
STEP: Saw pod success
Apr  9 07:20:21.491: INFO: Pod "pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:20:21.519: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:20:21.588: INFO: Waiting for pod pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:20:21.615: INFO: Pod pod-projected-configmaps-ee2df271-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:20:21.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7861" for this suite.
Apr  9 07:20:27.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:20:28.734: INFO: namespace projected-7861 deletion completed in 7.090816219s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:20:28.735: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:20:29.035: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32" in namespace "downward-api-2342" to be "success or failure"
Apr  9 07:20:29.063: INFO: Pod "downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.351443ms
Apr  9 07:20:31.091: INFO: Pod "downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055995866s
STEP: Saw pod success
Apr  9 07:20:31.091: INFO: Pod "downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:20:31.119: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:20:31.184: INFO: Waiting for pod downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:20:31.211: INFO: Pod downwardapi-volume-f3e6b2d2-5a97-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:20:31.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2342" for this suite.
Apr  9 07:20:37.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:20:38.329: INFO: namespace downward-api-2342 deletion completed in 7.09032961s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:20:38.330: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr  9 07:20:38.593: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config --namespace=kubectl-5447 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr  9 07:20:41.362: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr  9 07:20:41.362: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:20:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5447" for this suite.
Apr  9 07:20:49.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:20:50.550: INFO: namespace kubectl-5447 deletion completed in 7.106185802s
•SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:20:50.551: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8437
Apr  9 07:20:52.900: INFO: Started pod liveness-exec in namespace container-probe-8437
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 07:20:52.928: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:24:54.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8437" for this suite.
Apr  9 07:25:00.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:25:01.444: INFO: namespace container-probe-8437 deletion completed in 7.107497158s
•SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:25:01.444: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-968065bb-5a98-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:25:01.862: INFO: Waiting up to 5m0s for pod "pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32" in namespace "configmap-1027" to be "success or failure"
Apr  9 07:25:01.889: INFO: Pod "pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.355146ms
Apr  9 07:25:03.917: INFO: Pod "pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055168181s
STEP: Saw pod success
Apr  9 07:25:03.917: INFO: Pod "pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:25:03.944: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:25:04.013: INFO: Waiting for pod pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:25:04.039: INFO: Pod pod-configmaps-96849a51-5a98-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:25:04.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1027" for this suite.
Apr  9 07:25:10.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:25:11.165: INFO: namespace configmap-1027 deletion completed in 7.096462398s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:25:11.166: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr  9 07:25:11.430: INFO: Asynchronously running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix242681303/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:25:11.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9286" for this suite.
Apr  9 07:25:17.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:25:18.633: INFO: namespace kubectl-9286 deletion completed in 7.102981662s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:25:18.634: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 07:25:19.043: INFO: Waiting up to 5m0s for pod "downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32" in namespace "downward-api-934" to be "success or failure"
Apr  9 07:25:19.070: INFO: Pod "downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.163687ms
Apr  9 07:25:21.098: INFO: Pod "downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055479774s
STEP: Saw pod success
Apr  9 07:25:21.098: INFO: Pod "downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:25:21.126: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:25:21.189: INFO: Waiting for pod downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:25:21.216: INFO: Pod downward-api-a0c1b384-5a98-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:25:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-934" for this suite.
Apr  9 07:25:27.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:25:28.333: INFO: namespace downward-api-934 deletion completed in 7.08915386s
•
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:25:28.333: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2033
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-2033
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2033
Apr  9 07:25:28.761: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Apr  9 07:25:38.789: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr  9 07:25:38.817: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:25:39.702: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:25:39.702: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:25:39.702: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 07:25:39.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  9 07:25:49.758: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 07:25:49.758: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:25:49.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999572s
Apr  9 07:25:50.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.972449232s
Apr  9 07:25:51.924: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.943734629s
Apr  9 07:25:52.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.91569401s
Apr  9 07:25:53.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.887521427s
Apr  9 07:25:55.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.859438594s
Apr  9 07:25:56.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.831242246s
Apr  9 07:25:57.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.803169919s
Apr  9 07:25:58.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.77503255s
Apr  9 07:25:59.122: INFO: Verifying statefulset ss doesn't scale past 3 for another 746.188041ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2033
Apr  9 07:26:00.150: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 07:26:00.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 07:26:00.990: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 07:26:00.990: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 07:26:00.990: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 07:26:01.817: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  9 07:26:01.817: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 07:26:01.817: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 07:26:01.817: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 07:26:02.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  9 07:26:02.641: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 07:26:02.641: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 07:26:02.669: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:26:02.669: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 07:26:02.669: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr  9 07:26:02.697: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:26:03.544: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:26:03.544: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:26:03.544: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 07:26:03.545: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:26:04.331: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:26:04.331: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:26:04.331: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 07:26:04.332: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config exec --namespace=statefulset-2033 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 07:26:05.124: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 07:26:05.124: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 07:26:05.124: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 07:26:05.124: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:26:05.152: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr  9 07:26:15.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 07:26:15.208: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 07:26:15.208: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 07:26:15.290: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr  9 07:26:15.290: INFO: ss-0  ip-10-250-30-1.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  }]
Apr  9 07:26:15.290: INFO: ss-1  ip-10-250-14-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:15.290: INFO: ss-2  ip-10-250-30-1.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:15.290: INFO: 
Apr  9 07:26:15.290: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 07:26:16.318: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr  9 07:26:16.318: INFO: ss-0  ip-10-250-30-1.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  }]
Apr  9 07:26:16.318: INFO: ss-1  ip-10-250-14-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:16.318: INFO: ss-2  ip-10-250-30-1.eu-west-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:16.318: INFO: 
Apr  9 07:26:16.318: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 07:26:17.346: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr  9 07:26:17.346: INFO: ss-0  ip-10-250-30-1.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:28 +0000 UTC  }]
Apr  9 07:26:17.347: INFO: ss-1  ip-10-250-14-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:17.347: INFO: ss-2  ip-10-250-30-1.eu-west-1.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:17.347: INFO: 
Apr  9 07:26:17.347: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 07:26:18.374: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr  9 07:26:18.374: INFO: ss-1  ip-10-250-14-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:26:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:25:49 +0000 UTC  }]
Apr  9 07:26:18.374: INFO: 
Apr  9 07:26:18.374: INFO: StatefulSet ss has not reached scale 0, at 1
Apr  9 07:26:19.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.888114495s
Apr  9 07:26:20.430: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.860089542s
Apr  9 07:26:21.458: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.831998503s
Apr  9 07:26:22.486: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.804019656s
Apr  9 07:26:23.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.776152068s
Apr  9 07:26:24.542: INFO: Verifying statefulset ss doesn't scale past 0 for another 748.311535ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2033
Apr  9 07:26:25.570: INFO: Scaling statefulset ss to 0
Apr  9 07:26:25.652: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 07:26:25.680: INFO: Deleting all statefulset in ns statefulset-2033
Apr  9 07:26:25.707: INFO: Scaling statefulset ss to 0
Apr  9 07:26:25.790: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:26:25.818: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:26:25.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2033" for this suite.
Apr  9 07:26:32.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:26:33.039: INFO: namespace statefulset-2033 deletion completed in 7.108727895s
•SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:26:33.039: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  9 07:26:39.560: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:39.588: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:41.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:41.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:43.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:43.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:45.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:45.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:47.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:47.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:49.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:49.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:51.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:51.616: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 07:26:53.588: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 07:26:53.616: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:26:53.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3233" for this suite.
Apr  9 07:27:15.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:27:16.765: INFO: namespace container-lifecycle-hook-3233 deletion completed in 23.087292611s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:27:16.765: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 07:27:17.078: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4762'
Apr  9 07:27:17.343: INFO: stderr: ""
Apr  9 07:27:17.343: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr  9 07:27:22.393: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-4762 -o json'
Apr  9 07:27:22.635: INFO: stderr: ""
Apr  9 07:27:22.635: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.134/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-04-09T07:27:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4762\",\n        \"resourceVersion\": \"11642\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4762/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e7458136-5a98-11e9-afbf-1e9604a4a829\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zx5ps\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-30-1.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zx5ps\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zx5ps\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T07:27:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T07:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T07:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T07:27:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://00f8fedd842c106c70041973f591ac4a690e04e391fc37ed87bb918f24a98c73\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-09T07:27:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.30.1\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.134\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-09T07:27:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr  9 07:27:22.636: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config replace -f - --namespace=kubectl-4762'
Apr  9 07:27:23.008: INFO: stderr: ""
Apr  9 07:27:23.008: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr  9 07:27:23.036: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-4762'
Apr  9 07:27:24.439: INFO: stderr: ""
Apr  9 07:27:24.439: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:27:24.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4762" for this suite.
Apr  9 07:27:30.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:27:31.551: INFO: namespace kubectl-4762 deletion completed in 7.082470328s
•SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:27:31.551: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:27:31.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-962" for this suite.
Apr  9 07:27:38.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:27:39.021: INFO: namespace services-962 deletion completed in 7.086371083s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:27:39.022: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  9 07:27:45.536: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:45.564: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:47.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:47.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:49.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:49.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:51.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:51.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:53.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:53.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:55.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:55.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:57.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:57.593: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:27:59.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:27:59.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:28:01.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:28:01.592: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:28:03.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:28:03.593: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 07:28:05.564: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 07:28:05.592: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:28:05.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7710" for this suite.
Apr  9 07:28:27.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:28:28.743: INFO: namespace container-lifecycle-hook-7710 deletion completed in 23.084925623s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:28:28.744: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:28:31.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3710" for this suite.
Apr  9 07:29:09.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:29:10.289: INFO: namespace kubelet-test-3710 deletion completed in 39.104988057s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:29:10.289: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8289
Apr  9 07:29:12.659: INFO: Started pod liveness-http in namespace container-probe-8289
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 07:29:12.687: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:33:14.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8289" for this suite.
Apr  9 07:33:20.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:33:21.196: INFO: namespace container-probe-8289 deletion completed in 7.097871035s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:33:21.196: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:33:21.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32" in namespace "projected-2068" to be "success or failure"
Apr  9 07:33:21.537: INFO: Pod "downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.420506ms
Apr  9 07:33:23.565: INFO: Pod "downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055308585s
STEP: Saw pod success
Apr  9 07:33:23.565: INFO: Pod "downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:33:23.592: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:33:23.660: INFO: Waiting for pod downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:33:23.686: INFO: Pod downwardapi-volume-c054cccb-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:33:23.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2068" for this suite.
Apr  9 07:33:29.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:33:30.802: INFO: namespace projected-2068 deletion completed in 7.08776771s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:33:30.803: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1726
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-c60d1b88-5a99-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:33:31.131: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32" in namespace "projected-1726" to be "success or failure"
Apr  9 07:33:31.159: INFO: Pod "pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.812702ms
Apr  9 07:33:33.194: INFO: Pod "pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062602496s
STEP: Saw pod success
Apr  9 07:33:33.194: INFO: Pod "pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:33:33.222: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:33:33.286: INFO: Waiting for pod pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:33:33.313: INFO: Pod pod-projected-secrets-c6113d0a-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:33:33.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1726" for this suite.
Apr  9 07:33:39.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:33:40.425: INFO: namespace projected-1726 deletion completed in 7.083478948s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:33:40.427: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 07:33:40.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 07:33:40.754: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 07:33:40.781: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-41.eu-west-1.compute.internal before test
Apr  9 07:33:40.820: INFO: coredns-7f7f7978c8-96z5b from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:33:40.820: INFO: addons-kube2iam-dkzn8 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:33:40.820: INFO: vpn-shoot-77d7f4479f-6ss5n from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr  9 07:33:40.820: INFO: addons-kubernetes-dashboard-665df4b66d-8rnhz from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  9 07:33:40.820: INFO: metrics-server-845bbc9978-plq7d from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container metrics-server ready: true, restart count 0
Apr  9 07:33:40.820: INFO: node-exporter-zntwd from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container node-exporter ready: true, restart count 0
Apr  9 07:33:40.820: INFO: coredns-7f7f7978c8-fg8rc from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:33:40.820: INFO: blackbox-exporter-6dc58dcffc-4zzfb from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr  9 07:33:40.820: INFO: kube-proxy-m5x28 from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  9 07:33:40.820: INFO: calico-node-ds2qx from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:33:40.820: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-t75n5 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr  9 07:33:40.820: INFO: addons-nginx-ingress-controller-f88658d78-sg5bg from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.820: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr  9 07:33:40.820: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-30-1.eu-west-1.compute.internal before test
Apr  9 07:33:40.876: INFO: node-exporter-m5mnx from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.876: INFO: 	Container node-exporter ready: true, restart count 0
Apr  9 07:33:40.876: INFO: calico-node-8wb77 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.876: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:33:40.876: INFO: addons-kube2iam-tdsnm from kube-system started at 2019-04-09 06:35:41 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.876: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:33:40.876: INFO: kube-proxy-g7h72 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:33:40.876: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-10-250-14-41.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-250-30-1.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod addons-kube2iam-dkzn8 requesting resource cpu=10m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod addons-kube2iam-tdsnm requesting resource cpu=10m on Node ip-10-250-30-1.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod addons-kubernetes-dashboard-665df4b66d-8rnhz requesting resource cpu=50m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod addons-nginx-ingress-controller-f88658d78-sg5bg requesting resource cpu=100m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-t75n5 requesting resource cpu=0m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod blackbox-exporter-6dc58dcffc-4zzfb requesting resource cpu=5m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod calico-node-8wb77 requesting resource cpu=100m on Node ip-10-250-30-1.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod calico-node-ds2qx requesting resource cpu=100m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod coredns-7f7f7978c8-96z5b requesting resource cpu=50m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod coredns-7f7f7978c8-fg8rc requesting resource cpu=50m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod kube-proxy-g7h72 requesting resource cpu=20m on Node ip-10-250-30-1.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod kube-proxy-m5x28 requesting resource cpu=20m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod metrics-server-845bbc9978-plq7d requesting resource cpu=20m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod node-exporter-m5mnx requesting resource cpu=5m on Node ip-10-250-30-1.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod node-exporter-zntwd requesting resource cpu=5m on Node ip-10-250-14-41.eu-west-1.compute.internal
Apr  9 07:33:41.052: INFO: Pod vpn-shoot-77d7f4479f-6ss5n requesting resource cpu=50m on Node ip-10-250-14-41.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32.1593be04123833e4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4207/filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32 to ip-10-250-14-41.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32.1593be043bf66687], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32.1593be045f34810e], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32.1593be0461cdd1c7], Reason = [Created], Message = [Created container filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32.1593be046921a7d2], Reason = [Started], Message = [Started container filler-pod-cbff81b0-5a99-11e9-8c35-36e6c88ddb32]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32.1593be0413e0d42e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4207/filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32 to ip-10-250-30-1.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32.1593be043ce8c6c4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32.1593be0440b30682], Reason = [Created], Message = [Created container filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32.1593be04475aa6f0], Reason = [Started], Message = [Started container filler-pod-cc03f143-5a99-11e9-8c35-36e6c88ddb32]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1593be0494eb0f3f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-14-41.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-30-1.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:33:44.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4207" for this suite.
Apr  9 07:33:50.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:33:51.591: INFO: namespace sched-pred-4207 deletion completed in 7.090436437s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:33:51.591: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d2737b3a-5a99-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:33:51.938: INFO: Waiting up to 5m0s for pod "pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32" in namespace "secrets-787" to be "success or failure"
Apr  9 07:33:51.966: INFO: Pod "pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.398993ms
Apr  9 07:33:53.994: INFO: Pod "pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055600805s
STEP: Saw pod success
Apr  9 07:33:53.994: INFO: Pod "pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:33:54.021: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32 container secret-env-test: <nil>
STEP: delete the pod
Apr  9 07:33:54.117: INFO: Waiting for pod pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:33:54.144: INFO: Pod pod-secrets-d277c40b-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:33:54.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-787" for this suite.
Apr  9 07:34:00.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:34:01.278: INFO: namespace secrets-787 deletion completed in 7.087744497s
•SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:34:01.278: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:34:01.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32" in namespace "projected-4773" to be "success or failure"
Apr  9 07:34:01.600: INFO: Pod "downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.278001ms
Apr  9 07:34:03.628: INFO: Pod "downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055817974s
STEP: Saw pod success
Apr  9 07:34:03.628: INFO: Pod "downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:34:03.662: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:34:03.726: INFO: Waiting for pod downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:34:03.753: INFO: Pod downwardapi-volume-d83614f8-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:34:03.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4773" for this suite.
Apr  9 07:34:09.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:34:10.866: INFO: namespace projected-4773 deletion completed in 7.0851085s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:34:10.866: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7445
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 07:34:11.293: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 07:34:33.793: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.144:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7445 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:34:33.793: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:34:34.395: INFO: Found all expected endpoints: [netserver-0]
Apr  9 07:34:34.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7445 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:34:34.423: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:34:35.032: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:34:35.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7445" for this suite.
Apr  9 07:34:57.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:34:58.169: INFO: namespace pod-network-test-7445 deletion completed in 23.109519707s
•
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:34:58.170: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-fa25b040-5a99-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:34:58.535: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32" in namespace "projected-4730" to be "success or failure"
Apr  9 07:34:58.563: INFO: Pod "pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.106653ms
Apr  9 07:35:00.591: INFO: Pod "pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055266741s
STEP: Saw pod success
Apr  9 07:35:00.591: INFO: Pod "pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:35:00.618: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:35:00.683: INFO: Waiting for pod pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:35:00.710: INFO: Pod pod-projected-secrets-fa29dc2e-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:35:00.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4730" for this suite.
Apr  9 07:35:06.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:35:07.829: INFO: namespace projected-4730 deletion completed in 7.091056112s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:35:07.829: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ffe106b0-5a99-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:35:08.152: INFO: Waiting up to 5m0s for pod "pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32" in namespace "secrets-3432" to be "success or failure"
Apr  9 07:35:08.179: INFO: Pod "pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.107746ms
Apr  9 07:35:10.207: INFO: Pod "pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054952303s
STEP: Saw pod success
Apr  9 07:35:10.207: INFO: Pod "pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:35:10.234: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:35:10.298: INFO: Waiting for pod pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:35:10.325: INFO: Pod pod-secrets-ffe55c7d-5a99-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:35:10.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3432" for this suite.
Apr  9 07:35:16.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:35:17.443: INFO: namespace secrets-3432 deletion completed in 7.089283998s
•SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:35:17.443: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr  9 07:35:17.785: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr  9 07:35:17.785: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:20.224: INFO: stderr: ""
Apr  9 07:35:20.224: INFO: stdout: "service/redis-slave created\n"
Apr  9 07:35:20.224: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr  9 07:35:20.224: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:20.598: INFO: stderr: ""
Apr  9 07:35:20.598: INFO: stdout: "service/redis-master created\n"
Apr  9 07:35:20.598: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr  9 07:35:20.598: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:20.938: INFO: stderr: ""
Apr  9 07:35:20.938: INFO: stdout: "service/frontend created\n"
Apr  9 07:35:20.938: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr  9 07:35:20.939: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:21.299: INFO: stderr: ""
Apr  9 07:35:21.299: INFO: stdout: "deployment.apps/frontend created\n"
Apr  9 07:35:21.299: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  9 07:35:21.299: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:21.628: INFO: stderr: ""
Apr  9 07:35:21.628: INFO: stdout: "deployment.apps/redis-master created\n"
Apr  9 07:35:21.628: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr  9 07:35:21.628: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2867'
Apr  9 07:35:22.015: INFO: stderr: ""
Apr  9 07:35:22.015: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr  9 07:35:22.015: INFO: Waiting for all frontend pods to be Running.
Apr  9 07:35:47.069: INFO: Waiting for frontend to serve content.
Apr  9 07:35:47.190: INFO: Trying to add a new entry to the guestbook.
Apr  9 07:35:47.238: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr  9 07:35:47.356: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:47.637: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:47.638: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 07:35:47.638: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:47.879: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:47.879: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 07:35:47.879: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:48.123: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:48.123: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 07:35:48.123: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:48.375: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:48.375: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 07:35:48.375: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:48.609: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:48.609: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 07:35:48.609: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2867'
Apr  9 07:35:48.853: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 07:35:48.853: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:35:48.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2867" for this suite.
Apr  9 07:36:34.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:36:35.971: INFO: namespace kubectl-2867 deletion completed in 47.089281271s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:36:35.972: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:36:36.334: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr  9 07:36:41.363: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 07:36:41.363: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 07:36:43.584: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-6859,SelfLink:/apis/apps/v1/namespaces/deployment-6859/deployments/test-cleanup-deployment,UID:37828d80-5a9a-11e9-afbf-1e9604a4a829,ResourceVersion:13249,Generation:1,CreationTimestamp:2019-04-09 07:36:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-09 07:36:41 +0000 UTC 2019-04-09 07:36:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-09 07:36:43 +0000 UTC 2019-04-09 07:36:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr  9 07:36:43.612: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-6859,SelfLink:/apis/apps/v1/namespaces/deployment-6859/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:3783776e-5a9a-11e9-afbf-1e9604a4a829,ResourceVersion:13242,Generation:1,CreationTimestamp:2019-04-09 07:36:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 37828d80-5a9a-11e9-afbf-1e9604a4a829 0xc002347b97 0xc002347b98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 07:36:43.640: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-wlb2k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-wlb2k,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-6859,SelfLink:/api/v1/namespaces/deployment-6859/pods/test-cleanup-deployment-55cbfbc8f5-wlb2k,UID:3783b65b-5a9a-11e9-afbf-1e9604a4a829,ResourceVersion:13241,Generation:0,CreationTimestamp:2019-04-09 07:36:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.153/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 3783776e-5a9a-11e9-afbf-1e9604a4a829 0xc001a9e147 0xc001a9e148}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pftzx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pftzx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pftzx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a9e1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a9e1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:36:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:36:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:36:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:36:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.153,StartTime:2019-04-09 07:36:41 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-09 07:36:42 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e970a4209744bd8832481d513ef95d82737774b1c402cde1142c30fe71032efb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:36:43.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6859" for this suite.
Apr  9 07:36:49.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:36:50.755: INFO: namespace deployment-6859 deletion completed in 7.087598386s
•SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:36:50.756: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:36:51.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32" in namespace "downward-api-2945" to be "success or failure"
Apr  9 07:36:51.137: INFO: Pod "downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.707079ms
Apr  9 07:36:53.166: INFO: Pod "downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057409902s
STEP: Saw pod success
Apr  9 07:36:53.166: INFO: Pod "downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:36:53.194: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:36:53.256: INFO: Waiting for pod downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:36:53.283: INFO: Pod downwardapi-volume-3d434207-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:36:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2945" for this suite.
Apr  9 07:36:59.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:37:00.407: INFO: namespace downward-api-2945 deletion completed in 7.096059649s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:37:00.407: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  9 07:37:00.708: INFO: Waiting up to 5m0s for pod "pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32" in namespace "emptydir-8016" to be "success or failure"
Apr  9 07:37:00.735: INFO: Pod "pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 26.97833ms
Apr  9 07:37:02.763: INFO: Pod "pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055148275s
STEP: Saw pod success
Apr  9 07:37:02.763: INFO: Pod "pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:37:02.798: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:37:02.861: INFO: Waiting for pod pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:37:02.888: INFO: Pod pod-42fc2911-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:37:02.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8016" for this suite.
Apr  9 07:37:08.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:37:10.008: INFO: namespace emptydir-8016 deletion completed in 7.092073856s
•SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:37:10.008: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-48b5049b-5a9a-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:37:10.337: INFO: Waiting up to 5m0s for pod "pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32" in namespace "configmap-933" to be "success or failure"
Apr  9 07:37:10.364: INFO: Pod "pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.357003ms
Apr  9 07:37:12.393: INFO: Pod "pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056218055s
STEP: Saw pod success
Apr  9 07:37:12.393: INFO: Pod "pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:37:12.421: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:37:12.485: INFO: Waiting for pod pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:37:12.512: INFO: Pod pod-configmaps-48b952d8-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:37:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-933" for this suite.
Apr  9 07:37:18.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:37:19.634: INFO: namespace configmap-933 deletion completed in 7.093308031s
•S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:37:19.634: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5678
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-4e81573b-5a9a-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4e81573b-5a9a-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:37:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5678" for this suite.
Apr  9 07:37:46.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:37:47.393: INFO: namespace configmap-5678 deletion completed in 23.087586989s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:37:47.394: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:37:49.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2810" for this suite.
Apr  9 07:38:35.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:38:36.940: INFO: namespace kubelet-test-2810 deletion completed in 47.090960672s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:38:36.941: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7c900835-5a9a-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:38:37.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32" in namespace "configmap-3449" to be "success or failure"
Apr  9 07:38:37.362: INFO: Pod "pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.270314ms
Apr  9 07:38:39.392: INFO: Pod "pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057362865s
STEP: Saw pod success
Apr  9 07:38:39.392: INFO: Pod "pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:38:39.420: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:38:39.599: INFO: Waiting for pod pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:38:39.626: INFO: Pod pod-configmaps-7c942aac-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:38:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3449" for this suite.
Apr  9 07:38:45.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:38:46.742: INFO: namespace configmap-3449 deletion completed in 7.087011915s
•SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:38:46.742: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  9 07:38:47.107: INFO: Waiting up to 5m0s for pod "pod-82674551-5a9a-11e9-8c35-36e6c88ddb32" in namespace "emptydir-1296" to be "success or failure"
Apr  9 07:38:47.135: INFO: Pod "pod-82674551-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.236598ms
Apr  9 07:38:49.162: INFO: Pod "pod-82674551-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055082305s
STEP: Saw pod success
Apr  9 07:38:49.162: INFO: Pod "pod-82674551-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:38:49.190: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-82674551-5a9a-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:38:49.292: INFO: Waiting for pod pod-82674551-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:38:49.319: INFO: Pod pod-82674551-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:38:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1296" for this suite.
Apr  9 07:38:55.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:38:56.439: INFO: namespace emptydir-1296 deletion completed in 7.091756315s
•S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:38:56.439: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0409 07:39:07.011938    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 07:39:07.012: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:39:07.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9221" for this suite.
Apr  9 07:39:13.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:39:14.107: INFO: namespace gc-9221 deletion completed in 7.067523165s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:39:14.107: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-92ac7b9c-5a9a-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:39:14.433: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32" in namespace "projected-9994" to be "success or failure"
Apr  9 07:39:14.461: INFO: Pod "pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.734886ms
Apr  9 07:39:16.489: INFO: Pod "pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055758662s
STEP: Saw pod success
Apr  9 07:39:16.489: INFO: Pod "pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:39:16.516: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:39:16.580: INFO: Waiting for pod pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:39:16.607: INFO: Pod pod-projected-configmaps-92b0b92c-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:39:16.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9994" for this suite.
Apr  9 07:39:22.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:39:23.736: INFO: namespace projected-9994 deletion completed in 7.100942626s
•S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:39:23.736: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:39:24.077: INFO: Creating ReplicaSet my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32
Apr  9 07:39:24.148: INFO: Pod name my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32: Found 1 pods out of 1
Apr  9 07:39:24.148: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32" is running
Apr  9 07:39:26.221: INFO: Pod "my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32-srrfq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:39:24 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:39:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:39:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 07:39:24 +0000 UTC Reason: Message:}])
Apr  9 07:39:26.221: INFO: Trying to dial the pod
Apr  9 07:39:31.393: INFO: Controller my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32: Got expected result from replica 1 [my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32-srrfq]: "my-hostname-basic-9874de79-5a9a-11e9-8c35-36e6c88ddb32-srrfq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:39:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7560" for this suite.
Apr  9 07:39:37.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:39:38.506: INFO: namespace replicaset-7560 deletion completed in 7.085110534s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:39:38.507: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  9 07:39:43.074: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:43.101: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:45.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:45.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:47.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:47.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:49.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:49.132: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:51.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:51.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:53.103: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:53.137: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:55.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:55.129: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:57.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:57.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:39:59.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:39:59.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:01.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:01.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:03.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:03.131: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:05.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:05.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:07.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:07.130: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:09.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:09.129: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:11.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:11.129: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 07:40:13.102: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 07:40:13.129: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:40:13.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-382" for this suite.
Apr  9 07:40:35.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:40:36.252: INFO: namespace container-lifecycle-hook-382 deletion completed in 23.094971949s
•SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:40:36.253: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 07:40:36.592: INFO: PodSpec: initContainers in spec.initContainers
Apr  9 07:41:19.353: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c3adddb4-5a9a-11e9-8c35-36e6c88ddb32", GenerateName:"", Namespace:"init-container-7350", SelfLink:"/api/v1/namespaces/init-container-7350/pods/pod-init-c3adddb4-5a9a-11e9-8c35-36e6c88ddb32", UID:"c3bb009b-5a9a-11e9-afbf-1e9604a4a829", ResourceVersion:"14089", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690392436, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"592585711"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.167/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9n764", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020d8ec0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9n764", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9n764", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9n764", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002732b68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-30-1.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00287e3c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002732c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002732dc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002732dc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002732dcc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690392436, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690392436, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690392436, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690392436, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.30.1", PodIP:"100.96.1.167", StartTime:(*v1.Time)(0xc002e43ca0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c54bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c54c40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://1fc6bd976eed89ad5f42bd59260bf42c1b7dc7d2b58d21e6ba585caf467b1f8a"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002e43ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002e43cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:41:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7350" for this suite.
Apr  9 07:41:41.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:41:42.474: INFO: namespace init-container-7350 deletion completed in 23.092245737s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:41:42.474: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-eb21441a-5a9a-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:41:42.836: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32" in namespace "projected-4258" to be "success or failure"
Apr  9 07:41:42.863: INFO: Pod "pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.148242ms
Apr  9 07:41:44.891: INFO: Pod "pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055035657s
STEP: Saw pod success
Apr  9 07:41:44.891: INFO: Pod "pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:41:44.919: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:41:44.982: INFO: Waiting for pod pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:41:45.009: INFO: Pod pod-projected-secrets-eb256f10-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:41:45.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4258" for this suite.
Apr  9 07:41:51.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:41:52.130: INFO: namespace projected-4258 deletion completed in 7.092616824s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:41:52.132: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:41:52.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32" in namespace "downward-api-7854" to be "success or failure"
Apr  9 07:41:52.453: INFO: Pod "downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.439384ms
Apr  9 07:41:54.481: INFO: Pod "downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055434509s
STEP: Saw pod success
Apr  9 07:41:54.481: INFO: Pod "downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:41:54.509: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:41:54.573: INFO: Waiting for pod downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:41:54.600: INFO: Pod downwardapi-volume-f0dc9459-5a9a-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:41:54.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7854" for this suite.
Apr  9 07:42:00.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:42:01.770: INFO: namespace downward-api-7854 deletion completed in 7.141871237s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:42:01.770: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:42:02.097: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:42:04.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5712" for this suite.
Apr  9 07:42:44.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:42:45.506: INFO: namespace pods-5712 deletion completed in 41.086643933s
•S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:42:45.506: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-10ae0b4c-5a9b-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:42:45.835: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32" in namespace "projected-300" to be "success or failure"
Apr  9 07:42:45.862: INFO: Pod "pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.631062ms
Apr  9 07:42:47.890: INFO: Pod "pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055612854s
STEP: Saw pod success
Apr  9 07:42:47.890: INFO: Pod "pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:42:47.918: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 07:42:47.982: INFO: Waiting for pod pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:42:48.009: INFO: Pod pod-projected-configmaps-10b2336e-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:42:48.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-300" for this suite.
Apr  9 07:42:54.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:42:55.134: INFO: namespace projected-300 deletion completed in 7.096391876s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:42:55.134: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-l75f
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 07:42:55.484: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l75f" in namespace "subpath-9244" to be "success or failure"
Apr  9 07:42:55.511: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Pending", Reason="", readiness=false. Elapsed: 27.237118ms
Apr  9 07:42:57.539: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 2.055654327s
Apr  9 07:42:59.626: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 4.142219557s
Apr  9 07:43:01.654: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 6.170475686s
Apr  9 07:43:03.682: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 8.198658117s
Apr  9 07:43:05.710: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 10.226777397s
Apr  9 07:43:07.739: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 12.254979443s
Apr  9 07:43:09.767: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 14.282868842s
Apr  9 07:43:11.794: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 16.310541045s
Apr  9 07:43:13.823: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 18.339090489s
Apr  9 07:43:15.851: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Running", Reason="", readiness=true. Elapsed: 20.367316152s
Apr  9 07:43:17.879: INFO: Pod "pod-subpath-test-configmap-l75f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.395271828s
STEP: Saw pod success
Apr  9 07:43:17.879: INFO: Pod "pod-subpath-test-configmap-l75f" satisfied condition "success or failure"
Apr  9 07:43:17.906: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-subpath-test-configmap-l75f container test-container-subpath-configmap-l75f: <nil>
STEP: delete the pod
Apr  9 07:43:17.971: INFO: Waiting for pod pod-subpath-test-configmap-l75f to disappear
Apr  9 07:43:17.998: INFO: Pod pod-subpath-test-configmap-l75f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l75f
Apr  9 07:43:17.998: INFO: Deleting pod "pod-subpath-test-configmap-l75f" in namespace "subpath-9244"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:43:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9244" for this suite.
Apr  9 07:43:24.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:43:25.468: INFO: namespace subpath-9244 deletion completed in 7.414270894s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:43:25.469: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  9 07:43:25.813: INFO: Waiting up to 5m0s for pod "pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32" in namespace "emptydir-7914" to be "success or failure"
Apr  9 07:43:25.841: INFO: Pod "pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.553069ms
Apr  9 07:43:27.869: INFO: Pod "pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055649292s
STEP: Saw pod success
Apr  9 07:43:27.869: INFO: Pod "pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:43:27.896: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:43:27.960: INFO: Waiting for pod pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:43:27.987: INFO: Pod pod-28865c5f-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:43:27.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7914" for this suite.
Apr  9 07:43:34.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:43:35.661: INFO: namespace emptydir-7914 deletion completed in 7.646185004s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:43:35.662: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 07:43:36.210: INFO: Number of nodes with available pods: 0
Apr  9 07:43:36.210: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:37.272: INFO: Number of nodes with available pods: 1
Apr  9 07:43:37.272: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:38.266: INFO: Number of nodes with available pods: 2
Apr  9 07:43:38.267: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr  9 07:43:38.405: INFO: Number of nodes with available pods: 1
Apr  9 07:43:38.405: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:39.460: INFO: Number of nodes with available pods: 1
Apr  9 07:43:39.460: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:40.460: INFO: Number of nodes with available pods: 1
Apr  9 07:43:40.460: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:41.460: INFO: Number of nodes with available pods: 1
Apr  9 07:43:41.460: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:42.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:42.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:43.460: INFO: Number of nodes with available pods: 1
Apr  9 07:43:43.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:44.462: INFO: Number of nodes with available pods: 1
Apr  9 07:43:44.462: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:45.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:45.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:46.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:46.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:47.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:47.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:48.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:48.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:49.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:49.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:50.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:50.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:51.460: INFO: Number of nodes with available pods: 1
Apr  9 07:43:51.460: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:52.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:52.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:53.461: INFO: Number of nodes with available pods: 1
Apr  9 07:43:53.461: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:43:54.461: INFO: Number of nodes with available pods: 2
Apr  9 07:43:54.461: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1804, will wait for the garbage collector to delete the pods
Apr  9 07:43:54.595: INFO: Deleting DaemonSet.extensions daemon-set took: 29.583799ms
Apr  9 07:43:54.696: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.431203ms
Apr  9 07:44:01.925: INFO: Number of nodes with available pods: 0
Apr  9 07:44:01.925: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 07:44:01.954: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1804/daemonsets","resourceVersion":"14596"},"items":null}

Apr  9 07:44:01.982: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1804/pods","resourceVersion":"14596"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:44:02.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1804" for this suite.
Apr  9 07:44:08.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:44:09.186: INFO: namespace daemonsets-1804 deletion completed in 7.090926812s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:44:09.187: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr  9 07:44:11.618: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-4291e511-5a9b-11e9-8c35-36e6c88ddb32,GenerateName:,Namespace:events-8371,SelfLink:/api/v1/namespaces/events-8371/pods/send-events-4291e511-5a9b-11e9-8c35-36e6c88ddb32,UID:42938b8a-5a9b-11e9-afbf-1e9604a4a829,ResourceVersion:14631,Generation:0,CreationTimestamp:2019-04-09 07:44:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 479916630,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.176/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kwrbb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kwrbb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-kwrbb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002baaa30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002baaa50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:44:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:44:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:44:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 07:44:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.176,StartTime:2019-04-09 07:44:09 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-09 07:44:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://d48602cff7614839df51a72c23b4bf8acf965e228e5ba4312dc289b1ff527a3e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr  9 07:44:13.644: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr  9 07:44:15.670: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:44:15.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8371" for this suite.
Apr  9 07:44:53.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:44:54.759: INFO: namespace events-8371 deletion completed in 39.035632608s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:44:54.760: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-5416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr  9 07:44:55.038: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5416" to be "success or failure"
Apr  9 07:44:55.064: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 25.326404ms
Apr  9 07:44:57.090: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051602104s
STEP: Saw pod success
Apr  9 07:44:57.090: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr  9 07:44:57.115: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr  9 07:44:57.183: INFO: Waiting for pod pod-host-path-test to disappear
Apr  9 07:44:57.209: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:44:57.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5416" for this suite.
Apr  9 07:45:03.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:45:04.246: INFO: namespace hostpath-5416 deletion completed in 7.011467049s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:45:04.246: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 07:45:04.575: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5967'
Apr  9 07:45:04.837: INFO: stderr: ""
Apr  9 07:45:04.837: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr  9 07:45:04.863: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-5967'
Apr  9 07:45:11.838: INFO: stderr: ""
Apr  9 07:45:11.838: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:45:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5967" for this suite.
Apr  9 07:45:17.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:45:18.944: INFO: namespace kubectl-5967 deletion completed in 7.079120287s
•SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:45:18.944: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:45:19.274: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:45:26.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2788" for this suite.
Apr  9 07:46:04.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:46:05.727: INFO: namespace pods-2788 deletion completed in 39.008391569s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:46:05.727: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hf8bf in namespace proxy-1953
I0409 07:46:06.036211    6827 runners.go:184] Created replication controller with name: proxy-service-hf8bf, namespace: proxy-1953, replica count: 1
I0409 07:46:07.086734    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 07:46:08.087246    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 07:46:09.087492    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:10.087827    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:11.088143    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:12.088383    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:13.088618    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:14.088832    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:15.089079    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:16.089322    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 07:46:17.089662    6827 runners.go:184] proxy-service-hf8bf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  9 07:46:17.116: INFO: setup took 11.1348356s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr  9 07:46:17.165: INFO: (0) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 48.374424ms)
Apr  9 07:46:17.166: INFO: (0) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 49.258361ms)
Apr  9 07:46:17.166: INFO: (0) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 49.343044ms)
Apr  9 07:46:17.170: INFO: (0) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 54.171656ms)
Apr  9 07:46:17.170: INFO: (0) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 54.070078ms)
Apr  9 07:46:17.171: INFO: (0) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 54.984491ms)
Apr  9 07:46:17.171: INFO: (0) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 55.043854ms)
Apr  9 07:46:17.171: INFO: (0) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 55.155814ms)
Apr  9 07:46:17.183: INFO: (0) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 66.383888ms)
Apr  9 07:46:17.183: INFO: (0) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 66.569794ms)
Apr  9 07:46:17.183: INFO: (0) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 67.15701ms)
Apr  9 07:46:17.185: INFO: (0) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 68.677609ms)
Apr  9 07:46:17.186: INFO: (0) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 69.734578ms)
Apr  9 07:46:17.187: INFO: (0) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 70.960717ms)
Apr  9 07:46:17.188: INFO: (0) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 71.979ms)
Apr  9 07:46:17.190: INFO: (0) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 73.1655ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.972145ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.879576ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.516226ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.473992ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.275067ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.722036ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.810651ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.288841ms)
Apr  9 07:46:17.218: INFO: (1) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 27.659688ms)
Apr  9 07:46:17.219: INFO: (1) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.550588ms)
Apr  9 07:46:17.219: INFO: (1) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 27.977022ms)
Apr  9 07:46:17.219: INFO: (1) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 29.228381ms)
Apr  9 07:46:17.220: INFO: (1) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.763476ms)
Apr  9 07:46:17.220: INFO: (1) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 29.686733ms)
Apr  9 07:46:17.220: INFO: (1) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 29.852279ms)
Apr  9 07:46:17.221: INFO: (1) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.669684ms)
Apr  9 07:46:17.249: INFO: (2) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.378967ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.588931ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.843036ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.11916ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.751194ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.423908ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 27.657114ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.025535ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.384897ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.463502ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 28.361795ms)
Apr  9 07:46:17.250: INFO: (2) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.276438ms)
Apr  9 07:46:17.252: INFO: (2) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.706691ms)
Apr  9 07:46:17.252: INFO: (2) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 29.610456ms)
Apr  9 07:46:17.252: INFO: (2) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 29.690416ms)
Apr  9 07:46:17.252: INFO: (2) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 30.061366ms)
Apr  9 07:46:17.280: INFO: (3) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.730748ms)
Apr  9 07:46:17.281: INFO: (3) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 28.625053ms)
Apr  9 07:46:17.281: INFO: (3) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.741115ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.783531ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 29.280448ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.120395ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 29.256228ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 29.162795ms)
Apr  9 07:46:17.282: INFO: (3) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 29.582557ms)
Apr  9 07:46:17.283: INFO: (3) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 30.463616ms)
Apr  9 07:46:17.283: INFO: (3) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 30.11608ms)
Apr  9 07:46:17.283: INFO: (3) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 29.860018ms)
Apr  9 07:46:17.284: INFO: (3) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 31.514105ms)
Apr  9 07:46:17.284: INFO: (3) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 31.415985ms)
Apr  9 07:46:17.285: INFO: (3) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 31.937352ms)
Apr  9 07:46:17.285: INFO: (3) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 32.353764ms)
Apr  9 07:46:17.312: INFO: (4) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 26.701152ms)
Apr  9 07:46:17.313: INFO: (4) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.494612ms)
Apr  9 07:46:17.313: INFO: (4) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.4235ms)
Apr  9 07:46:17.313: INFO: (4) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.827345ms)
Apr  9 07:46:17.313: INFO: (4) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.763523ms)
Apr  9 07:46:17.313: INFO: (4) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.703569ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.853122ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.640997ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.484208ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.0308ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.878359ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 27.865625ms)
Apr  9 07:46:17.314: INFO: (4) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 28.615191ms)
Apr  9 07:46:17.315: INFO: (4) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.187663ms)
Apr  9 07:46:17.316: INFO: (4) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 30.522467ms)
Apr  9 07:46:17.316: INFO: (4) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.36904ms)
Apr  9 07:46:17.343: INFO: (5) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.213157ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.24738ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.245935ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.131232ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.111287ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.106117ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 28.112846ms)
Apr  9 07:46:17.344: INFO: (5) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.098965ms)
Apr  9 07:46:17.345: INFO: (5) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.178613ms)
Apr  9 07:46:17.345: INFO: (5) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.120484ms)
Apr  9 07:46:17.346: INFO: (5) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.25412ms)
Apr  9 07:46:17.346: INFO: (5) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 29.210954ms)
Apr  9 07:46:17.346: INFO: (5) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 29.766173ms)
Apr  9 07:46:17.347: INFO: (5) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 30.783149ms)
Apr  9 07:46:17.348: INFO: (5) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 31.425939ms)
Apr  9 07:46:17.348: INFO: (5) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 31.463939ms)
Apr  9 07:46:17.375: INFO: (6) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.396966ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.812012ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.933341ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.063007ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.038033ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.033343ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.157253ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.011632ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.218793ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.142042ms)
Apr  9 07:46:17.376: INFO: (6) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.167186ms)
Apr  9 07:46:17.377: INFO: (6) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.023401ms)
Apr  9 07:46:17.378: INFO: (6) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 29.638969ms)
Apr  9 07:46:17.378: INFO: (6) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 29.673137ms)
Apr  9 07:46:17.379: INFO: (6) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.6231ms)
Apr  9 07:46:17.379: INFO: (6) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 30.636766ms)
Apr  9 07:46:17.406: INFO: (7) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.42425ms)
Apr  9 07:46:17.406: INFO: (7) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.81597ms)
Apr  9 07:46:17.406: INFO: (7) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.682042ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.568222ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.843323ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 27.890574ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 28.097954ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.96371ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 27.724307ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 27.76218ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.926141ms)
Apr  9 07:46:17.407: INFO: (7) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.906851ms)
Apr  9 07:46:17.408: INFO: (7) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 29.434525ms)
Apr  9 07:46:17.409: INFO: (7) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 30.076819ms)
Apr  9 07:46:17.410: INFO: (7) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.916985ms)
Apr  9 07:46:17.410: INFO: (7) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 31.190791ms)
Apr  9 07:46:17.443: INFO: (8) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 33.158173ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 33.360444ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 33.285742ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 33.281452ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 33.370652ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 33.341978ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 33.362204ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 33.376087ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 33.310374ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 33.357501ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 33.34077ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 33.471608ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 33.394066ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 33.509156ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 33.387208ms)
Apr  9 07:46:17.444: INFO: (8) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 33.389324ms)
Apr  9 07:46:17.471: INFO: (9) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.149671ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.508184ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.639891ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 27.541443ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.771842ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.909453ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.775618ms)
Apr  9 07:46:17.472: INFO: (9) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.090899ms)
Apr  9 07:46:17.473: INFO: (9) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.905479ms)
Apr  9 07:46:17.473: INFO: (9) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.915759ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 34.5688ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 35.039867ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 34.65029ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 34.898704ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 34.68372ms)
Apr  9 07:46:17.479: INFO: (9) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 34.718875ms)
Apr  9 07:46:17.507: INFO: (10) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.07809ms)
Apr  9 07:46:17.507: INFO: (10) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.387665ms)
Apr  9 07:46:17.507: INFO: (10) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.063749ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.335653ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 28.352808ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.387478ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.48841ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.126235ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 29.256868ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 29.086344ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 29.101994ms)
Apr  9 07:46:17.508: INFO: (10) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 29.199121ms)
Apr  9 07:46:17.509: INFO: (10) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 29.994898ms)
Apr  9 07:46:17.509: INFO: (10) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 30.165991ms)
Apr  9 07:46:17.511: INFO: (10) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 31.503702ms)
Apr  9 07:46:17.511: INFO: (10) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 31.64699ms)
Apr  9 07:46:17.539: INFO: (11) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.065629ms)
Apr  9 07:46:17.539: INFO: (11) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.137925ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.461198ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.532976ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.540763ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.579502ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.715274ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.855766ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 29.053026ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 29.027762ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.982261ms)
Apr  9 07:46:17.540: INFO: (11) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 29.190381ms)
Apr  9 07:46:17.541: INFO: (11) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 30.0434ms)
Apr  9 07:46:17.541: INFO: (11) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 30.13723ms)
Apr  9 07:46:17.542: INFO: (11) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.81389ms)
Apr  9 07:46:17.542: INFO: (11) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 31.049228ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.358206ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.420468ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.922043ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 28.415316ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.555474ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.503563ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 28.647091ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.508388ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.925083ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.854393ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 29.128233ms)
Apr  9 07:46:17.571: INFO: (12) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.799979ms)
Apr  9 07:46:17.573: INFO: (12) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 30.406587ms)
Apr  9 07:46:17.574: INFO: (12) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 31.581652ms)
Apr  9 07:46:17.575: INFO: (12) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 32.128653ms)
Apr  9 07:46:17.575: INFO: (12) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 31.810244ms)
Apr  9 07:46:17.603: INFO: (13) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.09281ms)
Apr  9 07:46:17.603: INFO: (13) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.159683ms)
Apr  9 07:46:17.603: INFO: (13) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 28.084093ms)
Apr  9 07:46:17.603: INFO: (13) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.192497ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.681395ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.518858ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.995665ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 29.053112ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 28.955877ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.920553ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 29.006133ms)
Apr  9 07:46:17.604: INFO: (13) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.183439ms)
Apr  9 07:46:17.605: INFO: (13) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 29.362558ms)
Apr  9 07:46:17.605: INFO: (13) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.757501ms)
Apr  9 07:46:17.606: INFO: (13) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 31.004185ms)
Apr  9 07:46:17.607: INFO: (13) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 32.091708ms)
Apr  9 07:46:17.635: INFO: (14) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.881066ms)
Apr  9 07:46:17.635: INFO: (14) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.797593ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 28.175604ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 28.309839ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.246889ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.259975ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 28.253794ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.335097ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.399713ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.368413ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.504502ms)
Apr  9 07:46:17.636: INFO: (14) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 28.443465ms)
Apr  9 07:46:17.638: INFO: (14) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 30.096177ms)
Apr  9 07:46:17.638: INFO: (14) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 30.152437ms)
Apr  9 07:46:17.639: INFO: (14) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 31.182009ms)
Apr  9 07:46:17.639: INFO: (14) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 31.2237ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.630134ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.780937ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.932303ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.777872ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.728937ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 27.772207ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.926731ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.857028ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.975472ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.962104ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.303715ms)
Apr  9 07:46:17.667: INFO: (15) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.645674ms)
Apr  9 07:46:17.669: INFO: (15) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 29.809536ms)
Apr  9 07:46:17.669: INFO: (15) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 29.605018ms)
Apr  9 07:46:17.669: INFO: (15) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 30.567529ms)
Apr  9 07:46:17.669: INFO: (15) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.426073ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 29.992929ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.955639ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 29.89776ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 29.864914ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 30.095364ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 30.190862ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 30.245026ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 30.235567ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 30.696606ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 30.571047ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 30.66871ms)
Apr  9 07:46:17.700: INFO: (16) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 30.8035ms)
Apr  9 07:46:17.701: INFO: (16) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 31.436198ms)
Apr  9 07:46:17.701: INFO: (16) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 31.700241ms)
Apr  9 07:46:17.703: INFO: (16) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 32.84186ms)
Apr  9 07:46:17.703: INFO: (16) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 32.789456ms)
Apr  9 07:46:17.730: INFO: (17) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.040527ms)
Apr  9 07:46:17.730: INFO: (17) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.627643ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.739361ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 27.647465ms)
Apr  9 07:46:17.730: INFO: (17) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.729404ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.786721ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.799906ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 27.709338ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.815086ms)
Apr  9 07:46:17.731: INFO: (17) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 28.67927ms)
Apr  9 07:46:17.732: INFO: (17) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.639521ms)
Apr  9 07:46:17.732: INFO: (17) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.701176ms)
Apr  9 07:46:17.732: INFO: (17) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 29.492133ms)
Apr  9 07:46:17.732: INFO: (17) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.471995ms)
Apr  9 07:46:17.732: INFO: (17) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 29.630097ms)
Apr  9 07:46:17.733: INFO: (17) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 30.634364ms)
Apr  9 07:46:17.761: INFO: (18) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.17676ms)
Apr  9 07:46:17.761: INFO: (18) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.592044ms)
Apr  9 07:46:17.761: INFO: (18) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.297066ms)
Apr  9 07:46:17.761: INFO: (18) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 27.384402ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 28.535703ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.305517ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 28.604524ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 28.58645ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 28.569803ms)
Apr  9 07:46:17.762: INFO: (18) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 28.707349ms)
Apr  9 07:46:17.763: INFO: (18) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 28.985276ms)
Apr  9 07:46:17.763: INFO: (18) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 29.041411ms)
Apr  9 07:46:17.764: INFO: (18) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 30.328772ms)
Apr  9 07:46:17.764: INFO: (18) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 30.470357ms)
Apr  9 07:46:17.765: INFO: (18) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 31.462801ms)
Apr  9 07:46:17.765: INFO: (18) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 31.225903ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:162/proxy/: bar (200; 27.868906ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:1080/proxy/rewriteme">test<... (200; 27.723763ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:160/proxy/: foo (200; 27.386768ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:443/proxy/tlsrewritem... (200; 27.594234ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname2/proxy/: bar (200; 28.219688ms)
Apr  9 07:46:17.793: INFO: (19) /api/v1/namespaces/proxy-1953/services/proxy-service-hf8bf:portname1/proxy/: foo (200; 27.91685ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:462/proxy/: tls qux (200; 27.631434ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:1080/proxy/rewriteme">... (200; 28.076687ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/: <a href="/api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k/proxy/rewriteme">test</a> (200; 27.846677ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/pods/https:proxy-service-hf8bf-l695k:460/proxy/: tls baz (200; 28.251649ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname2/proxy/: tls qux (200; 27.561548ms)
Apr  9 07:46:17.794: INFO: (19) /api/v1/namespaces/proxy-1953/services/https:proxy-service-hf8bf:tlsportname1/proxy/: tls baz (200; 27.872312ms)
Apr  9 07:46:17.795: INFO: (19) /api/v1/namespaces/proxy-1953/pods/proxy-service-hf8bf-l695k:162/proxy/: bar (200; 29.083879ms)
Apr  9 07:46:17.795: INFO: (19) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname2/proxy/: bar (200; 28.688995ms)
Apr  9 07:46:17.796: INFO: (19) /api/v1/namespaces/proxy-1953/pods/http:proxy-service-hf8bf-l695k:160/proxy/: foo (200; 29.523287ms)
Apr  9 07:46:17.796: INFO: (19) /api/v1/namespaces/proxy-1953/services/http:proxy-service-hf8bf:portname1/proxy/: foo (200; 30.0663ms)
STEP: deleting ReplicationController proxy-service-hf8bf in namespace proxy-1953, will wait for the garbage collector to delete the pods
Apr  9 07:46:17.898: INFO: Deleting ReplicationController proxy-service-hf8bf took: 26.67855ms
Apr  9 07:46:17.998: INFO: Terminating ReplicationController proxy-service-hf8bf pods took: 100.295259ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:46:19.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1953" for this suite.
Apr  9 07:46:26.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:46:27.026: INFO: namespace proxy-1953 deletion completed in 7.001916729s
•S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:46:27.026: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 07:46:27.279: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 07:46:27.330: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 07:46:27.355: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-14-41.eu-west-1.compute.internal before test
Apr  9 07:46:27.390: INFO: vpn-shoot-77d7f4479f-6ss5n from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr  9 07:46:27.390: INFO: addons-kube2iam-dkzn8 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:46:27.390: INFO: addons-kubernetes-dashboard-665df4b66d-8rnhz from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  9 07:46:27.390: INFO: metrics-server-845bbc9978-plq7d from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container metrics-server ready: true, restart count 0
Apr  9 07:46:27.390: INFO: kube-proxy-m5x28 from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  9 07:46:27.390: INFO: node-exporter-zntwd from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container node-exporter ready: true, restart count 0
Apr  9 07:46:27.390: INFO: coredns-7f7f7978c8-fg8rc from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:46:27.390: INFO: blackbox-exporter-6dc58dcffc-4zzfb from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr  9 07:46:27.390: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-t75n5 from kube-system started at 2019-04-09 06:35:39 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr  9 07:46:27.390: INFO: calico-node-ds2qx from kube-system started at 2019-04-09 06:35:19 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:46:27.390: INFO: addons-nginx-ingress-controller-f88658d78-sg5bg from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr  9 07:46:27.390: INFO: coredns-7f7f7978c8-96z5b from kube-system started at 2019-04-09 06:35:40 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.390: INFO: 	Container coredns ready: true, restart count 0
Apr  9 07:46:27.390: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-30-1.eu-west-1.compute.internal before test
Apr  9 07:46:27.434: INFO: calico-node-8wb77 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.434: INFO: 	Container calico-node ready: true, restart count 0
Apr  9 07:46:27.434: INFO: addons-kube2iam-tdsnm from kube-system started at 2019-04-09 06:35:41 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.434: INFO: 	Container kube2iam ready: true, restart count 0
Apr  9 07:46:27.434: INFO: kube-proxy-g7h72 from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.434: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  9 07:46:27.434: INFO: node-exporter-m5mnx from kube-system started at 2019-04-09 06:35:22 +0000 UTC (1 container statuses recorded)
Apr  9 07:46:27.434: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-96115df7-5a9b-11e9-8c35-36e6c88ddb32 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-96115df7-5a9b-11e9-8c35-36e6c88ddb32 off the node ip-10-250-30-1.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-96115df7-5a9b-11e9-8c35-36e6c88ddb32
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:46:31.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6120" for this suite.
Apr  9 07:46:43.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:46:44.826: INFO: namespace sched-pred-6120 deletion completed in 13.000752327s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:46:44.826: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 07:46:45.106: INFO: Waiting up to 5m0s for pod "downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32" in namespace "downward-api-2846" to be "success or failure"
Apr  9 07:46:45.132: INFO: Pod "downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.413323ms
Apr  9 07:46:47.158: INFO: Pod "downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052001093s
STEP: Saw pod success
Apr  9 07:46:47.158: INFO: Pod "downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:46:47.183: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:46:47.244: INFO: Waiting for pod downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:46:47.269: INFO: Pod downward-api-9f508a5c-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:46:47.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2846" for this suite.
Apr  9 07:46:53.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:46:54.331: INFO: namespace downward-api-2846 deletion completed in 7.036632575s
•SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:46:54.331: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-a4fabaae-5a9b-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:46:54.635: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32" in namespace "projected-4719" to be "success or failure"
Apr  9 07:46:54.660: INFO: Pod "pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.010182ms
Apr  9 07:46:56.686: INFO: Pod "pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050946965s
STEP: Saw pod success
Apr  9 07:46:56.686: INFO: Pod "pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:46:56.711: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:46:56.780: INFO: Waiting for pod pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:46:56.805: INFO: Pod pod-projected-secrets-a4fe8eed-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:46:56.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4719" for this suite.
Apr  9 07:47:02.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:47:03.838: INFO: namespace projected-4719 deletion completed in 7.007720878s
•SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:47:03.838: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4760.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4760.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4760.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4760.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 99.171.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.171.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.171.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.171.99_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4760.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4760.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4760.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4760.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4760.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4760.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 99.171.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.171.99_udp@PTR;check="$$(dig +tcp +noall +answer +search 99.171.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.171.99_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 07:47:06.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:06.487: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:06.514: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:06.541: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:07.126: INFO: Unable to read jessie_udp@dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:07.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:07.181: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:07.208: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local from pod dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32: the server could not find the requested resource (get pods dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32)
Apr  9 07:47:07.711: INFO: Lookups using dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32 failed for: [wheezy_udp@dns-test-service.dns-4760.svc.cluster.local wheezy_tcp@dns-test-service.dns-4760.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local jessie_udp@dns-test-service.dns-4760.svc.cluster.local jessie_tcp@dns-test-service.dns-4760.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4760.svc.cluster.local]

Apr  9 07:47:14.337: INFO: DNS probes using dns-4760/dns-test-aab9de0c-5a9b-11e9-8c35-36e6c88ddb32 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:47:14.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4760" for this suite.
Apr  9 07:47:20.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:47:21.450: INFO: namespace dns-4760 deletion completed in 6.999738382s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:47:21.451: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4504
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 07:47:21.775: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 07:47:44.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.187:8080/dial?request=hostName&protocol=http&host=100.96.0.46&port=8080&tries=1'] Namespace:pod-network-test-4504 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:47:44.212: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:47:44.902: INFO: Waiting for endpoints: map[]
Apr  9 07:47:44.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.187:8080/dial?request=hostName&protocol=http&host=100.96.1.186&port=8080&tries=1'] Namespace:pod-network-test-4504 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:47:44.928: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:47:45.539: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:47:45.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4504" for this suite.
Apr  9 07:48:07.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:48:08.569: INFO: namespace pod-network-test-4504 deletion completed in 23.003746463s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:48:08.569: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3892.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3892.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3892.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3892.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3892.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3892.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 07:48:11.656: INFO: DNS probes using dns-3892/dns-test-d13a98ab-5a9b-11e9-8c35-36e6c88ddb32 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:48:11.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3892" for this suite.
Apr  9 07:48:17.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:48:18.722: INFO: namespace dns-3892 deletion completed in 7.011031149s
•SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:48:18.722: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:48:19.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6394" for this suite.
Apr  9 07:48:25.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:48:26.062: INFO: namespace kubelet-test-6394 deletion completed in 7.005759988s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:48:26.062: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-dbb0f2a0-5a9b-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:48:26.427: INFO: Waiting up to 5m0s for pod "pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32" in namespace "secrets-3886" to be "success or failure"
Apr  9 07:48:26.451: INFO: Pod "pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 24.823596ms
Apr  9 07:48:28.478: INFO: Pod "pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050983429s
STEP: Saw pod success
Apr  9 07:48:28.478: INFO: Pod "pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:48:28.503: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:48:28.565: INFO: Waiting for pod pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:48:28.590: INFO: Pod pod-secrets-dbb4beeb-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:48:28.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3886" for this suite.
Apr  9 07:48:34.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:48:35.617: INFO: namespace secrets-3886 deletion completed in 7.001060818s
•SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:48:35.618: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 07:48:36.076: INFO: Number of nodes with available pods: 0
Apr  9 07:48:36.076: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:48:37.127: INFO: Number of nodes with available pods: 0
Apr  9 07:48:37.127: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:48:38.128: INFO: Number of nodes with available pods: 2
Apr  9 07:48:38.128: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr  9 07:48:38.255: INFO: Number of nodes with available pods: 1
Apr  9 07:48:38.255: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:48:39.306: INFO: Number of nodes with available pods: 1
Apr  9 07:48:39.306: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:48:40.307: INFO: Number of nodes with available pods: 2
Apr  9 07:48:40.307: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8547, will wait for the garbage collector to delete the pods
Apr  9 07:48:40.458: INFO: Deleting DaemonSet.extensions daemon-set took: 26.186961ms
Apr  9 07:48:40.859: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.358337ms
Apr  9 07:48:51.884: INFO: Number of nodes with available pods: 0
Apr  9 07:48:51.884: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 07:48:51.909: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8547/daemonsets","resourceVersion":"15548"},"items":null}

Apr  9 07:48:51.934: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8547/pods","resourceVersion":"15548"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:48:52.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8547" for this suite.
Apr  9 07:48:58.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:48:59.041: INFO: namespace daemonsets-8547 deletion completed in 7.004040804s
•SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:48:59.042: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8214/configmap-test-ef5c0aba-5a9b-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:48:59.426: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32" in namespace "configmap-8214" to be "success or failure"
Apr  9 07:48:59.451: INFO: Pod "pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.220093ms
Apr  9 07:49:01.477: INFO: Pod "pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050836987s
STEP: Saw pod success
Apr  9 07:49:01.477: INFO: Pod "pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:49:01.502: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32 container env-test: <nil>
STEP: delete the pod
Apr  9 07:49:01.562: INFO: Waiting for pod pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:49:01.587: INFO: Pod pod-configmaps-ef5ff067-5a9b-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:49:01.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8214" for this suite.
Apr  9 07:49:07.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:49:08.616: INFO: namespace configmap-8214 deletion completed in 7.003314319s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:49:08.617: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7880
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-f5098f53-5a9b-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:49:11.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7880" for this suite.
Apr  9 07:49:33.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:49:34.182: INFO: namespace configmap-7880 deletion completed in 23.007306261s
•SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:49:34.182: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6498/configmap-test-0447f49e-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume configMaps
Apr  9 07:49:34.526: INFO: Waiting up to 5m0s for pod "pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32" in namespace "configmap-6498" to be "success or failure"
Apr  9 07:49:34.552: INFO: Pod "pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.339987ms
Apr  9 07:49:36.579: INFO: Pod "pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052257005s
STEP: Saw pod success
Apr  9 07:49:36.579: INFO: Pod "pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:49:36.604: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32 container env-test: <nil>
STEP: delete the pod
Apr  9 07:49:36.663: INFO: Waiting for pod pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:49:36.688: INFO: Pod pod-configmaps-044bcb97-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:49:36.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6498" for this suite.
Apr  9 07:49:42.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:49:43.712: INFO: namespace configmap-6498 deletion completed in 6.998381949s
•S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:49:43.712: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-09f195b2-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:49:44.025: INFO: Waiting up to 5m0s for pod "pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32" in namespace "secrets-8005" to be "success or failure"
Apr  9 07:49:44.050: INFO: Pod "pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.288814ms
Apr  9 07:49:46.076: INFO: Pod "pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050849251s
STEP: Saw pod success
Apr  9 07:49:46.076: INFO: Pod "pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:49:46.102: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:49:46.206: INFO: Waiting for pod pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:49:46.231: INFO: Pod pod-secrets-09f567c7-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:49:46.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8005" for this suite.
Apr  9 07:49:52.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:49:53.299: INFO: namespace secrets-8005 deletion completed in 7.042018194s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:49:53.299: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:49:53.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32" in namespace "projected-3530" to be "success or failure"
Apr  9 07:49:53.686: INFO: Pod "downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.369342ms
Apr  9 07:49:55.712: INFO: Pod "downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051404774s
STEP: Saw pod success
Apr  9 07:49:55.712: INFO: Pod "downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:49:55.737: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:49:55.797: INFO: Waiting for pod downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:49:55.822: INFO: Pod downwardapi-volume-0fb38797-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:49:55.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3530" for this suite.
Apr  9 07:50:01.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:50:02.877: INFO: namespace projected-3530 deletion completed in 7.028786711s
•SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:50:02.877: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9248
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr  9 07:50:03.203: INFO: Waiting up to 5m0s for pod "pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32" in namespace "emptydir-9248" to be "success or failure"
Apr  9 07:50:03.229: INFO: Pod "pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.980436ms
Apr  9 07:50:05.255: INFO: Pod "pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05166436s
STEP: Saw pod success
Apr  9 07:50:05.255: INFO: Pod "pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:50:05.280: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:50:05.342: INFO: Waiting for pod pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:50:05.367: INFO: Pod pod-1563afe2-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:50:05.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9248" for this suite.
Apr  9 07:50:11.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:50:12.392: INFO: namespace emptydir-9248 deletion completed in 6.999461965s
•SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:50:12.392: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-88lw
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 07:50:12.753: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-88lw" in namespace "subpath-2015" to be "success or failure"
Apr  9 07:50:12.778: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Pending", Reason="", readiness=false. Elapsed: 24.947743ms
Apr  9 07:50:14.804: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 2.051021955s
Apr  9 07:50:16.830: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 4.077234921s
Apr  9 07:50:18.856: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 6.103261872s
Apr  9 07:50:20.882: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 8.129303222s
Apr  9 07:50:22.908: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 10.155414769s
Apr  9 07:50:24.934: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 12.181257537s
Apr  9 07:50:26.960: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 14.206983147s
Apr  9 07:50:28.986: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 16.233311923s
Apr  9 07:50:31.012: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 18.259111426s
Apr  9 07:50:33.038: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Running", Reason="", readiness=true. Elapsed: 20.285113481s
Apr  9 07:50:35.064: INFO: Pod "pod-subpath-test-downwardapi-88lw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.31131218s
STEP: Saw pod success
Apr  9 07:50:35.064: INFO: Pod "pod-subpath-test-downwardapi-88lw" satisfied condition "success or failure"
Apr  9 07:50:35.090: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-88lw container test-container-subpath-downwardapi-88lw: <nil>
STEP: delete the pod
Apr  9 07:50:35.149: INFO: Waiting for pod pod-subpath-test-downwardapi-88lw to disappear
Apr  9 07:50:35.174: INFO: Pod pod-subpath-test-downwardapi-88lw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-88lw
Apr  9 07:50:35.174: INFO: Deleting pod "pod-subpath-test-downwardapi-88lw" in namespace "subpath-2015"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:50:35.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2015" for this suite.
Apr  9 07:50:41.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:50:42.233: INFO: namespace subpath-2015 deletion completed in 7.008334805s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:50:42.234: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:50:42.517: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32" in namespace "projected-1601" to be "success or failure"
Apr  9 07:50:42.543: INFO: Pod "downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.337461ms
Apr  9 07:50:44.568: INFO: Pod "downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051165833s
STEP: Saw pod success
Apr  9 07:50:44.568: INFO: Pod "downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:50:44.595: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:50:44.655: INFO: Waiting for pod downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:50:44.681: INFO: Pod downwardapi-volume-2cd2691c-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:50:44.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1601" for this suite.
Apr  9 07:50:50.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:50:51.705: INFO: namespace projected-1601 deletion completed in 6.9973285s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:50:51.705: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:50:52.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5631" for this suite.
Apr  9 07:51:14.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:51:15.084: INFO: namespace pods-5631 deletion completed in 23.031085533s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:51:15.085: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr  9 07:51:15.401: INFO: Waiting up to 5m0s for pod "client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32" in namespace "containers-6928" to be "success or failure"
Apr  9 07:51:15.426: INFO: Pod "client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.131144ms
Apr  9 07:51:17.452: INFO: Pod "client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050647484s
STEP: Saw pod success
Apr  9 07:51:17.452: INFO: Pod "client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:51:17.477: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:51:17.537: INFO: Waiting for pod client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:51:17.562: INFO: Pod client-containers-406c22d1-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:51:17.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6928" for this suite.
Apr  9 07:51:23.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:51:24.626: INFO: namespace containers-6928 deletion completed in 7.03793491s
•
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:51:24.626: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr  9 07:51:24.904: INFO: Waiting up to 5m0s for pod "client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32" in namespace "containers-581" to be "success or failure"
Apr  9 07:51:24.929: INFO: Pod "client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 24.919208ms
Apr  9 07:51:26.955: INFO: Pod "client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050740275s
STEP: Saw pod success
Apr  9 07:51:26.955: INFO: Pod "client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:51:26.980: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 07:51:27.041: INFO: Waiting for pod client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:51:27.066: INFO: Pod client-containers-46165219-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:51:27.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-581" for this suite.
Apr  9 07:51:33.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:51:34.576: INFO: namespace containers-581 deletion completed in 7.484122999s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:51:34.577: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  9 07:51:41.166: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:41.191: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:43.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:43.217: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:45.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:45.218: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:47.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:47.217: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:49.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:49.219: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:51.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:51.217: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 07:51:53.191: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 07:51:53.217: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:51:53.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2642" for this suite.
Apr  9 07:52:15.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:52:16.307: INFO: namespace container-lifecycle-hook-2642 deletion completed in 23.063939185s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:52:16.308: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr  9 07:52:16.576: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config cluster-info'
Apr  9 07:52:18.509: INFO: stderr: ""
Apr  9 07:52:18.509: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:52:18.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4216" for this suite.
Apr  9 07:52:24.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:52:25.542: INFO: namespace kubectl-4216 deletion completed in 7.007354915s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:52:25.542: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7602
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-6a8f0cbc-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating secret with name s-test-opt-upd-6a8f0d10-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6a8f0cbc-5a9c-11e9-8c35-36e6c88ddb32
STEP: Updating secret s-test-opt-upd-6a8f0d10-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating secret with name s-test-opt-create-6a8f0d27-5a9c-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:52:30.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7602" for this suite.
Apr  9 07:52:52.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:52:54.037: INFO: namespace projected-7602 deletion completed in 23.27929592s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:52:54.038: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 07:52:54.414: INFO: Waiting up to 5m0s for pod "downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32" in namespace "downward-api-7367" to be "success or failure"
Apr  9 07:52:54.440: INFO: Pod "downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.461187ms
Apr  9 07:52:56.467: INFO: Pod "downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052017527s
STEP: Saw pod success
Apr  9 07:52:56.467: INFO: Pod "downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:52:56.492: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:52:56.553: INFO: Waiting for pod downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:52:56.578: INFO: Pod downward-api-7b7035e9-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:52:56.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7367" for this suite.
Apr  9 07:53:02.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:53:03.674: INFO: namespace downward-api-7367 deletion completed in 7.069287055s
•SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:53:03.674: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr  9 07:53:08.177: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:08.177: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:08.755: INFO: Exec stderr: ""
Apr  9 07:53:08.756: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:08.756: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:09.345: INFO: Exec stderr: ""
Apr  9 07:53:09.345: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:09.345: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:10.007: INFO: Exec stderr: ""
Apr  9 07:53:10.007: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:10.007: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:10.664: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr  9 07:53:10.664: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:10.664: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:11.240: INFO: Exec stderr: ""
Apr  9 07:53:11.241: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:11.241: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:11.804: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr  9 07:53:11.804: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:11.804: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:12.351: INFO: Exec stderr: ""
Apr  9 07:53:12.351: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:12.351: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:12.902: INFO: Exec stderr: ""
Apr  9 07:53:12.902: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:12.902: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:13.464: INFO: Exec stderr: ""
Apr  9 07:53:13.464: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5085 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 07:53:13.464: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 07:53:14.088: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:53:14.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5085" for this suite.
Apr  9 07:54:04.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:54:05.202: INFO: namespace e2e-kubelet-etc-hosts-5085 deletion completed in 51.087861761s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:54:05.203: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5923
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a5d45aa1-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a5d45aa1-5a9c-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:54:11.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5923" for this suite.
Apr  9 07:54:33.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:54:34.929: INFO: namespace projected-5923 deletion completed in 23.080582305s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:54:34.929: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9855
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-b798120e-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating secret with name s-test-opt-upd-b7981264-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b798120e-5a9c-11e9-8c35-36e6c88ddb32
STEP: Updating secret s-test-opt-upd-b7981264-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating secret with name s-test-opt-create-b798127f-5a9c-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:54:42.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9855" for this suite.
Apr  9 07:55:04.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:55:05.181: INFO: namespace secrets-9855 deletion completed in 23.090088281s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:55:05.181: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-c993aef3-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 07:55:05.538: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32" in namespace "projected-4483" to be "success or failure"
Apr  9 07:55:05.566: INFO: Pod "pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.602924ms
Apr  9 07:55:07.594: INFO: Pod "pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056097256s
STEP: Saw pod success
Apr  9 07:55:07.595: INFO: Pod "pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:55:07.622: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 07:55:07.689: INFO: Waiting for pod pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:55:07.716: INFO: Pod pod-projected-secrets-c997f4a9-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:55:07.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4483" for this suite.
Apr  9 07:55:13.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:55:14.835: INFO: namespace projected-4483 deletion completed in 7.090510458s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:55:14.835: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-cf5be9d8-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating secret with name secret-projected-all-test-volume-cf5be9c1-5a9c-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr  9 07:55:15.265: INFO: Waiting up to 5m0s for pod "projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32" in namespace "projected-2837" to be "success or failure"
Apr  9 07:55:15.293: INFO: Pod "projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.182158ms
Apr  9 07:55:17.320: INFO: Pod "projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05497953s
STEP: Saw pod success
Apr  9 07:55:17.320: INFO: Pod "projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:55:17.348: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr  9 07:55:17.417: INFO: Waiting for pod projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:55:17.444: INFO: Pod projected-volume-cf5be985-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:55:17.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2837" for this suite.
Apr  9 07:55:23.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:55:24.628: INFO: namespace projected-2837 deletion completed in 7.156082049s
•SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:55:24.628: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7017
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-159
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:55:49.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7323" for this suite.
Apr  9 07:55:55.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:55:56.695: INFO: namespace namespaces-7323 deletion completed in 7.084982127s
STEP: Destroying namespace "nsdeletetest-7017" for this suite.
Apr  9 07:55:56.722: INFO: Namespace nsdeletetest-7017 was already deleted
STEP: Destroying namespace "nsdeletetest-159" for this suite.
Apr  9 07:56:02.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:56:03.829: INFO: namespace nsdeletetest-159 deletion completed in 7.106809925s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:56:03.830: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:56:04.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32" in namespace "projected-1738" to be "success or failure"
Apr  9 07:56:04.251: INFO: Pod "downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.601951ms
Apr  9 07:56:06.280: INFO: Pod "downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055879791s
STEP: Saw pod success
Apr  9 07:56:06.280: INFO: Pod "downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:56:06.307: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:56:06.372: INFO: Waiting for pod downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:56:06.399: INFO: Pod downwardapi-volume-ec92ba75-5a9c-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:56:06.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1738" for this suite.
Apr  9 07:56:12.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:56:13.520: INFO: namespace projected-1738 deletion completed in 7.092988404s
•
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:56:13.520: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 07:56:13.931: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 07:56:14.014: INFO: Number of nodes with available pods: 0
Apr  9 07:56:14.014: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:56:15.069: INFO: Number of nodes with available pods: 1
Apr  9 07:56:15.069: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:56:16.070: INFO: Number of nodes with available pods: 2
Apr  9 07:56:16.070: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr  9 07:56:16.266: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:16.266: INFO: Wrong image for pod: daemon-set-nmh9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:17.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:17.321: INFO: Wrong image for pod: daemon-set-nmh9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:18.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:18.321: INFO: Wrong image for pod: daemon-set-nmh9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:19.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:19.321: INFO: Wrong image for pod: daemon-set-nmh9w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:19.321: INFO: Pod daemon-set-nmh9w is not available
Apr  9 07:56:20.322: INFO: Pod daemon-set-9w47q is not available
Apr  9 07:56:20.322: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:21.321: INFO: Pod daemon-set-9w47q is not available
Apr  9 07:56:21.322: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:22.321: INFO: Pod daemon-set-9w47q is not available
Apr  9 07:56:22.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:23.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:24.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:24.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:25.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:25.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:26.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:26.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:27.322: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:27.322: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:28.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:28.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:29.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:29.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:30.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:30.321: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:31.321: INFO: Wrong image for pod: daemon-set-frhtr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 07:56:31.322: INFO: Pod daemon-set-frhtr is not available
Apr  9 07:56:32.321: INFO: Pod daemon-set-cxsc2 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr  9 07:56:32.403: INFO: Number of nodes with available pods: 1
Apr  9 07:56:32.403: INFO: Node ip-10-250-30-1.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 07:56:33.459: INFO: Number of nodes with available pods: 2
Apr  9 07:56:33.459: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2467, will wait for the garbage collector to delete the pods
Apr  9 07:56:33.703: INFO: Deleting DaemonSet.extensions daemon-set took: 28.690229ms
Apr  9 07:56:34.103: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.375887ms
Apr  9 07:56:41.931: INFO: Number of nodes with available pods: 0
Apr  9 07:56:41.931: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 07:56:41.958: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2467/daemonsets","resourceVersion":"17070"},"items":null}

Apr  9 07:56:41.985: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2467/pods","resourceVersion":"17070"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:56:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2467" for this suite.
Apr  9 07:56:48.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:56:49.199: INFO: namespace daemonsets-2467 deletion completed in 7.102121011s
•
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:56:49.199: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr  9 07:56:49.585: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17098,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 07:56:49.585: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17098,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr  9 07:56:59.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17120,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr  9 07:56:59.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17120,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr  9 07:57:09.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17141,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 07:57:09.696: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17141,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr  9 07:57:19.725: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17164,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 07:57:19.726: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-a,UID:079e1acf-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17164,Generation:0,CreationTimestamp:2019-04-09 07:56:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr  9 07:57:29.754: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-b,UID:1f8f70d7-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17185,Generation:0,CreationTimestamp:2019-04-09 07:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 07:57:29.755: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-b,UID:1f8f70d7-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17185,Generation:0,CreationTimestamp:2019-04-09 07:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr  9 07:57:39.784: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-b,UID:1f8f70d7-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17207,Generation:0,CreationTimestamp:2019-04-09 07:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 07:57:39.784: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2583,SelfLink:/api/v1/namespaces/watch-2583/configmaps/e2e-watch-test-configmap-b,UID:1f8f70d7-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:17207,Generation:0,CreationTimestamp:2019-04-09 07:57:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:57:49.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2583" for this suite.
Apr  9 07:57:55.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:57:56.902: INFO: namespace watch-2583 deletion completed in 7.09046083s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:57:56.903: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:57:59.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5542" for this suite.
Apr  9 07:58:05.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:58:06.561: INFO: namespace emptydir-wrapper-5542 deletion completed in 7.094967276s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:58:06.561: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-35b2ac72-5a9d-11e9-8c35-36e6c88ddb32
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:58:06.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2845" for this suite.
Apr  9 07:58:13.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:58:14.019: INFO: namespace configmap-2845 deletion completed in 7.086292607s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:58:14.019: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 07:58:14.287: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3154'
Apr  9 07:58:14.594: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Apr  9 07:58:14.594: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr  9 07:58:14.621: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-3154'
Apr  9 07:58:14.877: INFO: stderr: ""
Apr  9 07:58:14.877: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:58:14.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3154" for this suite.
Apr  9 07:58:36.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:58:37.989: INFO: namespace kubectl-3154 deletion completed in 23.083873861s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:58:37.990: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6286
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6286
STEP: Creating statefulset with conflicting port in namespace statefulset-6286
STEP: Waiting until pod test-pod will start running in namespace statefulset-6286
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6286
Apr  9 07:58:40.475: INFO: Observed stateful pod in namespace: statefulset-6286, name: ss-0, uid: 49b1fd84-5a9d-11e9-afbf-1e9604a4a829, status phase: Failed. Waiting for statefulset controller to delete.
Apr  9 07:58:40.495: INFO: Observed stateful pod in namespace: statefulset-6286, name: ss-0, uid: 49b1fd84-5a9d-11e9-afbf-1e9604a4a829, status phase: Failed. Waiting for statefulset controller to delete.
Apr  9 07:58:40.497: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6286
STEP: Removing pod with conflicting port in namespace statefulset-6286
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6286 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 07:58:42.582: INFO: Deleting all statefulset in ns statefulset-6286
Apr  9 07:58:42.609: INFO: Scaling statefulset ss to 0
Apr  9 07:58:52.721: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 07:58:52.748: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:58:52.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6286" for this suite.
Apr  9 07:58:58.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:58:59.943: INFO: namespace statefulset-6286 deletion completed in 7.084032765s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:58:59.943: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 07:59:00.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32" in namespace "downward-api-8444" to be "success or failure"
Apr  9 07:59:00.336: INFO: Pod "downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.152554ms
Apr  9 07:59:02.363: INFO: Pod "downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055028187s
STEP: Saw pod success
Apr  9 07:59:02.364: INFO: Pod "downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:59:02.391: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 07:59:02.456: INFO: Waiting for pod downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:59:02.482: INFO: Pod downwardapi-volume-5587215e-5a9d-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:59:02.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8444" for this suite.
Apr  9 07:59:08.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:59:09.595: INFO: namespace downward-api-8444 deletion completed in 7.085031615s
•SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:59:09.595: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 07:59:09.907: INFO: Waiting up to 5m0s for pod "downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32" in namespace "downward-api-2517" to be "success or failure"
Apr  9 07:59:09.934: INFO: Pod "downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.275873ms
Apr  9 07:59:11.962: INFO: Pod "downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055504364s
STEP: Saw pod success
Apr  9 07:59:11.963: INFO: Pod "downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 07:59:11.990: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32 container dapi-container: <nil>
STEP: delete the pod
Apr  9 07:59:12.054: INFO: Waiting for pod downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 07:59:12.082: INFO: Pod downward-api-5b3fc717-5a9d-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 07:59:12.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2517" for this suite.
Apr  9 07:59:18.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 07:59:19.195: INFO: namespace downward-api-2517 deletion completed in 7.084594223s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 07:59:19.195: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr  9 07:59:20.994: INFO: Pod name wrapped-volume-race-61ca3d69-5a9d-11e9-8c35-36e6c88ddb32: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-61ca3d69-5a9d-11e9-8c35-36e6c88ddb32 in namespace emptydir-wrapper-492, will wait for the garbage collector to delete the pods
Apr  9 07:59:31.330: INFO: Deleting ReplicationController wrapped-volume-race-61ca3d69-5a9d-11e9-8c35-36e6c88ddb32 took: 28.844232ms
Apr  9 07:59:31.431: INFO: Terminating ReplicationController wrapped-volume-race-61ca3d69-5a9d-11e9-8c35-36e6c88ddb32 pods took: 100.264094ms
STEP: Creating RC which spawns configmap-volume pods
Apr  9 08:00:09.594: INFO: Pod name wrapped-volume-race-7ebedb91-5a9d-11e9-8c35-36e6c88ddb32: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7ebedb91-5a9d-11e9-8c35-36e6c88ddb32 in namespace emptydir-wrapper-492, will wait for the garbage collector to delete the pods
Apr  9 08:00:13.877: INFO: Deleting ReplicationController wrapped-volume-race-7ebedb91-5a9d-11e9-8c35-36e6c88ddb32 took: 28.923977ms
Apr  9 08:00:14.277: INFO: Terminating ReplicationController wrapped-volume-race-7ebedb91-5a9d-11e9-8c35-36e6c88ddb32 pods took: 400.248392ms
STEP: Creating RC which spawns configmap-volume pods
Apr  9 08:00:49.594: INFO: Pod name wrapped-volume-race-969d844d-5a9d-11e9-8c35-36e6c88ddb32: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-969d844d-5a9d-11e9-8c35-36e6c88ddb32 in namespace emptydir-wrapper-492, will wait for the garbage collector to delete the pods
Apr  9 08:00:53.873: INFO: Deleting ReplicationController wrapped-volume-race-969d844d-5a9d-11e9-8c35-36e6c88ddb32 took: 29.371323ms
Apr  9 08:00:54.173: INFO: Terminating ReplicationController wrapped-volume-race-969d844d-5a9d-11e9-8c35-36e6c88ddb32 pods took: 300.430699ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:01:29.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-492" for this suite.
Apr  9 08:01:35.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:01:36.719: INFO: namespace emptydir-wrapper-492 deletion completed in 7.001787765s
•SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:01:36.719: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-6csl
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 08:01:37.051: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6csl" in namespace "subpath-9575" to be "success or failure"
Apr  9 08:01:37.076: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Pending", Reason="", readiness=false. Elapsed: 25.156894ms
Apr  9 08:01:39.102: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 2.050780161s
Apr  9 08:01:41.136: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 4.084454661s
Apr  9 08:01:43.162: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 6.11035532s
Apr  9 08:01:45.187: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 8.13603959s
Apr  9 08:01:47.214: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 10.162313596s
Apr  9 08:01:49.239: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 12.188023634s
Apr  9 08:01:51.267: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 14.21524007s
Apr  9 08:01:53.293: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 16.241464908s
Apr  9 08:01:55.323: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 18.271437284s
Apr  9 08:01:57.349: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Running", Reason="", readiness=true. Elapsed: 20.297233162s
Apr  9 08:01:59.377: INFO: Pod "pod-subpath-test-secret-6csl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.325674034s
STEP: Saw pod success
Apr  9 08:01:59.377: INFO: Pod "pod-subpath-test-secret-6csl" satisfied condition "success or failure"
Apr  9 08:01:59.402: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-subpath-test-secret-6csl container test-container-subpath-secret-6csl: <nil>
STEP: delete the pod
Apr  9 08:01:59.491: INFO: Waiting for pod pod-subpath-test-secret-6csl to disappear
Apr  9 08:01:59.516: INFO: Pod pod-subpath-test-secret-6csl no longer exists
STEP: Deleting pod pod-subpath-test-secret-6csl
Apr  9 08:01:59.516: INFO: Deleting pod "pod-subpath-test-secret-6csl" in namespace "subpath-9575"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:01:59.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9575" for this suite.
Apr  9 08:02:05.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:06.647: INFO: namespace subpath-9575 deletion completed in 7.080946144s
•SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:06.647: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2309
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c4ce0259-5a9d-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 08:02:07.024: INFO: Waiting up to 5m0s for pod "pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32" in namespace "secrets-2309" to be "success or failure"
Apr  9 08:02:07.049: INFO: Pod "pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 25.055934ms
Apr  9 08:02:09.075: INFO: Pod "pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051094462s
STEP: Saw pod success
Apr  9 08:02:09.420: INFO: Pod "pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:02:09.446: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:02:09.505: INFO: Waiting for pod pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:02:09.530: INFO: Pod pod-secrets-c4d1eee7-5a9d-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:09.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2309" for this suite.
Apr  9 08:02:15.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:16.555: INFO: namespace secrets-2309 deletion completed in 6.99822094s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:16.556: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:02:16.873: INFO: Creating deployment "test-recreate-deployment"
Apr  9 08:02:16.899: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr  9 08:02:16.949: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr  9 08:02:16.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393736, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393736, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393736, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393736, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:02:19.000: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr  9 08:02:19.052: INFO: Updating deployment test-recreate-deployment
Apr  9 08:02:19.052: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 08:02:19.119: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-6157,SelfLink:/apis/apps/v1/namespaces/deployment-6157/deployments/test-recreate-deployment,UID:cab64b58-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18357,Generation:2,CreationTimestamp:2019-04-09 08:02:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-09 08:02:19 +0000 UTC 2019-04-09 08:02:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-09 08:02:19 +0000 UTC 2019-04-09 08:02:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr  9 08:02:19.145: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-6157,SelfLink:/apis/apps/v1/namespaces/deployment-6157/replicasets/test-recreate-deployment-c9cbd8684,UID:cc0065b2-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18354,Generation:1,CreationTimestamp:2019-04-09 08:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cab64b58-5a9d-11e9-afbf-1e9604a4a829 0xc002ee3a80 0xc002ee3a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:02:19.145: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr  9 08:02:19.146: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-6157,SelfLink:/apis/apps/v1/namespaces/deployment-6157/replicasets/test-recreate-deployment-7d57d5ff7c,UID:cab6a379-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18348,Generation:2,CreationTimestamp:2019-04-09 08:02:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cab64b58-5a9d-11e9-afbf-1e9604a4a829 0xc002ee39b7 0xc002ee39b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:02:19.173: INFO: Pod "test-recreate-deployment-c9cbd8684-tkd4v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-tkd4v,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-6157,SelfLink:/api/v1/namespaces/deployment-6157/pods/test-recreate-deployment-c9cbd8684-tkd4v,UID:cc009f8f-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18355,Generation:0,CreationTimestamp:2019-04-09 08:02:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 cc0065b2-5a9d-11e9-afbf-1e9604a4a829 0xc002e784a0 0xc002e784a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-psg2t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-psg2t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-psg2t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e78570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e78590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:19 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:,StartTime:2019-04-09 08:02:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:19.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6157" for this suite.
Apr  9 08:02:25.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:26.208: INFO: namespace deployment-6157 deletion completed in 7.009300923s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:26.209: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3497
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:02:26.498: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:26.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3497" for this suite.
Apr  9 08:02:32.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:33.816: INFO: namespace custom-resource-definition-3497 deletion completed in 7.023518783s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:33.816: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:02:34.070: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr  9 08:02:34.121: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 08:02:36.172: INFO: Creating deployment "test-rolling-update-deployment"
Apr  9 08:02:36.198: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr  9 08:02:36.248: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr  9 08:02:36.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393756, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393756, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393756, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690393756, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:02:38.299: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 08:02:38.377: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5884,SelfLink:/apis/apps/v1/namespaces/deployment-5884/deployments/test-rolling-update-deployment,UID:d637139e-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18469,Generation:1,CreationTimestamp:2019-04-09 08:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-09 08:02:36 +0000 UTC 2019-04-09 08:02:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-09 08:02:37 +0000 UTC 2019-04-09 08:02:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr  9 08:02:38.403: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-5884,SelfLink:/apis/apps/v1/namespaces/deployment-5884/replicasets/test-rolling-update-deployment-67599b4d9,UID:d638f5b2-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18462,Generation:1,CreationTimestamp:2019-04-09 08:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d637139e-5a9d-11e9-afbf-1e9604a4a829 0xc000436fc0 0xc000436fc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 08:02:38.403: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr  9 08:02:38.403: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5884,SelfLink:/apis/apps/v1/namespaces/deployment-5884/replicasets/test-rolling-update-controller,UID:d4f65c31-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18468,Generation:2,CreationTimestamp:2019-04-09 08:02:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d637139e-5a9d-11e9-afbf-1e9604a4a829 0xc000436ed7 0xc000436ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:02:38.429: INFO: Pod "test-rolling-update-deployment-67599b4d9-2jcwl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-2jcwl,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-5884,SelfLink:/api/v1/namespaces/deployment-5884/pods/test-rolling-update-deployment-67599b4d9-2jcwl,UID:d639374a-5a9d-11e9-afbf-1e9604a4a829,ResourceVersion:18461,Generation:0,CreationTimestamp:2019-04-09 08:02:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.225/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 d638f5b2-5a9d-11e9-afbf-1e9604a4a829 0xc000437a30 0xc000437a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mkzjf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mkzjf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mkzjf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-30-1.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000437aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000437ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:02:36 +0000 UTC  }],Message:,Reason:,HostIP:10.250.30.1,PodIP:100.96.1.225,StartTime:2019-04-09 08:02:36 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-09 08:02:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ff8a9d574405898328b1ca5c1d0e4b5c26747a042d465aff22bcf1cf6ab4cae0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:38.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5884" for this suite.
Apr  9 08:02:44.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:45.457: INFO: namespace deployment-5884 deletion completed in 7.00334206s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:45.457: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr  9 08:02:45.776: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config api-versions'
Apr  9 08:02:46.097: INFO: stderr: ""
Apr  9 08:02:46.097: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3218" for this suite.
Apr  9 08:02:52.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:02:53.326: INFO: namespace kubectl-3218 deletion completed in 7.202935436s
•SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:02:53.326: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  9 08:02:56.258: INFO: Successfully updated pod "pod-update-e0959561-5a9d-11e9-8c35-36e6c88ddb32"
STEP: verifying the updated pod is in kubernetes
Apr  9 08:02:56.310: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:02:56.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7837" for this suite.
Apr  9 08:03:18.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:03:19.337: INFO: namespace pods-7837 deletion completed in 23.001297143s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:03:19.337: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:03:19.669: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:03:22.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3251" for this suite.
Apr  9 08:03:28.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:03:29.401: INFO: namespace init-container-3251 deletion completed in 7.004613138s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:03:29.401: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr  9 08:03:29.672: INFO: Asynchronously running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:03:29.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8018" for this suite.
Apr  9 08:03:35.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:03:36.947: INFO: namespace kubectl-8018 deletion completed in 7.036825616s
•SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:03:36.947: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0409 08:04:17.479074    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 08:04:17.479: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:04:17.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9360" for this suite.
Apr  9 08:04:23.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:04:24.685: INFO: namespace gc-9360 deletion completed in 7.178225594s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:04:24.685: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr  9 08:04:27.648: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl exec --namespace=svcaccounts-8449 pod-service-account-1769a1be-5a9e-11e9-8c35-36e6c88ddb32 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr  9 08:04:28.492: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl exec --namespace=svcaccounts-8449 pod-service-account-1769a1be-5a9e-11e9-8c35-36e6c88ddb32 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr  9 08:04:29.347: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl exec --namespace=svcaccounts-8449 pod-service-account-1769a1be-5a9e-11e9-8c35-36e6c88ddb32 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:04:30.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8449" for this suite.
Apr  9 08:04:36.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:04:37.368: INFO: namespace svcaccounts-8449 deletion completed in 7.102678913s
•SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:04:37.368: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:04:37.815: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr  9 08:04:37.870: INFO: Number of nodes with available pods: 0
Apr  9 08:04:37.870: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr  9 08:04:37.982: INFO: Number of nodes with available pods: 0
Apr  9 08:04:37.982: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:39.010: INFO: Number of nodes with available pods: 0
Apr  9 08:04:39.010: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:40.011: INFO: Number of nodes with available pods: 1
Apr  9 08:04:40.011: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr  9 08:04:40.122: INFO: Number of nodes with available pods: 0
Apr  9 08:04:40.122: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr  9 08:04:40.178: INFO: Number of nodes with available pods: 0
Apr  9 08:04:40.178: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:41.206: INFO: Number of nodes with available pods: 0
Apr  9 08:04:41.206: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:42.206: INFO: Number of nodes with available pods: 0
Apr  9 08:04:42.206: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:43.206: INFO: Number of nodes with available pods: 0
Apr  9 08:04:43.206: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:44.206: INFO: Number of nodes with available pods: 0
Apr  9 08:04:44.206: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:45.206: INFO: Number of nodes with available pods: 0
Apr  9 08:04:45.206: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:04:46.207: INFO: Number of nodes with available pods: 1
Apr  9 08:04:46.207: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8803, will wait for the garbage collector to delete the pods
Apr  9 08:04:46.367: INFO: Deleting DaemonSet.extensions daemon-set took: 28.370545ms
Apr  9 08:04:46.968: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.301098ms
Apr  9 08:04:49.595: INFO: Number of nodes with available pods: 0
Apr  9 08:04:49.595: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 08:04:49.623: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8803/daemonsets","resourceVersion":"18992"},"items":null}

Apr  9 08:04:49.650: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8803/pods","resourceVersion":"18992"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:04:49.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8803" for this suite.
Apr  9 08:04:55.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:04:56.875: INFO: namespace daemonsets-8803 deletion completed in 7.084983794s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:04:56.875: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9181
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 08:04:57.178: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 08:05:17.652: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.235 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9181 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:05:17.652: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 08:05:19.282: INFO: Found all expected endpoints: [netserver-0]
Apr  9 08:05:19.309: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.73 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9181 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:05:19.309: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 08:05:20.892: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:05:20.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9181" for this suite.
Apr  9 08:05:43.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:05:44.015: INFO: namespace pod-network-test-9181 deletion completed in 23.092618677s
•SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:05:44.015: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-4654ff47-5a9e-11e9-8c35-36e6c88ddb32
STEP: Creating a pod to test consume secrets
Apr  9 08:05:44.339: INFO: Waiting up to 5m0s for pod "pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32" in namespace "secrets-8910" to be "success or failure"
Apr  9 08:05:44.367: INFO: Pod "pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.569728ms
Apr  9 08:05:46.394: INFO: Pod "pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055137896s
STEP: Saw pod success
Apr  9 08:05:46.394: INFO: Pod "pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:05:46.426: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:05:46.490: INFO: Waiting for pod pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:05:46.517: INFO: Pod pod-secrets-46592ff8-5a9e-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:05:46.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8910" for this suite.
Apr  9 08:05:52.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:05:53.799: INFO: namespace secrets-8910 deletion completed in 7.253532378s
•S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:05:53.799: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1093
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-4c350e5a-5a9e-11e9-8c35-36e6c88ddb32
STEP: Creating configMap with name cm-test-opt-upd-4c350ea0-5a9e-11e9-8c35-36e6c88ddb32
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4c350e5a-5a9e-11e9-8c35-36e6c88ddb32
STEP: Updating configmap cm-test-opt-upd-4c350ea0-5a9e-11e9-8c35-36e6c88ddb32
STEP: Creating configMap with name cm-test-opt-create-4c350eb7-5a9e-11e9-8c35-36e6c88ddb32
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:05:58.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1093" for this suite.
Apr  9 08:06:20.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:06:21.962: INFO: namespace projected-1093 deletion completed in 23.089729295s
•SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:06:21.962: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:06:22.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32" in namespace "downward-api-3519" to be "success or failure"
Apr  9 08:06:22.337: INFO: Pod "downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 29.826714ms
Apr  9 08:06:24.365: INFO: Pod "downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.058019886s
STEP: Saw pod success
Apr  9 08:06:24.366: INFO: Pod "downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:06:24.393: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 08:06:24.457: INFO: Waiting for pod downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:06:24.484: INFO: Pod downwardapi-volume-5cfad085-5a9e-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:06:24.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3519" for this suite.
Apr  9 08:06:30.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:06:31.600: INFO: namespace downward-api-3519 deletion completed in 7.088263578s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:06:31.601: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 08:06:31.876: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8539'
Apr  9 08:06:33.657: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Apr  9 08:06:33.657: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr  9 08:06:33.711: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-z8mlr]
Apr  9 08:06:33.711: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-z8mlr" in namespace "kubectl-8539" to be "running and ready"
Apr  9 08:06:33.738: INFO: Pod "e2e-test-nginx-rc-z8mlr": Phase="Pending", Reason="", readiness=false. Elapsed: 27.33925ms
Apr  9 08:06:35.768: INFO: Pod "e2e-test-nginx-rc-z8mlr": Phase="Running", Reason="", readiness=true. Elapsed: 2.057228095s
Apr  9 08:06:35.768: INFO: Pod "e2e-test-nginx-rc-z8mlr" satisfied condition "running and ready"
Apr  9 08:06:35.768: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-z8mlr]
Apr  9 08:06:35.769: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-8539'
Apr  9 08:06:36.087: INFO: stderr: ""
Apr  9 08:06:36.087: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr  9 08:06:36.087: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-8539'
Apr  9 08:06:36.320: INFO: stderr: ""
Apr  9 08:06:36.320: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:06:36.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8539" for this suite.
Apr  9 08:06:58.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:06:59.449: INFO: namespace kubectl-8539 deletion completed in 23.091351383s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:06:59.449: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr  9 08:06:59.776: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-7515'
Apr  9 08:07:00.206: INFO: stderr: ""
Apr  9 08:07:00.206: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 08:07:01.234: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:07:01.234: INFO: Found 0 / 1
Apr  9 08:07:02.235: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:07:02.235: INFO: Found 1 / 1
Apr  9 08:07:02.235: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr  9 08:07:02.262: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:07:02.262: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 08:07:02.262: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config patch pod redis-master-pwj7h --namespace=kubectl-7515 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr  9 08:07:02.572: INFO: stderr: ""
Apr  9 08:07:02.572: INFO: stdout: "pod/redis-master-pwj7h patched\n"
STEP: checking annotations
Apr  9 08:07:02.600: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:07:02.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:07:02.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7515" for this suite.
Apr  9 08:07:24.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:07:25.727: INFO: namespace kubectl-7515 deletion completed in 23.09856842s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:07:25.728: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:07:26.138: INFO: (0) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 32.214014ms)
Apr  9 08:07:26.181: INFO: (1) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 42.711495ms)
Apr  9 08:07:26.211: INFO: (2) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.753563ms)
Apr  9 08:07:26.240: INFO: (3) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.384297ms)
Apr  9 08:07:26.269: INFO: (4) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.023267ms)
Apr  9 08:07:26.300: INFO: (5) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 30.678607ms)
Apr  9 08:07:26.329: INFO: (6) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.222681ms)
Apr  9 08:07:26.359: INFO: (7) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.623417ms)
Apr  9 08:07:26.388: INFO: (8) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.509973ms)
Apr  9 08:07:26.418: INFO: (9) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.266583ms)
Apr  9 08:07:26.447: INFO: (10) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.046831ms)
Apr  9 08:07:26.476: INFO: (11) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.788152ms)
Apr  9 08:07:26.505: INFO: (12) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.442275ms)
Apr  9 08:07:26.535: INFO: (13) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.692487ms)
Apr  9 08:07:26.564: INFO: (14) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.342744ms)
Apr  9 08:07:26.594: INFO: (15) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.669973ms)
Apr  9 08:07:26.623: INFO: (16) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.226474ms)
Apr  9 08:07:26.652: INFO: (17) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 28.880602ms)
Apr  9 08:07:26.682: INFO: (18) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.346777ms)
Apr  9 08:07:26.711: INFO: (19) /api/v1/nodes/ip-10-250-14-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 29.616751ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:07:26.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8747" for this suite.
Apr  9 08:07:32.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:07:33.833: INFO: namespace proxy-8747 deletion completed in 7.093175983s
•SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:07:33.833: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4146
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:07:34.317: INFO: Create a RollingUpdate DaemonSet
Apr  9 08:07:34.345: INFO: Check that daemon pods launch on every node of the cluster
Apr  9 08:07:34.400: INFO: Number of nodes with available pods: 0
Apr  9 08:07:34.400: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:07:35.482: INFO: Number of nodes with available pods: 0
Apr  9 08:07:35.482: INFO: Node ip-10-250-14-41.eu-west-1.compute.internal is running more than one daemon pod
Apr  9 08:07:36.456: INFO: Number of nodes with available pods: 2
Apr  9 08:07:36.456: INFO: Number of running nodes: 2, number of available pods: 2
Apr  9 08:07:36.456: INFO: Update the DaemonSet to trigger a rollout
Apr  9 08:07:36.512: INFO: Updating DaemonSet daemon-set
Apr  9 08:07:42.595: INFO: Roll back the DaemonSet before rollout is complete
Apr  9 08:07:42.650: INFO: Updating DaemonSet daemon-set
Apr  9 08:07:42.650: INFO: Make sure DaemonSet rollback is complete
Apr  9 08:07:42.686: INFO: Wrong image for pod: daemon-set-zmj8d. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr  9 08:07:42.686: INFO: Pod daemon-set-zmj8d is not available
Apr  9 08:07:43.741: INFO: Wrong image for pod: daemon-set-zmj8d. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr  9 08:07:43.741: INFO: Pod daemon-set-zmj8d is not available
Apr  9 08:07:44.741: INFO: Pod daemon-set-nlsqh is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4146, will wait for the garbage collector to delete the pods
Apr  9 08:07:44.929: INFO: Deleting DaemonSet.extensions daemon-set took: 28.279151ms
Apr  9 08:07:45.330: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.377449ms
Apr  9 08:07:49.362: INFO: Number of nodes with available pods: 0
Apr  9 08:07:49.362: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 08:07:49.390: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4146/daemonsets","resourceVersion":"19597"},"items":null}

Apr  9 08:07:49.417: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4146/pods","resourceVersion":"19597"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:07:49.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4146" for this suite.
Apr  9 08:07:55.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:07:56.776: INFO: namespace daemonsets-4146 deletion completed in 7.248321293s
•SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:07:56.776: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 08:07:59.783: INFO: Successfully updated pod "labelsupdate957c45de-5a9e-11e9-8c35-36e6c88ddb32"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:08:03.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-516" for this suite.
Apr  9 08:08:26.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:08:27.013: INFO: namespace downward-api-516 deletion completed in 23.090181124s
•SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:08:27.013: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:08:29.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4757" for this suite.
Apr  9 08:08:51.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:08:52.561: INFO: namespace replication-controller-4757 deletion completed in 23.083278137s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:08:52.562: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4290
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 08:08:52.826: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 08:09:11.296: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.248:8080/dial?request=hostName&protocol=udp&host=100.96.1.247&port=8081&tries=1'] Namespace:pod-network-test-4290 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:09:11.296: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 08:09:11.985: INFO: Waiting for endpoints: map[]
Apr  9 08:09:12.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.248:8080/dial?request=hostName&protocol=udp&host=100.96.0.75&port=8081&tries=1'] Namespace:pod-network-test-4290 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:09:12.012: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
Apr  9 08:09:12.660: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:09:12.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4290" for this suite.
Apr  9 08:09:34.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:09:35.697: INFO: namespace pod-network-test-4290 deletion completed in 23.008437078s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:09:35.697: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0409 08:09:46.332972    6827 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 08:09:46.333: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:09:46.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3694" for this suite.
Apr  9 08:09:52.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:09:53.441: INFO: namespace gc-3694 deletion completed in 7.082915525s
•SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:09:53.441: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:09:53.779: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:09:57.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7024" for this suite.
Apr  9 08:10:04.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:05.009: INFO: namespace init-container-7024 deletion completed in 7.090766754s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:05.010: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:10:05.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32" in namespace "downward-api-2228" to be "success or failure"
Apr  9 08:10:05.335: INFO: Pod "downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.577687ms
Apr  9 08:10:07.363: INFO: Pod "downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055466254s
STEP: Saw pod success
Apr  9 08:10:07.363: INFO: Pod "downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:10:07.390: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32 container client-container: <nil>
STEP: delete the pod
Apr  9 08:10:07.457: INFO: Waiting for pod downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:10:07.486: INFO: Pod downwardapi-volume-e1e5dc5b-5a9e-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:10:07.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2228" for this suite.
Apr  9 08:10:13.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:14.618: INFO: namespace downward-api-2228 deletion completed in 7.104255156s
•
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:14.618: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  9 08:10:14.913: INFO: Waiting up to 5m0s for pod "pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32" in namespace "emptydir-3868" to be "success or failure"
Apr  9 08:10:14.941: INFO: Pod "pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.492115ms
Apr  9 08:10:16.969: INFO: Pod "pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055673749s
STEP: Saw pod success
Apr  9 08:10:16.969: INFO: Pod "pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:10:16.997: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 08:10:17.063: INFO: Waiting for pod pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:10:17.090: INFO: Pod pod-e79fb351-5a9e-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:10:17.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3868" for this suite.
Apr  9 08:10:23.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:24.290: INFO: namespace emptydir-3868 deletion completed in 7.171172618s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:24.290: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr  9 08:10:25.193: INFO: created pod pod-service-account-defaultsa
Apr  9 08:10:25.193: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr  9 08:10:25.221: INFO: created pod pod-service-account-mountsa
Apr  9 08:10:25.221: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr  9 08:10:25.248: INFO: created pod pod-service-account-nomountsa
Apr  9 08:10:25.249: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr  9 08:10:25.276: INFO: created pod pod-service-account-defaultsa-mountspec
Apr  9 08:10:25.276: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr  9 08:10:25.304: INFO: created pod pod-service-account-mountsa-mountspec
Apr  9 08:10:25.304: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr  9 08:10:25.332: INFO: created pod pod-service-account-nomountsa-mountspec
Apr  9 08:10:25.332: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr  9 08:10:25.359: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr  9 08:10:25.359: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr  9 08:10:25.388: INFO: created pod pod-service-account-mountsa-nomountspec
Apr  9 08:10:25.388: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr  9 08:10:25.415: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr  9 08:10:25.416: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:10:25.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3642" for this suite.
Apr  9 08:10:31.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:32.532: INFO: namespace svcaccounts-3642 deletion completed in 7.088147764s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:32.532: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr  9 08:10:32.985: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2134,SelfLink:/api/v1/namespaces/watch-2134/configmaps/e2e-watch-test-resource-version,UID:f24e7de3-5a9e-11e9-afbf-1e9604a4a829,ResourceVersion:20297,Generation:0,CreationTimestamp:2019-04-09 08:10:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 08:10:32.985: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2134,SelfLink:/api/v1/namespaces/watch-2134/configmaps/e2e-watch-test-resource-version,UID:f24e7de3-5a9e-11e9-afbf-1e9604a4a829,ResourceVersion:20298,Generation:0,CreationTimestamp:2019-04-09 08:10:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:10:32.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2134" for this suite.
Apr  9 08:10:39.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:40.095: INFO: namespace watch-2134 deletion completed in 7.082088282s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:40.095: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  9 08:10:40.404: INFO: Waiting up to 5m0s for pod "pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32" in namespace "emptydir-4187" to be "success or failure"
Apr  9 08:10:40.432: INFO: Pod "pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 27.232734ms
Apr  9 08:10:42.459: INFO: Pod "pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055068103s
STEP: Saw pod success
Apr  9 08:10:42.459: INFO: Pod "pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32" satisfied condition "success or failure"
Apr  9 08:10:42.486: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32 container test-container: <nil>
STEP: delete the pod
Apr  9 08:10:42.552: INFO: Waiting for pod pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32 to disappear
Apr  9 08:10:42.579: INFO: Pod pod-f6d0f182-5a9e-11e9-8c35-36e6c88ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:10:42.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4187" for this suite.
Apr  9 08:10:48.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:10:49.690: INFO: namespace emptydir-4187 deletion completed in 7.082400902s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:10:49.690: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-pnrn
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 08:10:50.058: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pnrn" in namespace "subpath-362" to be "success or failure"
Apr  9 08:10:50.085: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Pending", Reason="", readiness=false. Elapsed: 27.122566ms
Apr  9 08:10:52.113: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 2.055171005s
Apr  9 08:10:54.141: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 4.083047586s
Apr  9 08:10:56.169: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.111235839s
Apr  9 08:10:58.197: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 8.139281695s
Apr  9 08:11:00.226: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 10.167576554s
Apr  9 08:11:02.254: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 12.196058489s
Apr  9 08:11:04.283: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 14.224730266s
Apr  9 08:11:06.312: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 16.253784746s
Apr  9 08:11:08.340: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 18.281836932s
Apr  9 08:11:10.368: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Running", Reason="", readiness=true. Elapsed: 20.309621299s
Apr  9 08:11:12.396: INFO: Pod "pod-subpath-test-projected-pnrn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.337811894s
STEP: Saw pod success
Apr  9 08:11:12.396: INFO: Pod "pod-subpath-test-projected-pnrn" satisfied condition "success or failure"
Apr  9 08:11:12.423: INFO: Trying to get logs from node ip-10-250-30-1.eu-west-1.compute.internal pod pod-subpath-test-projected-pnrn container test-container-subpath-projected-pnrn: <nil>
STEP: delete the pod
Apr  9 08:11:12.487: INFO: Waiting for pod pod-subpath-test-projected-pnrn to disappear
Apr  9 08:11:12.514: INFO: Pod pod-subpath-test-projected-pnrn no longer exists
STEP: Deleting pod pod-subpath-test-projected-pnrn
Apr  9 08:11:12.514: INFO: Deleting pod "pod-subpath-test-projected-pnrn" in namespace "subpath-362"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:11:12.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-362" for this suite.
Apr  9 08:11:18.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:11:19.648: INFO: namespace subpath-362 deletion completed in 7.078547768s
•SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:11:19.648: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr  9 08:11:20.084: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4290,SelfLink:/api/v1/namespaces/watch-4290/configmaps/e2e-watch-test-watch-closed,UID:0e71bc9e-5a9f-11e9-afbf-1e9604a4a829,ResourceVersion:20450,Generation:0,CreationTimestamp:2019-04-09 08:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 08:11:20.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4290,SelfLink:/api/v1/namespaces/watch-4290/configmaps/e2e-watch-test-watch-closed,UID:0e71bc9e-5a9f-11e9-afbf-1e9604a4a829,ResourceVersion:20451,Generation:0,CreationTimestamp:2019-04-09 08:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr  9 08:11:20.194: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4290,SelfLink:/api/v1/namespaces/watch-4290/configmaps/e2e-watch-test-watch-closed,UID:0e71bc9e-5a9f-11e9-afbf-1e9604a4a829,ResourceVersion:20452,Generation:0,CreationTimestamp:2019-04-09 08:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 08:11:20.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4290,SelfLink:/api/v1/namespaces/watch-4290/configmaps/e2e-watch-test-watch-closed,UID:0e71bc9e-5a9f-11e9-afbf-1e9604a4a829,ResourceVersion:20453,Generation:0,CreationTimestamp:2019-04-09 08:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:11:20.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4290" for this suite.
Apr  9 08:11:26.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:11:27.313: INFO: namespace watch-4290 deletion completed in 7.091051134s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:11:27.313: INFO: >>> kubeConfig: /tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr  9 08:11:27.577: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config create -f - --namespace=kubectl-2471'
Apr  9 08:11:27.990: INFO: stderr: ""
Apr  9 08:11:27.990: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 08:11:27.991: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2471'
Apr  9 08:11:28.214: INFO: stderr: ""
Apr  9 08:11:28.214: INFO: stdout: "update-demo-nautilus-2npsj update-demo-nautilus-844vj "
Apr  9 08:11:28.214: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-2npsj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2471'
Apr  9 08:11:28.407: INFO: stderr: ""
Apr  9 08:11:28.407: INFO: stdout: ""
Apr  9 08:11:28.407: INFO: update-demo-nautilus-2npsj is created but not running
Apr  9 08:11:33.407: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2471'
Apr  9 08:11:33.616: INFO: stderr: ""
Apr  9 08:11:33.616: INFO: stdout: "update-demo-nautilus-2npsj update-demo-nautilus-844vj "
Apr  9 08:11:33.616: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-2npsj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2471'
Apr  9 08:11:33.831: INFO: stderr: ""
Apr  9 08:11:33.831: INFO: stdout: "true"
Apr  9 08:11:33.831: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-2npsj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2471'
Apr  9 08:11:34.048: INFO: stderr: ""
Apr  9 08:11:34.048: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 08:11:34.048: INFO: validating pod update-demo-nautilus-2npsj
Apr  9 08:11:34.167: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 08:11:34.167: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 08:11:34.167: INFO: update-demo-nautilus-2npsj is verified up and running
Apr  9 08:11:34.167: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-844vj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2471'
Apr  9 08:11:34.378: INFO: stderr: ""
Apr  9 08:11:34.378: INFO: stdout: "true"
Apr  9 08:11:34.378: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods update-demo-nautilus-844vj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2471'
Apr  9 08:11:34.593: INFO: stderr: ""
Apr  9 08:11:34.593: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 08:11:34.593: INFO: validating pod update-demo-nautilus-844vj
Apr  9 08:11:34.711: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 08:11:34.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 08:11:34.711: INFO: update-demo-nautilus-844vj is verified up and running
STEP: using delete to clean up resources
Apr  9 08:11:34.711: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2471'
Apr  9 08:11:34.929: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:11:34.929: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  9 08:11:34.929: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2471'
Apr  9 08:11:35.155: INFO: stderr: "No resources found.\n"
Apr  9 08:11:35.155: INFO: stdout: ""
Apr  9 08:11:35.155: INFO: Running '/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/setup/scripts/kubectl --server=https://api.pub-aws-ckifw.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/build/0b081913/git-kubernetes-dev_landscape-dev-garden-master_master/../tm/shoot.config get pods -l name=update-demo --namespace=kubectl-2471 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 08:11:35.360: INFO: stderr: ""
Apr  9 08:11:35.360: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:11:35.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2471" for this suite.
Apr  9 08:11:57.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:11:58.476: INFO: namespace kubectl-2471 deletion completed in 23.087583221s
•SSSSSSSSSSSSSSSSSApr  9 08:11:58.476: INFO: Running AfterSuite actions on all nodes
Apr  9 08:11:58.476: INFO: Running AfterSuite actions on node 1
Apr  9 08:11:58.476: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5444.602 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Flaked | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h30m46.368034833s
Test Suite Passed
