I0511 02:10:53.265560      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-966612213
I0511 02:10:53.265672      16 e2e.go:240] Starting e2e run "008b7a7b-7392-11e9-80a1-becd2a02efc9" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1557540652 - Will randomize all specs
Will run 204 of 3584 specs

May 11 02:10:53.440: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:10:53.444: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 11 02:10:53.460: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 11 02:10:53.498: INFO: 32 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 11 02:10:53.498: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
May 11 02:10:53.498: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 11 02:10:53.506: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May 11 02:10:53.506: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 11 02:10:53.506: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
May 11 02:10:53.506: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'portworx' (0 seconds elapsed)
May 11 02:10:53.506: INFO: e2e test version: v1.14.1
May 11 02:10:53.506: INFO: kube-apiserver version: v1.14.1
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:10:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename watch
May 11 02:11:19.645: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 11 02:11:19.654: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9153,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 11 02:11:19.654: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9153,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 11 02:11:29.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9179,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 11 02:11:29.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9179,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 11 02:11:39.668: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9205,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 11 02:11:39.669: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9205,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 11 02:11:49.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9231,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 11 02:11:49.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-a,UID:111d7009-7392-11e9-b14d-000c2942e394,ResourceVersion:9231,Generation:0,CreationTimestamp:2019-05-11 02:11:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 11 02:11:59.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-b,UID:28f90423-7392-11e9-b14d-000c2942e394,ResourceVersion:9257,Generation:0,CreationTimestamp:2019-05-11 02:12:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 11 02:11:59.683: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-b,UID:28f90423-7392-11e9-b14d-000c2942e394,ResourceVersion:9257,Generation:0,CreationTimestamp:2019-05-11 02:12:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 11 02:12:09.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-b,UID:28f90423-7392-11e9-b14d-000c2942e394,ResourceVersion:9283,Generation:0,CreationTimestamp:2019-05-11 02:12:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 11 02:12:09.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3411,SelfLink:/api/v1/namespaces/watch-3411/configmaps/e2e-watch-test-configmap-b,UID:28f90423-7392-11e9-b14d-000c2942e394,ResourceVersion:9283,Generation:0,CreationTimestamp:2019-05-11 02:12:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:12:19.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3411" for this suite.
May 11 02:12:25.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:12:25.767: INFO: namespace watch-3411 deletion completed in 6.072545366s

• [SLOW TEST:92.261 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:12:25.768: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 11 02:12:25.806: INFO: Waiting up to 5m0s for pod "downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9" in namespace "downward-api-5425" to be "success or failure"
May 11 02:12:25.809: INFO: Pod "downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603834ms
May 11 02:12:27.813: INFO: Pod "downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006384425s
May 11 02:12:29.816: INFO: Pod "downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009345409s
STEP: Saw pod success
May 11 02:12:29.816: INFO: Pod "downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:12:29.818: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 02:12:29.945: INFO: Waiting for pod downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:12:29.947: INFO: Pod downward-api-38459ed9-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:12:29.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5425" for this suite.
May 11 02:12:35.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:12:36.018: INFO: namespace downward-api-5425 deletion completed in 6.068448193s

• [SLOW TEST:10.250 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:12:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 11 02:12:36.045: INFO: Waiting up to 5m0s for pod "pod-3e6010e6-7392-11e9-80a1-becd2a02efc9" in namespace "emptydir-880" to be "success or failure"
May 11 02:12:36.047: INFO: Pod "pod-3e6010e6-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.945955ms
May 11 02:12:38.050: INFO: Pod "pod-3e6010e6-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005186438s
May 11 02:12:40.054: INFO: Pod "pod-3e6010e6-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009051836s
STEP: Saw pod success
May 11 02:12:40.054: INFO: Pod "pod-3e6010e6-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:12:40.057: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-3e6010e6-7392-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:12:40.069: INFO: Waiting for pod pod-3e6010e6-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:12:40.071: INFO: Pod pod-3e6010e6-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:12:40.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-880" for this suite.
May 11 02:12:46.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:12:46.153: INFO: namespace emptydir-880 deletion completed in 6.079180244s

• [SLOW TEST:10.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:12:46.153: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 11 02:12:46.177: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:12:51.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1564" for this suite.
May 11 02:13:13.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:13:13.363: INFO: namespace init-container-1564 deletion completed in 22.081093071s

• [SLOW TEST:27.210 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:13:13.363: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 11 02:13:15.412: INFO: Pod pod-hostip-54a3bf90-7392-11e9-80a1-becd2a02efc9 has hostIP: 70.0.48.171
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:13:15.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-894" for this suite.
May 11 02:13:37.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:13:37.495: INFO: namespace pods-894 deletion completed in 22.080121469s

• [SLOW TEST:24.132 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:13:37.496: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 11 02:13:37.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-5405'
May 11 02:13:37.938: INFO: stderr: ""
May 11 02:13:37.938: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 11 02:13:38.941: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:38.941: INFO: Found 0 / 1
May 11 02:13:39.943: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:39.943: INFO: Found 0 / 1
May 11 02:13:40.941: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:40.941: INFO: Found 0 / 1
May 11 02:13:41.941: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:41.941: INFO: Found 1 / 1
May 11 02:13:41.941: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 11 02:13:41.943: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:41.943: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 02:13:41.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 patch pod redis-master-hzwlm --namespace=kubectl-5405 -p {"metadata":{"annotations":{"x":"y"}}}'
May 11 02:13:42.036: INFO: stderr: ""
May 11 02:13:42.036: INFO: stdout: "pod/redis-master-hzwlm patched\n"
STEP: checking annotations
May 11 02:13:42.038: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:13:42.038: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:13:42.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5405" for this suite.
May 11 02:14:04.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:14:04.111: INFO: namespace kubectl-5405 deletion completed in 22.070845958s

• [SLOW TEST:26.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:14:04.111: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 11 02:14:04.142: INFO: Waiting up to 5m0s for pod "client-containers-72e29797-7392-11e9-80a1-becd2a02efc9" in namespace "containers-4272" to be "success or failure"
May 11 02:14:04.144: INFO: Pod "client-containers-72e29797-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392512ms
May 11 02:14:06.147: INFO: Pod "client-containers-72e29797-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00554861s
May 11 02:14:08.150: INFO: Pod "client-containers-72e29797-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008268245s
STEP: Saw pod success
May 11 02:14:08.150: INFO: Pod "client-containers-72e29797-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:14:08.153: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod client-containers-72e29797-7392-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:14:08.164: INFO: Waiting for pod client-containers-72e29797-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:14:08.166: INFO: Pod client-containers-72e29797-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:14:08.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4272" for this suite.
May 11 02:14:14.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:14:14.242: INFO: namespace containers-4272 deletion completed in 6.072894264s

• [SLOW TEST:10.131 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:14:14.242: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:14:14.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8653'
May 11 02:14:14.365: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 11 02:14:14.366: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 11 02:14:16.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8653'
May 11 02:14:16.499: INFO: stderr: ""
May 11 02:14:16.499: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:14:16.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8653" for this suite.
May 11 02:16:18.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:16:18.587: INFO: namespace kubectl-8653 deletion completed in 2m2.076674574s

• [SLOW TEST:124.345 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:16:18.588: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9
May 11 02:16:18.621: INFO: Pod name my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9: Found 0 pods out of 1
May 11 02:16:23.628: INFO: Pod name my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9: Found 1 pods out of 1
May 11 02:16:23.628: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9" are running
May 11 02:16:23.630: INFO: Pod "my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9-c7qcp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 02:16:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 02:16:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 02:16:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 02:16:19 +0000 UTC Reason: Message:}])
May 11 02:16:23.631: INFO: Trying to dial the pod
May 11 02:16:28.646: INFO: Controller my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9: Got expected result from replica 1 [my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9-c7qcp]: "my-hostname-basic-c30a530c-7392-11e9-80a1-becd2a02efc9-c7qcp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:16:28.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9550" for this suite.
May 11 02:16:34.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:16:34.736: INFO: namespace replication-controller-9550 deletion completed in 6.087794203s

• [SLOW TEST:16.149 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:16:34.736: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ccaa0af0-7392-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:16:34.791: INFO: Waiting up to 5m0s for pod "pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9" in namespace "secrets-7675" to be "success or failure"
May 11 02:16:34.794: INFO: Pod "pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.219534ms
May 11 02:16:36.798: INFO: Pod "pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007101517s
May 11 02:16:38.801: INFO: Pod "pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010642664s
STEP: Saw pod success
May 11 02:16:38.801: INFO: Pod "pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:16:38.804: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 02:16:38.817: INFO: Waiting for pod pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:16:38.818: INFO: Pod pod-secrets-ccada54c-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:16:38.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7675" for this suite.
May 11 02:16:44.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:16:44.893: INFO: namespace secrets-7675 deletion completed in 6.072574951s
STEP: Destroying namespace "secret-namespace-4775" for this suite.
May 11 02:16:50.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:16:50.960: INFO: namespace secret-namespace-4775 deletion completed in 6.066958424s

• [SLOW TEST:16.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:16:50.960: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 11 02:16:53.503: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9"
May 11 02:16:53.503: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9" in namespace "pods-8545" to be "terminated due to deadline exceeded"
May 11 02:16:53.505: INFO: Pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.114184ms
May 11 02:16:55.509: INFO: Pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005668165s
May 11 02:16:57.514: INFO: Pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01079361s
May 11 02:16:57.514: INFO: Pod "pod-update-activedeadlineseconds-d6551011-7392-11e9-80a1-becd2a02efc9" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:16:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8545" for this suite.
May 11 02:17:03.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:17:03.610: INFO: namespace pods-8545 deletion completed in 6.09071334s

• [SLOW TEST:12.650 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:17:03.611: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 11 02:17:03.645: INFO: Waiting up to 5m0s for pod "var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9" in namespace "var-expansion-3083" to be "success or failure"
May 11 02:17:03.647: INFO: Pod "var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099048ms
May 11 02:17:05.650: INFO: Pod "var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004715638s
STEP: Saw pod success
May 11 02:17:05.650: INFO: Pod "var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:17:05.652: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 02:17:05.664: INFO: Waiting for pod var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:17:05.666: INFO: Pod var-expansion-dde0a4a4-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:17:05.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3083" for this suite.
May 11 02:17:11.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:17:11.740: INFO: namespace var-expansion-3083 deletion completed in 6.071275704s

• [SLOW TEST:8.129 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:17:11.740: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2rm8b in namespace proxy-9940
I0511 02:17:11.797232      16 runners.go:184] Created replication controller with name: proxy-service-2rm8b, namespace: proxy-9940, replica count: 1
I0511 02:17:12.847847      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 02:17:13.848063      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 02:17:14.848279      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 02:17:15.848548      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 02:17:16.848865      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 02:17:17.849109      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0511 02:17:18.849326      16 runners.go:184] proxy-service-2rm8b Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 02:17:18.852: INFO: setup took 7.084062132s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 11 02:17:18.856: INFO: (0) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 4.485406ms)
May 11 02:17:18.856: INFO: (0) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 4.351335ms)
May 11 02:17:18.861: INFO: (0) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 9.098055ms)
May 11 02:17:18.861: INFO: (0) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 8.769127ms)
May 11 02:17:18.861: INFO: (0) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 8.943309ms)
May 11 02:17:18.861: INFO: (0) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 8.924156ms)
May 11 02:17:18.866: INFO: (0) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 14.331022ms)
May 11 02:17:18.867: INFO: (0) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 14.558842ms)
May 11 02:17:18.867: INFO: (0) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 14.487324ms)
May 11 02:17:18.871: INFO: (0) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 18.659749ms)
May 11 02:17:18.871: INFO: (0) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 18.685507ms)
May 11 02:17:18.874: INFO: (0) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 21.57102ms)
May 11 02:17:18.874: INFO: (0) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 21.591734ms)
May 11 02:17:18.874: INFO: (0) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 21.976906ms)
May 11 02:17:18.875: INFO: (0) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 22.639638ms)
May 11 02:17:18.879: INFO: (0) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 27.147134ms)
May 11 02:17:18.886: INFO: (1) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 5.5187ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 7.06362ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 6.701797ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 6.366066ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 7.45348ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 7.038477ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 6.550974ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 6.873959ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 7.156368ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 6.456778ms)
May 11 02:17:18.887: INFO: (1) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 6.819839ms)
May 11 02:17:18.889: INFO: (1) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 9.266158ms)
May 11 02:17:18.889: INFO: (1) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.490591ms)
May 11 02:17:18.889: INFO: (1) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 8.652244ms)
May 11 02:17:18.889: INFO: (1) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 9.404155ms)
May 11 02:17:18.889: INFO: (1) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 9.180338ms)
May 11 02:17:18.893: INFO: (2) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 3.636248ms)
May 11 02:17:18.893: INFO: (2) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 4.048074ms)
May 11 02:17:18.895: INFO: (2) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.581769ms)
May 11 02:17:18.895: INFO: (2) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.905567ms)
May 11 02:17:18.895: INFO: (2) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 6.049685ms)
May 11 02:17:18.895: INFO: (2) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 6.275825ms)
May 11 02:17:18.895: INFO: (2) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 6.214363ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 6.389024ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 6.388922ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 6.21158ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 6.628193ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 6.733785ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 6.634761ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 6.668128ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 6.698837ms)
May 11 02:17:18.896: INFO: (2) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 6.797018ms)
May 11 02:17:18.900: INFO: (3) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 3.178173ms)
May 11 02:17:18.900: INFO: (3) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 3.810124ms)
May 11 02:17:18.900: INFO: (3) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 3.464392ms)
May 11 02:17:18.900: INFO: (3) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 3.290552ms)
May 11 02:17:18.901: INFO: (3) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 4.061954ms)
May 11 02:17:18.901: INFO: (3) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 4.243279ms)
May 11 02:17:18.901: INFO: (3) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 4.790153ms)
May 11 02:17:18.902: INFO: (3) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 5.083074ms)
May 11 02:17:18.902: INFO: (3) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 5.18097ms)
May 11 02:17:18.902: INFO: (3) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 5.803955ms)
May 11 02:17:18.902: INFO: (3) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.661366ms)
May 11 02:17:18.902: INFO: (3) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 6.0526ms)
May 11 02:17:18.903: INFO: (3) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 5.940343ms)
May 11 02:17:18.903: INFO: (3) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.80393ms)
May 11 02:17:18.903: INFO: (3) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 6.042232ms)
May 11 02:17:18.903: INFO: (3) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 6.468891ms)
May 11 02:17:18.908: INFO: (4) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 4.862214ms)
May 11 02:17:18.908: INFO: (4) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 5.3679ms)
May 11 02:17:18.908: INFO: (4) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.619259ms)
May 11 02:17:18.908: INFO: (4) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.493075ms)
May 11 02:17:18.908: INFO: (4) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.40585ms)
May 11 02:17:18.909: INFO: (4) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 5.458879ms)
May 11 02:17:18.909: INFO: (4) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 5.394951ms)
May 11 02:17:18.909: INFO: (4) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 5.70339ms)
May 11 02:17:18.909: INFO: (4) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 5.709728ms)
May 11 02:17:18.909: INFO: (4) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.631495ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 7.409067ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.574893ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 7.320921ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.870401ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 7.670917ms)
May 11 02:17:18.911: INFO: (4) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.05369ms)
May 11 02:17:18.926: INFO: (5) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 14.626413ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 15.058493ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 15.329647ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 15.110581ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 15.236276ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 16.025807ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 16.45575ms)
May 11 02:17:18.927: INFO: (5) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 16.217624ms)
May 11 02:17:18.928: INFO: (5) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 15.107714ms)
May 11 02:17:18.929: INFO: (5) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 17.453633ms)
May 11 02:17:18.931: INFO: (5) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 19.126916ms)
May 11 02:17:18.931: INFO: (5) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 19.080398ms)
May 11 02:17:18.931: INFO: (5) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 18.704122ms)
May 11 02:17:18.931: INFO: (5) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 18.852997ms)
May 11 02:17:18.931: INFO: (5) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 19.99597ms)
May 11 02:17:18.932: INFO: (5) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 20.038131ms)
May 11 02:17:18.939: INFO: (6) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 5.952866ms)
May 11 02:17:18.939: INFO: (6) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 7.127904ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 7.06421ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 6.48281ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 6.550538ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 6.870698ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 7.726489ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 7.127189ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 6.781354ms)
May 11 02:17:18.940: INFO: (6) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 8.182955ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.92202ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.802602ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 8.206126ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 8.17167ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 7.973658ms)
May 11 02:17:18.941: INFO: (6) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 8.039661ms)
May 11 02:17:18.945: INFO: (7) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 3.256041ms)
May 11 02:17:18.945: INFO: (7) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 3.554143ms)
May 11 02:17:18.945: INFO: (7) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 3.767969ms)
May 11 02:17:18.945: INFO: (7) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 3.510961ms)
May 11 02:17:18.945: INFO: (7) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 3.697859ms)
May 11 02:17:18.946: INFO: (7) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 4.874656ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 5.490364ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.591328ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.681622ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 5.891293ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 5.710117ms)
May 11 02:17:18.947: INFO: (7) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 5.855166ms)
May 11 02:17:18.949: INFO: (7) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 7.1942ms)
May 11 02:17:18.949: INFO: (7) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.143106ms)
May 11 02:17:18.949: INFO: (7) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.354163ms)
May 11 02:17:18.951: INFO: (7) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.923713ms)
May 11 02:17:18.953: INFO: (8) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 2.916127ms)
May 11 02:17:18.954: INFO: (8) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 2.768643ms)
May 11 02:17:18.954: INFO: (8) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 2.906562ms)
May 11 02:17:18.955: INFO: (8) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 4.123397ms)
May 11 02:17:18.955: INFO: (8) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 4.04582ms)
May 11 02:17:18.956: INFO: (8) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 4.096382ms)
May 11 02:17:18.956: INFO: (8) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 4.819636ms)
May 11 02:17:18.956: INFO: (8) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 4.485455ms)
May 11 02:17:18.958: INFO: (8) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 6.58043ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 8.108456ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.710373ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.033753ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 8.58219ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.904203ms)
May 11 02:17:18.959: INFO: (8) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 8.320683ms)
May 11 02:17:18.960: INFO: (8) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 8.574832ms)
May 11 02:17:18.965: INFO: (9) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 4.508616ms)
May 11 02:17:18.965: INFO: (9) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 4.862136ms)
May 11 02:17:18.965: INFO: (9) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 4.834552ms)
May 11 02:17:18.965: INFO: (9) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 4.801515ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 5.536508ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 5.517418ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 5.679542ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.944239ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 5.809804ms)
May 11 02:17:18.966: INFO: (9) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.947213ms)
May 11 02:17:18.967: INFO: (9) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.126764ms)
May 11 02:17:18.968: INFO: (9) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 8.302306ms)
May 11 02:17:18.968: INFO: (9) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 8.132308ms)
May 11 02:17:18.968: INFO: (9) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 8.388729ms)
May 11 02:17:18.968: INFO: (9) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 8.239863ms)
May 11 02:17:18.968: INFO: (9) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 8.264303ms)
May 11 02:17:18.972: INFO: (10) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 3.540034ms)
May 11 02:17:18.972: INFO: (10) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 3.475596ms)
May 11 02:17:18.972: INFO: (10) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 3.53416ms)
May 11 02:17:18.973: INFO: (10) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 4.89612ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 4.884761ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 5.295623ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.097644ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 5.373488ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 5.279842ms)
May 11 02:17:18.974: INFO: (10) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.434609ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 7.05307ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 7.398844ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.220982ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.244576ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 7.471462ms)
May 11 02:17:18.976: INFO: (10) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 7.383286ms)
May 11 02:17:18.982: INFO: (11) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.337965ms)
May 11 02:17:18.982: INFO: (11) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.021265ms)
May 11 02:17:18.983: INFO: (11) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 6.076674ms)
May 11 02:17:18.983: INFO: (11) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 6.037874ms)
May 11 02:17:18.983: INFO: (11) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 5.847854ms)
May 11 02:17:18.983: INFO: (11) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 6.040111ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 7.208232ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.986267ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 7.184541ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 7.862326ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 7.691787ms)
May 11 02:17:18.984: INFO: (11) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 7.665151ms)
May 11 02:17:18.985: INFO: (11) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 8.057788ms)
May 11 02:17:18.985: INFO: (11) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 8.219661ms)
May 11 02:17:18.985: INFO: (11) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 8.31509ms)
May 11 02:17:18.985: INFO: (11) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.678787ms)
May 11 02:17:18.987: INFO: (12) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 2.487195ms)
May 11 02:17:18.989: INFO: (12) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 4.247068ms)
May 11 02:17:18.989: INFO: (12) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 4.485861ms)
May 11 02:17:18.990: INFO: (12) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 4.445724ms)
May 11 02:17:18.990: INFO: (12) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 4.503187ms)
May 11 02:17:18.990: INFO: (12) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.22871ms)
May 11 02:17:18.991: INFO: (12) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 4.964572ms)
May 11 02:17:18.991: INFO: (12) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 5.348221ms)
May 11 02:17:18.991: INFO: (12) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 5.536158ms)
May 11 02:17:18.991: INFO: (12) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.162098ms)
May 11 02:17:18.991: INFO: (12) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.666853ms)
May 11 02:17:18.992: INFO: (12) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 6.163107ms)
May 11 02:17:18.992: INFO: (12) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 6.7107ms)
May 11 02:17:18.992: INFO: (12) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 6.225394ms)
May 11 02:17:18.992: INFO: (12) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 6.698087ms)
May 11 02:17:18.992: INFO: (12) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 6.239062ms)
May 11 02:17:18.998: INFO: (13) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.277808ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 7.226477ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 7.792328ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 7.24906ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 8.041133ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 7.417635ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 7.113513ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 7.775289ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 7.742817ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 8.031923ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 7.167477ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 7.296506ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 7.958218ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 7.712443ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 7.803794ms)
May 11 02:17:19.000: INFO: (13) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 7.393355ms)
May 11 02:17:19.003: INFO: (14) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 2.386358ms)
May 11 02:17:19.004: INFO: (14) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 3.873536ms)
May 11 02:17:19.005: INFO: (14) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 3.87624ms)
May 11 02:17:19.005: INFO: (14) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 3.873406ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 5.092225ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.363766ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 5.257842ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 5.35064ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 5.130312ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 5.206478ms)
May 11 02:17:19.006: INFO: (14) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 5.58421ms)
May 11 02:17:19.007: INFO: (14) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 6.690751ms)
May 11 02:17:19.007: INFO: (14) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 6.38434ms)
May 11 02:17:19.007: INFO: (14) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 6.558811ms)
May 11 02:17:19.007: INFO: (14) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 6.658086ms)
May 11 02:17:19.007: INFO: (14) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 6.6358ms)
May 11 02:17:19.009: INFO: (15) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 2.175107ms)
May 11 02:17:19.019: INFO: (15) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 11.989862ms)
May 11 02:17:19.022: INFO: (15) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 14.296048ms)
May 11 02:17:19.022: INFO: (15) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 14.790839ms)
May 11 02:17:19.023: INFO: (15) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 15.597894ms)
May 11 02:17:19.023: INFO: (15) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 16.087253ms)
May 11 02:17:19.024: INFO: (15) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 16.31886ms)
May 11 02:17:19.024: INFO: (15) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 16.645541ms)
May 11 02:17:19.025: INFO: (15) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 17.147309ms)
May 11 02:17:19.028: INFO: (15) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 20.80257ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 21.202736ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 21.387858ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 21.408564ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 21.575249ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 21.824487ms)
May 11 02:17:19.029: INFO: (15) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 21.972032ms)
May 11 02:17:19.040: INFO: (16) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 10.584045ms)
May 11 02:17:19.040: INFO: (16) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 10.621937ms)
May 11 02:17:19.040: INFO: (16) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 10.086723ms)
May 11 02:17:19.041: INFO: (16) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 11.378969ms)
May 11 02:17:19.041: INFO: (16) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 11.196239ms)
May 11 02:17:19.041: INFO: (16) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 12.028745ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 11.986102ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 11.079154ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 11.805719ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 12.209552ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 11.523617ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 11.84129ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 11.593978ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 12.05842ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 12.165444ms)
May 11 02:17:19.042: INFO: (16) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 11.66824ms)
May 11 02:17:19.052: INFO: (17) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 9.713456ms)
May 11 02:17:19.052: INFO: (17) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 9.068502ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 10.576634ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 10.075046ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 10.894755ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 11.786857ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 11.093472ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 11.554841ms)
May 11 02:17:19.054: INFO: (17) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 10.878952ms)
May 11 02:17:19.055: INFO: (17) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 12.668243ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 13.832431ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 13.324583ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 14.337767ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 14.513883ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 13.805879ms)
May 11 02:17:19.057: INFO: (17) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 14.035436ms)
May 11 02:17:19.068: INFO: (18) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 10.673358ms)
May 11 02:17:19.068: INFO: (18) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 10.165387ms)
May 11 02:17:19.069: INFO: (18) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 11.328117ms)
May 11 02:17:19.069: INFO: (18) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 10.975295ms)
May 11 02:17:19.069: INFO: (18) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 11.366125ms)
May 11 02:17:19.070: INFO: (18) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 11.785727ms)
May 11 02:17:19.070: INFO: (18) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 12.009817ms)
May 11 02:17:19.070: INFO: (18) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 12.75775ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 12.565823ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 13.231891ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 13.083987ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 12.820824ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 13.858881ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 13.353239ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 13.532546ms)
May 11 02:17:19.071: INFO: (18) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 13.725267ms)
May 11 02:17:19.081: INFO: (19) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 9.440089ms)
May 11 02:17:19.083: INFO: (19) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:162/proxy/: bar (200; 10.891567ms)
May 11 02:17:19.084: INFO: (19) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">... (200; 11.654242ms)
May 11 02:17:19.084: INFO: (19) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 12.572546ms)
May 11 02:17:19.084: INFO: (19) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:443/proxy/tlsrewritem... (200; 11.739275ms)
May 11 02:17:19.085: INFO: (19) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:460/proxy/: tls baz (200; 12.81578ms)
May 11 02:17:19.085: INFO: (19) /api/v1/namespaces/proxy-9940/pods/https:proxy-service-2rm8b-7xzx4:462/proxy/: tls qux (200; 12.489122ms)
May 11 02:17:19.085: INFO: (19) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname1/proxy/: foo (200; 13.191981ms)
May 11 02:17:19.085: INFO: (19) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname2/proxy/: bar (200; 13.090054ms)
May 11 02:17:19.085: INFO: (19) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4/proxy/rewriteme">test</a> (200; 13.056879ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9940/pods/proxy-service-2rm8b-7xzx4:1080/proxy/rewriteme">test<... (200; 13.019286ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname1/proxy/: tls baz (200; 13.181318ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/services/proxy-service-2rm8b:portname2/proxy/: bar (200; 13.472982ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/services/https:proxy-service-2rm8b:tlsportname2/proxy/: tls qux (200; 13.539227ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/services/http:proxy-service-2rm8b:portname1/proxy/: foo (200; 13.610982ms)
May 11 02:17:19.086: INFO: (19) /api/v1/namespaces/proxy-9940/pods/http:proxy-service-2rm8b-7xzx4:160/proxy/: foo (200; 13.774062ms)
STEP: deleting ReplicationController proxy-service-2rm8b in namespace proxy-9940, will wait for the garbage collector to delete the pods
May 11 02:17:19.148: INFO: Deleting ReplicationController proxy-service-2rm8b took: 7.955569ms
May 11 02:17:19.548: INFO: Terminating ReplicationController proxy-service-2rm8b pods took: 400.292007ms
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:17:21.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9940" for this suite.
May 11 02:17:27.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:17:27.326: INFO: namespace proxy-9940 deletion completed in 6.072750372s

• [SLOW TEST:15.586 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:17:27.326: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ec021860-7392-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:17:27.355: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9" in namespace "configmap-5564" to be "success or failure"
May 11 02:17:27.356: INFO: Pod "pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656284ms
May 11 02:17:29.359: INFO: Pod "pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004447447s
STEP: Saw pod success
May 11 02:17:29.359: INFO: Pod "pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:17:29.361: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:17:29.374: INFO: Waiting for pod pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:17:29.376: INFO: Pod pod-configmaps-ec026ee3-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:17:29.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5564" for this suite.
May 11 02:17:35.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:17:35.446: INFO: namespace configmap-5564 deletion completed in 6.067027562s

• [SLOW TEST:8.120 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:17:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:17:35.477: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9" in namespace "downward-api-2717" to be "success or failure"
May 11 02:17:35.479: INFO: Pod "downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388316ms
May 11 02:17:37.483: INFO: Pod "downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006448171s
May 11 02:17:39.488: INFO: Pod "downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011089586s
STEP: Saw pod success
May 11 02:17:39.488: INFO: Pod "downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:17:39.491: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-3 pod downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:17:39.662: INFO: Waiting for pod downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9 to disappear
May 11 02:17:39.664: INFO: Pod downwardapi-volume-f0d9b93a-7392-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:17:39.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2717" for this suite.
May 11 02:17:45.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:17:45.748: INFO: namespace downward-api-2717 deletion completed in 6.082249605s

• [SLOW TEST:10.301 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:17:45.749: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 11 02:17:45.794: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:45.795: INFO: Number of nodes with available pods: 0
May 11 02:17:45.795: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:17:46.798: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:46.801: INFO: Number of nodes with available pods: 0
May 11 02:17:46.801: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:17:47.799: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:47.801: INFO: Number of nodes with available pods: 1
May 11 02:17:47.801: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:48.798: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:48.801: INFO: Number of nodes with available pods: 2
May 11 02:17:48.801: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:49.799: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:49.801: INFO: Number of nodes with available pods: 3
May 11 02:17:49.801: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 11 02:17:49.812: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:49.814: INFO: Number of nodes with available pods: 2
May 11 02:17:49.814: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:50.817: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:50.819: INFO: Number of nodes with available pods: 2
May 11 02:17:50.819: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:51.818: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:51.820: INFO: Number of nodes with available pods: 2
May 11 02:17:51.820: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:52.818: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:52.821: INFO: Number of nodes with available pods: 2
May 11 02:17:52.821: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:53.818: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:53.821: INFO: Number of nodes with available pods: 2
May 11 02:17:53.821: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:54.819: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:54.823: INFO: Number of nodes with available pods: 2
May 11 02:17:54.823: INFO: Node craig-k8s-certification-2-sprout-hero-2 is running more than one daemon pod
May 11 02:17:55.819: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:17:55.823: INFO: Number of nodes with available pods: 3
May 11 02:17:55.823: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6639, will wait for the garbage collector to delete the pods
May 11 02:17:55.884: INFO: Deleting DaemonSet.extensions daemon-set took: 4.107522ms
May 11 02:17:56.284: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.248306ms
May 11 02:17:59.189: INFO: Number of nodes with available pods: 0
May 11 02:17:59.189: INFO: Number of running nodes: 0, number of available pods: 0
May 11 02:17:59.192: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6639/daemonsets","resourceVersion":"10914"},"items":null}

May 11 02:17:59.193: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6639/pods","resourceVersion":"10914"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:17:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6639" for this suite.
May 11 02:18:05.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:18:05.284: INFO: namespace daemonsets-6639 deletion completed in 6.08019637s

• [SLOW TEST:19.536 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:18:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 11 02:18:05.564: INFO: Pod name wrapped-volume-race-02b24eb3-7393-11e9-80a1-becd2a02efc9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-02b24eb3-7393-11e9-80a1-becd2a02efc9 in namespace emptydir-wrapper-5242, will wait for the garbage collector to delete the pods
May 11 02:18:19.662: INFO: Deleting ReplicationController wrapped-volume-race-02b24eb3-7393-11e9-80a1-becd2a02efc9 took: 4.582323ms
May 11 02:18:20.062: INFO: Terminating ReplicationController wrapped-volume-race-02b24eb3-7393-11e9-80a1-becd2a02efc9 pods took: 400.265522ms
STEP: Creating RC which spawns configmap-volume pods
May 11 02:19:02.675: INFO: Pod name wrapped-volume-race-24d1d057-7393-11e9-80a1-becd2a02efc9: Found 0 pods out of 5
May 11 02:19:07.681: INFO: Pod name wrapped-volume-race-24d1d057-7393-11e9-80a1-becd2a02efc9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-24d1d057-7393-11e9-80a1-becd2a02efc9 in namespace emptydir-wrapper-5242, will wait for the garbage collector to delete the pods
May 11 02:19:17.756: INFO: Deleting ReplicationController wrapped-volume-race-24d1d057-7393-11e9-80a1-becd2a02efc9 took: 5.215822ms
May 11 02:19:18.256: INFO: Terminating ReplicationController wrapped-volume-race-24d1d057-7393-11e9-80a1-becd2a02efc9 pods took: 500.270417ms
STEP: Creating RC which spawns configmap-volume pods
May 11 02:20:02.673: INFO: Pod name wrapped-volume-race-489434cf-7393-11e9-80a1-becd2a02efc9: Found 0 pods out of 5
May 11 02:20:07.680: INFO: Pod name wrapped-volume-race-489434cf-7393-11e9-80a1-becd2a02efc9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-489434cf-7393-11e9-80a1-becd2a02efc9 in namespace emptydir-wrapper-5242, will wait for the garbage collector to delete the pods
May 11 02:20:17.761: INFO: Deleting ReplicationController wrapped-volume-race-489434cf-7393-11e9-80a1-becd2a02efc9 took: 4.786418ms
May 11 02:20:18.161: INFO: Terminating ReplicationController wrapped-volume-race-489434cf-7393-11e9-80a1-becd2a02efc9 pods took: 400.239121ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:20:55.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5242" for this suite.
May 11 02:21:01.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:21:01.520: INFO: namespace emptydir-wrapper-5242 deletion completed in 6.07236391s

• [SLOW TEST:176.236 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:21:01.521: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3057
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3057
STEP: Creating statefulset with conflicting port in namespace statefulset-3057
STEP: Waiting until pod test-pod will start running in namespace statefulset-3057
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3057
May 11 02:21:05.567: INFO: Observed stateful pod in namespace: statefulset-3057, name: ss-0, uid: 6e1ce56a-7393-11e9-b14d-000c2942e394, status phase: Pending. Waiting for statefulset controller to delete.
May 11 02:21:05.761: INFO: Observed stateful pod in namespace: statefulset-3057, name: ss-0, uid: 6e1ce56a-7393-11e9-b14d-000c2942e394, status phase: Failed. Waiting for statefulset controller to delete.
May 11 02:21:05.768: INFO: Observed stateful pod in namespace: statefulset-3057, name: ss-0, uid: 6e1ce56a-7393-11e9-b14d-000c2942e394, status phase: Failed. Waiting for statefulset controller to delete.
May 11 02:21:05.772: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3057
STEP: Removing pod with conflicting port in namespace statefulset-3057
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3057 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 11 02:21:09.790: INFO: Deleting all statefulset in ns statefulset-3057
May 11 02:21:09.793: INFO: Scaling statefulset ss to 0
May 11 02:21:29.804: INFO: Waiting for statefulset status.replicas updated to 0
May 11 02:21:29.808: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:21:29.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3057" for this suite.
May 11 02:21:35.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:21:35.898: INFO: namespace statefulset-3057 deletion completed in 6.077247929s

• [SLOW TEST:34.377 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:21:35.898: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 11 02:21:38.454: INFO: Successfully updated pod "labelsupdate802b08f7-7393-11e9-80a1-becd2a02efc9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:21:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3379" for this suite.
May 11 02:22:02.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:22:02.555: INFO: namespace downward-api-3379 deletion completed in 22.075777603s

• [SLOW TEST:26.658 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:22:02.555: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-900f2f2f-7393-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:22:02.588: INFO: Waiting up to 5m0s for pod "pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9" in namespace "secrets-6622" to be "success or failure"
May 11 02:22:02.593: INFO: Pod "pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337421ms
May 11 02:22:04.597: INFO: Pod "pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008343762s
STEP: Saw pod success
May 11 02:22:04.597: INFO: Pod "pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:22:04.599: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 02:22:04.614: INFO: Waiting for pod pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9 to disappear
May 11 02:22:04.616: INFO: Pod pod-secrets-900f85d3-7393-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:22:04.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6622" for this suite.
May 11 02:22:10.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:22:10.699: INFO: namespace secrets-6622 deletion completed in 6.08129881s

• [SLOW TEST:8.144 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:22:10.700: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2745.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2745.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2745.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2745.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2745.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2745.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 02:22:26.763: INFO: DNS probes using dns-2745/dns-test-94e98874-7393-11e9-80a1-becd2a02efc9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:22:26.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2745" for this suite.
May 11 02:22:32.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:22:32.843: INFO: namespace dns-2745 deletion completed in 6.066502019s

• [SLOW TEST:22.143 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:22:32.843: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 11 02:22:32.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:32.886: INFO: Number of nodes with available pods: 0
May 11 02:22:32.886: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:22:33.890: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:33.892: INFO: Number of nodes with available pods: 0
May 11 02:22:33.892: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:22:34.890: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:34.892: INFO: Number of nodes with available pods: 3
May 11 02:22:34.892: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 11 02:22:34.904: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:34.906: INFO: Number of nodes with available pods: 2
May 11 02:22:34.906: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:22:35.909: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:35.911: INFO: Number of nodes with available pods: 2
May 11 02:22:35.911: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:22:36.909: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:22:36.911: INFO: Number of nodes with available pods: 3
May 11 02:22:36.911: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9051, will wait for the garbage collector to delete the pods
May 11 02:22:36.970: INFO: Deleting DaemonSet.extensions daemon-set took: 4.468739ms
May 11 02:22:37.370: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.292671ms
May 11 02:22:42.774: INFO: Number of nodes with available pods: 0
May 11 02:22:42.774: INFO: Number of running nodes: 0, number of available pods: 0
May 11 02:22:42.777: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9051/daemonsets","resourceVersion":"13050"},"items":null}

May 11 02:22:42.780: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9051/pods","resourceVersion":"13050"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:22:42.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9051" for this suite.
May 11 02:22:48.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:22:48.888: INFO: namespace daemonsets-9051 deletion completed in 6.092937457s

• [SLOW TEST:16.045 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:22:48.888: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0511 02:23:28.942379      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:23:28.942: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:23:28.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1593" for this suite.
May 11 02:23:34.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:23:35.111: INFO: namespace gc-1593 deletion completed in 6.166485278s

• [SLOW TEST:46.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:23:35.111: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-c73c7637-7393-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:23:35.162: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9" in namespace "projected-2237" to be "success or failure"
May 11 02:23:35.167: INFO: Pod "pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456715ms
May 11 02:23:37.169: INFO: Pod "pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007125365s
May 11 02:23:39.174: INFO: Pod "pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011988725s
STEP: Saw pod success
May 11 02:23:39.174: INFO: Pod "pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:23:39.176: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:23:39.195: INFO: Waiting for pod pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9 to disappear
May 11 02:23:39.197: INFO: Pod pod-projected-configmaps-c73d3d01-7393-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:23:39.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2237" for this suite.
May 11 02:23:45.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:23:45.285: INFO: namespace projected-2237 deletion completed in 6.082736082s

• [SLOW TEST:10.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:23:45.285: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-cd4bb40e-7393-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:23:45.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9" in namespace "configmap-4057" to be "success or failure"
May 11 02:23:45.334: INFO: Pod "pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.713745ms
May 11 02:23:47.338: INFO: Pod "pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011381219s
May 11 02:23:49.344: INFO: Pod "pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017180135s
STEP: Saw pod success
May 11 02:23:49.344: INFO: Pod "pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:23:49.349: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:23:49.364: INFO: Waiting for pod pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9 to disappear
May 11 02:23:49.366: INFO: Pod pod-configmaps-cd4c1abb-7393-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:23:49.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4057" for this suite.
May 11 02:23:55.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:23:55.437: INFO: namespace configmap-4057 deletion completed in 6.068955335s

• [SLOW TEST:10.152 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:23:55.437: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 11 02:23:55.465: INFO: Waiting up to 5m0s for pod "client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9" in namespace "containers-2312" to be "success or failure"
May 11 02:23:55.468: INFO: Pod "client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213155ms
May 11 02:23:57.472: INFO: Pod "client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007290659s
May 11 02:23:59.476: INFO: Pod "client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010675484s
STEP: Saw pod success
May 11 02:23:59.476: INFO: Pod "client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:23:59.478: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:23:59.493: INFO: Waiting for pod client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9 to disappear
May 11 02:23:59.495: INFO: Pod client-containers-d35745d0-7393-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:23:59.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2312" for this suite.
May 11 02:24:05.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:24:05.563: INFO: namespace containers-2312 deletion completed in 6.065625919s

• [SLOW TEST:10.126 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:24:05.563: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 11 02:24:06.147: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 11 02:24:08.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 02:24:10.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 02:24:12.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138246, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 02:24:16.005: INFO: Waited 1.820786739s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:24:16.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6022" for this suite.
May 11 02:24:22.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:24:22.767: INFO: namespace aggregator-6022 deletion completed in 6.173641306s

• [SLOW TEST:17.204 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:24:22.767: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-pvmf
STEP: Creating a pod to test atomic-volume-subpath
May 11 02:24:22.805: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pvmf" in namespace "subpath-4141" to be "success or failure"
May 11 02:24:22.807: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671373ms
May 11 02:24:24.813: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008453044s
May 11 02:24:26.816: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011346172s
May 11 02:24:28.820: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015566073s
May 11 02:24:30.824: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 8.019207273s
May 11 02:24:32.828: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 10.02336526s
May 11 02:24:34.833: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 12.028026528s
May 11 02:24:36.838: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 14.033618047s
May 11 02:24:38.842: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 16.037132713s
May 11 02:24:40.846: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 18.041635173s
May 11 02:24:42.851: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Running", Reason="", readiness=true. Elapsed: 20.046457264s
May 11 02:24:44.855: INFO: Pod "pod-subpath-test-configmap-pvmf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049842471s
STEP: Saw pod success
May 11 02:24:44.855: INFO: Pod "pod-subpath-test-configmap-pvmf" satisfied condition "success or failure"
May 11 02:24:44.858: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-subpath-test-configmap-pvmf container test-container-subpath-configmap-pvmf: <nil>
STEP: delete the pod
May 11 02:24:44.874: INFO: Waiting for pod pod-subpath-test-configmap-pvmf to disappear
May 11 02:24:44.878: INFO: Pod pod-subpath-test-configmap-pvmf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pvmf
May 11 02:24:44.878: INFO: Deleting pod "pod-subpath-test-configmap-pvmf" in namespace "subpath-4141"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:24:44.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4141" for this suite.
May 11 02:24:50.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:24:50.958: INFO: namespace subpath-4141 deletion completed in 6.07462794s

• [SLOW TEST:28.191 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:24:50.958: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0511 02:24:52.019459      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:24:52.019: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:24:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-825" for this suite.
May 11 02:24:58.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:24:58.100: INFO: namespace gc-825 deletion completed in 6.07785878s

• [SLOW TEST:7.141 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:24:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:24:58.127: INFO: Creating deployment "test-recreate-deployment"
May 11 02:24:58.130: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 11 02:24:58.135: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 11 02:25:00.141: INFO: Waiting deployment "test-recreate-deployment" to complete
May 11 02:25:00.143: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 11 02:25:00.150: INFO: Updating deployment test-recreate-deployment
May 11 02:25:00.150: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 11 02:25:00.194: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3247,SelfLink:/apis/apps/v1/namespaces/deployment-3247/deployments/test-recreate-deployment,UID:f8f741d6-7393-11e9-b14d-000c2942e394,ResourceVersion:14081,Generation:2,CreationTimestamp:2019-05-11 02:24:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-11 02:25:00 +0000 UTC 2019-05-11 02:25:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-11 02:25:00 +0000 UTC 2019-05-11 02:24:58 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 11 02:25:00.198: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3247,SelfLink:/apis/apps/v1/namespaces/deployment-3247/replicasets/test-recreate-deployment-c9cbd8684,UID:fa2e891f-7393-11e9-b14d-000c2942e394,ResourceVersion:14078,Generation:1,CreationTimestamp:2019-05-11 02:25:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8f741d6-7393-11e9-b14d-000c2942e394 0xc00178a590 0xc00178a591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 02:25:00.198: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 11 02:25:00.198: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3247,SelfLink:/apis/apps/v1/namespaces/deployment-3247/replicasets/test-recreate-deployment-7d57d5ff7c,UID:f8f7b788-7393-11e9-b14d-000c2942e394,ResourceVersion:14070,Generation:2,CreationTimestamp:2019-05-11 02:24:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8f741d6-7393-11e9-b14d-000c2942e394 0xc00178a4a7 0xc00178a4a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 02:25:00.201: INFO: Pod "test-recreate-deployment-c9cbd8684-g7vls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-g7vls,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3247,SelfLink:/api/v1/namespaces/deployment-3247/pods/test-recreate-deployment-c9cbd8684-g7vls,UID:fa2ef6c9-7393-11e9-b14d-000c2942e394,ResourceVersion:14082,Generation:0,CreationTimestamp:2019-05-11 02:25:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 fa2e891f-7393-11e9-b14d-000c2942e394 0xc00178ae60 0xc00178ae61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d7bfs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d7bfs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d7bfs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00178aec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00178aee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:24:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:24:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:24:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:25:00 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:,StartTime:2019-05-11 02:24:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:25:00.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3247" for this suite.
May 11 02:25:06.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:25:06.290: INFO: namespace deployment-3247 deletion completed in 6.086109915s

• [SLOW TEST:8.189 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:25:06.290: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 11 02:25:06.317: INFO: Waiting up to 5m0s for pod "pod-fd92693a-7393-11e9-80a1-becd2a02efc9" in namespace "emptydir-5865" to be "success or failure"
May 11 02:25:06.320: INFO: Pod "pod-fd92693a-7393-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.565316ms
May 11 02:25:08.323: INFO: Pod "pod-fd92693a-7393-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006335008s
STEP: Saw pod success
May 11 02:25:08.323: INFO: Pod "pod-fd92693a-7393-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:25:08.325: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-fd92693a-7393-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:25:08.339: INFO: Waiting for pod pod-fd92693a-7393-11e9-80a1-becd2a02efc9 to disappear
May 11 02:25:08.342: INFO: Pod pod-fd92693a-7393-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:25:08.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5865" for this suite.
May 11 02:25:14.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:25:14.426: INFO: namespace emptydir-5865 deletion completed in 6.081255456s

• [SLOW TEST:8.136 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:25:14.426: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6844
May 11 02:25:18.461: INFO: Started pod liveness-http in namespace container-probe-6844
STEP: checking the pod's current state and verifying that restartCount is present
May 11 02:25:18.464: INFO: Initial restart count of pod liveness-http is 0
May 11 02:25:38.513: INFO: Restart count of pod container-probe-6844/liveness-http is now 1 (20.048647534s elapsed)
May 11 02:25:56.557: INFO: Restart count of pod container-probe-6844/liveness-http is now 2 (38.092493144s elapsed)
May 11 02:26:16.597: INFO: Restart count of pod container-probe-6844/liveness-http is now 3 (58.132688729s elapsed)
May 11 02:26:38.644: INFO: Restart count of pod container-probe-6844/liveness-http is now 4 (1m20.180155558s elapsed)
May 11 02:27:40.776: INFO: Restart count of pod container-probe-6844/liveness-http is now 5 (2m22.31209707s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:27:40.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6844" for this suite.
May 11 02:27:46.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:27:46.896: INFO: namespace container-probe-6844 deletion completed in 6.107315565s

• [SLOW TEST:152.470 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:27:46.896: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:27:46.925: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9" in namespace "downward-api-7330" to be "success or failure"
May 11 02:27:46.927: INFO: Pod "downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50071ms
May 11 02:27:48.932: INFO: Pod "downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00745966s
May 11 02:27:50.935: INFO: Pod "downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010774094s
STEP: Saw pod success
May 11 02:27:50.935: INFO: Pod "downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:27:50.938: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:27:50.951: INFO: Waiting for pod downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9 to disappear
May 11 02:27:50.953: INFO: Pod downwardapi-volume-5d4d3c58-7394-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:27:50.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7330" for this suite.
May 11 02:27:56.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:27:57.035: INFO: namespace downward-api-7330 deletion completed in 6.07417729s

• [SLOW TEST:10.139 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:27:57.035: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 11 02:27:57.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-918'
May 11 02:27:57.528: INFO: stderr: ""
May 11 02:27:57.528: INFO: stdout: "pod/pause created\n"
May 11 02:27:57.528: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 11 02:27:57.528: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-918" to be "running and ready"
May 11 02:27:57.532: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090002ms
May 11 02:27:59.534: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006685957s
May 11 02:27:59.534: INFO: Pod "pause" satisfied condition "running and ready"
May 11 02:27:59.534: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 11 02:27:59.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 label pods pause testing-label=testing-label-value --namespace=kubectl-918'
May 11 02:27:59.637: INFO: stderr: ""
May 11 02:27:59.637: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 11 02:27:59.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pod pause -L testing-label --namespace=kubectl-918'
May 11 02:27:59.740: INFO: stderr: ""
May 11 02:27:59.740: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 11 02:27:59.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 label pods pause testing-label- --namespace=kubectl-918'
May 11 02:27:59.829: INFO: stderr: ""
May 11 02:27:59.829: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 11 02:27:59.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pod pause -L testing-label --namespace=kubectl-918'
May 11 02:27:59.928: INFO: stderr: ""
May 11 02:27:59.928: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 11 02:27:59.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-918'
May 11 02:28:00.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 02:28:00.039: INFO: stdout: "pod \"pause\" force deleted\n"
May 11 02:28:00.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get rc,svc -l name=pause --no-headers --namespace=kubectl-918'
May 11 02:28:00.153: INFO: stderr: "No resources found.\n"
May 11 02:28:00.153: INFO: stdout: ""
May 11 02:28:00.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -l name=pause --namespace=kubectl-918 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 02:28:00.265: INFO: stderr: ""
May 11 02:28:00.265: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:28:00.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-918" for this suite.
May 11 02:28:06.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:28:06.354: INFO: namespace kubectl-918 deletion completed in 6.086592852s

• [SLOW TEST:9.319 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:28:06.355: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-68e7c246-7394-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:28:06.396: INFO: Waiting up to 5m0s for pod "pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9" in namespace "secrets-6635" to be "success or failure"
May 11 02:28:06.400: INFO: Pod "pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.834726ms
May 11 02:28:08.403: INFO: Pod "pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00687125s
STEP: Saw pod success
May 11 02:28:08.403: INFO: Pod "pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:28:08.405: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 02:28:08.419: INFO: Waiting for pod pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9 to disappear
May 11 02:28:08.420: INFO: Pod pod-secrets-68e837ff-7394-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:28:08.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6635" for this suite.
May 11 02:28:14.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:28:14.500: INFO: namespace secrets-6635 deletion completed in 6.077171529s

• [SLOW TEST:8.146 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:28:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:28:16.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3993" for this suite.
May 11 02:28:54.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:28:54.649: INFO: namespace kubelet-test-3993 deletion completed in 38.104862536s

• [SLOW TEST:40.149 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:28:54.649: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:28:54.677: INFO: Creating deployment "nginx-deployment"
May 11 02:28:54.682: INFO: Waiting for observed generation 1
May 11 02:28:56.687: INFO: Waiting for all required pods to come up
May 11 02:28:56.690: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 11 02:28:58.699: INFO: Waiting for deployment "nginx-deployment" to complete
May 11 02:28:58.704: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 11 02:28:58.709: INFO: Updating deployment nginx-deployment
May 11 02:28:58.709: INFO: Waiting for observed generation 2
May 11 02:29:00.715: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 11 02:29:00.717: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 11 02:29:00.719: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 11 02:29:00.724: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 11 02:29:00.724: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 11 02:29:00.725: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 11 02:29:00.728: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 11 02:29:00.728: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 11 02:29:00.732: INFO: Updating deployment nginx-deployment
May 11 02:29:00.732: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 11 02:29:00.738: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 11 02:29:00.741: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 11 02:29:00.766: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1648,SelfLink:/apis/apps/v1/namespaces/deployment-1648/deployments/nginx-deployment,UID:85f6128f-7394-11e9-b14d-000c2942e394,ResourceVersion:15226,Generation:3,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-11 02:28:59 +0000 UTC 2019-05-11 02:28:55 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-11 02:29:01 +0000 UTC 2019-05-11 02:29:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 11 02:29:00.776: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1648,SelfLink:/apis/apps/v1/namespaces/deployment-1648/replicasets/nginx-deployment-5f9595f595,UID:885d1eae-7394-11e9-b14d-000c2942e394,ResourceVersion:15219,Generation:3,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 85f6128f-7394-11e9-b14d-000c2942e394 0xc0020ec987 0xc0020ec988}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 02:29:00.776: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 11 02:29:00.776: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1648,SelfLink:/apis/apps/v1/namespaces/deployment-1648/replicasets/nginx-deployment-6f478d8d8,UID:85f69b71-7394-11e9-b14d-000c2942e394,ResourceVersion:15216,Generation:3,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 85f6128f-7394-11e9-b14d-000c2942e394 0xc0020eca57 0xc0020eca58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 11 02:29:00.795: INFO: Pod "nginx-deployment-5f9595f595-5v6ks" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5v6ks,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-5v6ks,UID:899723f8-7394-11e9-b14d-000c2942e394,ResourceVersion:15243,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1857 0xc001bd1858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd18c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd18e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.795: INFO: Pod "nginx-deployment-5f9595f595-8hp8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-8hp8d,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-8hp8d,UID:885e2ced-7394-11e9-b14d-000c2942e394,ResourceVersion:15164,Generation:0,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1947 0xc001bd1948}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd19b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd19d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:59 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:,StartTime:2019-05-11 02:28:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-9bds6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9bds6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-9bds6,UID:89970b70-7394-11e9-b14d-000c2942e394,ResourceVersion:15242,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1ab0 0xc001bd1ab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd1b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd1b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-ck8tz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-ck8tz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-ck8tz,UID:885e21b1-7394-11e9-b14d-000c2942e394,ResourceVersion:15162,Generation:0,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1ba7 0xc001bd1ba8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd1c10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd1c30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:59 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.168,PodIP:,StartTime:2019-05-11 02:28:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-gkzfx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gkzfx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-gkzfx,UID:885da0b8-7394-11e9-b14d-000c2942e394,ResourceVersion:15155,Generation:0,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1d00 0xc001bd1d01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd1d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd1d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:59 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.169,PodIP:,StartTime:2019-05-11 02:28:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-gqrp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gqrp5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-gqrp5,UID:89943177-7394-11e9-b14d-000c2942e394,ResourceVersion:15239,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1e60 0xc001bd1e61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd1ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bd1ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-h7t5j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-h7t5j,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-h7t5j,UID:8863bbc4-7394-11e9-b14d-000c2942e394,ResourceVersion:15183,Generation:0,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc001bd1f70 0xc001bd1f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bd1fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:59 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:,StartTime:2019-05-11 02:28:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-hx2vq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hx2vq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-hx2vq,UID:88629f33-7394-11e9-b14d-000c2942e394,ResourceVersion:15182,Generation:0,CreationTimestamp:2019-05-11 02:28:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc00121e0d0 0xc00121e0d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:59 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.168,PodIP:,StartTime:2019-05-11 02:28:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.796: INFO: Pod "nginx-deployment-5f9595f595-j5sgg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j5sgg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-j5sgg,UID:89933c48-7394-11e9-b14d-000c2942e394,ResourceVersion:15233,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc00121e250 0xc00121e251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e2c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e2e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-5f9595f595-tll2q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tll2q,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-tll2q,UID:89973530-7394-11e9-b14d-000c2942e394,ResourceVersion:15245,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc00121e370 0xc00121e371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-5f9595f595-wnt8f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wnt8f,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-wnt8f,UID:899727f4-7394-11e9-b14d-000c2942e394,ResourceVersion:15244,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc00121e467 0xc00121e468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e500}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-5f9595f595-z5mgd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-z5mgd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-5f9595f595-z5mgd,UID:89941bee-7394-11e9-b14d-000c2942e394,ResourceVersion:15240,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 885d1eae-7394-11e9-b14d-000c2942e394 0xc00121e577 0xc00121e578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-6f478d8d8-27jgx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-27jgx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-27jgx,UID:85f9af19-7394-11e9-b14d-000c2942e394,ResourceVersion:15105,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121e6a0 0xc00121e6a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.58,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ee30f5e73908a78bb919ad60c661d836b9023bc1aff2da64f121442defb18d2a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-6f478d8d8-2hnk9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2hnk9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-2hnk9,UID:8994badf-7394-11e9-b14d-000c2942e394,ResourceVersion:15251,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121e810 0xc00121e811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.797: INFO: Pod "nginx-deployment-6f478d8d8-2ndjk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2ndjk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-2ndjk,UID:89976848-7394-11e9-b14d-000c2942e394,ResourceVersion:15246,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121e960 0xc00121e961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121e9c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121e9e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.798: INFO: Pod "nginx-deployment-6f478d8d8-7ks27" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7ks27,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-7ks27,UID:8997880a-7394-11e9-b14d-000c2942e394,ResourceVersion:15247,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121ea47 0xc00121ea48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121eab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121ead0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.798: INFO: Pod "nginx-deployment-6f478d8d8-87xzz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-87xzz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-87xzz,UID:8997a84c-7394-11e9-b14d-000c2942e394,ResourceVersion:15250,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121eb57 0xc00121eb58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121ebd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121ebf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.798: INFO: Pod "nginx-deployment-6f478d8d8-9j8lf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9j8lf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-9j8lf,UID:85f7f79c-7394-11e9-b14d-000c2942e394,ResourceVersion:15121,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121ec67 0xc00121ec68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121ecd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121ecf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.168,PodIP:10.233.93.13,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d8d634b88d39142f96fd0efd65badc9eac6cffd249acefd09adc3ff130333fca}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.798: INFO: Pod "nginx-deployment-6f478d8d8-bg7t5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bg7t5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-bg7t5,UID:89935a40-7394-11e9-b14d-000c2942e394,ResourceVersion:15234,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121edc0 0xc00121edc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121ee30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121ee50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.799: INFO: Pod "nginx-deployment-6f478d8d8-gm52t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gm52t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-gm52t,UID:8994ff26-7394-11e9-b14d-000c2942e394,ResourceVersion:15252,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121ef00 0xc00121ef01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121ef90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121efb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.799: INFO: Pod "nginx-deployment-6f478d8d8-jpt5h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jpt5h,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-jpt5h,UID:89979fe7-7394-11e9-b14d-000c2942e394,ResourceVersion:15249,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f050 0xc00121f051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f0c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f0e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.799: INFO: Pod "nginx-deployment-6f478d8d8-jrsfv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jrsfv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-jrsfv,UID:85f813d2-7394-11e9-b14d-000c2942e394,ResourceVersion:15106,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f147 0xc00121f148}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.169,PodIP:10.233.122.13,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e7788e8640973516690caeaa6c6cadee63789be4e1585f72d66a05c24b480e9e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.799: INFO: Pod "nginx-deployment-6f478d8d8-lf5jn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lf5jn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-lf5jn,UID:89937861-7394-11e9-b14d-000c2942e394,ResourceVersion:15232,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f2a0 0xc00121f2a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-mdh6v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mdh6v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-mdh6v,UID:85f99571-7394-11e9-b14d-000c2942e394,ResourceVersion:15115,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f3b0 0xc00121f3b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.56,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://140590bd4dab6204eddbb7098898a43c8f1a5d3a11d9e0d5525b7f5bce4b0f90}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-ngjlt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ngjlt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-ngjlt,UID:85f78840-7394-11e9-b14d-000c2942e394,ResourceVersion:15100,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f510 0xc00121f511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.55,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0f75f4b584a9edba65320cba9354ab775108238ca259a688bed8ff8c11cf7a0e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-qhdqt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qhdqt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-qhdqt,UID:8994e4b5-7394-11e9-b14d-000c2942e394,ResourceVersion:15253,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f670 0xc00121f671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-qvcnj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qvcnj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-qvcnj,UID:85f9ce59-7394-11e9-b14d-000c2942e394,ResourceVersion:15097,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f770 0xc00121f771}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.169,PodIP:10.233.122.15,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://aa143c218a8e244305ceb68a20b5326e099e037be485258fec63088ac5070dbb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-s959q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s959q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-s959q,UID:85fcb3ea-7394-11e9-b14d-000c2942e394,ResourceVersion:15110,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121f8f0 0xc00121f8f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121f950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121f970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.57,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:55 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ca26b6243f0d2605f9a6a38be9a657c7066289d5042017a16f615fec9a48e86f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.800: INFO: Pod "nginx-deployment-6f478d8d8-scl2q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-scl2q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-scl2q,UID:85fca09c-7394-11e9-b14d-000c2942e394,ResourceVersion:15102,Generation:0,CreationTimestamp:2019-05-11 02:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121fa50 0xc00121fa51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121fab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121fad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:28:55 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.169,PodIP:10.233.122.14,StartTime:2019-05-11 02:28:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 02:28:56 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bf8c5ace171a81faf21a140ab6366ba340ee260a7b7f4e8f786adde3b633377b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.801: INFO: Pod "nginx-deployment-6f478d8d8-srvf9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-srvf9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-srvf9,UID:89950336-7394-11e9-b14d-000c2942e394,ResourceVersion:15255,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121fbb0 0xc00121fbb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121fc40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121fc60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.801: INFO: Pod "nginx-deployment-6f478d8d8-zc294" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zc294,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-zc294,UID:89926855-7394-11e9-b14d-000c2942e394,ResourceVersion:15224,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121fce0 0xc00121fce1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121fd40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121fd60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:29:01 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 02:29:00.801: INFO: Pod "nginx-deployment-6f478d8d8-zt2bw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zt2bw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1648,SelfLink:/api/v1/namespaces/deployment-1648/pods/nginx-deployment-6f478d8d8-zt2bw,UID:8997964f-7394-11e9-b14d-000c2942e394,ResourceVersion:15248,Generation:0,CreationTimestamp:2019-05-11 02:29:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85f69b71-7394-11e9-b14d-000c2942e394 0xc00121fde0 0xc00121fde1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-njt6w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-njt6w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-njt6w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00121fe40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00121fe60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:29:00.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1648" for this suite.
May 11 02:29:06.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:29:07.077: INFO: namespace deployment-1648 deletion completed in 6.262186776s

• [SLOW TEST:12.428 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:29:07.077: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-8d1d733f-7394-11e9-80a1-becd2a02efc9
STEP: Creating secret with name s-test-opt-upd-8d1d73c4-7394-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8d1d733f-7394-11e9-80a1-becd2a02efc9
STEP: Updating secret s-test-opt-upd-8d1d73c4-7394-11e9-80a1-becd2a02efc9
STEP: Creating secret with name s-test-opt-create-8d1d73e8-7394-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:30:25.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3965" for this suite.
May 11 02:30:47.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:30:47.927: INFO: namespace secrets-3965 deletion completed in 22.070238998s

• [SLOW TEST:100.850 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:30:47.928: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 11 02:30:55.986: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:30:55.989: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:30:57.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:30:57.993: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:30:59.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:30:59.992: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:01.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:01.993: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:03.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:03.993: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:05.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:05.992: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:07.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:07.994: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:09.990: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:09.993: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:11.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:11.993: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:13.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:13.992: INFO: Pod pod-with-prestop-exec-hook still exists
May 11 02:31:15.989: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 11 02:31:15.992: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:31:16.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3229" for this suite.
May 11 02:31:38.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:31:38.080: INFO: namespace container-lifecycle-hook-3229 deletion completed in 22.073947355s

• [SLOW TEST:50.152 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:31:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:31:38.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1966'
May 11 02:31:38.230: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 11 02:31:38.230: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 11 02:31:38.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1966'
May 11 02:31:38.330: INFO: stderr: ""
May 11 02:31:38.330: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:31:38.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1966" for this suite.
May 11 02:31:44.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:31:44.418: INFO: namespace kubectl-1966 deletion completed in 6.084464994s

• [SLOW TEST:6.338 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:31:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:31:49.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3765" for this suite.
May 11 02:32:11.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:32:11.543: INFO: namespace replication-controller-3765 deletion completed in 22.07980884s

• [SLOW TEST:27.125 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:32:11.544: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:32:11.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 version'
May 11 02:32:11.666: INFO: stderr: ""
May 11 02:32:11.666: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:32:11.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6629" for this suite.
May 11 02:32:17.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:32:17.742: INFO: namespace kubectl-6629 deletion completed in 6.072709855s

• [SLOW TEST:6.199 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:32:17.742: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:32:17.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8410'
May 11 02:32:17.869: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 11 02:32:17.869: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
May 11 02:32:17.877: INFO: scanned /root for discovery docs: <nil>
May 11 02:32:17.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8410'
May 11 02:32:33.680: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 11 02:32:33.680: INFO: stdout: "Created e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9\nScaling up e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 11 02:32:33.680: INFO: stdout: "Created e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9\nScaling up e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 11 02:32:33.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8410'
May 11 02:32:33.786: INFO: stderr: ""
May 11 02:32:33.786: INFO: stdout: "e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9-r2bx2 "
May 11 02:32:33.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9-r2bx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8410'
May 11 02:32:33.890: INFO: stderr: ""
May 11 02:32:33.890: INFO: stdout: "true"
May 11 02:32:33.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9-r2bx2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8410'
May 11 02:32:33.996: INFO: stderr: ""
May 11 02:32:33.996: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 11 02:32:33.996: INFO: e2e-test-nginx-rc-9fddd6873e1681c846437fbdf4da0ec9-r2bx2 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 11 02:32:33.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete rc e2e-test-nginx-rc --namespace=kubectl-8410'
May 11 02:32:34.109: INFO: stderr: ""
May 11 02:32:34.109: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:32:34.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8410" for this suite.
May 11 02:32:56.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:32:56.188: INFO: namespace kubectl-8410 deletion completed in 22.07521746s

• [SLOW TEST:38.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:32:56.188: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 11 02:32:56.216: INFO: Waiting up to 5m0s for pod "pod-15a75c82-7395-11e9-80a1-becd2a02efc9" in namespace "emptydir-4493" to be "success or failure"
May 11 02:32:56.219: INFO: Pod "pod-15a75c82-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873192ms
May 11 02:32:58.222: INFO: Pod "pod-15a75c82-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006123468s
STEP: Saw pod success
May 11 02:32:58.222: INFO: Pod "pod-15a75c82-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:32:58.225: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-15a75c82-7395-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:32:58.238: INFO: Waiting for pod pod-15a75c82-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:32:58.239: INFO: Pod pod-15a75c82-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:32:58.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4493" for this suite.
May 11 02:33:04.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:33:04.322: INFO: namespace emptydir-4493 deletion completed in 6.079617273s

• [SLOW TEST:8.134 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:33:04.322: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 11 02:33:04.366: INFO: Waiting up to 5m0s for pod "pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9" in namespace "emptydir-6118" to be "success or failure"
May 11 02:33:04.369: INFO: Pod "pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.576608ms
May 11 02:33:06.373: INFO: Pod "pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006665695s
STEP: Saw pod success
May 11 02:33:06.373: INFO: Pod "pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:33:06.375: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:33:06.393: INFO: Waiting for pod pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:33:06.395: INFO: Pod pod-1a82ba2e-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:33:06.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6118" for this suite.
May 11 02:33:12.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:33:12.462: INFO: namespace emptydir-6118 deletion completed in 6.064809624s

• [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:33:12.462: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 11 02:33:12.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 11 02:33:12.490: INFO: Waiting for terminating namespaces to be deleted...
May 11 02:33:12.492: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-1 before test
May 11 02:33:12.502: INFO: calico-node-mrtgz from kube-system started at 2019-05-11 00:58:53 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:33:12.502: INFO: portworx-mmdlz from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container portworx ready: true, restart count 0
May 11 02:33:12.502: INFO: kube-proxy-hkkn5 from kube-system started at 2019-05-11 00:59:06 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:33:12.502: INFO: calico-kube-controllers-8b6f9c4bd-sw8gv from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 11 02:33:12.502: INFO: stork-scheduler-6bbd49d544-f4zsx from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:33:12.502: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-z45cd from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:33:12.502: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:33:12.502: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-1 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:33:12.502: INFO: nodelocaldns-p9tj4 from kube-system started at 2019-05-11 00:59:23 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:33:12.502: INFO: stork-6c6dcf79d6-dkkf4 from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container stork ready: true, restart count 0
May 11 02:33:12.502: INFO: kubernetes-dashboard-6c7466966c-wq669 from kube-system started at 2019-05-11 00:59:25 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.502: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 11 02:33:12.502: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-2 before test
May 11 02:33:12.513: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-2jslh from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:33:12.513: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:33:12.513: INFO: kube-proxy-n9pvn from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:33:12.513: INFO: stork-scheduler-6bbd49d544-qbncg from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:33:12.513: INFO: sonobuoy-e2e-job-f101e66479304f83 from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container e2e ready: true, restart count 0
May 11 02:33:12.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:33:12.513: INFO: dns-autoscaler-56c969bdb8-hv98f from kube-system started at 2019-05-11 00:59:22 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container autoscaler ready: true, restart count 0
May 11 02:33:12.513: INFO: nodelocaldns-ssrkc from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:33:12.513: INFO: portworx-bpm2q from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container portworx ready: true, restart count 0
May 11 02:33:12.513: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-2 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:33:12.513: INFO: calico-node-zldrd from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:33:12.513: INFO: stork-6c6dcf79d6-cv8rs from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.513: INFO: 	Container stork ready: true, restart count 0
May 11 02:33:12.513: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-3 before test
May 11 02:33:12.549: INFO: calico-node-4s655 from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:33:12.549: INFO: stork-scheduler-6bbd49d544-5bphb from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container stork-scheduler ready: true, restart count 1
May 11 02:33:12.549: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-11 02:09:22 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 11 02:33:12.549: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-h79lj from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:33:12.549: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:33:12.549: INFO: coredns-5466d46f97-sclc9 from kube-system started at 2019-05-11 00:59:26 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container coredns ready: true, restart count 0
May 11 02:33:12.549: INFO: kube-proxy-pg9lg from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:33:12.549: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-3 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:33:12.549: INFO: nodelocaldns-ngb8l from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:33:12.549: INFO: stork-6c6dcf79d6-zdldn from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container stork ready: true, restart count 2
May 11 02:33:12.549: INFO: portworx-npbrq from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:33:12.549: INFO: 	Container portworx ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159d8030c0fe0345], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:33:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5367" for this suite.
May 11 02:33:19.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:33:19.654: INFO: namespace sched-pred-5367 deletion completed in 6.080553314s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.193 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:33:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5418
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 02:33:19.675: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 11 02:33:47.739: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.93.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5418 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:33:47.739: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:33:48.999: INFO: Found all expected endpoints: [netserver-0]
May 11 02:33:49.003: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.90.74 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5418 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:33:49.003: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:33:50.251: INFO: Found all expected endpoints: [netserver-1]
May 11 02:33:50.254: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.122.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5418 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:33:50.254: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:33:51.486: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:33:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5418" for this suite.
May 11 02:34:13.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:34:13.580: INFO: namespace pod-network-test-5418 deletion completed in 22.088707618s

• [SLOW TEST:53.925 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:34:13.580: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0511 02:34:23.651168      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:34:23.651: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:34:23.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6552" for this suite.
May 11 02:34:29.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:34:29.719: INFO: namespace gc-6552 deletion completed in 6.06539803s

• [SLOW TEST:16.138 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:34:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 11 02:34:29.740: INFO: PodSpec: initContainers in spec.initContainers
May 11 02:35:20.193: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4d669f0a-7395-11e9-80a1-becd2a02efc9", GenerateName:"", Namespace:"init-container-8810", SelfLink:"/api/v1/namespaces/init-container-8810/pods/pod-init-4d669f0a-7395-11e9-80a1-becd2a02efc9", UID:"4dacbbaa-7395-11e9-b14d-000c2942e394", ResourceVersion:"17579", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693138870, loc:(*time.Location)(0x8a060e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"740318579"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hvkwl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001737340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hvkwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hvkwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hvkwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0026dcc68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"craig-k8s-certification-2-sprout-hero-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002820e40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0026dcce0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0026dcd00)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0026dcd08), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0026dcd0c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138869, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138869, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138869, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693138870, loc:(*time.Location)(0x8a060e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"70.0.48.171", PodIP:"10.233.90.79", StartTime:(*v1.Time)(0xc001073660), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001b19490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001b19500)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://8d83c585ef12ac3d018e1f9f65fe4bfc00366b8bc1353609f8a8edefce2e776d"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0010736a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001073680), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:35:20.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8810" for this suite.
May 11 02:35:42.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:35:42.285: INFO: namespace init-container-8810 deletion completed in 22.086054623s

• [SLOW TEST:72.566 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:35:42.285: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7775
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 02:35:42.309: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 11 02:36:08.395: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.81:8080/dial?request=hostName&protocol=udp&host=10.233.122.30&port=8081&tries=1'] Namespace:pod-network-test-7775 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:36:08.395: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:36:08.687: INFO: Waiting for endpoints: map[]
May 11 02:36:08.689: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.81:8080/dial?request=hostName&protocol=udp&host=10.233.93.31&port=8081&tries=1'] Namespace:pod-network-test-7775 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:36:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:36:08.863: INFO: Waiting for endpoints: map[]
May 11 02:36:08.865: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.81:8080/dial?request=hostName&protocol=udp&host=10.233.90.80&port=8081&tries=1'] Namespace:pod-network-test-7775 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:36:08.865: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:36:09.074: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:36:09.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7775" for this suite.
May 11 02:36:25.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:36:25.164: INFO: namespace pod-network-test-7775 deletion completed in 16.085088507s

• [SLOW TEST:42.878 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:36:25.164: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0511 02:36:35.215012      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:36:35.215: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:36:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9926" for this suite.
May 11 02:36:41.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:36:41.288: INFO: namespace gc-9926 deletion completed in 6.071287101s

• [SLOW TEST:16.124 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:36:41.289: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 11 02:36:41.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 api-versions'
May 11 02:36:41.403: INFO: stderr: ""
May 11 02:36:41.403: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nportworx.io/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nstork.libopenstorage.org/v1alpha1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:36:41.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2032" for this suite.
May 11 02:36:47.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:36:47.483: INFO: namespace kubectl-2032 deletion completed in 6.077090805s

• [SLOW TEST:6.195 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:36:47.484: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 11 02:36:47.561: INFO: Pod name pod-release: Found 0 pods out of 1
May 11 02:36:52.566: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:36:53.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4214" for this suite.
May 11 02:36:59.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:36:59.655: INFO: namespace replication-controller-4214 deletion completed in 6.067176722s

• [SLOW TEST:12.171 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:36:59.655: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a6c5539a-7395-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:36:59.686: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9" in namespace "projected-7878" to be "success or failure"
May 11 02:36:59.688: INFO: Pod "pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626805ms
May 11 02:37:01.691: INFO: Pod "pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005784119s
STEP: Saw pod success
May 11 02:37:01.692: INFO: Pod "pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:37:01.694: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 02:37:01.707: INFO: Waiting for pod pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:37:01.709: INFO: Pod pod-projected-secrets-a6c5b621-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:37:01.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7878" for this suite.
May 11 02:37:07.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:37:07.775: INFO: namespace projected-7878 deletion completed in 6.063440201s

• [SLOW TEST:8.121 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:37:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:37:07.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9" in namespace "downward-api-9266" to be "success or failure"
May 11 02:37:07.810: INFO: Pod "downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4528ms
May 11 02:37:09.813: INFO: Pod "downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007036629s
STEP: Saw pod success
May 11 02:37:09.813: INFO: Pod "downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:37:09.817: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:37:09.834: INFO: Waiting for pod downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:37:09.836: INFO: Pod downwardapi-volume-ab9cc84e-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:37:09.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9266" for this suite.
May 11 02:37:15.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:37:15.936: INFO: namespace downward-api-9266 deletion completed in 6.096985778s

• [SLOW TEST:8.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:37:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 11 02:37:15.966: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 11 02:37:15.971: INFO: Waiting for terminating namespaces to be deleted...
May 11 02:37:15.973: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-1 before test
May 11 02:37:15.978: INFO: calico-node-mrtgz from kube-system started at 2019-05-11 00:58:53 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:37:15.978: INFO: portworx-mmdlz from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container portworx ready: true, restart count 0
May 11 02:37:15.978: INFO: kube-proxy-hkkn5 from kube-system started at 2019-05-11 00:59:06 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:37:15.978: INFO: calico-kube-controllers-8b6f9c4bd-sw8gv from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 11 02:37:15.978: INFO: stork-scheduler-6bbd49d544-f4zsx from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:37:15.978: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-z45cd from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:37:15.978: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:37:15.978: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-1 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:37:15.978: INFO: nodelocaldns-p9tj4 from kube-system started at 2019-05-11 00:59:23 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:37:15.978: INFO: stork-6c6dcf79d6-dkkf4 from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container stork ready: true, restart count 0
May 11 02:37:15.978: INFO: kubernetes-dashboard-6c7466966c-wq669 from kube-system started at 2019-05-11 00:59:25 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.978: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 11 02:37:15.978: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-2 before test
May 11 02:37:15.985: INFO: dns-autoscaler-56c969bdb8-hv98f from kube-system started at 2019-05-11 00:59:22 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container autoscaler ready: true, restart count 0
May 11 02:37:15.985: INFO: nodelocaldns-ssrkc from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:37:15.985: INFO: portworx-bpm2q from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container portworx ready: true, restart count 0
May 11 02:37:15.985: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-2 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:37:15.985: INFO: calico-node-zldrd from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:37:15.985: INFO: stork-6c6dcf79d6-cv8rs from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container stork ready: true, restart count 0
May 11 02:37:15.985: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-2jslh from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:37:15.985: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:37:15.985: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:37:15.985: INFO: kube-proxy-n9pvn from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.986: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:37:15.986: INFO: stork-scheduler-6bbd49d544-qbncg from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.986: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:37:15.986: INFO: sonobuoy-e2e-job-f101e66479304f83 from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:37:15.986: INFO: 	Container e2e ready: true, restart count 0
May 11 02:37:15.986: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:37:15.986: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-3 before test
May 11 02:37:15.993: INFO: kube-proxy-pg9lg from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:37:15.993: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-3 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:37:15.993: INFO: nodelocaldns-ngb8l from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:37:15.993: INFO: stork-6c6dcf79d6-zdldn from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container stork ready: true, restart count 2
May 11 02:37:15.993: INFO: portworx-npbrq from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container portworx ready: true, restart count 0
May 11 02:37:15.993: INFO: calico-node-4s655 from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:37:15.993: INFO: stork-scheduler-6bbd49d544-5bphb from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container stork-scheduler ready: true, restart count 1
May 11 02:37:15.993: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-11 02:09:22 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 11 02:37:15.993: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-h79lj from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:37:15.993: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:37:15.993: INFO: coredns-5466d46f97-sclc9 from kube-system started at 2019-05-11 00:59:26 +0000 UTC (1 container statuses recorded)
May 11 02:37:15.993: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b1b28392-7395-11e9-80a1-becd2a02efc9 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b1b28392-7395-11e9-80a1-becd2a02efc9 off the node craig-k8s-certification-2-sprout-hero-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b1b28392-7395-11e9-80a1-becd2a02efc9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:37:20.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7565" for this suite.
May 11 02:37:28.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:37:28.112: INFO: namespace sched-pred-7565 deletion completed in 8.065583023s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:12.176 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:37:28.112: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3126
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 02:37:28.133: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 11 02:37:46.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.93.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3126 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:37:46.227: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:37:46.450: INFO: Found all expected endpoints: [netserver-0]
May 11 02:37:46.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.122.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3126 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:37:46.454: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:37:46.683: INFO: Found all expected endpoints: [netserver-1]
May 11 02:37:46.687: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.90.89:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3126 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:37:46.687: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:37:46.911: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:37:46.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3126" for this suite.
May 11 02:38:08.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:38:09.001: INFO: namespace pod-network-test-3126 deletion completed in 22.086085933s

• [SLOW TEST:40.889 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:38:09.001: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 11 02:38:09.553: INFO: created pod pod-service-account-defaultsa
May 11 02:38:09.553: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 11 02:38:09.559: INFO: created pod pod-service-account-mountsa
May 11 02:38:09.559: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 11 02:38:09.563: INFO: created pod pod-service-account-nomountsa
May 11 02:38:09.563: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 11 02:38:09.568: INFO: created pod pod-service-account-defaultsa-mountspec
May 11 02:38:09.568: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 11 02:38:09.572: INFO: created pod pod-service-account-mountsa-mountspec
May 11 02:38:09.572: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 11 02:38:09.576: INFO: created pod pod-service-account-nomountsa-mountspec
May 11 02:38:09.576: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 11 02:38:09.581: INFO: created pod pod-service-account-defaultsa-nomountspec
May 11 02:38:09.581: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 11 02:38:09.588: INFO: created pod pod-service-account-mountsa-nomountspec
May 11 02:38:09.588: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 11 02:38:09.592: INFO: created pod pod-service-account-nomountsa-nomountspec
May 11 02:38:09.592: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:38:09.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6641" for this suite.
May 11 02:38:15.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:38:15.680: INFO: namespace svcaccounts-6641 deletion completed in 6.084071746s

• [SLOW TEST:6.679 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:38:15.680: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 11 02:38:15.708: INFO: Waiting up to 5m0s for pod "downward-api-d4160563-7395-11e9-80a1-becd2a02efc9" in namespace "downward-api-3389" to be "success or failure"
May 11 02:38:15.710: INFO: Pod "downward-api-d4160563-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692211ms
May 11 02:38:17.713: INFO: Pod "downward-api-d4160563-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005227634s
May 11 02:38:19.717: INFO: Pod "downward-api-d4160563-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009070969s
STEP: Saw pod success
May 11 02:38:19.717: INFO: Pod "downward-api-d4160563-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:38:19.719: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downward-api-d4160563-7395-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 02:38:19.732: INFO: Waiting for pod downward-api-d4160563-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:38:19.733: INFO: Pod downward-api-d4160563-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:38:19.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3389" for this suite.
May 11 02:38:25.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:38:25.807: INFO: namespace downward-api-3389 deletion completed in 6.071158776s

• [SLOW TEST:10.127 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:38:25.807: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:38:25.843: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 11 02:38:25.848: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:25.850: INFO: Number of nodes with available pods: 0
May 11 02:38:25.850: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:38:26.854: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:26.856: INFO: Number of nodes with available pods: 0
May 11 02:38:26.856: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:38:27.855: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:27.857: INFO: Number of nodes with available pods: 1
May 11 02:38:27.857: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:38:28.855: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:28.858: INFO: Number of nodes with available pods: 3
May 11 02:38:28.858: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 11 02:38:28.875: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:28.875: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:28.875: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:28.879: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:29.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:29.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:29.883: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:29.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:30.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:30.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:30.884: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:30.890: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:31.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:31.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:31.883: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:31.883: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:31.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:32.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:32.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:32.882: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:32.882: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:32.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:33.885: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:33.886: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:33.886: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:33.886: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:33.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:34.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:34.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:34.884: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:34.884: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:34.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:35.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:35.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:35.882: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:35.882: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:35.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:36.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:36.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:36.884: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:36.884: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:36.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:37.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:37.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:37.883: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:37.883: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:37.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:38.885: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:38.885: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:38.885: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:38.885: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:38.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:39.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:39.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:39.882: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:39.882: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:39.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:40.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:40.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:40.884: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:40.884: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:40.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:41.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:41.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:41.883: INFO: Wrong image for pod: daemon-set-s2kcn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:41.883: INFO: Pod daemon-set-s2kcn is not available
May 11 02:38:41.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:42.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:42.882: INFO: Pod daemon-set-2w2sd is not available
May 11 02:38:42.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:42.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:43.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:43.882: INFO: Pod daemon-set-2w2sd is not available
May 11 02:38:43.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:43.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:44.882: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:44.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:44.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:45.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:45.883: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:45.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:45.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:46.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:46.884: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:46.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:46.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:47.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:47.883: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:47.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:47.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:48.884: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:48.884: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:48.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:48.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:49.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:49.883: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:49.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:49.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:50.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:50.883: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:50.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:50.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:51.883: INFO: Wrong image for pod: daemon-set-2q4dv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:51.883: INFO: Pod daemon-set-2q4dv is not available
May 11 02:38:51.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:51.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:52.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:52.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:52.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:53.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:53.882: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:53.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:54.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:54.882: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:54.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:55.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:55.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:55.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:56.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:56.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:56.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:57.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:57.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:57.889: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:58.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:58.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:58.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:38:59.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:38:59.882: INFO: Pod daemon-set-pn6ln is not available
May 11 02:38:59.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:00.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:00.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:39:00.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:01.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:01.884: INFO: Pod daemon-set-pn6ln is not available
May 11 02:39:01.890: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:02.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:02.882: INFO: Pod daemon-set-pn6ln is not available
May 11 02:39:02.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:03.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:03.882: INFO: Pod daemon-set-pn6ln is not available
May 11 02:39:03.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:04.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:04.883: INFO: Pod daemon-set-pn6ln is not available
May 11 02:39:04.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:05.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:05.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:06.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:06.883: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:06.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:07.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:07.883: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:07.887: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:08.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:08.883: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:08.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:09.882: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:09.882: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:09.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:10.883: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:10.883: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:10.885: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:11.884: INFO: Wrong image for pod: daemon-set-b45s7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 11 02:39:11.884: INFO: Pod daemon-set-b45s7 is not available
May 11 02:39:11.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:12.883: INFO: Pod daemon-set-mjqst is not available
May 11 02:39:12.886: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 11 02:39:12.888: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:12.897: INFO: Number of nodes with available pods: 2
May 11 02:39:12.897: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:39:13.902: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:13.905: INFO: Number of nodes with available pods: 2
May 11 02:39:13.905: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:39:14.900: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:14.903: INFO: Number of nodes with available pods: 2
May 11 02:39:14.903: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:39:15.902: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:15.905: INFO: Number of nodes with available pods: 2
May 11 02:39:15.905: INFO: Node craig-k8s-certification-2-sprout-hero-3 is running more than one daemon pod
May 11 02:39:16.903: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 02:39:16.906: INFO: Number of nodes with available pods: 3
May 11 02:39:16.906: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5502, will wait for the garbage collector to delete the pods
May 11 02:39:16.977: INFO: Deleting DaemonSet.extensions daemon-set took: 4.738975ms
May 11 02:39:17.377: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.227675ms
May 11 02:39:22.681: INFO: Number of nodes with available pods: 0
May 11 02:39:22.681: INFO: Number of running nodes: 0, number of available pods: 0
May 11 02:39:22.683: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5502/daemonsets","resourceVersion":"19108"},"items":null}

May 11 02:39:22.686: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5502/pods","resourceVersion":"19108"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:39:22.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5502" for this suite.
May 11 02:39:28.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:39:28.778: INFO: namespace daemonsets-5502 deletion completed in 6.078856107s

• [SLOW TEST:62.970 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:39:28.778: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:39:28.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9" in namespace "projected-484" to be "success or failure"
May 11 02:39:28.810: INFO: Pod "downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997016ms
May 11 02:39:30.813: INFO: Pod "downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005816307s
May 11 02:39:32.816: INFO: Pod "downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008863435s
STEP: Saw pod success
May 11 02:39:32.816: INFO: Pod "downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:39:32.819: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:39:32.835: INFO: Waiting for pod downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9 to disappear
May 11 02:39:32.837: INFO: Pod downwardapi-volume-ffa812bc-7395-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:39:32.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-484" for this suite.
May 11 02:39:38.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:39:38.910: INFO: namespace projected-484 deletion completed in 6.070045676s

• [SLOW TEST:10.133 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:39:38.910: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:39:38.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9" in namespace "downward-api-5990" to be "success or failure"
May 11 02:39:38.941: INFO: Pod "downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140488ms
May 11 02:39:40.946: INFO: Pod "downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006605618s
May 11 02:39:42.951: INFO: Pod "downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011462934s
STEP: Saw pod success
May 11 02:39:42.951: INFO: Pod "downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:39:42.954: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-3 pod downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:39:43.001: INFO: Waiting for pod downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:39:43.013: INFO: Pod downwardapi-volume-05b21c8e-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:39:43.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5990" for this suite.
May 11 02:39:49.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:39:49.105: INFO: namespace downward-api-5990 deletion completed in 6.089591694s

• [SLOW TEST:10.195 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:39:49.105: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-0bc5b62d-7396-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:39:49.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9" in namespace "projected-7465" to be "success or failure"
May 11 02:39:49.141: INFO: Pod "pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.421069ms
May 11 02:39:51.146: INFO: Pod "pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008254211s
May 11 02:39:53.151: INFO: Pod "pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012798601s
STEP: Saw pod success
May 11 02:39:53.151: INFO: Pod "pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:39:53.153: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 02:39:53.167: INFO: Waiting for pod pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:39:53.170: INFO: Pod pod-projected-secrets-0bc61cdc-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:39:53.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7465" for this suite.
May 11 02:39:59.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:39:59.269: INFO: namespace projected-7465 deletion completed in 6.096500766s

• [SLOW TEST:10.163 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:39:59.269: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6109
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 11 02:39:59.318: INFO: Found 0 stateful pods, waiting for 3
May 11 02:40:09.322: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:40:09.322: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:40:09.322: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 11 02:40:09.350: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 11 02:40:19.378: INFO: Updating stateful set ss2
May 11 02:40:19.383: INFO: Waiting for Pod statefulset-6109/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 02:40:29.390: INFO: Waiting for Pod statefulset-6109/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 11 02:40:39.427: INFO: Found 2 stateful pods, waiting for 3
May 11 02:40:49.430: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:40:49.430: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:40:49.430: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 11 02:40:49.451: INFO: Updating stateful set ss2
May 11 02:40:49.457: INFO: Waiting for Pod statefulset-6109/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 02:40:59.462: INFO: Waiting for Pod statefulset-6109/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 02:41:09.481: INFO: Updating stateful set ss2
May 11 02:41:09.489: INFO: Waiting for StatefulSet statefulset-6109/ss2 to complete update
May 11 02:41:09.489: INFO: Waiting for Pod statefulset-6109/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 02:41:19.496: INFO: Waiting for StatefulSet statefulset-6109/ss2 to complete update
May 11 02:41:19.496: INFO: Waiting for Pod statefulset-6109/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 11 02:41:29.496: INFO: Deleting all statefulset in ns statefulset-6109
May 11 02:41:29.502: INFO: Scaling statefulset ss2 to 0
May 11 02:41:49.518: INFO: Waiting for statefulset status.replicas updated to 0
May 11 02:41:49.522: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:41:49.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6109" for this suite.
May 11 02:41:55.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:41:55.600: INFO: namespace statefulset-6109 deletion completed in 6.066090307s

• [SLOW TEST:116.331 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:41:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:42:01.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3463" for this suite.
May 11 02:42:07.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:42:07.759: INFO: namespace namespaces-3463 deletion completed in 6.076528984s
STEP: Destroying namespace "nsdeletetest-4914" for this suite.
May 11 02:42:07.761: INFO: Namespace nsdeletetest-4914 was already deleted
STEP: Destroying namespace "nsdeletetest-5313" for this suite.
May 11 02:42:13.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:42:13.849: INFO: namespace nsdeletetest-5313 deletion completed in 6.087789129s

• [SLOW TEST:18.248 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:42:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-620ce58e-7396-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:42:13.888: INFO: Waiting up to 5m0s for pod "pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9" in namespace "secrets-418" to be "success or failure"
May 11 02:42:13.890: INFO: Pod "pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030017ms
May 11 02:42:15.893: INFO: Pod "pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004943791s
STEP: Saw pod success
May 11 02:42:15.893: INFO: Pod "pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:42:15.895: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 02:42:15.911: INFO: Waiting for pod pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:42:15.913: INFO: Pod pod-secrets-620d483f-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:42:15.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-418" for this suite.
May 11 02:42:21.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:42:21.991: INFO: namespace secrets-418 deletion completed in 6.07602621s

• [SLOW TEST:8.142 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:42:21.991: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-66e6aa6f-7396-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-66e6aa6f-7396-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:42:26.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-714" for this suite.
May 11 02:42:48.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:42:48.128: INFO: namespace configmap-714 deletion completed in 22.072173591s

• [SLOW TEST:26.137 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:42:48.128: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-lkbg
STEP: Creating a pod to test atomic-volume-subpath
May 11 02:42:48.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lkbg" in namespace "subpath-5237" to be "success or failure"
May 11 02:42:48.165: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013745ms
May 11 02:42:50.168: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006035992s
May 11 02:42:52.171: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 4.008561002s
May 11 02:42:54.175: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 6.012856951s
May 11 02:42:56.179: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 8.017093084s
May 11 02:42:58.184: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 10.021327945s
May 11 02:43:00.188: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 12.025279124s
May 11 02:43:02.192: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 14.029314764s
May 11 02:43:04.196: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 16.033626812s
May 11 02:43:06.199: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 18.036899839s
May 11 02:43:08.203: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 20.040610477s
May 11 02:43:10.206: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Running", Reason="", readiness=true. Elapsed: 22.043766757s
May 11 02:43:12.209: INFO: Pod "pod-subpath-test-secret-lkbg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0469341s
STEP: Saw pod success
May 11 02:43:12.209: INFO: Pod "pod-subpath-test-secret-lkbg" satisfied condition "success or failure"
May 11 02:43:12.212: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-subpath-test-secret-lkbg container test-container-subpath-secret-lkbg: <nil>
STEP: delete the pod
May 11 02:43:12.227: INFO: Waiting for pod pod-subpath-test-secret-lkbg to disappear
May 11 02:43:12.229: INFO: Pod pod-subpath-test-secret-lkbg no longer exists
STEP: Deleting pod pod-subpath-test-secret-lkbg
May 11 02:43:12.229: INFO: Deleting pod "pod-subpath-test-secret-lkbg" in namespace "subpath-5237"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:43:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5237" for this suite.
May 11 02:43:18.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:43:18.319: INFO: namespace subpath-5237 deletion completed in 6.085431657s

• [SLOW TEST:30.191 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:43:18.319: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-4517/secret-test-8878f10a-7396-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:43:18.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9" in namespace "secrets-4517" to be "success or failure"
May 11 02:43:18.352: INFO: Pod "pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625419ms
May 11 02:43:20.356: INFO: Pod "pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005818324s
STEP: Saw pod success
May 11 02:43:20.356: INFO: Pod "pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:43:20.358: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9 container env-test: <nil>
STEP: delete the pod
May 11 02:43:20.369: INFO: Waiting for pod pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:43:20.370: INFO: Pod pod-configmaps-887959cf-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:43:20.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4517" for this suite.
May 11 02:43:26.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:43:26.435: INFO: namespace secrets-4517 deletion completed in 6.061751775s

• [SLOW TEST:8.116 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:43:26.435: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:43:50.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6913" for this suite.
May 11 02:43:56.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:43:56.671: INFO: namespace namespaces-6913 deletion completed in 6.066114961s
STEP: Destroying namespace "nsdeletetest-2538" for this suite.
May 11 02:43:56.673: INFO: Namespace nsdeletetest-2538 was already deleted
STEP: Destroying namespace "nsdeletetest-6904" for this suite.
May 11 02:44:02.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:44:02.747: INFO: namespace nsdeletetest-6904 deletion completed in 6.074597736s

• [SLOW TEST:36.312 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:44:02.747: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 11 02:44:02.777: INFO: Waiting up to 5m0s for pod "downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9" in namespace "downward-api-3389" to be "success or failure"
May 11 02:44:02.779: INFO: Pod "downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011133ms
May 11 02:44:04.783: INFO: Pod "downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005787069s
May 11 02:44:06.785: INFO: Pod "downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008197425s
STEP: Saw pod success
May 11 02:44:06.785: INFO: Pod "downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:44:06.787: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-3 pod downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 02:44:06.801: INFO: Waiting for pod downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:44:06.803: INFO: Pod downward-api-a2f4717f-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:44:06.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3389" for this suite.
May 11 02:44:12.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:44:12.866: INFO: namespace downward-api-3389 deletion completed in 6.06129219s

• [SLOW TEST:10.119 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:44:12.867: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:44:12.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9" in namespace "projected-775" to be "success or failure"
May 11 02:44:12.898: INFO: Pod "downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779865ms
May 11 02:44:14.901: INFO: Pod "downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005343199s
STEP: Saw pod success
May 11 02:44:14.901: INFO: Pod "downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:44:14.903: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:44:14.916: INFO: Waiting for pod downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:44:14.917: INFO: Pod downwardapi-volume-a8fc68b7-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:44:14.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-775" for this suite.
May 11 02:44:20.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:44:20.985: INFO: namespace projected-775 deletion completed in 6.065398733s

• [SLOW TEST:8.119 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:44:20.986: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 11 02:44:25.028: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:25.248: INFO: Exec stderr: ""
May 11 02:44:25.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:25.479: INFO: Exec stderr: ""
May 11 02:44:25.479: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:25.677: INFO: Exec stderr: ""
May 11 02:44:25.677: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:25.677: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:25.897: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 11 02:44:25.897: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:26.096: INFO: Exec stderr: ""
May 11 02:44:26.096: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:26.096: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:26.300: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 11 02:44:26.300: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:26.300: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:26.485: INFO: Exec stderr: ""
May 11 02:44:26.485: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:26.485: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:26.691: INFO: Exec stderr: ""
May 11 02:44:26.691: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:26.872: INFO: Exec stderr: ""
May 11 02:44:26.872: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2516 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 02:44:26.872: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 02:44:27.077: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:44:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2516" for this suite.
May 11 02:45:17.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:45:17.151: INFO: namespace e2e-kubelet-etc-hosts-2516 deletion completed in 50.070795631s

• [SLOW TEST:56.166 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:45:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-cf4d0012-7396-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:45:17.179: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9" in namespace "projected-6837" to be "success or failure"
May 11 02:45:17.182: INFO: Pod "pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186574ms
May 11 02:45:19.186: INFO: Pod "pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006986789s
STEP: Saw pod success
May 11 02:45:19.186: INFO: Pod "pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:45:19.189: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 02:45:19.203: INFO: Waiting for pod pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:45:19.206: INFO: Pod pod-projected-secrets-cf4d4cd2-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:45:19.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6837" for this suite.
May 11 02:45:25.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:45:25.293: INFO: namespace projected-6837 deletion completed in 6.084294546s

• [SLOW TEST:8.141 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:45:25.293: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:45:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4793'
May 11 02:45:25.539: INFO: stderr: ""
May 11 02:45:25.539: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 11 02:45:25.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete pods e2e-test-nginx-pod --namespace=kubectl-4793'
May 11 02:45:32.556: INFO: stderr: ""
May 11 02:45:32.556: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:45:32.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4793" for this suite.
May 11 02:45:38.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:45:38.652: INFO: namespace kubectl-4793 deletion completed in 6.093482498s

• [SLOW TEST:13.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:45:38.652: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:45:38.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3156'
May 11 02:45:38.785: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 11 02:45:38.785: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 11 02:45:38.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete jobs e2e-test-nginx-job --namespace=kubectl-3156'
May 11 02:45:38.881: INFO: stderr: ""
May 11 02:45:38.881: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:45:38.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3156" for this suite.
May 11 02:45:44.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:45:44.952: INFO: namespace kubectl-3156 deletion completed in 6.067437698s

• [SLOW TEST:6.299 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:45:44.952: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-dfdf5d4d-7396-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:45:44.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9" in namespace "configmap-5825" to be "success or failure"
May 11 02:45:44.983: INFO: Pod "pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.84512ms
May 11 02:45:46.987: INFO: Pod "pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005455119s
STEP: Saw pod success
May 11 02:45:46.987: INFO: Pod "pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:45:46.990: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:45:47.016: INFO: Waiting for pod pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9 to disappear
May 11 02:45:47.018: INFO: Pod pod-configmaps-dfdfac9d-7396-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:45:47.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5825" for this suite.
May 11 02:45:53.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:45:53.096: INFO: namespace configmap-5825 deletion completed in 6.074737347s

• [SLOW TEST:8.145 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:45:53.097: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-9293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9293 to expose endpoints map[]
May 11 02:45:53.128: INFO: successfully validated that service multi-endpoint-test in namespace services-9293 exposes endpoints map[] (4.405109ms elapsed)
STEP: Creating pod pod1 in namespace services-9293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9293 to expose endpoints map[pod1:[100]]
May 11 02:45:56.156: INFO: successfully validated that service multi-endpoint-test in namespace services-9293 exposes endpoints map[pod1:[100]] (3.022800293s elapsed)
STEP: Creating pod pod2 in namespace services-9293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9293 to expose endpoints map[pod1:[100] pod2:[101]]
May 11 02:45:59.192: INFO: successfully validated that service multi-endpoint-test in namespace services-9293 exposes endpoints map[pod1:[100] pod2:[101]] (3.033049795s elapsed)
STEP: Deleting pod pod1 in namespace services-9293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9293 to expose endpoints map[pod2:[101]]
May 11 02:45:59.207: INFO: successfully validated that service multi-endpoint-test in namespace services-9293 exposes endpoints map[pod2:[101]] (10.341659ms elapsed)
STEP: Deleting pod pod2 in namespace services-9293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9293 to expose endpoints map[]
May 11 02:46:00.219: INFO: successfully validated that service multi-endpoint-test in namespace services-9293 exposes endpoints map[] (1.005855573s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:46:00.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9293" for this suite.
May 11 02:46:06.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:46:06.316: INFO: namespace services-9293 deletion completed in 6.075971891s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:13.220 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:46:06.316: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0511 02:46:36.877385      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:46:36.877: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:46:36.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2841" for this suite.
May 11 02:46:42.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:46:42.971: INFO: namespace gc-2841 deletion completed in 6.090876877s

• [SLOW TEST:36.654 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:46:42.971: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0511 02:46:49.022892      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 11 02:46:49.022: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:46:49.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-76" for this suite.
May 11 02:46:55.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:46:55.099: INFO: namespace gc-76 deletion completed in 6.074908392s

• [SLOW TEST:12.128 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:46:55.100: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 11 02:46:55.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-2200'
May 11 02:46:55.346: INFO: stderr: ""
May 11 02:46:55.346: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 02:46:55.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2200'
May 11 02:46:55.455: INFO: stderr: ""
May 11 02:46:55.455: INFO: stdout: "update-demo-nautilus-b86dm update-demo-nautilus-mj2zm "
May 11 02:46:55.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-b86dm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2200'
May 11 02:46:55.553: INFO: stderr: ""
May 11 02:46:55.553: INFO: stdout: ""
May 11 02:46:55.553: INFO: update-demo-nautilus-b86dm is created but not running
May 11 02:47:00.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2200'
May 11 02:47:00.649: INFO: stderr: ""
May 11 02:47:00.649: INFO: stdout: "update-demo-nautilus-b86dm update-demo-nautilus-mj2zm "
May 11 02:47:00.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-b86dm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2200'
May 11 02:47:00.737: INFO: stderr: ""
May 11 02:47:00.737: INFO: stdout: "true"
May 11 02:47:00.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-b86dm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2200'
May 11 02:47:00.825: INFO: stderr: ""
May 11 02:47:00.825: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:47:00.825: INFO: validating pod update-demo-nautilus-b86dm
May 11 02:47:00.830: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:47:00.830: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:47:00.830: INFO: update-demo-nautilus-b86dm is verified up and running
May 11 02:47:00.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-mj2zm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2200'
May 11 02:47:00.920: INFO: stderr: ""
May 11 02:47:00.920: INFO: stdout: "true"
May 11 02:47:00.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-mj2zm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2200'
May 11 02:47:01.002: INFO: stderr: ""
May 11 02:47:01.002: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:47:01.002: INFO: validating pod update-demo-nautilus-mj2zm
May 11 02:47:01.005: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:47:01.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:47:01.005: INFO: update-demo-nautilus-mj2zm is verified up and running
STEP: using delete to clean up resources
May 11 02:47:01.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-2200'
May 11 02:47:01.096: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 02:47:01.096: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 11 02:47:01.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2200'
May 11 02:47:01.194: INFO: stderr: "No resources found.\n"
May 11 02:47:01.194: INFO: stdout: ""
May 11 02:47:01.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -l name=update-demo --namespace=kubectl-2200 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 02:47:01.280: INFO: stderr: ""
May 11 02:47:01.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:47:01.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2200" for this suite.
May 11 02:47:07.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:47:07.353: INFO: namespace kubectl-2200 deletion completed in 6.06870811s

• [SLOW TEST:12.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:47:07.353: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-10fd6853-7397-11e9-80a1-becd2a02efc9
STEP: Creating configMap with name cm-test-opt-upd-10fd688d-7397-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-10fd6853-7397-11e9-80a1-becd2a02efc9
STEP: Updating configmap cm-test-opt-upd-10fd688d-7397-11e9-80a1-becd2a02efc9
STEP: Creating configMap with name cm-test-opt-create-10fd68a3-7397-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:47:11.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9296" for this suite.
May 11 02:47:33.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:47:33.546: INFO: namespace projected-9296 deletion completed in 22.088950895s

• [SLOW TEST:26.193 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:47:33.546: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:47:33.579: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:47:37.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6034" for this suite.
May 11 02:48:15.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:48:15.692: INFO: namespace pods-6034 deletion completed in 38.067211027s

• [SLOW TEST:42.145 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:48:15.692: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:48:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7908" for this suite.
May 11 02:48:21.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:48:21.788: INFO: namespace services-7908 deletion completed in 6.071620935s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.096 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:48:21.788: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 11 02:48:21.822: INFO: Waiting up to 5m0s for pod "pod-3d5b9416-7397-11e9-80a1-becd2a02efc9" in namespace "emptydir-713" to be "success or failure"
May 11 02:48:21.824: INFO: Pod "pod-3d5b9416-7397-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90571ms
May 11 02:48:23.826: INFO: Pod "pod-3d5b9416-7397-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004871671s
STEP: Saw pod success
May 11 02:48:23.827: INFO: Pod "pod-3d5b9416-7397-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:48:23.829: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-3d5b9416-7397-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:48:23.842: INFO: Waiting for pod pod-3d5b9416-7397-11e9-80a1-becd2a02efc9 to disappear
May 11 02:48:23.843: INFO: Pod pod-3d5b9416-7397-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:48:23.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-713" for this suite.
May 11 02:48:29.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:48:29.923: INFO: namespace emptydir-713 deletion completed in 6.077571967s

• [SLOW TEST:8.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:48:29.923: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 11 02:48:29.955: INFO: Waiting up to 5m0s for pod "pod-42346fb5-7397-11e9-80a1-becd2a02efc9" in namespace "emptydir-9958" to be "success or failure"
May 11 02:48:29.957: INFO: Pod "pod-42346fb5-7397-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536652ms
May 11 02:48:31.961: INFO: Pod "pod-42346fb5-7397-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006002912s
STEP: Saw pod success
May 11 02:48:31.961: INFO: Pod "pod-42346fb5-7397-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:48:31.963: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-42346fb5-7397-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:48:31.978: INFO: Waiting for pod pod-42346fb5-7397-11e9-80a1-becd2a02efc9 to disappear
May 11 02:48:31.980: INFO: Pod pod-42346fb5-7397-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:48:31.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9958" for this suite.
May 11 02:48:37.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:48:38.087: INFO: namespace emptydir-9958 deletion completed in 6.104203846s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:48:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 02:48:38.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9" in namespace "downward-api-7959" to be "success or failure"
May 11 02:48:38.117: INFO: Pod "downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.244061ms
May 11 02:48:40.120: INFO: Pod "downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007209281s
STEP: Saw pod success
May 11 02:48:40.120: INFO: Pod "downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:48:40.122: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 02:48:40.136: INFO: Waiting for pod downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9 to disappear
May 11 02:48:40.138: INFO: Pod downwardapi-volume-47116c1a-7397-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:48:40.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7959" for this suite.
May 11 02:48:46.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:48:46.210: INFO: namespace downward-api-7959 deletion completed in 6.068812721s

• [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:48:46.211: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-4be9bc43-7397-11e9-80a1-becd2a02efc9
STEP: Creating secret with name s-test-opt-upd-4be9bccc-7397-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4be9bc43-7397-11e9-80a1-becd2a02efc9
STEP: Updating secret s-test-opt-upd-4be9bccc-7397-11e9-80a1-becd2a02efc9
STEP: Creating secret with name s-test-opt-create-4be9bd04-7397-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:48:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2213" for this suite.
May 11 02:49:12.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:49:12.414: INFO: namespace projected-2213 deletion completed in 22.081163367s

• [SLOW TEST:26.204 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:49:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 11 02:49:12.438: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 11 02:49:12.443: INFO: Waiting for terminating namespaces to be deleted...
May 11 02:49:12.445: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-1 before test
May 11 02:49:12.453: INFO: kubernetes-dashboard-6c7466966c-wq669 from kube-system started at 2019-05-11 00:59:25 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May 11 02:49:12.453: INFO: calico-node-mrtgz from kube-system started at 2019-05-11 00:58:53 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:49:12.453: INFO: portworx-mmdlz from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container portworx ready: true, restart count 0
May 11 02:49:12.453: INFO: kube-proxy-hkkn5 from kube-system started at 2019-05-11 00:59:06 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:49:12.453: INFO: calico-kube-controllers-8b6f9c4bd-sw8gv from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 11 02:49:12.453: INFO: stork-scheduler-6bbd49d544-f4zsx from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:49:12.453: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-z45cd from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:49:12.453: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:49:12.453: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-1 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:49:12.453: INFO: nodelocaldns-p9tj4 from kube-system started at 2019-05-11 00:59:23 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:49:12.453: INFO: stork-6c6dcf79d6-dkkf4 from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.453: INFO: 	Container stork ready: true, restart count 0
May 11 02:49:12.453: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-2 before test
May 11 02:49:12.459: INFO: kube-proxy-n9pvn from kube-system started at 2019-05-11 00:59:03 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:49:12.460: INFO: stork-scheduler-6bbd49d544-qbncg from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container stork-scheduler ready: true, restart count 0
May 11 02:49:12.460: INFO: sonobuoy-e2e-job-f101e66479304f83 from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container e2e ready: true, restart count 0
May 11 02:49:12.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:49:12.460: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-2jslh from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:49:12.460: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:49:12.460: INFO: dns-autoscaler-56c969bdb8-hv98f from kube-system started at 2019-05-11 00:59:22 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container autoscaler ready: true, restart count 0
May 11 02:49:12.460: INFO: nodelocaldns-ssrkc from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:49:12.460: INFO: portworx-bpm2q from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container portworx ready: true, restart count 0
May 11 02:49:12.460: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-2 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:49:12.460: INFO: calico-node-zldrd from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:49:12.460: INFO: stork-6c6dcf79d6-cv8rs from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.460: INFO: 	Container stork ready: true, restart count 0
May 11 02:49:12.460: INFO: 
Logging pods the kubelet thinks is on node craig-k8s-certification-2-sprout-hero-3 before test
May 11 02:49:12.465: INFO: kube-proxy-pg9lg from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container kube-proxy ready: true, restart count 0
May 11 02:49:12.465: INFO: nginx-proxy-craig-k8s-certification-2-sprout-hero-3 from kube-system started at <nil> (0 container statuses recorded)
May 11 02:49:12.465: INFO: nodelocaldns-ngb8l from kube-system started at 2019-05-11 00:59:24 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container node-cache ready: true, restart count 0
May 11 02:49:12.465: INFO: stork-6c6dcf79d6-zdldn from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container stork ready: true, restart count 2
May 11 02:49:12.465: INFO: portworx-npbrq from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container portworx ready: true, restart count 0
May 11 02:49:12.465: INFO: calico-node-4s655 from kube-system started at 2019-05-11 00:58:54 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container calico-node ready: true, restart count 0
May 11 02:49:12.465: INFO: stork-scheduler-6bbd49d544-5bphb from kube-system started at 2019-05-11 02:02:44 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container stork-scheduler ready: true, restart count 1
May 11 02:49:12.465: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-11 02:09:22 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 11 02:49:12.465: INFO: sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-h79lj from heptio-sonobuoy started at 2019-05-11 02:09:28 +0000 UTC (2 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 11 02:49:12.465: INFO: 	Container systemd-logs ready: true, restart count 0
May 11 02:49:12.465: INFO: coredns-5466d46f97-sclc9 from kube-system started at 2019-05-11 00:59:26 +0000 UTC (1 container statuses recorded)
May 11 02:49:12.465: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node craig-k8s-certification-2-sprout-hero-1
STEP: verifying the node has the label node craig-k8s-certification-2-sprout-hero-2
STEP: verifying the node has the label node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod sonobuoy requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod sonobuoy-e2e-job-f101e66479304f83 requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-2jslh requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-h79lj requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-z45cd requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod calico-kube-controllers-8b6f9c4bd-sw8gv requesting resource cpu=30m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod calico-node-4s655 requesting resource cpu=150m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod calico-node-mrtgz requesting resource cpu=150m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod calico-node-zldrd requesting resource cpu=150m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod coredns-5466d46f97-sclc9 requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod dns-autoscaler-56c969bdb8-hv98f requesting resource cpu=20m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod kube-proxy-hkkn5 requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod kube-proxy-n9pvn requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod kube-proxy-pg9lg requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod kubernetes-dashboard-6c7466966c-wq669 requesting resource cpu=50m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod nginx-proxy-craig-k8s-certification-2-sprout-hero-1 requesting resource cpu=25m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod nginx-proxy-craig-k8s-certification-2-sprout-hero-2 requesting resource cpu=25m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod nginx-proxy-craig-k8s-certification-2-sprout-hero-3 requesting resource cpu=25m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod nodelocaldns-ngb8l requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.507: INFO: Pod nodelocaldns-p9tj4 requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod nodelocaldns-ssrkc requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod portworx-bpm2q requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.507: INFO: Pod portworx-mmdlz requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.507: INFO: Pod portworx-npbrq requesting resource cpu=0m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.508: INFO: Pod stork-6c6dcf79d6-cv8rs requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-2
May 11 02:49:12.508: INFO: Pod stork-6c6dcf79d6-dkkf4 requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.508: INFO: Pod stork-6c6dcf79d6-zdldn requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.508: INFO: Pod stork-scheduler-6bbd49d544-5bphb requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-3
May 11 02:49:12.508: INFO: Pod stork-scheduler-6bbd49d544-f4zsx requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-1
May 11 02:49:12.508: INFO: Pod stork-scheduler-6bbd49d544-qbncg requesting resource cpu=100m on Node craig-k8s-certification-2-sprout-hero-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9.159d8110428df9e2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-854/filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9 to craig-k8s-certification-2-sprout-hero-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9.159d8110450c5981], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9.159d811047371752], Reason = [Created], Message = [Created container filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9.159d811052891346], Reason = [Started], Message = [Started container filler-pod-5b9249b2-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9.159d811042c051bc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-854/filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9 to craig-k8s-certification-2-sprout-hero-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9.159d81105bc9817c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9.159d81105d8b68a5], Reason = [Created], Message = [Created container filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9.159d8110696722f1], Reason = [Started], Message = [Started container filler-pod-5b932cd3-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9.159d811042ff27e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-854/filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9 to craig-k8s-certification-2-sprout-hero-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9.159d81105baafb78], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9.159d8110a7f2e193], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9.159d8110a9c1e0b0], Reason = [Created], Message = [Created container filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9.159d8110b545e518], Reason = [Started], Message = [Started container filler-pod-5b93c774-7397-11e9-80a1-becd2a02efc9]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159d811132961ec9], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node craig-k8s-certification-2-sprout-hero-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node craig-k8s-certification-2-sprout-hero-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node craig-k8s-certification-2-sprout-hero-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:49:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-854" for this suite.
May 11 02:49:23.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:49:23.649: INFO: namespace sched-pred-854 deletion completed in 6.066553571s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.234 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:49:23.649: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 11 02:49:26.202: INFO: Successfully updated pod "annotationupdate623a9947-7397-11e9-80a1-becd2a02efc9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:49:28.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3286" for this suite.
May 11 02:49:50.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:49:50.286: INFO: namespace projected-3286 deletion completed in 22.068319751s

• [SLOW TEST:26.637 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:49:50.287: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 11 02:49:50.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 cluster-info'
May 11 02:49:50.403: INFO: stderr: ""
May 11 02:49:50.403: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:49:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7394" for this suite.
May 11 02:49:56.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:49:56.470: INFO: namespace kubectl-7394 deletion completed in 6.064041899s

• [SLOW TEST:6.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:49:56.470: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:49:56.503: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 11 02:49:56.507: INFO: Number of nodes with available pods: 0
May 11 02:49:56.507: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 11 02:49:56.518: INFO: Number of nodes with available pods: 0
May 11 02:49:56.518: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:49:57.523: INFO: Number of nodes with available pods: 0
May 11 02:49:57.523: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:49:58.522: INFO: Number of nodes with available pods: 1
May 11 02:49:58.522: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 11 02:49:58.535: INFO: Number of nodes with available pods: 1
May 11 02:49:58.535: INFO: Number of running nodes: 0, number of available pods: 1
May 11 02:49:59.538: INFO: Number of nodes with available pods: 0
May 11 02:49:59.538: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 11 02:49:59.544: INFO: Number of nodes with available pods: 0
May 11 02:49:59.544: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:00.547: INFO: Number of nodes with available pods: 0
May 11 02:50:00.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:01.547: INFO: Number of nodes with available pods: 0
May 11 02:50:01.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:02.547: INFO: Number of nodes with available pods: 0
May 11 02:50:02.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:03.547: INFO: Number of nodes with available pods: 0
May 11 02:50:03.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:04.547: INFO: Number of nodes with available pods: 0
May 11 02:50:04.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:05.547: INFO: Number of nodes with available pods: 0
May 11 02:50:05.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:06.547: INFO: Number of nodes with available pods: 0
May 11 02:50:06.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:07.547: INFO: Number of nodes with available pods: 0
May 11 02:50:07.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:08.547: INFO: Number of nodes with available pods: 0
May 11 02:50:08.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:09.547: INFO: Number of nodes with available pods: 0
May 11 02:50:09.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:10.548: INFO: Number of nodes with available pods: 0
May 11 02:50:10.548: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:11.547: INFO: Number of nodes with available pods: 0
May 11 02:50:11.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:12.547: INFO: Number of nodes with available pods: 0
May 11 02:50:12.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:13.547: INFO: Number of nodes with available pods: 0
May 11 02:50:13.547: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 02:50:14.547: INFO: Number of nodes with available pods: 1
May 11 02:50:14.547: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-679, will wait for the garbage collector to delete the pods
May 11 02:50:14.611: INFO: Deleting DaemonSet.extensions daemon-set took: 5.089397ms
May 11 02:50:15.011: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.359434ms
May 11 02:50:22.614: INFO: Number of nodes with available pods: 0
May 11 02:50:22.614: INFO: Number of running nodes: 0, number of available pods: 0
May 11 02:50:22.617: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-679/daemonsets","resourceVersion":"22833"},"items":null}

May 11 02:50:22.620: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-679/pods","resourceVersion":"22833"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:50:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-679" for this suite.
May 11 02:50:28.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:50:28.754: INFO: namespace daemonsets-679 deletion completed in 6.111376616s

• [SLOW TEST:32.284 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:50:28.754: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3484
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3484
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3484
May 11 02:50:28.791: INFO: Found 0 stateful pods, waiting for 1
May 11 02:50:38.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 11 02:50:38.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 02:50:39.088: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 02:50:39.088: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 02:50:39.088: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 02:50:39.091: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 11 02:50:49.097: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 02:50:49.097: INFO: Waiting for statefulset status.replicas updated to 0
May 11 02:50:49.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999653s
May 11 02:50:50.113: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996702403s
May 11 02:50:51.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992870785s
May 11 02:50:52.123: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986978948s
May 11 02:50:53.126: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983357856s
May 11 02:50:54.130: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980131397s
May 11 02:50:55.134: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976900716s
May 11 02:50:56.138: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972678346s
May 11 02:50:57.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968098204s
May 11 02:50:58.146: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.334404ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3484
May 11 02:50:59.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 02:50:59.426: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 02:50:59.426: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 02:50:59.426: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 02:50:59.429: INFO: Found 1 stateful pods, waiting for 3
May 11 02:51:09.433: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:51:09.434: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 02:51:09.434: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 11 02:51:09.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 02:51:09.743: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 02:51:09.743: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 02:51:09.743: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 02:51:09.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 02:51:10.057: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 02:51:10.057: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 02:51:10.057: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 02:51:10.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 02:51:10.448: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 02:51:10.448: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 02:51:10.448: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 02:51:10.448: INFO: Waiting for statefulset status.replicas updated to 0
May 11 02:51:10.451: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 11 02:51:20.456: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 02:51:20.456: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 11 02:51:20.456: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 11 02:51:20.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999304s
May 11 02:51:21.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996138479s
May 11 02:51:22.472: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992016573s
May 11 02:51:23.476: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988075719s
May 11 02:51:24.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98415307s
May 11 02:51:25.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980296414s
May 11 02:51:26.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974390803s
May 11 02:51:27.493: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970760656s
May 11 02:51:28.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967426624s
May 11 02:51:29.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.358289ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3484
May 11 02:51:30.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 02:51:30.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 02:51:30.830: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 02:51:30.830: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 02:51:30.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 02:51:31.139: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 02:51:31.139: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 02:51:31.139: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 02:51:31.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-3484 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 02:51:31.448: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 02:51:31.448: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 02:51:31.448: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 02:51:31.448: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 11 02:52:01.464: INFO: Deleting all statefulset in ns statefulset-3484
May 11 02:52:01.466: INFO: Scaling statefulset ss to 0
May 11 02:52:01.473: INFO: Waiting for statefulset status.replicas updated to 0
May 11 02:52:01.474: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:52:01.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3484" for this suite.
May 11 02:52:07.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:52:07.562: INFO: namespace statefulset-3484 deletion completed in 6.075466947s

• [SLOW TEST:98.807 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:52:07.562: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7837
May 11 02:52:09.601: INFO: Started pod liveness-exec in namespace container-probe-7837
STEP: checking the pod's current state and verifying that restartCount is present
May 11 02:52:09.604: INFO: Initial restart count of pod liveness-exec is 0
May 11 02:53:05.712: INFO: Restart count of pod container-probe-7837/liveness-exec is now 1 (56.107753665s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:53:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7837" for this suite.
May 11 02:53:11.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:53:11.802: INFO: namespace container-probe-7837 deletion completed in 6.080031566s

• [SLOW TEST:64.240 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:53:11.802: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:53:11.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5890'
May 11 02:53:11.925: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 11 02:53:11.925: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 11 02:53:11.930: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-slqtd]
May 11 02:53:11.930: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-slqtd" in namespace "kubectl-5890" to be "running and ready"
May 11 02:53:11.933: INFO: Pod "e2e-test-nginx-rc-slqtd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.630041ms
May 11 02:53:13.936: INFO: Pod "e2e-test-nginx-rc-slqtd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00570438s
May 11 02:53:13.936: INFO: Pod "e2e-test-nginx-rc-slqtd" satisfied condition "running and ready"
May 11 02:53:13.936: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-slqtd]
May 11 02:53:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 logs rc/e2e-test-nginx-rc --namespace=kubectl-5890'
May 11 02:53:14.053: INFO: stderr: ""
May 11 02:53:14.053: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 11 02:53:14.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete rc e2e-test-nginx-rc --namespace=kubectl-5890'
May 11 02:53:14.147: INFO: stderr: ""
May 11 02:53:14.148: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:53:14.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5890" for this suite.
May 11 02:53:36.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:53:36.223: INFO: namespace kubectl-5890 deletion completed in 22.072460364s

• [SLOW TEST:24.421 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:53:36.223: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 11 02:53:36.249: INFO: Waiting up to 5m0s for pod "pod-f8c55351-7397-11e9-80a1-becd2a02efc9" in namespace "emptydir-9667" to be "success or failure"
May 11 02:53:36.251: INFO: Pod "pod-f8c55351-7397-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588944ms
May 11 02:53:38.255: INFO: Pod "pod-f8c55351-7397-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005766034s
STEP: Saw pod success
May 11 02:53:38.255: INFO: Pod "pod-f8c55351-7397-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:53:38.257: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-f8c55351-7397-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:53:38.271: INFO: Waiting for pod pod-f8c55351-7397-11e9-80a1-becd2a02efc9 to disappear
May 11 02:53:38.273: INFO: Pod pod-f8c55351-7397-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:53:38.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9667" for this suite.
May 11 02:53:44.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:53:44.347: INFO: namespace emptydir-9667 deletion completed in 6.070782717s

• [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:53:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 11 02:53:44.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-5221'
May 11 02:53:44.578: INFO: stderr: ""
May 11 02:53:44.578: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 02:53:44.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:53:44.672: INFO: stderr: ""
May 11 02:53:44.672: INFO: stdout: "update-demo-nautilus-ww54g update-demo-nautilus-wzbp5 "
May 11 02:53:44.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:44.753: INFO: stderr: ""
May 11 02:53:44.753: INFO: stdout: ""
May 11 02:53:44.753: INFO: update-demo-nautilus-ww54g is created but not running
May 11 02:53:49.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:53:49.859: INFO: stderr: ""
May 11 02:53:49.859: INFO: stdout: "update-demo-nautilus-ww54g update-demo-nautilus-wzbp5 "
May 11 02:53:49.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:49.952: INFO: stderr: ""
May 11 02:53:49.952: INFO: stdout: "true"
May 11 02:53:49.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:50.062: INFO: stderr: ""
May 11 02:53:50.062: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:53:50.062: INFO: validating pod update-demo-nautilus-ww54g
May 11 02:53:50.067: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:53:50.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:53:50.067: INFO: update-demo-nautilus-ww54g is verified up and running
May 11 02:53:50.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-wzbp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:50.172: INFO: stderr: ""
May 11 02:53:50.172: INFO: stdout: "true"
May 11 02:53:50.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-wzbp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:50.270: INFO: stderr: ""
May 11 02:53:50.270: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:53:50.270: INFO: validating pod update-demo-nautilus-wzbp5
May 11 02:53:50.274: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:53:50.274: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:53:50.274: INFO: update-demo-nautilus-wzbp5 is verified up and running
STEP: scaling down the replication controller
May 11 02:53:50.276: INFO: scanned /root for discovery docs: <nil>
May 11 02:53:50.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5221'
May 11 02:53:51.405: INFO: stderr: ""
May 11 02:53:51.406: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 02:53:51.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:53:51.535: INFO: stderr: ""
May 11 02:53:51.535: INFO: stdout: "update-demo-nautilus-ww54g update-demo-nautilus-wzbp5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 11 02:53:56.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:53:56.631: INFO: stderr: ""
May 11 02:53:56.632: INFO: stdout: "update-demo-nautilus-ww54g "
May 11 02:53:56.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:56.723: INFO: stderr: ""
May 11 02:53:56.723: INFO: stdout: "true"
May 11 02:53:56.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:56.814: INFO: stderr: ""
May 11 02:53:56.814: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:53:56.814: INFO: validating pod update-demo-nautilus-ww54g
May 11 02:53:56.818: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:53:56.818: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:53:56.818: INFO: update-demo-nautilus-ww54g is verified up and running
STEP: scaling up the replication controller
May 11 02:53:56.820: INFO: scanned /root for discovery docs: <nil>
May 11 02:53:56.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5221'
May 11 02:53:57.938: INFO: stderr: ""
May 11 02:53:57.938: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 02:53:57.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:53:58.036: INFO: stderr: ""
May 11 02:53:58.036: INFO: stdout: "update-demo-nautilus-9dhcj update-demo-nautilus-ww54g "
May 11 02:53:58.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-9dhcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:53:58.124: INFO: stderr: ""
May 11 02:53:58.124: INFO: stdout: ""
May 11 02:53:58.124: INFO: update-demo-nautilus-9dhcj is created but not running
May 11 02:54:03.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5221'
May 11 02:54:03.242: INFO: stderr: ""
May 11 02:54:03.242: INFO: stdout: "update-demo-nautilus-9dhcj update-demo-nautilus-ww54g "
May 11 02:54:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-9dhcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:54:03.348: INFO: stderr: ""
May 11 02:54:03.348: INFO: stdout: "true"
May 11 02:54:03.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-9dhcj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:54:03.454: INFO: stderr: ""
May 11 02:54:03.454: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:54:03.454: INFO: validating pod update-demo-nautilus-9dhcj
May 11 02:54:03.496: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:54:03.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:54:03.496: INFO: update-demo-nautilus-9dhcj is verified up and running
May 11 02:54:03.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:54:03.594: INFO: stderr: ""
May 11 02:54:03.594: INFO: stdout: "true"
May 11 02:54:03.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-ww54g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5221'
May 11 02:54:03.698: INFO: stderr: ""
May 11 02:54:03.698: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 02:54:03.698: INFO: validating pod update-demo-nautilus-ww54g
May 11 02:54:03.702: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 02:54:03.702: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 02:54:03.702: INFO: update-demo-nautilus-ww54g is verified up and running
STEP: using delete to clean up resources
May 11 02:54:03.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-5221'
May 11 02:54:03.808: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 02:54:03.808: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 11 02:54:03.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5221'
May 11 02:54:03.923: INFO: stderr: "No resources found.\n"
May 11 02:54:03.923: INFO: stdout: ""
May 11 02:54:03.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -l name=update-demo --namespace=kubectl-5221 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 02:54:04.030: INFO: stderr: ""
May 11 02:54:04.030: INFO: stdout: "update-demo-nautilus-9dhcj\nupdate-demo-nautilus-ww54g\n"
May 11 02:54:04.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5221'
May 11 02:54:04.650: INFO: stderr: "No resources found.\n"
May 11 02:54:04.650: INFO: stdout: ""
May 11 02:54:04.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -l name=update-demo --namespace=kubectl-5221 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 02:54:04.752: INFO: stderr: ""
May 11 02:54:04.752: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:54:04.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5221" for this suite.
May 11 02:54:26.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:54:26.827: INFO: namespace kubectl-5221 deletion completed in 22.071020263s

• [SLOW TEST:42.480 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:54:26.827: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-3925
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3925
STEP: Deleting pre-stop pod
May 11 02:54:39.879: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:54:39.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3925" for this suite.
May 11 02:55:17.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:55:17.959: INFO: namespace prestop-3925 deletion completed in 38.068793196s

• [SLOW TEST:51.132 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:55:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 11 02:55:17.981: INFO: namespace kubectl-7605
May 11 02:55:17.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-7605'
May 11 02:55:18.202: INFO: stderr: ""
May 11 02:55:18.202: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 11 02:55:19.206: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:55:19.206: INFO: Found 0 / 1
May 11 02:55:20.206: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:55:20.206: INFO: Found 1 / 1
May 11 02:55:20.206: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 11 02:55:20.209: INFO: Selector matched 1 pods for map[app:redis]
May 11 02:55:20.209: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 02:55:20.209: INFO: wait on redis-master startup in kubectl-7605 
May 11 02:55:20.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 logs redis-master-cn7sr redis-master --namespace=kubectl-7605'
May 11 02:55:20.323: INFO: stderr: ""
May 11 02:55:20.323: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 May 02:55:19.021 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 May 02:55:19.021 # Server started, Redis version 3.2.12\n1:M 11 May 02:55:19.022 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 May 02:55:19.022 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 11 02:55:20.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7605'
May 11 02:55:20.435: INFO: stderr: ""
May 11 02:55:20.435: INFO: stdout: "service/rm2 exposed\n"
May 11 02:55:20.438: INFO: Service rm2 in namespace kubectl-7605 found.
STEP: exposing service
May 11 02:55:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7605'
May 11 02:55:22.555: INFO: stderr: ""
May 11 02:55:22.555: INFO: stdout: "service/rm3 exposed\n"
May 11 02:55:22.558: INFO: Service rm3 in namespace kubectl-7605 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:55:24.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7605" for this suite.
May 11 02:55:46.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:55:46.653: INFO: namespace kubectl-7605 deletion completed in 22.086306135s

• [SLOW TEST:28.694 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:55:46.653: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:56:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9552" for this suite.
May 11 02:56:18.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:56:18.933: INFO: namespace container-runtime-9552 deletion completed in 6.078879571s

• [SLOW TEST:32.280 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:56:18.934: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 11 02:56:20.973: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-59c1ddc0-7398-11e9-80a1-becd2a02efc9,GenerateName:,Namespace:events-6051,SelfLink:/api/v1/namespaces/events-6051/pods/send-events-59c1ddc0-7398-11e9-80a1-becd2a02efc9,UID:5a084ed7-7398-11e9-b14d-000c2942e394,ResourceVersion:24492,Generation:0,CreationTimestamp:2019-05-11 02:56:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 961149758,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mfq4t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mfq4t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mfq4t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027a29f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027a2a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:56:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:56:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:56:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:56:19 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.137,StartTime:2019-05-11 02:56:18 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-11 02:56:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://9d5b349f755afb15d2adcc49cf084355980fb347f2af64e22ab35f4b1dd0f9e0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 11 02:56:22.976: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 11 02:56:24.979: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:56:24.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6051" for this suite.
May 11 02:57:02.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:57:03.070: INFO: namespace events-6051 deletion completed in 38.081243598s

• [SLOW TEST:44.136 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:57:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-74113021-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:57:03.108: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9" in namespace "projected-9875" to be "success or failure"
May 11 02:57:03.109: INFO: Pod "pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58398ms
May 11 02:57:05.112: INFO: Pod "pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004288354s
STEP: Saw pod success
May 11 02:57:05.112: INFO: Pod "pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:57:05.114: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:57:05.132: INFO: Waiting for pod pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:57:05.134: INFO: Pod pod-projected-configmaps-7411848c-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:57:05.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9875" for this suite.
May 11 02:57:11.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:57:11.220: INFO: namespace projected-9875 deletion completed in 6.081077069s

• [SLOW TEST:8.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:57:11.220: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-78ebba3e-7398-11e9-80a1-becd2a02efc9
STEP: Creating configMap with name cm-test-opt-upd-78ebba78-7398-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-78ebba3e-7398-11e9-80a1-becd2a02efc9
STEP: Updating configmap cm-test-opt-upd-78ebba78-7398-11e9-80a1-becd2a02efc9
STEP: Creating configMap with name cm-test-opt-create-78ebba90-7398-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:57:15.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4486" for this suite.
May 11 02:57:37.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:57:37.391: INFO: namespace configmap-4486 deletion completed in 22.079208709s

• [SLOW TEST:26.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:57:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-88853c2a-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 02:57:37.423: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9" in namespace "projected-345" to be "success or failure"
May 11 02:57:37.425: INFO: Pod "pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486795ms
May 11 02:57:39.429: INFO: Pod "pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006140797s
STEP: Saw pod success
May 11 02:57:39.429: INFO: Pod "pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:57:39.432: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 02:57:39.447: INFO: Waiting for pod pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:57:39.450: INFO: Pod pod-projected-secrets-8885901e-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:57:39.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-345" for this suite.
May 11 02:57:45.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:57:45.534: INFO: namespace projected-345 deletion completed in 6.08122992s

• [SLOW TEST:8.143 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:57:45.535: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 11 02:57:45.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6632'
May 11 02:57:45.799: INFO: stderr: ""
May 11 02:57:45.799: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 11 02:57:50.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pod e2e-test-nginx-pod --namespace=kubectl-6632 -o json'
May 11 02:57:50.942: INFO: stderr: ""
May 11 02:57:50.942: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-11T02:57:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6632\",\n        \"resourceVersion\": \"24878\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6632/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8dc68e61-7398-11e9-b14d-000c2942e394\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-z5v78\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"craig-k8s-certification-2-sprout-hero-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-z5v78\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-z5v78\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-11T02:57:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-11T02:57:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-11T02:57:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-11T02:57:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://4afb5a307ed96bc712470b90401b6a548457099fda121adc242c46deb7415300\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-11T02:57:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"70.0.48.171\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.90.140\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-11T02:57:45Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 11 02:57:50.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 replace -f - --namespace=kubectl-6632'
May 11 02:57:51.181: INFO: stderr: ""
May 11 02:57:51.181: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 11 02:57:51.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete pods e2e-test-nginx-pod --namespace=kubectl-6632'
May 11 02:58:02.562: INFO: stderr: ""
May 11 02:58:02.562: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:02.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6632" for this suite.
May 11 02:58:08.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:08.640: INFO: namespace kubectl-6632 deletion completed in 6.075773519s

• [SLOW TEST:23.106 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:08.641: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 02:58:08.665: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 11 02:58:08.670: INFO: Pod name sample-pod: Found 0 pods out of 1
May 11 02:58:13.675: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 02:58:13.675: INFO: Creating deployment "test-rolling-update-deployment"
May 11 02:58:13.679: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 11 02:58:13.683: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 11 02:58:15.688: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 11 02:58:15.690: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 11 02:58:15.698: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-362,SelfLink:/apis/apps/v1/namespaces/deployment-362/deployments/test-rolling-update-deployment,UID:9e6839cb-7398-11e9-b14d-000c2942e394,ResourceVersion:25030,Generation:1,CreationTimestamp:2019-05-11 02:58:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-11 02:58:14 +0000 UTC 2019-05-11 02:58:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-11 02:58:15 +0000 UTC 2019-05-11 02:58:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 11 02:58:15.700: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-362,SelfLink:/apis/apps/v1/namespaces/deployment-362/replicasets/test-rolling-update-deployment-67599b4d9,UID:9e6a4ec9-7398-11e9-b14d-000c2942e394,ResourceVersion:25020,Generation:1,CreationTimestamp:2019-05-11 02:58:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9e6839cb-7398-11e9-b14d-000c2942e394 0xc0027fb2c0 0xc0027fb2c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 11 02:58:15.700: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 11 02:58:15.700: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-362,SelfLink:/apis/apps/v1/namespaces/deployment-362/replicasets/test-rolling-update-controller,UID:9b6badd9-7398-11e9-b14d-000c2942e394,ResourceVersion:25029,Generation:2,CreationTimestamp:2019-05-11 02:58:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 9e6839cb-7398-11e9-b14d-000c2942e394 0xc0027fb177 0xc0027fb178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 02:58:15.703: INFO: Pod "test-rolling-update-deployment-67599b4d9-52r7p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-52r7p,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-362,SelfLink:/api/v1/namespaces/deployment-362/pods/test-rolling-update-deployment-67599b4d9-52r7p,UID:9e6ac311-7398-11e9-b14d-000c2942e394,ResourceVersion:25019,Generation:0,CreationTimestamp:2019-05-11 02:58:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 9e6a4ec9-7398-11e9-b14d-000c2942e394 0xc0020ec490 0xc0020ec491}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-82m8w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-82m8w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-82m8w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020ec4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020ec510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:58:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:58:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:58:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 02:58:14 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.142,StartTime:2019-05-11 02:58:13 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-11 02:58:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://448092f5b1fd719f1c3dc2f6ccb17d30166e73c830e0f819a8d6bd70e4f4b1ef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:15.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-362" for this suite.
May 11 02:58:21.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:21.788: INFO: namespace deployment-362 deletion completed in 6.081921075s

• [SLOW TEST:13.147 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:21.788: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a2fb1486-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:58:21.816: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9" in namespace "projected-8558" to be "success or failure"
May 11 02:58:21.818: INFO: Pod "pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321779ms
May 11 02:58:23.821: INFO: Pod "pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005380086s
STEP: Saw pod success
May 11 02:58:23.821: INFO: Pod "pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:58:23.823: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:58:23.835: INFO: Waiting for pod pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:58:23.837: INFO: Pod pod-projected-configmaps-a2fb6e7e-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:23.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8558" for this suite.
May 11 02:58:29.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:29.914: INFO: namespace projected-8558 deletion completed in 6.074827056s

• [SLOW TEST:8.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-a7d302ff-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:58:29.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9" in namespace "configmap-2511" to be "success or failure"
May 11 02:58:29.946: INFO: Pod "pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534636ms
May 11 02:58:31.950: INFO: Pod "pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007899638s
STEP: Saw pod success
May 11 02:58:31.950: INFO: Pod "pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:58:31.952: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:58:31.972: INFO: Waiting for pod pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:58:31.974: INFO: Pod pod-configmaps-a7d352f8-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2511" for this suite.
May 11 02:58:37.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:38.062: INFO: namespace configmap-2511 deletion completed in 6.085877427s

• [SLOW TEST:8.148 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:38.063: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 11 02:58:38.089: INFO: Waiting up to 5m0s for pod "pod-acae860e-7398-11e9-80a1-becd2a02efc9" in namespace "emptydir-40" to be "success or failure"
May 11 02:58:38.092: INFO: Pod "pod-acae860e-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.218462ms
May 11 02:58:40.095: INFO: Pod "pod-acae860e-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006479329s
STEP: Saw pod success
May 11 02:58:40.095: INFO: Pod "pod-acae860e-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:58:40.098: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-acae860e-7398-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:58:40.112: INFO: Waiting for pod pod-acae860e-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:58:40.113: INFO: Pod pod-acae860e-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:40.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-40" for this suite.
May 11 02:58:46.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:46.191: INFO: namespace emptydir-40 deletion completed in 6.074906108s

• [SLOW TEST:8.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:46.191: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 11 02:58:46.234: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-997,SelfLink:/api/v1/namespaces/watch-997/configmaps/e2e-watch-test-resource-version,UID:b1cda548-7398-11e9-b14d-000c2942e394,ResourceVersion:25261,Generation:0,CreationTimestamp:2019-05-11 02:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 11 02:58:46.234: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-997,SelfLink:/api/v1/namespaces/watch-997/configmaps/e2e-watch-test-resource-version,UID:b1cda548-7398-11e9-b14d-000c2942e394,ResourceVersion:25262,Generation:0,CreationTimestamp:2019-05-11 02:58:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:58:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-997" for this suite.
May 11 02:58:52.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:58:52.312: INFO: namespace watch-997 deletion completed in 6.074888483s

• [SLOW TEST:6.121 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:58:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-v8qf
STEP: Creating a pod to test atomic-volume-subpath
May 11 02:58:52.346: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v8qf" in namespace "subpath-7543" to be "success or failure"
May 11 02:58:52.349: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908468ms
May 11 02:58:54.352: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006074013s
May 11 02:58:56.356: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010485401s
May 11 02:58:58.361: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 6.014685143s
May 11 02:59:00.365: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 8.019082829s
May 11 02:59:02.368: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 10.02226377s
May 11 02:59:04.373: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 12.026881036s
May 11 02:59:06.376: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 14.030217852s
May 11 02:59:08.380: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 16.034365481s
May 11 02:59:10.384: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 18.038443073s
May 11 02:59:12.389: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Running", Reason="", readiness=true. Elapsed: 20.042703098s
May 11 02:59:14.393: INFO: Pod "pod-subpath-test-configmap-v8qf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.047408536s
STEP: Saw pod success
May 11 02:59:14.393: INFO: Pod "pod-subpath-test-configmap-v8qf" satisfied condition "success or failure"
May 11 02:59:14.396: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-subpath-test-configmap-v8qf container test-container-subpath-configmap-v8qf: <nil>
STEP: delete the pod
May 11 02:59:14.409: INFO: Waiting for pod pod-subpath-test-configmap-v8qf to disappear
May 11 02:59:14.411: INFO: Pod pod-subpath-test-configmap-v8qf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v8qf
May 11 02:59:14.411: INFO: Deleting pod "pod-subpath-test-configmap-v8qf" in namespace "subpath-7543"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:59:14.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7543" for this suite.
May 11 02:59:20.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:59:20.493: INFO: namespace subpath-7543 deletion completed in 6.078206979s

• [SLOW TEST:28.181 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:59:20.494: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 11 02:59:20.520: INFO: Waiting up to 5m0s for pod "pod-c5f90534-7398-11e9-80a1-becd2a02efc9" in namespace "emptydir-1185" to be "success or failure"
May 11 02:59:20.522: INFO: Pod "pod-c5f90534-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01919ms
May 11 02:59:22.525: INFO: Pod "pod-c5f90534-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005206275s
STEP: Saw pod success
May 11 02:59:22.526: INFO: Pod "pod-c5f90534-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:59:22.528: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-c5f90534-7398-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:59:22.541: INFO: Waiting for pod pod-c5f90534-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:59:22.543: INFO: Pod pod-c5f90534-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:59:22.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1185" for this suite.
May 11 02:59:28.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:59:28.622: INFO: namespace emptydir-1185 deletion completed in 6.076325798s

• [SLOW TEST:8.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:59:28.622: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 11 02:59:28.654: INFO: Waiting up to 5m0s for pod "pod-cad1d957-7398-11e9-80a1-becd2a02efc9" in namespace "emptydir-1650" to be "success or failure"
May 11 02:59:28.658: INFO: Pod "pod-cad1d957-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.212687ms
May 11 02:59:30.661: INFO: Pod "pod-cad1d957-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007223172s
STEP: Saw pod success
May 11 02:59:30.661: INFO: Pod "pod-cad1d957-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:59:30.663: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-cad1d957-7398-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:59:30.677: INFO: Waiting for pod pod-cad1d957-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:59:30.678: INFO: Pod pod-cad1d957-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:59:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1650" for this suite.
May 11 02:59:36.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:59:36.761: INFO: namespace emptydir-1650 deletion completed in 6.080425462s

• [SLOW TEST:8.140 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:59:36.762: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cfac4875-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 02:59:36.799: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9" in namespace "projected-6569" to be "success or failure"
May 11 02:59:36.802: INFO: Pod "pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943217ms
May 11 02:59:38.805: INFO: Pod "pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006064484s
STEP: Saw pod success
May 11 02:59:38.805: INFO: Pod "pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:59:38.808: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 02:59:38.819: INFO: Waiting for pod pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:59:38.821: INFO: Pod pod-projected-configmaps-cfacb971-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:59:38.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6569" for this suite.
May 11 02:59:44.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:59:44.893: INFO: namespace projected-6569 deletion completed in 6.069924132s

• [SLOW TEST:8.131 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:59:44.893: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 11 02:59:44.917: INFO: Waiting up to 5m0s for pod "client-containers-d483c369-7398-11e9-80a1-becd2a02efc9" in namespace "containers-2664" to be "success or failure"
May 11 02:59:44.919: INFO: Pod "client-containers-d483c369-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923131ms
May 11 02:59:46.922: INFO: Pod "client-containers-d483c369-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005079443s
STEP: Saw pod success
May 11 02:59:46.922: INFO: Pod "client-containers-d483c369-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 02:59:46.924: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod client-containers-d483c369-7398-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 02:59:46.936: INFO: Waiting for pod client-containers-d483c369-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 02:59:46.938: INFO: Pod client-containers-d483c369-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 02:59:46.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2664" for this suite.
May 11 02:59:52.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 02:59:53.026: INFO: namespace containers-2664 deletion completed in 6.085854144s

• [SLOW TEST:8.133 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 02:59:53.027: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-dc2b
STEP: Creating a pod to test atomic-volume-subpath
May 11 02:59:53.060: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dc2b" in namespace "subpath-7059" to be "success or failure"
May 11 02:59:53.063: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.131896ms
May 11 02:59:55.067: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007444048s
May 11 02:59:57.070: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.010376703s
May 11 02:59:59.073: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 6.013306759s
May 11 03:00:01.077: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 8.017274335s
May 11 03:00:03.081: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 10.021491352s
May 11 03:00:05.085: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 12.02473425s
May 11 03:00:07.089: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 14.028758159s
May 11 03:00:09.093: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 16.033290502s
May 11 03:00:11.097: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 18.036793585s
May 11 03:00:13.101: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Running", Reason="", readiness=true. Elapsed: 20.041418927s
May 11 03:00:15.106: INFO: Pod "pod-subpath-test-projected-dc2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045819341s
STEP: Saw pod success
May 11 03:00:15.106: INFO: Pod "pod-subpath-test-projected-dc2b" satisfied condition "success or failure"
May 11 03:00:15.109: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-subpath-test-projected-dc2b container test-container-subpath-projected-dc2b: <nil>
STEP: delete the pod
May 11 03:00:15.127: INFO: Waiting for pod pod-subpath-test-projected-dc2b to disappear
May 11 03:00:15.133: INFO: Pod pod-subpath-test-projected-dc2b no longer exists
STEP: Deleting pod pod-subpath-test-projected-dc2b
May 11 03:00:15.133: INFO: Deleting pod "pod-subpath-test-projected-dc2b" in namespace "subpath-7059"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:00:15.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7059" for this suite.
May 11 03:00:21.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:00:21.224: INFO: namespace subpath-7059 deletion completed in 6.086464639s

• [SLOW TEST:28.198 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:00:21.225: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ea2ce414-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:00:21.261: INFO: Waiting up to 5m0s for pod "pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9" in namespace "secrets-7877" to be "success or failure"
May 11 03:00:21.264: INFO: Pod "pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9957ms
May 11 03:00:23.267: INFO: Pod "pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006271423s
STEP: Saw pod success
May 11 03:00:23.267: INFO: Pod "pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:00:23.270: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 03:00:23.286: INFO: Waiting for pod pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 03:00:23.289: INFO: Pod pod-secrets-ea2d475e-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:00:23.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7877" for this suite.
May 11 03:00:29.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:00:29.362: INFO: namespace secrets-7877 deletion completed in 6.070086685s

• [SLOW TEST:8.138 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:00:29.362: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ef056992-7398-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:00:29.390: INFO: Waiting up to 5m0s for pod "pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9" in namespace "secrets-6431" to be "success or failure"
May 11 03:00:29.393: INFO: Pod "pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853689ms
May 11 03:00:31.396: INFO: Pod "pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006608252s
STEP: Saw pod success
May 11 03:00:31.396: INFO: Pod "pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:00:31.399: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 03:00:31.411: INFO: Waiting for pod pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 03:00:31.413: INFO: Pod pod-secrets-ef05c5d6-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:00:31.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6431" for this suite.
May 11 03:00:37.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:00:37.489: INFO: namespace secrets-6431 deletion completed in 6.073847099s

• [SLOW TEST:8.127 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:00:37.490: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 11 03:00:37.521: INFO: Waiting up to 5m0s for pod "downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9" in namespace "downward-api-6637" to be "success or failure"
May 11 03:00:37.524: INFO: Pod "downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733379ms
May 11 03:00:39.528: INFO: Pod "downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006528583s
STEP: Saw pod success
May 11 03:00:39.528: INFO: Pod "downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:00:39.531: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 03:00:39.545: INFO: Waiting for pod downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9 to disappear
May 11 03:00:39.547: INFO: Pod downward-api-f3de65a1-7398-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:00:39.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6637" for this suite.
May 11 03:00:45.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:00:45.626: INFO: namespace downward-api-6637 deletion completed in 6.076587187s

• [SLOW TEST:8.136 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:00:45.627: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6476
May 11 03:00:47.661: INFO: Started pod liveness-http in namespace container-probe-6476
STEP: checking the pod's current state and verifying that restartCount is present
May 11 03:00:47.663: INFO: Initial restart count of pod liveness-http is 0
May 11 03:01:09.706: INFO: Restart count of pod container-probe-6476/liveness-http is now 1 (22.043162507s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:01:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6476" for this suite.
May 11 03:01:15.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:01:15.791: INFO: namespace container-probe-6476 deletion completed in 6.073250859s

• [SLOW TEST:30.165 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:01:15.792: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:01:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7669" for this suite.
May 11 03:02:03.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:02:03.914: INFO: namespace kubelet-test-7669 deletion completed in 44.067372767s

• [SLOW TEST:48.122 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:02:03.914: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-27614705-7399-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:02:03.944: INFO: Waiting up to 5m0s for pod "pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9" in namespace "secrets-1589" to be "success or failure"
May 11 03:02:03.946: INFO: Pod "pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929059ms
May 11 03:02:05.949: INFO: Pod "pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005114734s
STEP: Saw pod success
May 11 03:02:05.949: INFO: Pod "pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:02:05.951: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9 container secret-env-test: <nil>
STEP: delete the pod
May 11 03:02:05.963: INFO: Waiting for pod pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:02:05.966: INFO: Pod pod-secrets-27618fad-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:02:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1589" for this suite.
May 11 03:02:11.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:02:12.040: INFO: namespace secrets-1589 deletion completed in 6.071610058s

• [SLOW TEST:8.126 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:02:12.040: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:02:12.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9" in namespace "projected-7475" to be "success or failure"
May 11 03:02:12.068: INFO: Pod "downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794955ms
May 11 03:02:14.071: INFO: Pod "downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005524059s
STEP: Saw pod success
May 11 03:02:14.071: INFO: Pod "downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:02:14.073: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:02:14.085: INFO: Waiting for pod downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:02:14.087: INFO: Pod downwardapi-volume-2c38ca2c-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:02:14.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7475" for this suite.
May 11 03:02:20.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:02:20.164: INFO: namespace projected-7475 deletion completed in 6.074463041s

• [SLOW TEST:8.125 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:02:20.165: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:02:20.202: INFO: Create a RollingUpdate DaemonSet
May 11 03:02:20.205: INFO: Check that daemon pods launch on every node of the cluster
May 11 03:02:20.209: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:20.211: INFO: Number of nodes with available pods: 0
May 11 03:02:20.211: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 03:02:21.215: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:21.218: INFO: Number of nodes with available pods: 0
May 11 03:02:21.218: INFO: Node craig-k8s-certification-2-sprout-hero-1 is running more than one daemon pod
May 11 03:02:22.216: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:22.221: INFO: Number of nodes with available pods: 3
May 11 03:02:22.221: INFO: Number of running nodes: 3, number of available pods: 3
May 11 03:02:22.221: INFO: Update the DaemonSet to trigger a rollout
May 11 03:02:22.228: INFO: Updating DaemonSet daemon-set
May 11 03:02:33.240: INFO: Roll back the DaemonSet before rollout is complete
May 11 03:02:33.245: INFO: Updating DaemonSet daemon-set
May 11 03:02:33.245: INFO: Make sure DaemonSet rollback is complete
May 11 03:02:33.247: INFO: Wrong image for pod: daemon-set-nfqkn. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 11 03:02:33.247: INFO: Pod daemon-set-nfqkn is not available
May 11 03:02:33.251: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:34.256: INFO: Wrong image for pod: daemon-set-nfqkn. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 11 03:02:34.256: INFO: Pod daemon-set-nfqkn is not available
May 11 03:02:34.262: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:35.254: INFO: Wrong image for pod: daemon-set-nfqkn. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 11 03:02:35.254: INFO: Pod daemon-set-nfqkn is not available
May 11 03:02:35.257: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 11 03:02:36.256: INFO: Pod daemon-set-fm45r is not available
May 11 03:02:36.260: INFO: DaemonSet pods can't tolerate node craig-k8s-certification-2-sprout-hero-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8030, will wait for the garbage collector to delete the pods
May 11 03:02:36.326: INFO: Deleting DaemonSet.extensions daemon-set took: 6.366838ms
May 11 03:02:36.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.283742ms
May 11 03:03:53.531: INFO: Number of nodes with available pods: 0
May 11 03:03:53.531: INFO: Number of running nodes: 0, number of available pods: 0
May 11 03:03:53.533: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8030/daemonsets","resourceVersion":"26693"},"items":null}

May 11 03:03:53.535: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8030/pods","resourceVersion":"26693"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:03:53.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8030" for this suite.
May 11 03:03:59.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:03:59.618: INFO: namespace daemonsets-8030 deletion completed in 6.067201533s

• [SLOW TEST:99.453 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:03:59.618: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:03:59.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2524" for this suite.
May 11 03:04:05.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:04:05.723: INFO: namespace kubelet-test-2524 deletion completed in 6.066463091s

• [SLOW TEST:6.105 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:04:05.723: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:04:05.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9" in namespace "projected-2747" to be "success or failure"
May 11 03:04:05.756: INFO: Pod "downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095396ms
May 11 03:04:07.760: INFO: Pod "downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006605504s
May 11 03:04:09.763: INFO: Pod "downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009862323s
STEP: Saw pod success
May 11 03:04:09.763: INFO: Pod "downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:04:09.765: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:04:09.778: INFO: Waiting for pod downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:04:09.785: INFO: Pod downwardapi-volume-6ffbcc53-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:04:09.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2747" for this suite.
May 11 03:04:15.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:04:15.866: INFO: namespace projected-2747 deletion completed in 6.077529825s

• [SLOW TEST:10.142 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:04:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-4875
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4875 to expose endpoints map[]
May 11 03:04:15.899: INFO: Get endpoints failed (2.911455ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May 11 03:04:16.902: INFO: successfully validated that service endpoint-test2 in namespace services-4875 exposes endpoints map[] (1.005549843s elapsed)
STEP: Creating pod pod1 in namespace services-4875
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4875 to expose endpoints map[pod1:[80]]
May 11 03:04:18.922: INFO: successfully validated that service endpoint-test2 in namespace services-4875 exposes endpoints map[pod1:[80]] (2.015329729s elapsed)
STEP: Creating pod pod2 in namespace services-4875
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4875 to expose endpoints map[pod1:[80] pod2:[80]]
May 11 03:04:20.944: INFO: successfully validated that service endpoint-test2 in namespace services-4875 exposes endpoints map[pod1:[80] pod2:[80]] (2.019161244s elapsed)
STEP: Deleting pod pod1 in namespace services-4875
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4875 to expose endpoints map[pod2:[80]]
May 11 03:04:21.958: INFO: successfully validated that service endpoint-test2 in namespace services-4875 exposes endpoints map[pod2:[80]] (1.01069929s elapsed)
STEP: Deleting pod pod2 in namespace services-4875
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4875 to expose endpoints map[]
May 11 03:04:22.970: INFO: successfully validated that service endpoint-test2 in namespace services-4875 exposes endpoints map[] (1.006759707s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:04:22.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4875" for this suite.
May 11 03:04:28.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:04:29.064: INFO: namespace services-4875 deletion completed in 6.074824085s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:13.198 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:04:29.064: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:04:29.097: INFO: (0) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.069656ms)
May 11 03:04:29.100: INFO: (1) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.061046ms)
May 11 03:04:29.102: INFO: (2) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.11841ms)
May 11 03:04:29.105: INFO: (3) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.993548ms)
May 11 03:04:29.108: INFO: (4) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.617492ms)
May 11 03:04:29.111: INFO: (5) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.487634ms)
May 11 03:04:29.113: INFO: (6) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.328329ms)
May 11 03:04:29.115: INFO: (7) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.190315ms)
May 11 03:04:29.118: INFO: (8) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.749087ms)
May 11 03:04:29.126: INFO: (9) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.704908ms)
May 11 03:04:29.129: INFO: (10) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.679972ms)
May 11 03:04:29.131: INFO: (11) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.274249ms)
May 11 03:04:29.133: INFO: (12) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.442886ms)
May 11 03:04:29.135: INFO: (13) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.982162ms)
May 11 03:04:29.138: INFO: (14) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.05409ms)
May 11 03:04:29.140: INFO: (15) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.152417ms)
May 11 03:04:29.142: INFO: (16) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.53929ms)
May 11 03:04:29.144: INFO: (17) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.243236ms)
May 11 03:04:29.147: INFO: (18) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.122441ms)
May 11 03:04:29.149: INFO: (19) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.933846ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:04:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6794" for this suite.
May 11 03:04:35.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:04:35.224: INFO: namespace proxy-6794 deletion completed in 6.073297677s

• [SLOW TEST:6.160 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:04:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-vxl8
STEP: Creating a pod to test atomic-volume-subpath
May 11 03:04:35.263: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vxl8" in namespace "subpath-3041" to be "success or failure"
May 11 03:04:35.269: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.676126ms
May 11 03:04:37.273: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00975336s
May 11 03:04:39.276: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.013673324s
May 11 03:04:41.280: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.017658359s
May 11 03:04:43.284: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.021350387s
May 11 03:04:45.287: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.024319375s
May 11 03:04:47.292: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.02882988s
May 11 03:04:49.295: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.032504063s
May 11 03:04:51.299: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.036421396s
May 11 03:04:53.304: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.040960168s
May 11 03:04:55.307: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.043789448s
May 11 03:04:57.309: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Running", Reason="", readiness=true. Elapsed: 22.046524517s
May 11 03:04:59.312: INFO: Pod "pod-subpath-test-downwardapi-vxl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049306141s
STEP: Saw pod success
May 11 03:04:59.312: INFO: Pod "pod-subpath-test-downwardapi-vxl8" satisfied condition "success or failure"
May 11 03:04:59.316: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-subpath-test-downwardapi-vxl8 container test-container-subpath-downwardapi-vxl8: <nil>
STEP: delete the pod
May 11 03:04:59.331: INFO: Waiting for pod pod-subpath-test-downwardapi-vxl8 to disappear
May 11 03:04:59.333: INFO: Pod pod-subpath-test-downwardapi-vxl8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vxl8
May 11 03:04:59.333: INFO: Deleting pod "pod-subpath-test-downwardapi-vxl8" in namespace "subpath-3041"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:04:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3041" for this suite.
May 11 03:05:05.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:05:05.418: INFO: namespace subpath-3041 deletion completed in 6.077580286s

• [SLOW TEST:30.193 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:05:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-9390c654-7399-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:05:05.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9" in namespace "projected-4145" to be "success or failure"
May 11 03:05:05.454: INFO: Pod "pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281448ms
May 11 03:05:07.458: INFO: Pod "pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005944739s
May 11 03:05:09.461: INFO: Pod "pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009388427s
STEP: Saw pod success
May 11 03:05:09.461: INFO: Pod "pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:05:09.463: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:05:09.477: INFO: Waiting for pod pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:05:09.479: INFO: Pod pod-projected-configmaps-93911972-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:05:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4145" for this suite.
May 11 03:05:15.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:05:15.560: INFO: namespace projected-4145 deletion completed in 6.079091724s

• [SLOW TEST:10.143 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:05:15.561: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 11 03:05:15.586: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-966612213 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:05:15.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7262" for this suite.
May 11 03:05:21.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:05:21.776: INFO: namespace kubectl-7262 deletion completed in 6.076076982s

• [SLOW TEST:6.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:05:21.776: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:05:21.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9" in namespace "projected-8501" to be "success or failure"
May 11 03:05:21.810: INFO: Pod "downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488751ms
May 11 03:05:23.813: INFO: Pod "downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006633027s
May 11 03:05:25.817: INFO: Pod "downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009887139s
STEP: Saw pod success
May 11 03:05:25.817: INFO: Pod "downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:05:25.819: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:05:25.834: INFO: Waiting for pod downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:05:25.836: INFO: Pod downwardapi-volume-9d50dc69-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:05:25.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8501" for this suite.
May 11 03:05:31.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:05:31.917: INFO: namespace projected-8501 deletion completed in 6.078557637s

• [SLOW TEST:10.141 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:05:31.917: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:05:31.948: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9" in namespace "projected-2557" to be "success or failure"
May 11 03:05:31.950: INFO: Pod "downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894826ms
May 11 03:05:33.955: INFO: Pod "downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007225239s
May 11 03:05:35.958: INFO: Pod "downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010071785s
STEP: Saw pod success
May 11 03:05:35.958: INFO: Pod "downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:05:35.960: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:05:35.971: INFO: Waiting for pod downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:05:35.972: INFO: Pod downwardapi-volume-a35c76c5-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:05:35.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2557" for this suite.
May 11 03:05:41.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:05:42.044: INFO: namespace projected-2557 deletion completed in 6.069135484s

• [SLOW TEST:10.127 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:05:42.044: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 11 03:05:47.096: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:05:48.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5236" for this suite.
May 11 03:06:10.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:06:10.197: INFO: namespace replicaset-5236 deletion completed in 22.087418661s

• [SLOW TEST:28.153 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:06:10.197: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-129
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 11 03:06:10.224: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 11 03:06:36.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.168:8080/dial?request=hostName&protocol=http&host=10.233.90.167&port=8080&tries=1'] Namespace:pod-network-test-129 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 03:06:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 03:06:36.529: INFO: Waiting for endpoints: map[]
May 11 03:06:36.532: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.168:8080/dial?request=hostName&protocol=http&host=10.233.122.53&port=8080&tries=1'] Namespace:pod-network-test-129 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 03:06:36.532: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 03:06:36.797: INFO: Waiting for endpoints: map[]
May 11 03:06:36.800: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.168:8080/dial?request=hostName&protocol=http&host=10.233.93.55&port=8080&tries=1'] Namespace:pod-network-test-129 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 11 03:06:36.800: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
May 11 03:06:37.016: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:06:37.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-129" for this suite.
May 11 03:06:59.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:06:59.092: INFO: namespace pod-network-test-129 deletion completed in 22.072142384s

• [SLOW TEST:48.895 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:06:59.093: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-d751df51-7399-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:06:59.123: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9" in namespace "configmap-3260" to be "success or failure"
May 11 03:06:59.126: INFO: Pod "pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410281ms
May 11 03:07:01.129: INFO: Pod "pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006125035s
STEP: Saw pod success
May 11 03:07:01.129: INFO: Pod "pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:07:01.134: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:07:01.150: INFO: Waiting for pod pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:07:01.152: INFO: Pod pod-configmaps-d7522d62-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:07:01.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3260" for this suite.
May 11 03:07:07.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:07:07.240: INFO: namespace configmap-3260 deletion completed in 6.085071459s

• [SLOW TEST:8.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:07:07.240: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 11 03:07:07.267: INFO: Waiting up to 5m0s for pod "var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9" in namespace "var-expansion-1146" to be "success or failure"
May 11 03:07:07.269: INFO: Pod "var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203973ms
May 11 03:07:09.274: INFO: Pod "var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007220834s
May 11 03:07:11.278: INFO: Pod "var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010793216s
STEP: Saw pod success
May 11 03:07:11.278: INFO: Pod "var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:07:11.282: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 03:07:11.302: INFO: Waiting for pod var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9 to disappear
May 11 03:07:11.304: INFO: Pod var-expansion-dc2cdc55-7399-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:07:11.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1146" for this suite.
May 11 03:07:17.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:07:17.380: INFO: namespace var-expansion-1146 deletion completed in 6.073786838s

• [SLOW TEST:10.140 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:07:17.380: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:07:17.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 version --client'
May 11 03:07:17.483: INFO: stderr: ""
May 11 03:07:17.483: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 11 03:07:17.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-1440'
May 11 03:07:17.688: INFO: stderr: ""
May 11 03:07:17.688: INFO: stdout: "replicationcontroller/redis-master created\n"
May 11 03:07:17.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-1440'
May 11 03:07:17.834: INFO: stderr: ""
May 11 03:07:17.834: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 11 03:07:18.838: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:07:18.839: INFO: Found 0 / 1
May 11 03:07:19.838: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:07:19.838: INFO: Found 1 / 1
May 11 03:07:19.838: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 11 03:07:19.841: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:07:19.841: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 11 03:07:19.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 describe pod redis-master-qn8s8 --namespace=kubectl-1440'
May 11 03:07:19.944: INFO: stderr: ""
May 11 03:07:19.944: INFO: stdout: "Name:               redis-master-qn8s8\nNamespace:          kubectl-1440\nPriority:           0\nPriorityClassName:  <none>\nNode:               craig-k8s-certification-2-sprout-hero-1/70.0.48.171\nStart Time:         Sat, 11 May 2019 03:07:17 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.233.90.171\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://432df58302fe87fd5ff4f54a161358abd5890b7e48dda48d762976c5662a08f7\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 11 May 2019 03:07:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-vn6nr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-vn6nr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-vn6nr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                              Message\n  ----    ------     ----  ----                                              -------\n  Normal  Scheduled  1s    default-scheduler                                 Successfully assigned kubectl-1440/redis-master-qn8s8 to craig-k8s-certification-2-sprout-hero-1\n  Normal  Pulled     1s    kubelet, craig-k8s-certification-2-sprout-hero-1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, craig-k8s-certification-2-sprout-hero-1  Created container redis-master\n  Normal  Started    1s    kubelet, craig-k8s-certification-2-sprout-hero-1  Started container redis-master\n"
May 11 03:07:19.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 describe rc redis-master --namespace=kubectl-1440'
May 11 03:07:20.059: INFO: stderr: ""
May 11 03:07:20.059: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1440\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-qn8s8\n"
May 11 03:07:20.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 describe service redis-master --namespace=kubectl-1440'
May 11 03:07:20.167: INFO: stderr: ""
May 11 03:07:20.167: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1440\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.41.14\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.90.171:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 11 03:07:20.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 describe node craig-k8s-certification-2-sprout-hero-0'
May 11 03:07:20.286: INFO: stderr: ""
May 11 03:07:20.286: INFO: stdout: "Name:               craig-k8s-certification-2-sprout-hero-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=craig-k8s-certification-2-sprout-hero-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 11 May 2019 00:57:20 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 11 May 2019 03:07:20 +0000   Sat, 11 May 2019 00:57:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 11 May 2019 03:07:20 +0000   Sat, 11 May 2019 00:57:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 11 May 2019 03:07:20 +0000   Sat, 11 May 2019 00:57:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 11 May 2019 03:07:20 +0000   Sat, 11 May 2019 00:59:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  70.0.48.177\n  Hostname:    craig-k8s-certification-2-sprout-hero-0\nCapacity:\n cpu:                4\n ephemeral-storage:  20507216Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8009764Ki\n pods:               110\nAllocatable:\n cpu:                3800m\n ephemeral-storage:  18899450235\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7407364Ki\n pods:               110\nSystem Info:\n Machine ID:                 7656e291d27f4b47a0968369c19b523e\n System UUID:                564D8753-6A58-8028-94A2-6B4A9D42E394\n Boot ID:                    c396dcb1-6517-4bdc-b378-7dcfcf0d72b7\n Kernel Version:             3.10.0-862.3.2.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.14.1\n Kube-Proxy Version:         v1.14.1\nPodCIDR:                     10.233.64.0/24\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-bf4501c8aa854d46-xkqkc            0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\n  kube-system                calico-node-bs46p                                                  150m (3%)     300m (7%)   64M (0%)         500M (6%)      128m\n  kube-system                coredns-5466d46f97-nkr4r                                           100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     128m\n  kube-system                kube-apiserver-craig-k8s-certification-2-sprout-hero-0             250m (6%)     0 (0%)      0 (0%)           0 (0%)         129m\n  kube-system                kube-controller-manager-craig-k8s-certification-2-sprout-hero-0    200m (5%)     0 (0%)      0 (0%)           0 (0%)         129m\n  kube-system                kube-proxy-tcjvn                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\n  kube-system                kube-scheduler-craig-k8s-certification-2-sprout-hero-0             100m (2%)     0 (0%)      0 (0%)           0 (0%)         129m\n  kube-system                nodelocaldns-hknwt                                                 100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     127m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                900m (23%)      300m (7%)\n  memory             210800640 (2%)  856515840 (11%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
May 11 03:07:20.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 describe namespace kubectl-1440'
May 11 03:07:20.376: INFO: stderr: ""
May 11 03:07:20.376: INFO: stdout: "Name:         kubectl-1440\nLabels:       e2e-framework=kubectl\n              e2e-run=008b7a7b-7392-11e9-80a1-becd2a02efc9\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:07:20.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1440" for this suite.
May 11 03:07:42.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:07:42.466: INFO: namespace kubectl-1440 deletion completed in 22.086256181s

• [SLOW TEST:25.086 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:07:42.466: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8011
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 11 03:07:42.505: INFO: Found 0 stateful pods, waiting for 3
May 11 03:07:52.510: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 03:07:52.510: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 03:07:52.510: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 11 03:07:52.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-8011 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:07:52.844: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:07:52.844: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:07:52.844: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 11 03:08:02.874: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 11 03:08:12.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-8011 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 03:08:13.189: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 03:08:13.189: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 03:08:13.189: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 03:08:23.212: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
May 11 03:08:23.212: INFO: Waiting for Pod statefulset-8011/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 03:08:23.212: INFO: Waiting for Pod statefulset-8011/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 03:08:33.218: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
May 11 03:08:33.218: INFO: Waiting for Pod statefulset-8011/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 11 03:08:43.219: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
STEP: Rolling back to a previous revision
May 11 03:08:53.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-8011 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:08:53.530: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:08:53.530: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:08:53.530: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 03:08:53.554: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 11 03:09:03.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-8011 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 03:09:03.918: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 03:09:03.918: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 03:09:03.918: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 03:09:13.937: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
May 11 03:09:13.937: INFO: Waiting for Pod statefulset-8011/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 11 03:09:13.937: INFO: Waiting for Pod statefulset-8011/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May 11 03:09:23.943: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
May 11 03:09:23.943: INFO: Waiting for Pod statefulset-8011/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 11 03:09:33.944: INFO: Waiting for StatefulSet statefulset-8011/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 11 03:09:43.944: INFO: Deleting all statefulset in ns statefulset-8011
May 11 03:09:43.947: INFO: Scaling statefulset ss2 to 0
May 11 03:10:13.957: INFO: Waiting for statefulset status.replicas updated to 0
May 11 03:10:13.959: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:10:13.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8011" for this suite.
May 11 03:10:19.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:10:20.043: INFO: namespace statefulset-8011 deletion completed in 6.070874954s

• [SLOW TEST:157.577 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:10:20.043: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 11 03:10:20.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28840,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 11 03:10:20.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28841,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 11 03:10:20.076: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28842,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 11 03:10:30.098: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28872,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 11 03:10:30.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28873,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 11 03:10:30.098: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6207,SelfLink:/api/v1/namespaces/watch-6207/configmaps/e2e-watch-test-label-changed,UID:4f5ef0e1-739a-11e9-b14d-000c2942e394,ResourceVersion:28874,Generation:0,CreationTimestamp:2019-05-11 03:10:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:10:30.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6207" for this suite.
May 11 03:10:36.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:10:36.176: INFO: namespace watch-6207 deletion completed in 6.074566037s

• [SLOW TEST:16.133 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:10:36.176: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:10:36.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9" in namespace "projected-4280" to be "success or failure"
May 11 03:10:36.206: INFO: Pod "downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.172341ms
May 11 03:10:38.209: INFO: Pod "downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006239537s
May 11 03:10:40.212: INFO: Pod "downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008785025s
STEP: Saw pod success
May 11 03:10:40.212: INFO: Pod "downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:10:40.214: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:10:40.227: INFO: Waiting for pod downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9 to disappear
May 11 03:10:40.228: INFO: Pod downwardapi-volume-58b5f4e8-739a-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:10:40.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4280" for this suite.
May 11 03:10:46.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:10:46.307: INFO: namespace projected-4280 deletion completed in 6.076135891s

• [SLOW TEST:10.131 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:10:46.308: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 11 03:10:46.333: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-966612213 proxy --unix-socket=/tmp/kubectl-proxy-unix986612228/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:10:46.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8066" for this suite.
May 11 03:10:52.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:10:52.496: INFO: namespace kubectl-8066 deletion completed in 6.078059483s

• [SLOW TEST:6.188 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:10:52.496: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 11 03:10:52.517: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:10:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9864" for this suite.
May 11 03:11:01.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:11:01.737: INFO: namespace init-container-9864 deletion completed in 6.069956014s

• [SLOW TEST:9.242 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:11:01.738: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:11:05.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3712" for this suite.
May 11 03:11:11.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:11:11.853: INFO: namespace kubelet-test-3712 deletion completed in 6.077284921s

• [SLOW TEST:10.116 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:11:11.854: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 11 03:11:16.408: INFO: Successfully updated pod "pod-update-6df9efb5-739a-11e9-80a1-becd2a02efc9"
STEP: verifying the updated pod is in kubernetes
May 11 03:11:16.413: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:11:16.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7124" for this suite.
May 11 03:11:38.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:11:38.499: INFO: namespace pods-7124 deletion completed in 22.083056322s

• [SLOW TEST:26.645 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:11:38.499: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-7ddc2d6c-739a-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:11:38.531: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9" in namespace "projected-4426" to be "success or failure"
May 11 03:11:38.534: INFO: Pod "pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895441ms
May 11 03:11:40.538: INFO: Pod "pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006592387s
STEP: Saw pod success
May 11 03:11:40.538: INFO: Pod "pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:11:40.541: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 03:11:40.553: INFO: Waiting for pod pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9 to disappear
May 11 03:11:40.556: INFO: Pod pod-projected-secrets-7ddc83c9-739a-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:11:40.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4426" for this suite.
May 11 03:11:46.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:11:46.638: INFO: namespace projected-4426 deletion completed in 6.077341608s

• [SLOW TEST:8.138 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:11:46.638: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-82b53f52-739a-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:11:46.665: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9" in namespace "projected-7940" to be "success or failure"
May 11 03:11:46.669: INFO: Pod "pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160855ms
May 11 03:11:48.674: INFO: Pod "pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150923s
May 11 03:11:50.678: INFO: Pod "pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012695942s
STEP: Saw pod success
May 11 03:11:50.678: INFO: Pod "pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:11:50.680: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 11 03:11:50.692: INFO: Waiting for pod pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9 to disappear
May 11 03:11:50.693: INFO: Pod pod-projected-secrets-82b59a58-739a-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:11:50.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7940" for this suite.
May 11 03:11:56.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:11:56.774: INFO: namespace projected-7940 deletion completed in 6.077837312s

• [SLOW TEST:10.136 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:11:56.774: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-1113/configmap-test-88c036bb-739a-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:11:56.802: INFO: Waiting up to 5m0s for pod "pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9" in namespace "configmap-1113" to be "success or failure"
May 11 03:11:56.804: INFO: Pod "pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.696537ms
May 11 03:11:58.806: INFO: Pod "pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003900164s
May 11 03:12:00.808: INFO: Pod "pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006216467s
STEP: Saw pod success
May 11 03:12:00.808: INFO: Pod "pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:12:00.810: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9 container env-test: <nil>
STEP: delete the pod
May 11 03:12:00.822: INFO: Waiting for pod pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9 to disappear
May 11 03:12:00.824: INFO: Pod pod-configmaps-88c08861-739a-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:12:00.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1113" for this suite.
May 11 03:12:06.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:12:06.912: INFO: namespace configmap-1113 deletion completed in 6.084390348s

• [SLOW TEST:10.138 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:12:06.912: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:12:08.961: INFO: Waiting up to 5m0s for pod "client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9" in namespace "pods-4072" to be "success or failure"
May 11 03:12:08.963: INFO: Pod "client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887428ms
May 11 03:12:10.965: INFO: Pod "client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004559488s
STEP: Saw pod success
May 11 03:12:10.966: INFO: Pod "client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:12:10.968: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9 container env3cont: <nil>
STEP: delete the pod
May 11 03:12:10.981: INFO: Waiting for pod client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9 to disappear
May 11 03:12:10.982: INFO: Pod client-envvars-8fffae39-739a-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:12:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4072" for this suite.
May 11 03:12:54.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:12:55.052: INFO: namespace pods-4072 deletion completed in 44.066627843s

• [SLOW TEST:48.140 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:12:55.052: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:12:55.073: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:12:57.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4115" for this suite.
May 11 03:13:47.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:13:47.384: INFO: namespace pods-4115 deletion completed in 50.070104473s

• [SLOW TEST:52.332 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:13:47.385: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-caae3020-739a-11e9-80a1-becd2a02efc9
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:13:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2679" for this suite.
May 11 03:13:53.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:13:53.480: INFO: namespace configmap-2679 deletion completed in 6.067980693s

• [SLOW TEST:6.095 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:13:53.480: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-429
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-429
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-429
May 11 03:13:53.510: INFO: Found 0 stateful pods, waiting for 1
May 11 03:14:03.515: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 11 03:14:03.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:14:03.860: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:14:03.860: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:14:03.860: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 03:14:03.865: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 11 03:14:13.869: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 03:14:13.869: INFO: Waiting for statefulset status.replicas updated to 0
May 11 03:14:13.879: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:13.879: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:13.879: INFO: 
May 11 03:14:13.879: INFO: StatefulSet ss has not reached scale 3, at 1
May 11 03:14:14.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996936702s
May 11 03:14:15.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993252699s
May 11 03:14:16.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989833982s
May 11 03:14:17.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986867208s
May 11 03:14:18.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982528406s
May 11 03:14:19.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977513751s
May 11 03:14:20.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972528728s
May 11 03:14:21.912: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969506811s
May 11 03:14:22.916: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.143012ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-429
May 11 03:14:23.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 03:14:24.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 11 03:14:24.220: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 03:14:24.220: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 03:14:24.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 03:14:24.533: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 11 03:14:24.534: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 03:14:24.534: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 03:14:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 11 03:14:24.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 11 03:14:24.859: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 11 03:14:24.859: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 11 03:14:24.862: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 11 03:14:34.866: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 11 03:14:34.866: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 11 03:14:34.866: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 11 03:14:34.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:14:35.178: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:14:35.178: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:14:35.178: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 03:14:35.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:14:35.464: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:14:35.464: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:14:35.464: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 03:14:35.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 exec --namespace=statefulset-429 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 11 03:14:35.753: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 11 03:14:35.753: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 11 03:14:35.753: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 11 03:14:35.753: INFO: Waiting for statefulset status.replicas updated to 0
May 11 03:14:35.756: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 11 03:14:45.768: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 11 03:14:45.768: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 11 03:14:45.768: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 11 03:14:45.777: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:45.777: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:45.778: INFO: ss-1  craig-k8s-certification-2-sprout-hero-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:45.778: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:45.778: INFO: 
May 11 03:14:45.778: INFO: StatefulSet ss has not reached scale 0, at 3
May 11 03:14:46.783: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:46.783: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:46.783: INFO: ss-1  craig-k8s-certification-2-sprout-hero-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:46.783: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:46.783: INFO: 
May 11 03:14:46.783: INFO: StatefulSet ss has not reached scale 0, at 3
May 11 03:14:47.787: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:47.787: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:47.787: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:47.787: INFO: 
May 11 03:14:47.787: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 03:14:48.791: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:48.791: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:48.791: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:48.791: INFO: 
May 11 03:14:48.791: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 03:14:49.796: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:49.796: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:49.796: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:49.796: INFO: 
May 11 03:14:49.796: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 03:14:50.800: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:50.800: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:50.800: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:50.800: INFO: 
May 11 03:14:50.800: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 03:14:51.804: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
May 11 03:14:51.804: INFO: ss-0  craig-k8s-certification-2-sprout-hero-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:13:53 +0000 UTC  }]
May 11 03:14:51.804: INFO: ss-2  craig-k8s-certification-2-sprout-hero-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:14:14 +0000 UTC  }]
May 11 03:14:51.804: INFO: 
May 11 03:14:51.804: INFO: StatefulSet ss has not reached scale 0, at 2
May 11 03:14:52.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.970638062s
May 11 03:14:53.811: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.966402322s
May 11 03:14:54.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 963.555637ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-429
May 11 03:14:55.819: INFO: Scaling statefulset ss to 0
May 11 03:14:55.827: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 11 03:14:55.829: INFO: Deleting all statefulset in ns statefulset-429
May 11 03:14:55.831: INFO: Scaling statefulset ss to 0
May 11 03:14:55.838: INFO: Waiting for statefulset status.replicas updated to 0
May 11 03:14:55.839: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:14:55.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-429" for this suite.
May 11 03:15:01.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:15:01.937: INFO: namespace statefulset-429 deletion completed in 6.086872236s

• [SLOW TEST:68.457 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:15:01.937: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 11 03:15:01.959: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:15:05.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2436" for this suite.
May 11 03:15:11.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:15:11.846: INFO: namespace init-container-2436 deletion completed in 6.084109346s

• [SLOW TEST:9.909 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:15:11.846: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 11 03:15:19.912: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:19.915: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:21.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:21.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:23.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:23.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:25.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:25.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:27.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:27.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:29.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:29.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:31.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:31.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:33.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:33.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:35.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:35.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:37.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:37.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:39.916: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:39.920: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:41.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:41.919: INFO: Pod pod-with-poststart-exec-hook still exists
May 11 03:15:43.915: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 11 03:15:43.919: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:15:43.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9502" for this suite.
May 11 03:16:05.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:16:05.999: INFO: namespace container-lifecycle-hook-9502 deletion completed in 22.07653798s

• [SLOW TEST:54.153 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:16:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:16:06.032: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 11 03:16:11.036: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 03:16:11.036: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 11 03:16:11.055: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1469,SelfLink:/apis/apps/v1/namespaces/deployment-1469/deployments/test-cleanup-deployment,UID:20917741-739b-11e9-b14d-000c2942e394,ResourceVersion:30496,Generation:1,CreationTimestamp:2019-05-11 03:16:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 11 03:16:11.059: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-1469,SelfLink:/apis/apps/v1/namespaces/deployment-1469/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:209358d2-739b-11e9-b14d-000c2942e394,ResourceVersion:30498,Generation:1,CreationTimestamp:2019-05-11 03:16:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 20917741-739b-11e9-b14d-000c2942e394 0xc0027ebd47 0xc0027ebd48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 03:16:11.059: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 11 03:16:11.059: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-1469,SelfLink:/apis/apps/v1/namespaces/deployment-1469/replicasets/test-cleanup-controller,UID:1d9425b8-739b-11e9-b14d-000c2942e394,ResourceVersion:30497,Generation:1,CreationTimestamp:2019-05-11 03:16:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 20917741-739b-11e9-b14d-000c2942e394 0xc0027ebc77 0xc0027ebc78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 11 03:16:11.064: INFO: Pod "test-cleanup-controller-wrnkg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-wrnkg,GenerateName:test-cleanup-controller-,Namespace:deployment-1469,SelfLink:/api/v1/namespaces/deployment-1469/pods/test-cleanup-controller-wrnkg,UID:1d95108d-739b-11e9-b14d-000c2942e394,ResourceVersion:30487,Generation:0,CreationTimestamp:2019-05-11 03:16:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 1d9425b8-739b-11e9-b14d-000c2942e394 0xc002b746c7 0xc002b746c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-l9fqg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l9fqg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-l9fqg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b74730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b74750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:16:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:16:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:16:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:16:06 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.189,StartTime:2019-05-11 03:16:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-11 03:16:06 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://74f09d22632dd01df2708d3e0c2c3feca2bbee6d047a3c88be8f88a12b28a24f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 11 03:16:11.064: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-s9g6b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-s9g6b,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-1469,SelfLink:/api/v1/namespaces/deployment-1469/pods/test-cleanup-deployment-55cbfbc8f5-s9g6b,UID:20940208-739b-11e9-b14d-000c2942e394,ResourceVersion:30500,Generation:0,CreationTimestamp:2019-05-11 03:16:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 209358d2-739b-11e9-b14d-000c2942e394 0xc002b74827 0xc002b74828}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-l9fqg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l9fqg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-l9fqg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b74890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b748b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:16:11.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1469" for this suite.
May 11 03:16:17.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:16:17.150: INFO: namespace deployment-1469 deletion completed in 6.077575613s

• [SLOW TEST:11.151 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:16:17.150: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-23f2692e-739b-11e9-80a1-becd2a02efc9
STEP: Creating secret with name secret-projected-all-test-volume-23f268fa-739b-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test Check all projections for projected volume plugin
May 11 03:16:17.181: INFO: Waiting up to 5m0s for pod "projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9" in namespace "projected-4190" to be "success or failure"
May 11 03:16:17.184: INFO: Pod "projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942663ms
May 11 03:16:19.188: INFO: Pod "projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006930252s
May 11 03:16:21.192: INFO: Pod "projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011183578s
STEP: Saw pod success
May 11 03:16:21.192: INFO: Pod "projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:16:21.195: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9 container projected-all-volume-test: <nil>
STEP: delete the pod
May 11 03:16:21.213: INFO: Waiting for pod projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:16:21.216: INFO: Pod projected-volume-23f268a0-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:16:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4190" for this suite.
May 11 03:16:27.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:16:27.316: INFO: namespace projected-4190 deletion completed in 6.09684375s

• [SLOW TEST:10.165 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:16:27.316: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:17:27.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6320" for this suite.
May 11 03:17:49.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:17:49.442: INFO: namespace container-probe-6320 deletion completed in 22.082286178s

• [SLOW TEST:82.126 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:17:49.442: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:17:49.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9" in namespace "projected-2266" to be "success or failure"
May 11 03:17:49.475: INFO: Pod "downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542478ms
May 11 03:17:51.481: INFO: Pod "downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008390158s
May 11 03:17:53.484: INFO: Pod "downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011335942s
STEP: Saw pod success
May 11 03:17:53.484: INFO: Pod "downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:17:53.486: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:17:53.501: INFO: Waiting for pod downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:17:53.503: INFO: Pod downwardapi-volume-5af58917-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:17:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2266" for this suite.
May 11 03:17:59.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:17:59.574: INFO: namespace projected-2266 deletion completed in 6.068699504s

• [SLOW TEST:10.132 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:17:59.575: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:18:03.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2032" for this suite.
May 11 03:18:43.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:18:43.696: INFO: namespace kubelet-test-2032 deletion completed in 40.071568794s

• [SLOW TEST:44.121 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:18:43.696: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 11 03:18:46.251: INFO: Successfully updated pod "annotationupdate7b4c08a3-739b-11e9-80a1-becd2a02efc9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:18:48.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8441" for this suite.
May 11 03:19:10.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:10.355: INFO: namespace downward-api-8441 deletion completed in 22.077126661s

• [SLOW TEST:26.658 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:10.355: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 11 03:19:10.387: INFO: Waiting up to 5m0s for pod "client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9" in namespace "containers-766" to be "success or failure"
May 11 03:19:10.393: INFO: Pod "client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194321ms
May 11 03:19:12.397: INFO: Pod "client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009687917s
STEP: Saw pod success
May 11 03:19:12.397: INFO: Pod "client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:19:12.399: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 03:19:12.419: INFO: Waiting for pod client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:19:12.421: INFO: Pod client-containers-8b3034bc-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:12.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-766" for this suite.
May 11 03:19:18.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:18.507: INFO: namespace containers-766 deletion completed in 6.083088714s

• [SLOW TEST:8.152 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-900dcfed-739b-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume secrets
May 11 03:19:18.553: INFO: Waiting up to 5m0s for pod "pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9" in namespace "secrets-6804" to be "success or failure"
May 11 03:19:18.557: INFO: Pod "pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604925ms
May 11 03:19:20.559: INFO: Pod "pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006090328s
STEP: Saw pod success
May 11 03:19:20.559: INFO: Pod "pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:19:20.561: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9 container secret-volume-test: <nil>
STEP: delete the pod
May 11 03:19:20.573: INFO: Waiting for pod pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:19:20.575: INFO: Pod pod-secrets-900e2c5d-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:20.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6804" for this suite.
May 11 03:19:26.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:26.688: INFO: namespace secrets-6804 deletion completed in 6.110376432s

• [SLOW TEST:8.181 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:26.688: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:19:26.714: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9" in namespace "projected-6537" to be "success or failure"
May 11 03:19:26.717: INFO: Pod "downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.24436ms
May 11 03:19:28.721: INFO: Pod "downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007611643s
STEP: Saw pod success
May 11 03:19:28.722: INFO: Pod "downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:19:28.725: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:19:28.739: INFO: Waiting for pod downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:19:28.744: INFO: Pod downwardapi-volume-94eb6960-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:28.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6537" for this suite.
May 11 03:19:34.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:34.834: INFO: namespace projected-6537 deletion completed in 6.087051642s

• [SLOW TEST:8.146 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:34.834: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:19:34.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9" in namespace "downward-api-191" to be "success or failure"
May 11 03:19:34.868: INFO: Pod "downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135179ms
May 11 03:19:36.871: INFO: Pod "downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005717462s
STEP: Saw pod success
May 11 03:19:36.871: INFO: Pod "downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:19:36.874: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:19:36.890: INFO: Waiting for pod downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:19:36.892: INFO: Pod downwardapi-volume-99c69f92-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:36.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-191" for this suite.
May 11 03:19:42.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:42.976: INFO: namespace downward-api-191 deletion completed in 6.080897947s

• [SLOW TEST:8.141 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:42.976: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:19:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:44.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1811" for this suite.
May 11 03:19:50.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:50.117: INFO: namespace custom-resource-definition-1811 deletion completed in 6.073092139s

• [SLOW TEST:7.141 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:50.118: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 11 03:19:50.144: INFO: Waiting up to 5m0s for pod "pod-a2e2a202-739b-11e9-80a1-becd2a02efc9" in namespace "emptydir-7110" to be "success or failure"
May 11 03:19:50.147: INFO: Pod "pod-a2e2a202-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367767ms
May 11 03:19:52.150: INFO: Pod "pod-a2e2a202-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006133201s
STEP: Saw pod success
May 11 03:19:52.150: INFO: Pod "pod-a2e2a202-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:19:52.152: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-a2e2a202-739b-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 03:19:52.163: INFO: Waiting for pod pod-a2e2a202-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:19:52.165: INFO: Pod pod-a2e2a202-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:19:52.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7110" for this suite.
May 11 03:19:58.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:19:58.241: INFO: namespace emptydir-7110 deletion completed in 6.073200005s

• [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:19:58.241: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 11 03:20:00.786: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1641 pod-service-account-a807e3ae-739b-11e9-80a1-becd2a02efc9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 11 03:20:01.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1641 pod-service-account-a807e3ae-739b-11e9-80a1-becd2a02efc9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 11 03:20:01.386: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1641 pod-service-account-a807e3ae-739b-11e9-80a1-becd2a02efc9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:20:01.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1641" for this suite.
May 11 03:20:07.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:20:07.788: INFO: namespace svcaccounts-1641 deletion completed in 6.080230795s

• [SLOW TEST:9.547 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:20:07.788: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ad6c31da-739b-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ad6c31da-739b-11e9-80a1-becd2a02efc9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:20:11.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-458" for this suite.
May 11 03:20:33.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:20:33.936: INFO: namespace projected-458 deletion completed in 22.074473193s

• [SLOW TEST:26.147 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:20:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 11 03:20:33.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 --namespace=kubectl-7095 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 11 03:20:36.183: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 11 03:20:36.183: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:20:38.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7095" for this suite.
May 11 03:20:44.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:20:44.266: INFO: namespace kubectl-7095 deletion completed in 6.075553693s

• [SLOW TEST:10.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:20:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c32971be-739b-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:20:44.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9" in namespace "projected-1922" to be "success or failure"
May 11 03:20:44.299: INFO: Pod "pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.845509ms
May 11 03:20:46.303: INFO: Pod "pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006759877s
STEP: Saw pod success
May 11 03:20:46.303: INFO: Pod "pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:20:46.305: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:20:46.317: INFO: Waiting for pod pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:20:46.319: INFO: Pod pod-projected-configmaps-c329bd14-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:20:46.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1922" for this suite.
May 11 03:20:52.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:20:52.403: INFO: namespace projected-1922 deletion completed in 6.081486099s

• [SLOW TEST:8.136 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:20:52.403: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c8032e5d-739b-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:20:52.436: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9" in namespace "projected-4196" to be "success or failure"
May 11 03:20:52.438: INFO: Pod "pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258398ms
May 11 03:20:54.442: INFO: Pod "pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005648561s
STEP: Saw pod success
May 11 03:20:54.442: INFO: Pod "pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:20:54.445: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:20:54.460: INFO: Waiting for pod pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:20:54.462: INFO: Pod pod-projected-configmaps-c803ac38-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:20:54.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4196" for this suite.
May 11 03:21:00.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:21:00.549: INFO: namespace projected-4196 deletion completed in 6.084809983s

• [SLOW TEST:8.146 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:21:00.549: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7709.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7709.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 03:21:02.596: INFO: DNS probes using dns-7709/dns-test-ccdd5347-739b-11e9-80a1-becd2a02efc9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:21:02.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7709" for this suite.
May 11 03:21:08.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:21:08.683: INFO: namespace dns-7709 deletion completed in 6.066709457s

• [SLOW TEST:8.134 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:21:08.683: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:21:08.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9" in namespace "downward-api-127" to be "success or failure"
May 11 03:21:08.711: INFO: Pod "downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064445ms
May 11 03:21:10.716: INFO: Pod "downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007076595s
STEP: Saw pod success
May 11 03:21:10.716: INFO: Pod "downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:21:10.719: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:21:10.735: INFO: Waiting for pod downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9 to disappear
May 11 03:21:10.736: INFO: Pod downwardapi-volume-d1b6cf7e-739b-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:21:10.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-127" for this suite.
May 11 03:21:16.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:21:16.807: INFO: namespace downward-api-127 deletion completed in 6.06776898s

• [SLOW TEST:8.124 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:21:16.807: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 11 03:21:16.845: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9324" to be "success or failure"
May 11 03:21:16.847: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102898ms
May 11 03:21:18.850: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005284719s
STEP: Saw pod success
May 11 03:21:18.850: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 11 03:21:18.852: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 11 03:21:18.863: INFO: Waiting for pod pod-host-path-test to disappear
May 11 03:21:18.865: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:21:18.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9324" for this suite.
May 11 03:21:24.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:21:24.956: INFO: namespace hostpath-9324 deletion completed in 6.087439302s

• [SLOW TEST:8.149 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:21:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5002
I0511 03:21:24.994105      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5002, replica count: 1
I0511 03:21:26.044641      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0511 03:21:27.044807      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 11 03:21:27.151: INFO: Created: latency-svc-s9dx4
May 11 03:21:27.155: INFO: Got endpoints: latency-svc-s9dx4 [10.61141ms]
May 11 03:21:27.164: INFO: Created: latency-svc-dxz5g
May 11 03:21:27.168: INFO: Got endpoints: latency-svc-dxz5g [12.788968ms]
May 11 03:21:27.169: INFO: Created: latency-svc-fh98n
May 11 03:21:27.171: INFO: Created: latency-svc-xvpzg
May 11 03:21:27.173: INFO: Got endpoints: latency-svc-fh98n [17.330273ms]
May 11 03:21:27.174: INFO: Got endpoints: latency-svc-xvpzg [18.057879ms]
May 11 03:21:27.176: INFO: Created: latency-svc-56wnm
May 11 03:21:27.181: INFO: Got endpoints: latency-svc-56wnm [25.17537ms]
May 11 03:21:27.181: INFO: Created: latency-svc-frhhb
May 11 03:21:27.183: INFO: Created: latency-svc-l9gbp
May 11 03:21:27.185: INFO: Got endpoints: latency-svc-frhhb [29.269132ms]
May 11 03:21:27.187: INFO: Created: latency-svc-f8vrg
May 11 03:21:27.187: INFO: Got endpoints: latency-svc-l9gbp [30.945378ms]
May 11 03:21:27.190: INFO: Created: latency-svc-qws4d
May 11 03:21:27.193: INFO: Got endpoints: latency-svc-f8vrg [37.078323ms]
May 11 03:21:27.195: INFO: Got endpoints: latency-svc-qws4d [39.244687ms]
May 11 03:21:27.196: INFO: Created: latency-svc-rrn4j
May 11 03:21:27.200: INFO: Created: latency-svc-4qfld
May 11 03:21:27.201: INFO: Got endpoints: latency-svc-rrn4j [45.26495ms]
May 11 03:21:27.202: INFO: Got endpoints: latency-svc-4qfld [45.646765ms]
May 11 03:21:27.205: INFO: Created: latency-svc-fgf8f
May 11 03:21:27.207: INFO: Got endpoints: latency-svc-fgf8f [12.031435ms]
May 11 03:21:27.211: INFO: Created: latency-svc-k9n6x
May 11 03:21:27.214: INFO: Created: latency-svc-skxz6
May 11 03:21:27.217: INFO: Got endpoints: latency-svc-k9n6x [60.423863ms]
May 11 03:21:27.218: INFO: Got endpoints: latency-svc-skxz6 [61.787263ms]
May 11 03:21:27.218: INFO: Created: latency-svc-pw6tv
May 11 03:21:27.223: INFO: Got endpoints: latency-svc-pw6tv [67.129178ms]
May 11 03:21:27.224: INFO: Created: latency-svc-h7h84
May 11 03:21:27.227: INFO: Created: latency-svc-cvwr9
May 11 03:21:27.228: INFO: Got endpoints: latency-svc-h7h84 [71.481281ms]
May 11 03:21:27.232: INFO: Created: latency-svc-559jm
May 11 03:21:27.234: INFO: Got endpoints: latency-svc-cvwr9 [77.290743ms]
May 11 03:21:27.235: INFO: Got endpoints: latency-svc-559jm [66.7892ms]
May 11 03:21:27.236: INFO: Created: latency-svc-xdq4c
May 11 03:21:27.239: INFO: Got endpoints: latency-svc-xdq4c [66.09009ms]
May 11 03:21:27.241: INFO: Created: latency-svc-p24ws
May 11 03:21:27.257: INFO: Got endpoints: latency-svc-p24ws [82.851919ms]
May 11 03:21:27.257: INFO: Created: latency-svc-t5p88
May 11 03:21:27.263: INFO: Got endpoints: latency-svc-t5p88 [81.981404ms]
May 11 03:21:27.263: INFO: Created: latency-svc-fvvwg
May 11 03:21:27.268: INFO: Created: latency-svc-728jm
May 11 03:21:27.270: INFO: Got endpoints: latency-svc-fvvwg [84.299111ms]
May 11 03:21:27.275: INFO: Got endpoints: latency-svc-728jm [87.724627ms]
May 11 03:21:27.275: INFO: Created: latency-svc-6qhmb
May 11 03:21:27.278: INFO: Got endpoints: latency-svc-6qhmb [85.265283ms]
May 11 03:21:27.282: INFO: Created: latency-svc-6pmz5
May 11 03:21:27.285: INFO: Created: latency-svc-td5gq
May 11 03:21:27.288: INFO: Got endpoints: latency-svc-6pmz5 [86.435279ms]
May 11 03:21:27.291: INFO: Got endpoints: latency-svc-td5gq [88.911518ms]
May 11 03:21:27.293: INFO: Created: latency-svc-lgj98
May 11 03:21:27.296: INFO: Created: latency-svc-9slgm
May 11 03:21:27.297: INFO: Got endpoints: latency-svc-lgj98 [89.801478ms]
May 11 03:21:27.301: INFO: Created: latency-svc-zbgtg
May 11 03:21:27.301: INFO: Got endpoints: latency-svc-9slgm [84.690257ms]
May 11 03:21:27.307: INFO: Created: latency-svc-vmvdm
May 11 03:21:27.307: INFO: Got endpoints: latency-svc-zbgtg [89.335874ms]
May 11 03:21:27.311: INFO: Got endpoints: latency-svc-vmvdm [87.497964ms]
May 11 03:21:27.314: INFO: Created: latency-svc-zjzvl
May 11 03:21:27.317: INFO: Got endpoints: latency-svc-zjzvl [89.614531ms]
May 11 03:21:27.320: INFO: Created: latency-svc-t6prq
May 11 03:21:27.322: INFO: Got endpoints: latency-svc-t6prq [88.566767ms]
May 11 03:21:27.325: INFO: Created: latency-svc-swxjt
May 11 03:21:27.328: INFO: Got endpoints: latency-svc-swxjt [92.649813ms]
May 11 03:21:27.331: INFO: Created: latency-svc-wbztp
May 11 03:21:27.336: INFO: Created: latency-svc-2nbqs
May 11 03:21:27.341: INFO: Created: latency-svc-plc87
May 11 03:21:27.344: INFO: Created: latency-svc-t6rjt
May 11 03:21:27.347: INFO: Created: latency-svc-9b77t
May 11 03:21:27.351: INFO: Created: latency-svc-jvfdm
May 11 03:21:27.354: INFO: Created: latency-svc-dwwcl
May 11 03:21:27.356: INFO: Got endpoints: latency-svc-wbztp [116.868828ms]
May 11 03:21:27.358: INFO: Created: latency-svc-ltjbk
May 11 03:21:27.362: INFO: Created: latency-svc-vcbjq
May 11 03:21:27.365: INFO: Created: latency-svc-sx5mj
May 11 03:21:27.367: INFO: Created: latency-svc-b5p64
May 11 03:21:27.373: INFO: Created: latency-svc-snxv4
May 11 03:21:27.378: INFO: Created: latency-svc-vxb2s
May 11 03:21:27.384: INFO: Created: latency-svc-5rzgp
May 11 03:21:27.387: INFO: Created: latency-svc-dpwvv
May 11 03:21:27.390: INFO: Created: latency-svc-crcqr
May 11 03:21:27.406: INFO: Got endpoints: latency-svc-2nbqs [149.378779ms]
May 11 03:21:27.415: INFO: Created: latency-svc-bdb5s
May 11 03:21:27.456: INFO: Got endpoints: latency-svc-plc87 [193.495603ms]
May 11 03:21:27.461: INFO: Created: latency-svc-lf6nw
May 11 03:21:27.507: INFO: Got endpoints: latency-svc-t6rjt [236.866729ms]
May 11 03:21:27.512: INFO: Created: latency-svc-lxnlg
May 11 03:21:27.555: INFO: Got endpoints: latency-svc-9b77t [280.12744ms]
May 11 03:21:27.563: INFO: Created: latency-svc-qpzx9
May 11 03:21:27.605: INFO: Got endpoints: latency-svc-jvfdm [326.480841ms]
May 11 03:21:27.611: INFO: Created: latency-svc-v9bd2
May 11 03:21:27.655: INFO: Got endpoints: latency-svc-dwwcl [367.100534ms]
May 11 03:21:27.661: INFO: Created: latency-svc-pmlqj
May 11 03:21:27.707: INFO: Got endpoints: latency-svc-ltjbk [416.087107ms]
May 11 03:21:27.714: INFO: Created: latency-svc-wwm2m
May 11 03:21:27.755: INFO: Got endpoints: latency-svc-vcbjq [458.023916ms]
May 11 03:21:27.762: INFO: Created: latency-svc-pxvhb
May 11 03:21:27.805: INFO: Got endpoints: latency-svc-sx5mj [503.702776ms]
May 11 03:21:27.812: INFO: Created: latency-svc-djn8j
May 11 03:21:27.855: INFO: Got endpoints: latency-svc-b5p64 [547.833445ms]
May 11 03:21:27.863: INFO: Created: latency-svc-mrxc8
May 11 03:21:27.906: INFO: Got endpoints: latency-svc-snxv4 [595.033217ms]
May 11 03:21:27.916: INFO: Created: latency-svc-72s68
May 11 03:21:27.955: INFO: Got endpoints: latency-svc-vxb2s [637.464241ms]
May 11 03:21:27.964: INFO: Created: latency-svc-xvxtm
May 11 03:21:28.004: INFO: Got endpoints: latency-svc-5rzgp [682.257324ms]
May 11 03:21:28.010: INFO: Created: latency-svc-ktdhf
May 11 03:21:28.055: INFO: Got endpoints: latency-svc-dpwvv [726.950137ms]
May 11 03:21:28.061: INFO: Created: latency-svc-9k9b8
May 11 03:21:28.105: INFO: Got endpoints: latency-svc-crcqr [749.007952ms]
May 11 03:21:28.111: INFO: Created: latency-svc-82k44
May 11 03:21:28.156: INFO: Got endpoints: latency-svc-bdb5s [749.3574ms]
May 11 03:21:28.162: INFO: Created: latency-svc-qf4g9
May 11 03:21:28.206: INFO: Got endpoints: latency-svc-lf6nw [749.459151ms]
May 11 03:21:28.212: INFO: Created: latency-svc-kndpf
May 11 03:21:28.254: INFO: Got endpoints: latency-svc-lxnlg [747.766974ms]
May 11 03:21:28.260: INFO: Created: latency-svc-cf8jq
May 11 03:21:28.305: INFO: Got endpoints: latency-svc-qpzx9 [750.028817ms]
May 11 03:21:28.310: INFO: Created: latency-svc-m9q96
May 11 03:21:28.355: INFO: Got endpoints: latency-svc-v9bd2 [749.914292ms]
May 11 03:21:28.363: INFO: Created: latency-svc-rvf9s
May 11 03:21:28.405: INFO: Got endpoints: latency-svc-pmlqj [750.392129ms]
May 11 03:21:28.413: INFO: Created: latency-svc-zsvc4
May 11 03:21:28.457: INFO: Got endpoints: latency-svc-wwm2m [750.109567ms]
May 11 03:21:28.463: INFO: Created: latency-svc-m72fv
May 11 03:21:28.504: INFO: Got endpoints: latency-svc-pxvhb [749.011917ms]
May 11 03:21:28.510: INFO: Created: latency-svc-zfx6d
May 11 03:21:28.555: INFO: Got endpoints: latency-svc-djn8j [749.655052ms]
May 11 03:21:28.562: INFO: Created: latency-svc-lnc8l
May 11 03:21:28.604: INFO: Got endpoints: latency-svc-mrxc8 [749.176691ms]
May 11 03:21:28.611: INFO: Created: latency-svc-6n4vd
May 11 03:21:28.655: INFO: Got endpoints: latency-svc-72s68 [748.988492ms]
May 11 03:21:28.660: INFO: Created: latency-svc-smwhz
May 11 03:21:28.705: INFO: Got endpoints: latency-svc-xvxtm [750.377405ms]
May 11 03:21:28.711: INFO: Created: latency-svc-l9zvd
May 11 03:21:28.756: INFO: Got endpoints: latency-svc-ktdhf [751.386598ms]
May 11 03:21:28.761: INFO: Created: latency-svc-28l4m
May 11 03:21:28.804: INFO: Got endpoints: latency-svc-9k9b8 [749.726231ms]
May 11 03:21:28.809: INFO: Created: latency-svc-g98f7
May 11 03:21:28.855: INFO: Got endpoints: latency-svc-82k44 [750.079869ms]
May 11 03:21:28.862: INFO: Created: latency-svc-mccjw
May 11 03:21:28.905: INFO: Got endpoints: latency-svc-qf4g9 [749.573162ms]
May 11 03:21:28.911: INFO: Created: latency-svc-5l8sc
May 11 03:21:28.955: INFO: Got endpoints: latency-svc-kndpf [749.190438ms]
May 11 03:21:28.961: INFO: Created: latency-svc-dgq5v
May 11 03:21:29.004: INFO: Got endpoints: latency-svc-cf8jq [750.001733ms]
May 11 03:21:29.013: INFO: Created: latency-svc-tv554
May 11 03:21:29.055: INFO: Got endpoints: latency-svc-m9q96 [749.818617ms]
May 11 03:21:29.062: INFO: Created: latency-svc-8knw9
May 11 03:21:29.105: INFO: Got endpoints: latency-svc-rvf9s [749.989201ms]
May 11 03:21:29.112: INFO: Created: latency-svc-jrg5v
May 11 03:21:29.154: INFO: Got endpoints: latency-svc-zsvc4 [749.115934ms]
May 11 03:21:29.160: INFO: Created: latency-svc-zkhbj
May 11 03:21:29.204: INFO: Got endpoints: latency-svc-m72fv [746.948386ms]
May 11 03:21:29.210: INFO: Created: latency-svc-5pv5m
May 11 03:21:29.256: INFO: Got endpoints: latency-svc-zfx6d [751.273337ms]
May 11 03:21:29.261: INFO: Created: latency-svc-9dtx4
May 11 03:21:29.305: INFO: Got endpoints: latency-svc-lnc8l [749.988018ms]
May 11 03:21:29.311: INFO: Created: latency-svc-sbhq5
May 11 03:21:29.355: INFO: Got endpoints: latency-svc-6n4vd [750.420986ms]
May 11 03:21:29.361: INFO: Created: latency-svc-bj5n7
May 11 03:21:29.404: INFO: Got endpoints: latency-svc-smwhz [749.406171ms]
May 11 03:21:29.410: INFO: Created: latency-svc-bgtnp
May 11 03:21:29.455: INFO: Got endpoints: latency-svc-l9zvd [749.544389ms]
May 11 03:21:29.461: INFO: Created: latency-svc-82t26
May 11 03:21:29.505: INFO: Got endpoints: latency-svc-28l4m [748.638991ms]
May 11 03:21:29.510: INFO: Created: latency-svc-r7j68
May 11 03:21:29.555: INFO: Got endpoints: latency-svc-g98f7 [750.681003ms]
May 11 03:21:29.560: INFO: Created: latency-svc-j2l2q
May 11 03:21:29.607: INFO: Got endpoints: latency-svc-mccjw [751.654124ms]
May 11 03:21:29.614: INFO: Created: latency-svc-8hb7t
May 11 03:21:29.655: INFO: Got endpoints: latency-svc-5l8sc [749.365922ms]
May 11 03:21:29.660: INFO: Created: latency-svc-rbmld
May 11 03:21:29.705: INFO: Got endpoints: latency-svc-dgq5v [750.370367ms]
May 11 03:21:29.712: INFO: Created: latency-svc-rfgts
May 11 03:21:29.756: INFO: Got endpoints: latency-svc-tv554 [751.760692ms]
May 11 03:21:29.762: INFO: Created: latency-svc-fx2df
May 11 03:21:29.805: INFO: Got endpoints: latency-svc-8knw9 [750.164055ms]
May 11 03:21:29.811: INFO: Created: latency-svc-fdp9t
May 11 03:21:29.858: INFO: Got endpoints: latency-svc-jrg5v [753.114802ms]
May 11 03:21:29.866: INFO: Created: latency-svc-rt8wd
May 11 03:21:29.906: INFO: Got endpoints: latency-svc-zkhbj [751.120352ms]
May 11 03:21:29.911: INFO: Created: latency-svc-lhxqm
May 11 03:21:29.956: INFO: Got endpoints: latency-svc-5pv5m [751.537737ms]
May 11 03:21:29.961: INFO: Created: latency-svc-htr9x
May 11 03:21:30.005: INFO: Got endpoints: latency-svc-9dtx4 [748.907551ms]
May 11 03:21:30.011: INFO: Created: latency-svc-79j56
May 11 03:21:30.055: INFO: Got endpoints: latency-svc-sbhq5 [750.014953ms]
May 11 03:21:30.060: INFO: Created: latency-svc-g5k9f
May 11 03:21:30.105: INFO: Got endpoints: latency-svc-bj5n7 [749.857501ms]
May 11 03:21:30.111: INFO: Created: latency-svc-gnpgb
May 11 03:21:30.156: INFO: Got endpoints: latency-svc-bgtnp [751.084623ms]
May 11 03:21:30.161: INFO: Created: latency-svc-lpksc
May 11 03:21:30.206: INFO: Got endpoints: latency-svc-82t26 [750.810043ms]
May 11 03:21:30.211: INFO: Created: latency-svc-tsh9z
May 11 03:21:30.257: INFO: Got endpoints: latency-svc-r7j68 [752.229395ms]
May 11 03:21:30.263: INFO: Created: latency-svc-mpsr4
May 11 03:21:30.305: INFO: Got endpoints: latency-svc-j2l2q [749.50582ms]
May 11 03:21:30.309: INFO: Created: latency-svc-7dm8n
May 11 03:21:30.355: INFO: Got endpoints: latency-svc-8hb7t [748.094644ms]
May 11 03:21:30.362: INFO: Created: latency-svc-jkzw2
May 11 03:21:30.405: INFO: Got endpoints: latency-svc-rbmld [750.093262ms]
May 11 03:21:30.411: INFO: Created: latency-svc-mmfjw
May 11 03:21:30.456: INFO: Got endpoints: latency-svc-rfgts [750.511741ms]
May 11 03:21:30.461: INFO: Created: latency-svc-zpq5s
May 11 03:21:30.505: INFO: Got endpoints: latency-svc-fx2df [748.704133ms]
May 11 03:21:30.511: INFO: Created: latency-svc-7486g
May 11 03:21:30.558: INFO: Got endpoints: latency-svc-fdp9t [752.515397ms]
May 11 03:21:30.564: INFO: Created: latency-svc-knjjp
May 11 03:21:30.604: INFO: Got endpoints: latency-svc-rt8wd [746.218079ms]
May 11 03:21:30.610: INFO: Created: latency-svc-l9c9f
May 11 03:21:30.656: INFO: Got endpoints: latency-svc-lhxqm [750.471981ms]
May 11 03:21:30.662: INFO: Created: latency-svc-88ldj
May 11 03:21:30.705: INFO: Got endpoints: latency-svc-htr9x [749.059508ms]
May 11 03:21:30.710: INFO: Created: latency-svc-s4wv7
May 11 03:21:30.755: INFO: Got endpoints: latency-svc-79j56 [750.703433ms]
May 11 03:21:30.761: INFO: Created: latency-svc-4prcq
May 11 03:21:30.804: INFO: Got endpoints: latency-svc-g5k9f [749.665415ms]
May 11 03:21:30.810: INFO: Created: latency-svc-kztsn
May 11 03:21:30.854: INFO: Got endpoints: latency-svc-gnpgb [749.152037ms]
May 11 03:21:30.860: INFO: Created: latency-svc-s8t7r
May 11 03:21:30.905: INFO: Got endpoints: latency-svc-lpksc [749.025844ms]
May 11 03:21:30.911: INFO: Created: latency-svc-dlj4j
May 11 03:21:30.955: INFO: Got endpoints: latency-svc-tsh9z [749.007971ms]
May 11 03:21:30.962: INFO: Created: latency-svc-8q45g
May 11 03:21:31.005: INFO: Got endpoints: latency-svc-mpsr4 [747.746791ms]
May 11 03:21:31.010: INFO: Created: latency-svc-t4tmz
May 11 03:21:31.055: INFO: Got endpoints: latency-svc-7dm8n [750.096196ms]
May 11 03:21:31.061: INFO: Created: latency-svc-8smll
May 11 03:21:31.105: INFO: Got endpoints: latency-svc-jkzw2 [749.99118ms]
May 11 03:21:31.111: INFO: Created: latency-svc-gxf64
May 11 03:21:31.156: INFO: Got endpoints: latency-svc-mmfjw [750.924065ms]
May 11 03:21:31.162: INFO: Created: latency-svc-djs2q
May 11 03:21:31.207: INFO: Got endpoints: latency-svc-zpq5s [751.077487ms]
May 11 03:21:31.214: INFO: Created: latency-svc-lr42q
May 11 03:21:31.255: INFO: Got endpoints: latency-svc-7486g [749.733838ms]
May 11 03:21:31.262: INFO: Created: latency-svc-49k4w
May 11 03:21:31.306: INFO: Got endpoints: latency-svc-knjjp [747.929749ms]
May 11 03:21:31.314: INFO: Created: latency-svc-kp7m6
May 11 03:21:31.355: INFO: Got endpoints: latency-svc-l9c9f [750.346303ms]
May 11 03:21:31.362: INFO: Created: latency-svc-blsxz
May 11 03:21:31.404: INFO: Got endpoints: latency-svc-88ldj [748.227936ms]
May 11 03:21:31.410: INFO: Created: latency-svc-xqkhr
May 11 03:21:31.455: INFO: Got endpoints: latency-svc-s4wv7 [750.080554ms]
May 11 03:21:31.460: INFO: Created: latency-svc-7crhq
May 11 03:21:31.505: INFO: Got endpoints: latency-svc-4prcq [749.612392ms]
May 11 03:21:31.511: INFO: Created: latency-svc-twvmr
May 11 03:21:31.555: INFO: Got endpoints: latency-svc-kztsn [750.23587ms]
May 11 03:21:31.561: INFO: Created: latency-svc-kwvvx
May 11 03:21:31.604: INFO: Got endpoints: latency-svc-s8t7r [750.119059ms]
May 11 03:21:31.610: INFO: Created: latency-svc-52h97
May 11 03:21:31.659: INFO: Got endpoints: latency-svc-dlj4j [754.051665ms]
May 11 03:21:31.667: INFO: Created: latency-svc-hjv25
May 11 03:21:31.706: INFO: Got endpoints: latency-svc-8q45g [750.855366ms]
May 11 03:21:31.712: INFO: Created: latency-svc-qddsn
May 11 03:21:31.755: INFO: Got endpoints: latency-svc-t4tmz [750.188942ms]
May 11 03:21:31.761: INFO: Created: latency-svc-jwh7c
May 11 03:21:31.805: INFO: Got endpoints: latency-svc-8smll [750.169442ms]
May 11 03:21:31.811: INFO: Created: latency-svc-w4fdt
May 11 03:21:31.855: INFO: Got endpoints: latency-svc-gxf64 [750.376247ms]
May 11 03:21:31.864: INFO: Created: latency-svc-kr5vw
May 11 03:21:31.904: INFO: Got endpoints: latency-svc-djs2q [748.592004ms]
May 11 03:21:31.911: INFO: Created: latency-svc-zgl7g
May 11 03:21:31.955: INFO: Got endpoints: latency-svc-lr42q [748.099093ms]
May 11 03:21:31.964: INFO: Created: latency-svc-mb8wg
May 11 03:21:32.005: INFO: Got endpoints: latency-svc-49k4w [750.330079ms]
May 11 03:21:32.012: INFO: Created: latency-svc-7hwxh
May 11 03:21:32.055: INFO: Got endpoints: latency-svc-kp7m6 [748.90604ms]
May 11 03:21:32.063: INFO: Created: latency-svc-9892h
May 11 03:21:32.106: INFO: Got endpoints: latency-svc-blsxz [751.389973ms]
May 11 03:21:32.116: INFO: Created: latency-svc-5df4s
May 11 03:21:32.155: INFO: Got endpoints: latency-svc-xqkhr [750.640117ms]
May 11 03:21:32.161: INFO: Created: latency-svc-nm86h
May 11 03:21:32.205: INFO: Got endpoints: latency-svc-7crhq [750.397939ms]
May 11 03:21:32.213: INFO: Created: latency-svc-k5s9n
May 11 03:21:32.255: INFO: Got endpoints: latency-svc-twvmr [749.997613ms]
May 11 03:21:32.262: INFO: Created: latency-svc-jpn48
May 11 03:21:32.305: INFO: Got endpoints: latency-svc-kwvvx [750.636299ms]
May 11 03:21:32.310: INFO: Created: latency-svc-2bxlh
May 11 03:21:32.354: INFO: Got endpoints: latency-svc-52h97 [750.155241ms]
May 11 03:21:32.361: INFO: Created: latency-svc-7vfbk
May 11 03:21:32.404: INFO: Got endpoints: latency-svc-hjv25 [745.642022ms]
May 11 03:21:32.409: INFO: Created: latency-svc-rltd8
May 11 03:21:32.455: INFO: Got endpoints: latency-svc-qddsn [749.103352ms]
May 11 03:21:32.461: INFO: Created: latency-svc-95cjp
May 11 03:21:32.504: INFO: Got endpoints: latency-svc-jwh7c [749.493314ms]
May 11 03:21:32.511: INFO: Created: latency-svc-nmhm6
May 11 03:21:32.556: INFO: Got endpoints: latency-svc-w4fdt [750.54892ms]
May 11 03:21:32.562: INFO: Created: latency-svc-klw84
May 11 03:21:32.605: INFO: Got endpoints: latency-svc-kr5vw [749.896143ms]
May 11 03:21:32.612: INFO: Created: latency-svc-8w9hb
May 11 03:21:32.655: INFO: Got endpoints: latency-svc-zgl7g [750.780552ms]
May 11 03:21:32.662: INFO: Created: latency-svc-gfss5
May 11 03:21:32.706: INFO: Got endpoints: latency-svc-mb8wg [750.802038ms]
May 11 03:21:32.713: INFO: Created: latency-svc-t2vbk
May 11 03:21:32.755: INFO: Got endpoints: latency-svc-7hwxh [749.988995ms]
May 11 03:21:32.761: INFO: Created: latency-svc-f2jtn
May 11 03:21:32.805: INFO: Got endpoints: latency-svc-9892h [750.27929ms]
May 11 03:21:32.812: INFO: Created: latency-svc-tbdmc
May 11 03:21:32.855: INFO: Got endpoints: latency-svc-5df4s [748.783883ms]
May 11 03:21:32.863: INFO: Created: latency-svc-h57pd
May 11 03:21:32.905: INFO: Got endpoints: latency-svc-nm86h [749.902518ms]
May 11 03:21:32.916: INFO: Created: latency-svc-692r8
May 11 03:21:32.955: INFO: Got endpoints: latency-svc-k5s9n [750.005654ms]
May 11 03:21:32.962: INFO: Created: latency-svc-hxgcj
May 11 03:21:33.007: INFO: Got endpoints: latency-svc-jpn48 [751.715245ms]
May 11 03:21:33.012: INFO: Created: latency-svc-q6bbf
May 11 03:21:33.054: INFO: Got endpoints: latency-svc-2bxlh [748.794996ms]
May 11 03:21:33.060: INFO: Created: latency-svc-bpj7n
May 11 03:21:33.112: INFO: Got endpoints: latency-svc-7vfbk [757.731123ms]
May 11 03:21:33.118: INFO: Created: latency-svc-7f5c2
May 11 03:21:33.155: INFO: Got endpoints: latency-svc-rltd8 [750.517487ms]
May 11 03:21:33.162: INFO: Created: latency-svc-gjw6r
May 11 03:21:33.205: INFO: Got endpoints: latency-svc-95cjp [750.412198ms]
May 11 03:21:33.217: INFO: Created: latency-svc-xj65g
May 11 03:21:33.255: INFO: Got endpoints: latency-svc-nmhm6 [750.452508ms]
May 11 03:21:33.261: INFO: Created: latency-svc-cc9fw
May 11 03:21:33.305: INFO: Got endpoints: latency-svc-klw84 [749.555913ms]
May 11 03:21:33.319: INFO: Created: latency-svc-4lhkf
May 11 03:21:33.356: INFO: Got endpoints: latency-svc-8w9hb [750.588279ms]
May 11 03:21:33.361: INFO: Created: latency-svc-9gmmq
May 11 03:21:33.405: INFO: Got endpoints: latency-svc-gfss5 [749.80574ms]
May 11 03:21:33.413: INFO: Created: latency-svc-82bhp
May 11 03:21:33.456: INFO: Got endpoints: latency-svc-t2vbk [749.715348ms]
May 11 03:21:33.461: INFO: Created: latency-svc-9qwsd
May 11 03:21:33.506: INFO: Got endpoints: latency-svc-f2jtn [750.589424ms]
May 11 03:21:33.513: INFO: Created: latency-svc-rmc8p
May 11 03:21:33.554: INFO: Got endpoints: latency-svc-tbdmc [749.263889ms]
May 11 03:21:33.560: INFO: Created: latency-svc-vgr2l
May 11 03:21:33.607: INFO: Got endpoints: latency-svc-h57pd [751.472066ms]
May 11 03:21:33.617: INFO: Created: latency-svc-kj7np
May 11 03:21:33.655: INFO: Got endpoints: latency-svc-692r8 [749.780348ms]
May 11 03:21:33.661: INFO: Created: latency-svc-vpwv6
May 11 03:21:33.705: INFO: Got endpoints: latency-svc-hxgcj [749.963938ms]
May 11 03:21:33.716: INFO: Created: latency-svc-mb54m
May 11 03:21:33.755: INFO: Got endpoints: latency-svc-q6bbf [748.595798ms]
May 11 03:21:33.762: INFO: Created: latency-svc-jg2ld
May 11 03:21:33.805: INFO: Got endpoints: latency-svc-bpj7n [750.790423ms]
May 11 03:21:33.811: INFO: Created: latency-svc-bxb7n
May 11 03:21:33.855: INFO: Got endpoints: latency-svc-7f5c2 [742.391283ms]
May 11 03:21:33.860: INFO: Created: latency-svc-8sdm8
May 11 03:21:33.906: INFO: Got endpoints: latency-svc-gjw6r [750.628859ms]
May 11 03:21:33.914: INFO: Created: latency-svc-wddfx
May 11 03:21:33.955: INFO: Got endpoints: latency-svc-xj65g [749.59868ms]
May 11 03:21:33.961: INFO: Created: latency-svc-qmfqt
May 11 03:21:34.005: INFO: Got endpoints: latency-svc-cc9fw [750.466377ms]
May 11 03:21:34.012: INFO: Created: latency-svc-4jmf2
May 11 03:21:34.055: INFO: Got endpoints: latency-svc-4lhkf [749.892194ms]
May 11 03:21:34.060: INFO: Created: latency-svc-mbdrt
May 11 03:21:34.105: INFO: Got endpoints: latency-svc-9gmmq [748.717885ms]
May 11 03:21:34.113: INFO: Created: latency-svc-6rjf7
May 11 03:21:34.155: INFO: Got endpoints: latency-svc-82bhp [749.950757ms]
May 11 03:21:34.161: INFO: Created: latency-svc-zrqf2
May 11 03:21:34.206: INFO: Got endpoints: latency-svc-9qwsd [750.408581ms]
May 11 03:21:34.211: INFO: Created: latency-svc-r42vx
May 11 03:21:34.255: INFO: Got endpoints: latency-svc-rmc8p [748.81651ms]
May 11 03:21:34.259: INFO: Created: latency-svc-48jbr
May 11 03:21:34.306: INFO: Got endpoints: latency-svc-vgr2l [751.736119ms]
May 11 03:21:34.314: INFO: Created: latency-svc-8fr9g
May 11 03:21:34.355: INFO: Got endpoints: latency-svc-kj7np [748.203032ms]
May 11 03:21:34.360: INFO: Created: latency-svc-6vd9f
May 11 03:21:34.406: INFO: Got endpoints: latency-svc-vpwv6 [750.852357ms]
May 11 03:21:34.413: INFO: Created: latency-svc-jl5zf
May 11 03:21:34.456: INFO: Got endpoints: latency-svc-mb54m [750.450586ms]
May 11 03:21:34.462: INFO: Created: latency-svc-sksbx
May 11 03:21:34.505: INFO: Got endpoints: latency-svc-jg2ld [749.497097ms]
May 11 03:21:34.511: INFO: Created: latency-svc-7wtp8
May 11 03:21:34.555: INFO: Got endpoints: latency-svc-bxb7n [749.479946ms]
May 11 03:21:34.561: INFO: Created: latency-svc-lbhgv
May 11 03:21:34.606: INFO: Got endpoints: latency-svc-8sdm8 [751.384733ms]
May 11 03:21:34.611: INFO: Created: latency-svc-lv5st
May 11 03:21:34.656: INFO: Got endpoints: latency-svc-wddfx [749.864671ms]
May 11 03:21:34.661: INFO: Created: latency-svc-qbf6v
May 11 03:21:34.707: INFO: Got endpoints: latency-svc-qmfqt [751.527299ms]
May 11 03:21:34.712: INFO: Created: latency-svc-qcz2r
May 11 03:21:34.755: INFO: Got endpoints: latency-svc-4jmf2 [749.517521ms]
May 11 03:21:34.763: INFO: Created: latency-svc-bc4bv
May 11 03:21:34.805: INFO: Got endpoints: latency-svc-mbdrt [749.55107ms]
May 11 03:21:34.812: INFO: Created: latency-svc-w9j9h
May 11 03:21:34.855: INFO: Got endpoints: latency-svc-6rjf7 [749.90679ms]
May 11 03:21:34.862: INFO: Created: latency-svc-jz8mm
May 11 03:21:34.905: INFO: Got endpoints: latency-svc-zrqf2 [749.819076ms]
May 11 03:21:34.911: INFO: Created: latency-svc-klgr6
May 11 03:21:34.957: INFO: Got endpoints: latency-svc-r42vx [750.670583ms]
May 11 03:21:34.963: INFO: Created: latency-svc-gbmtk
May 11 03:21:35.007: INFO: Got endpoints: latency-svc-48jbr [752.671984ms]
May 11 03:21:35.058: INFO: Got endpoints: latency-svc-8fr9g [751.59438ms]
May 11 03:21:35.106: INFO: Got endpoints: latency-svc-6vd9f [751.380995ms]
May 11 03:21:35.156: INFO: Got endpoints: latency-svc-jl5zf [750.375522ms]
May 11 03:21:35.206: INFO: Got endpoints: latency-svc-sksbx [750.269635ms]
May 11 03:21:35.262: INFO: Got endpoints: latency-svc-7wtp8 [757.169984ms]
May 11 03:21:35.306: INFO: Got endpoints: latency-svc-lbhgv [751.078777ms]
May 11 03:21:35.356: INFO: Got endpoints: latency-svc-lv5st [749.473872ms]
May 11 03:21:35.405: INFO: Got endpoints: latency-svc-qbf6v [749.567503ms]
May 11 03:21:35.456: INFO: Got endpoints: latency-svc-qcz2r [749.5814ms]
May 11 03:21:35.505: INFO: Got endpoints: latency-svc-bc4bv [749.918303ms]
May 11 03:21:35.555: INFO: Got endpoints: latency-svc-w9j9h [750.433654ms]
May 11 03:21:35.605: INFO: Got endpoints: latency-svc-jz8mm [749.946146ms]
May 11 03:21:35.655: INFO: Got endpoints: latency-svc-klgr6 [750.215846ms]
May 11 03:21:35.705: INFO: Got endpoints: latency-svc-gbmtk [748.099601ms]
May 11 03:21:35.705: INFO: Latencies: [12.031435ms 12.788968ms 17.330273ms 18.057879ms 25.17537ms 29.269132ms 30.945378ms 37.078323ms 39.244687ms 45.26495ms 45.646765ms 60.423863ms 61.787263ms 66.09009ms 66.7892ms 67.129178ms 71.481281ms 77.290743ms 81.981404ms 82.851919ms 84.299111ms 84.690257ms 85.265283ms 86.435279ms 87.497964ms 87.724627ms 88.566767ms 88.911518ms 89.335874ms 89.614531ms 89.801478ms 92.649813ms 116.868828ms 149.378779ms 193.495603ms 236.866729ms 280.12744ms 326.480841ms 367.100534ms 416.087107ms 458.023916ms 503.702776ms 547.833445ms 595.033217ms 637.464241ms 682.257324ms 726.950137ms 742.391283ms 745.642022ms 746.218079ms 746.948386ms 747.746791ms 747.766974ms 747.929749ms 748.094644ms 748.099093ms 748.099601ms 748.203032ms 748.227936ms 748.592004ms 748.595798ms 748.638991ms 748.704133ms 748.717885ms 748.783883ms 748.794996ms 748.81651ms 748.90604ms 748.907551ms 748.988492ms 749.007952ms 749.007971ms 749.011917ms 749.025844ms 749.059508ms 749.103352ms 749.115934ms 749.152037ms 749.176691ms 749.190438ms 749.263889ms 749.3574ms 749.365922ms 749.406171ms 749.459151ms 749.473872ms 749.479946ms 749.493314ms 749.497097ms 749.50582ms 749.517521ms 749.544389ms 749.55107ms 749.555913ms 749.567503ms 749.573162ms 749.5814ms 749.59868ms 749.612392ms 749.655052ms 749.665415ms 749.715348ms 749.726231ms 749.733838ms 749.780348ms 749.80574ms 749.818617ms 749.819076ms 749.857501ms 749.864671ms 749.892194ms 749.896143ms 749.902518ms 749.90679ms 749.914292ms 749.918303ms 749.946146ms 749.950757ms 749.963938ms 749.988018ms 749.988995ms 749.989201ms 749.99118ms 749.997613ms 750.001733ms 750.005654ms 750.014953ms 750.028817ms 750.079869ms 750.080554ms 750.093262ms 750.096196ms 750.109567ms 750.119059ms 750.155241ms 750.164055ms 750.169442ms 750.188942ms 750.215846ms 750.23587ms 750.269635ms 750.27929ms 750.330079ms 750.346303ms 750.370367ms 750.375522ms 750.376247ms 750.377405ms 750.392129ms 750.397939ms 750.408581ms 750.412198ms 750.420986ms 750.433654ms 750.450586ms 750.452508ms 750.466377ms 750.471981ms 750.511741ms 750.517487ms 750.54892ms 750.588279ms 750.589424ms 750.628859ms 750.636299ms 750.640117ms 750.670583ms 750.681003ms 750.703433ms 750.780552ms 750.790423ms 750.802038ms 750.810043ms 750.852357ms 750.855366ms 750.924065ms 751.077487ms 751.078777ms 751.084623ms 751.120352ms 751.273337ms 751.380995ms 751.384733ms 751.386598ms 751.389973ms 751.472066ms 751.527299ms 751.537737ms 751.59438ms 751.654124ms 751.715245ms 751.736119ms 751.760692ms 752.229395ms 752.515397ms 752.671984ms 753.114802ms 754.051665ms 757.169984ms 757.731123ms]
May 11 03:21:35.705: INFO: 50 %ile: 749.665415ms
May 11 03:21:35.705: INFO: 90 %ile: 751.273337ms
May 11 03:21:35.705: INFO: 99 %ile: 757.169984ms
May 11 03:21:35.705: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:21:35.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5002" for this suite.
May 11 03:21:43.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:21:43.772: INFO: namespace svc-latency-5002 deletion completed in 8.061791799s

• [SLOW TEST:18.816 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:21:43.772: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 11 03:21:47.822: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 03:21:47.824: INFO: Pod pod-with-poststart-http-hook still exists
May 11 03:21:49.824: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 03:21:49.826: INFO: Pod pod-with-poststart-http-hook still exists
May 11 03:21:51.824: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 11 03:21:51.827: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:21:51.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6573" for this suite.
May 11 03:22:13.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:22:13.914: INFO: namespace container-lifecycle-hook-6573 deletion completed in 22.08477357s

• [SLOW TEST:30.142 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:22:13.915: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 11 03:22:13.944: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:22:22.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7215" for this suite.
May 11 03:22:28.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:22:28.639: INFO: namespace pods-7215 deletion completed in 6.075893191s

• [SLOW TEST:14.724 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:22:28.639: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:22:48.675: INFO: Container started at 2019-05-11 03:22:29 +0000 UTC, pod became ready at 2019-05-11 03:22:48 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:22:48.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7178" for this suite.
May 11 03:23:10.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:23:10.748: INFO: namespace container-probe-7178 deletion completed in 22.069977144s

• [SLOW TEST:42.109 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:23:10.749: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 11 03:23:10.776: INFO: Waiting up to 5m0s for pod "var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9" in namespace "var-expansion-9340" to be "success or failure"
May 11 03:23:10.780: INFO: Pod "var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131651ms
May 11 03:23:12.784: INFO: Pod "var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00822559s
STEP: Saw pod success
May 11 03:23:12.784: INFO: Pod "var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:23:12.787: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 03:23:12.808: INFO: Waiting for pod var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:23:12.809: INFO: Pod var-expansion-1a78b16f-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:23:12.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9340" for this suite.
May 11 03:23:18.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:23:18.893: INFO: namespace var-expansion-9340 deletion completed in 6.081503063s

• [SLOW TEST:8.144 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:23:18.894: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:23:18.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9" in namespace "downward-api-575" to be "success or failure"
May 11 03:23:18.924: INFO: Pod "downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.586046ms
May 11 03:23:20.926: INFO: Pod "downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007843524s
STEP: Saw pod success
May 11 03:23:20.926: INFO: Pod "downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:23:20.928: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:23:20.940: INFO: Waiting for pod downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:23:20.941: INFO: Pod downwardapi-volume-1f5336b5-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:23:20.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-575" for this suite.
May 11 03:23:26.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:23:27.071: INFO: namespace downward-api-575 deletion completed in 6.126707443s

• [SLOW TEST:8.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:23:27.071: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-24347000-739c-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:23:27.108: INFO: Waiting up to 5m0s for pod "pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9" in namespace "configmap-60" to be "success or failure"
May 11 03:23:27.111: INFO: Pod "pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726726ms
May 11 03:23:29.116: INFO: Pod "pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007482985s
STEP: Saw pod success
May 11 03:23:29.116: INFO: Pod "pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:23:29.118: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:23:29.132: INFO: Waiting for pod pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:23:29.133: INFO: Pod pod-configmaps-2434c7b3-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:23:29.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-60" for this suite.
May 11 03:23:35.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:23:35.208: INFO: namespace configmap-60 deletion completed in 6.072521068s

• [SLOW TEST:8.138 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:23:35.209: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:23:35.240: INFO: (0) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.655296ms)
May 11 03:23:35.243: INFO: (1) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.755233ms)
May 11 03:23:35.246: INFO: (2) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.216718ms)
May 11 03:23:35.249: INFO: (3) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.643096ms)
May 11 03:23:35.252: INFO: (4) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.648014ms)
May 11 03:23:35.255: INFO: (5) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.808723ms)
May 11 03:23:35.258: INFO: (6) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.638097ms)
May 11 03:23:35.261: INFO: (7) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.789507ms)
May 11 03:23:35.263: INFO: (8) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.748562ms)
May 11 03:23:35.265: INFO: (9) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.020649ms)
May 11 03:23:35.268: INFO: (10) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.808166ms)
May 11 03:23:35.270: INFO: (11) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.099715ms)
May 11 03:23:35.273: INFO: (12) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.187617ms)
May 11 03:23:35.275: INFO: (13) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.915361ms)
May 11 03:23:35.277: INFO: (14) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.113669ms)
May 11 03:23:35.279: INFO: (15) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.000778ms)
May 11 03:23:35.281: INFO: (16) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.293289ms)
May 11 03:23:35.286: INFO: (17) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.831459ms)
May 11 03:23:35.288: INFO: (18) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.042843ms)
May 11 03:23:35.292: INFO: (19) /api/v1/nodes/craig-k8s-certification-2-sprout-hero-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.143798ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:23:35.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5105" for this suite.
May 11 03:23:41.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:23:41.368: INFO: namespace proxy-5105 deletion completed in 6.073017064s

• [SLOW TEST:6.159 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:23:41.368: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 11 03:23:43.916: INFO: Successfully updated pod "labelsupdate2cb88fd1-739c-11e9-80a1-becd2a02efc9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:23:45.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3740" for this suite.
May 11 03:24:07.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:24:08.013: INFO: namespace projected-3740 deletion completed in 22.073731928s

• [SLOW TEST:26.645 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:24:08.013: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8677.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8677.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8677.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8677.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.62_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8677.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8677.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8677.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8677.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8677.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8677.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.29.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.29.62_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 11 03:24:10.068: INFO: Unable to read wheezy_udp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.071: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.073: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.076: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.090: INFO: Unable to read jessie_udp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.092: INFO: Unable to read jessie_tcp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.094: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.097: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:10.110: INFO: Lookups using dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9 failed for: [wheezy_udp@dns-test-service.dns-8677.svc.cluster.local wheezy_tcp@dns-test-service.dns-8677.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local jessie_udp@dns-test-service.dns-8677.svc.cluster.local jessie_tcp@dns-test-service.dns-8677.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local]

May 11 03:24:15.145: INFO: Unable to read jessie_udp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:15.148: INFO: Unable to read jessie_tcp@dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:15.150: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:15.152: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local from pod dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9: the server could not find the requested resource (get pods dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9)
May 11 03:24:15.166: INFO: Lookups using dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9 failed for: [jessie_udp@dns-test-service.dns-8677.svc.cluster.local jessie_tcp@dns-test-service.dns-8677.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8677.svc.cluster.local]

May 11 03:24:20.166: INFO: DNS probes using dns-8677/dns-test-3c9c4fc1-739c-11e9-80a1-becd2a02efc9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:24:20.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8677" for this suite.
May 11 03:24:26.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:24:26.291: INFO: namespace dns-8677 deletion completed in 6.078212523s

• [SLOW TEST:18.278 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:24:26.291: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 11 03:24:26.320: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4435,SelfLink:/api/v1/namespaces/watch-4435/configmaps/e2e-watch-test-watch-closed,UID:47c5f3e3-739c-11e9-b14d-000c2942e394,ResourceVersion:34233,Generation:0,CreationTimestamp:2019-05-11 03:24:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 11 03:24:26.320: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4435,SelfLink:/api/v1/namespaces/watch-4435/configmaps/e2e-watch-test-watch-closed,UID:47c5f3e3-739c-11e9-b14d-000c2942e394,ResourceVersion:34235,Generation:0,CreationTimestamp:2019-05-11 03:24:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 11 03:24:26.328: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4435,SelfLink:/api/v1/namespaces/watch-4435/configmaps/e2e-watch-test-watch-closed,UID:47c5f3e3-739c-11e9-b14d-000c2942e394,ResourceVersion:34237,Generation:0,CreationTimestamp:2019-05-11 03:24:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 11 03:24:26.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4435,SelfLink:/api/v1/namespaces/watch-4435/configmaps/e2e-watch-test-watch-closed,UID:47c5f3e3-739c-11e9-b14d-000c2942e394,ResourceVersion:34238,Generation:0,CreationTimestamp:2019-05-11 03:24:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:24:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4435" for this suite.
May 11 03:24:32.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:24:32.402: INFO: namespace watch-4435 deletion completed in 6.071198084s

• [SLOW TEST:6.111 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:24:32.403: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 11 03:24:32.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-6710'
May 11 03:24:32.638: INFO: stderr: ""
May 11 03:24:32.638: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 11 03:24:33.641: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:24:33.641: INFO: Found 0 / 1
May 11 03:24:34.642: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:24:34.642: INFO: Found 1 / 1
May 11 03:24:34.642: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 11 03:24:34.644: INFO: Selector matched 1 pods for map[app:redis]
May 11 03:24:34.644: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 11 03:24:34.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 logs redis-master-z7k7k redis-master --namespace=kubectl-6710'
May 11 03:24:34.756: INFO: stderr: ""
May 11 03:24:34.756: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 May 03:24:33.448 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 May 03:24:33.448 # Server started, Redis version 3.2.12\n1:M 11 May 03:24:33.448 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 May 03:24:33.448 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 11 03:24:34.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 log redis-master-z7k7k redis-master --namespace=kubectl-6710 --tail=1'
May 11 03:24:34.867: INFO: stderr: ""
May 11 03:24:34.867: INFO: stdout: "1:M 11 May 03:24:33.448 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 11 03:24:34.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 log redis-master-z7k7k redis-master --namespace=kubectl-6710 --limit-bytes=1'
May 11 03:24:34.976: INFO: stderr: ""
May 11 03:24:34.976: INFO: stdout: " "
STEP: exposing timestamps
May 11 03:24:34.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 log redis-master-z7k7k redis-master --namespace=kubectl-6710 --tail=1 --timestamps'
May 11 03:24:35.089: INFO: stderr: ""
May 11 03:24:35.089: INFO: stdout: "2019-05-11T03:24:33.448553792Z 1:M 11 May 03:24:33.448 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 11 03:24:37.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 log redis-master-z7k7k redis-master --namespace=kubectl-6710 --since=1s'
May 11 03:24:37.703: INFO: stderr: ""
May 11 03:24:37.703: INFO: stdout: ""
May 11 03:24:37.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 log redis-master-z7k7k redis-master --namespace=kubectl-6710 --since=24h'
May 11 03:24:37.819: INFO: stderr: ""
May 11 03:24:37.819: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 May 03:24:33.448 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 May 03:24:33.448 # Server started, Redis version 3.2.12\n1:M 11 May 03:24:33.448 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 May 03:24:33.448 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 11 03:24:37.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-6710'
May 11 03:24:37.931: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:24:37.931: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 11 03:24:37.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6710'
May 11 03:24:38.041: INFO: stderr: "No resources found.\n"
May 11 03:24:38.041: INFO: stdout: ""
May 11 03:24:38.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -l name=nginx --namespace=kubectl-6710 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 11 03:24:38.162: INFO: stderr: ""
May 11 03:24:38.162: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:24:38.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6710" for this suite.
May 11 03:24:44.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:24:44.240: INFO: namespace kubectl-6710 deletion completed in 6.075236801s

• [SLOW TEST:11.838 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:24:44.240: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 11 03:24:44.264: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 11 03:24:44.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:44.487: INFO: stderr: ""
May 11 03:24:44.487: INFO: stdout: "service/redis-slave created\n"
May 11 03:24:44.488: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 11 03:24:44.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:44.707: INFO: stderr: ""
May 11 03:24:44.707: INFO: stdout: "service/redis-master created\n"
May 11 03:24:44.707: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 11 03:24:44.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:44.867: INFO: stderr: ""
May 11 03:24:44.867: INFO: stdout: "service/frontend created\n"
May 11 03:24:44.867: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 11 03:24:44.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:45.016: INFO: stderr: ""
May 11 03:24:45.016: INFO: stdout: "deployment.apps/frontend created\n"
May 11 03:24:45.016: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 11 03:24:45.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:45.221: INFO: stderr: ""
May 11 03:24:45.221: INFO: stdout: "deployment.apps/redis-master created\n"
May 11 03:24:45.221: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 11 03:24:45.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-3806'
May 11 03:24:45.440: INFO: stderr: ""
May 11 03:24:45.440: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 11 03:24:45.440: INFO: Waiting for all frontend pods to be Running.
May 11 03:25:20.495: INFO: Waiting for frontend to serve content.
May 11 03:25:20.558: INFO: Trying to add a new entry to the guestbook.
May 11 03:25:20.571: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 11 03:25:20.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:20.725: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:20.725: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 11 03:25:20.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:20.850: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:20.850: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 11 03:25:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:20.963: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:20.963: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 11 03:25:20.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:21.073: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:21.073: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 11 03:25:21.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:21.162: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:21.162: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 11 03:25:21.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 delete --grace-period=0 --force -f - --namespace=kubectl-3806'
May 11 03:25:21.240: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 11 03:25:21.240: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:25:21.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3806" for this suite.
May 11 03:26:05.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:26:05.304: INFO: namespace kubectl-3806 deletion completed in 44.061830915s

• [SLOW TEST:81.064 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:26:05.304: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 11 03:26:05.328: INFO: Waiting up to 5m0s for pod "pod-82837294-739c-11e9-80a1-becd2a02efc9" in namespace "emptydir-6161" to be "success or failure"
May 11 03:26:05.330: INFO: Pod "pod-82837294-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758216ms
May 11 03:26:07.333: INFO: Pod "pod-82837294-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004419302s
STEP: Saw pod success
May 11 03:26:07.333: INFO: Pod "pod-82837294-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:26:07.335: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-82837294-739c-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 03:26:07.356: INFO: Waiting for pod pod-82837294-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:26:07.357: INFO: Pod pod-82837294-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:26:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6161" for this suite.
May 11 03:26:13.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:26:13.439: INFO: namespace emptydir-6161 deletion completed in 6.079215393s

• [SLOW TEST:8.135 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:26:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-875e71fc-739c-11e9-80a1-becd2a02efc9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:26:15.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4718" for this suite.
May 11 03:26:37.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:26:37.586: INFO: namespace configmap-4718 deletion completed in 22.081891773s

• [SLOW TEST:24.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:26:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-95c249a6-739c-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:26:37.620: INFO: Waiting up to 5m0s for pod "pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9" in namespace "configmap-9038" to be "success or failure"
May 11 03:26:37.622: INFO: Pod "pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249186ms
May 11 03:26:39.627: INFO: Pod "pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006819403s
May 11 03:26:41.632: INFO: Pod "pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011727118s
STEP: Saw pod success
May 11 03:26:41.632: INFO: Pod "pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:26:41.635: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9 container configmap-volume-test: <nil>
STEP: delete the pod
May 11 03:26:41.653: INFO: Waiting for pod pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:26:41.655: INFO: Pod pod-configmaps-95c2a7b8-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:26:41.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9038" for this suite.
May 11 03:26:47.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:26:47.730: INFO: namespace configmap-9038 deletion completed in 6.072269924s

• [SLOW TEST:10.143 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:26:47.730: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:26:49.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-857" for this suite.
May 11 03:26:55.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:26:55.874: INFO: namespace emptydir-wrapper-857 deletion completed in 6.080624421s

• [SLOW TEST:8.144 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:26:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:26:55.901: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 11 03:27:00.904: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 11 03:27:00.904: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 11 03:27:02.909: INFO: Creating deployment "test-rollover-deployment"
May 11 03:27:02.916: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 11 03:27:04.921: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 11 03:27:04.927: INFO: Ensure that both replica sets have 1 created replica
May 11 03:27:04.932: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 11 03:27:04.937: INFO: Updating deployment test-rollover-deployment
May 11 03:27:04.937: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 11 03:27:06.944: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 11 03:27:06.949: INFO: Make sure deployment "test-rollover-deployment" is complete
May 11 03:27:06.954: INFO: all replica sets need to contain the pod-template-hash label
May 11 03:27:06.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142026, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 03:27:08.961: INFO: all replica sets need to contain the pod-template-hash label
May 11 03:27:08.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142026, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 03:27:10.962: INFO: all replica sets need to contain the pod-template-hash label
May 11 03:27:10.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142026, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 03:27:12.961: INFO: all replica sets need to contain the pod-template-hash label
May 11 03:27:12.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142026, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 03:27:14.962: INFO: all replica sets need to contain the pod-template-hash label
May 11 03:27:14.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142026, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693142023, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 11 03:27:16.962: INFO: 
May 11 03:27:16.962: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 11 03:27:16.973: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3629,SelfLink:/apis/apps/v1/namespaces/deployment-3629/deployments/test-rollover-deployment,UID:a51cba8c-739c-11e9-b14d-000c2942e394,ResourceVersion:35191,Generation:2,CreationTimestamp:2019-05-11 03:27:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-11 03:27:03 +0000 UTC 2019-05-11 03:27:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-11 03:27:16 +0000 UTC 2019-05-11 03:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 11 03:27:16.976: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-3629,SelfLink:/apis/apps/v1/namespaces/deployment-3629/replicasets/test-rollover-deployment-766b4d6c9d,UID:a6523d9e-739c-11e9-b14d-000c2942e394,ResourceVersion:35180,Generation:2,CreationTimestamp:2019-05-11 03:27:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a51cba8c-739c-11e9-b14d-000c2942e394 0xc0026c1157 0xc0026c1158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 11 03:27:16.976: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 11 03:27:16.976: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3629,SelfLink:/apis/apps/v1/namespaces/deployment-3629/replicasets/test-rollover-controller,UID:a0ee9c8b-739c-11e9-b14d-000c2942e394,ResourceVersion:35189,Generation:2,CreationTimestamp:2019-05-11 03:26:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a51cba8c-739c-11e9-b14d-000c2942e394 0xc0026c0f67 0xc0026c0f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 03:27:16.976: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-3629,SelfLink:/apis/apps/v1/namespaces/deployment-3629/replicasets/test-rollover-deployment-6455657675,UID:a51e6f79-739c-11e9-b14d-000c2942e394,ResourceVersion:35136,Generation:2,CreationTimestamp:2019-05-11 03:27:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a51cba8c-739c-11e9-b14d-000c2942e394 0xc0026c1077 0xc0026c1078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 11 03:27:16.979: INFO: Pod "test-rollover-deployment-766b4d6c9d-lw5mm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-lw5mm,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-3629,SelfLink:/api/v1/namespaces/deployment-3629/pods/test-rollover-deployment-766b4d6c9d-lw5mm,UID:a654f10a-739c-11e9-b14d-000c2942e394,ResourceVersion:35152,Generation:0,CreationTimestamp:2019-05-11 03:27:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d a6523d9e-739c-11e9-b14d-000c2942e394 0xc001bfc467 0xc001bfc468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sljz4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sljz4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sljz4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:craig-k8s-certification-2-sprout-hero-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bfc4d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bfc4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:27:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:27:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:27:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-11 03:27:05 +0000 UTC  }],Message:,Reason:,HostIP:70.0.48.171,PodIP:10.233.90.226,StartTime:2019-05-11 03:27:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-11 03:27:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://97d92554e8751072689795654c836c58d8b3301c3e6be115f6575e801d624c75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:27:16.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3629" for this suite.
May 11 03:27:22.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:27:23.064: INFO: namespace deployment-3629 deletion completed in 6.081829334s

• [SLOW TEST:27.190 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:27:23.064: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 11 03:27:23.093: INFO: Waiting up to 5m0s for pod "pod-b0dd4089-739c-11e9-80a1-becd2a02efc9" in namespace "emptydir-892" to be "success or failure"
May 11 03:27:23.096: INFO: Pod "pod-b0dd4089-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942456ms
May 11 03:27:25.100: INFO: Pod "pod-b0dd4089-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006080888s
STEP: Saw pod success
May 11 03:27:25.100: INFO: Pod "pod-b0dd4089-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:27:25.103: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-b0dd4089-739c-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 03:27:25.116: INFO: Waiting for pod pod-b0dd4089-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:27:25.118: INFO: Pod pod-b0dd4089-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:27:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-892" for this suite.
May 11 03:27:31.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:27:31.212: INFO: namespace emptydir-892 deletion completed in 6.091504073s

• [SLOW TEST:8.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:27:31.213: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 11 03:27:31.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9" in namespace "downward-api-937" to be "success or failure"
May 11 03:27:31.250: INFO: Pod "downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786971ms
May 11 03:27:33.254: INFO: Pod "downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006338119s
STEP: Saw pod success
May 11 03:27:33.254: INFO: Pod "downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:27:33.256: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9 container client-container: <nil>
STEP: delete the pod
May 11 03:27:33.273: INFO: Waiting for pod downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:27:33.275: INFO: Pod downwardapi-volume-b5b9379f-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:27:33.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-937" for this suite.
May 11 03:27:39.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:27:39.347: INFO: namespace downward-api-937 deletion completed in 6.070071876s

• [SLOW TEST:8.135 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:27:39.348: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 11 03:27:39.378: INFO: Waiting up to 5m0s for pod "pod-ba922d61-739c-11e9-80a1-becd2a02efc9" in namespace "emptydir-1100" to be "success or failure"
May 11 03:27:39.380: INFO: Pod "pod-ba922d61-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274345ms
May 11 03:27:41.384: INFO: Pod "pod-ba922d61-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005870721s
STEP: Saw pod success
May 11 03:27:41.384: INFO: Pod "pod-ba922d61-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:27:41.387: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-ba922d61-739c-11e9-80a1-becd2a02efc9 container test-container: <nil>
STEP: delete the pod
May 11 03:27:41.399: INFO: Waiting for pod pod-ba922d61-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:27:41.400: INFO: Pod pod-ba922d61-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:27:41.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1100" for this suite.
May 11 03:27:47.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:27:47.471: INFO: namespace emptydir-1100 deletion completed in 6.06692516s

• [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:27:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 11 03:27:47.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 create -f - --namespace=kubectl-8197'
May 11 03:27:47.727: INFO: stderr: ""
May 11 03:27:47.727: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 03:27:47.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8197'
May 11 03:27:47.823: INFO: stderr: ""
May 11 03:27:47.823: INFO: stdout: "update-demo-nautilus-6h9qp update-demo-nautilus-pqbf6 "
May 11 03:27:47.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-6h9qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:27:47.923: INFO: stderr: ""
May 11 03:27:47.923: INFO: stdout: ""
May 11 03:27:47.923: INFO: update-demo-nautilus-6h9qp is created but not running
May 11 03:27:52.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8197'
May 11 03:27:53.030: INFO: stderr: ""
May 11 03:27:53.030: INFO: stdout: "update-demo-nautilus-6h9qp update-demo-nautilus-pqbf6 "
May 11 03:27:53.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-6h9qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:27:53.122: INFO: stderr: ""
May 11 03:27:53.122: INFO: stdout: "true"
May 11 03:27:53.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-6h9qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:27:53.215: INFO: stderr: ""
May 11 03:27:53.215: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 03:27:53.215: INFO: validating pod update-demo-nautilus-6h9qp
May 11 03:27:53.219: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 03:27:53.219: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 03:27:53.219: INFO: update-demo-nautilus-6h9qp is verified up and running
May 11 03:27:53.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-pqbf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:27:53.328: INFO: stderr: ""
May 11 03:27:53.328: INFO: stdout: "true"
May 11 03:27:53.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-nautilus-pqbf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:27:53.437: INFO: stderr: ""
May 11 03:27:53.437: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 11 03:27:53.437: INFO: validating pod update-demo-nautilus-pqbf6
May 11 03:27:53.441: INFO: got data: {
  "image": "nautilus.jpg"
}

May 11 03:27:53.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 11 03:27:53.441: INFO: update-demo-nautilus-pqbf6 is verified up and running
STEP: rolling-update to new replication controller
May 11 03:27:53.443: INFO: scanned /root for discovery docs: <nil>
May 11 03:27:53.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8197'
May 11 03:28:15.866: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 11 03:28:15.866: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 11 03:28:15.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8197'
May 11 03:28:15.968: INFO: stderr: ""
May 11 03:28:15.968: INFO: stdout: "update-demo-kitten-2jmks update-demo-kitten-jb4d2 "
May 11 03:28:15.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-kitten-2jmks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:28:16.085: INFO: stderr: ""
May 11 03:28:16.085: INFO: stdout: "true"
May 11 03:28:16.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-kitten-2jmks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:28:16.196: INFO: stderr: ""
May 11 03:28:16.196: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 11 03:28:16.196: INFO: validating pod update-demo-kitten-2jmks
May 11 03:28:16.200: INFO: got data: {
  "image": "kitten.jpg"
}

May 11 03:28:16.200: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 11 03:28:16.200: INFO: update-demo-kitten-2jmks is verified up and running
May 11 03:28:16.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-kitten-jb4d2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:28:16.306: INFO: stderr: ""
May 11 03:28:16.307: INFO: stdout: "true"
May 11 03:28:16.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966612213 get pods update-demo-kitten-jb4d2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8197'
May 11 03:28:16.426: INFO: stderr: ""
May 11 03:28:16.426: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 11 03:28:16.426: INFO: validating pod update-demo-kitten-jb4d2
May 11 03:28:16.429: INFO: got data: {
  "image": "kitten.jpg"
}

May 11 03:28:16.429: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 11 03:28:16.429: INFO: update-demo-kitten-jb4d2 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:28:16.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8197" for this suite.
May 11 03:28:38.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:28:38.502: INFO: namespace kubectl-8197 deletion completed in 22.06799675s

• [SLOW TEST:51.031 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:28:38.502: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 11 03:28:38.530: INFO: Waiting up to 5m0s for pod "downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9" in namespace "downward-api-8626" to be "success or failure"
May 11 03:28:38.533: INFO: Pod "downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.043099ms
May 11 03:28:40.536: INFO: Pod "downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006814019s
STEP: Saw pod success
May 11 03:28:40.536: INFO: Pod "downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:28:40.538: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-2 pod downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9 container dapi-container: <nil>
STEP: delete the pod
May 11 03:28:40.551: INFO: Waiting for pod downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9 to disappear
May 11 03:28:40.553: INFO: Pod downward-api-ddd3cc9e-739c-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:28:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8626" for this suite.
May 11 03:28:46.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:28:46.632: INFO: namespace downward-api-8626 deletion completed in 6.076387395s

• [SLOW TEST:8.130 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:28:46.632: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:28:46.656: INFO: Creating ReplicaSet my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9
May 11 03:28:46.660: INFO: Pod name my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9: Found 0 pods out of 1
May 11 03:28:51.663: INFO: Pod name my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9: Found 1 pods out of 1
May 11 03:28:51.663: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9" is running
May 11 03:28:51.665: INFO: Pod "my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9-wxx2n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 03:28:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 03:28:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 03:28:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-11 03:28:47 +0000 UTC Reason: Message:}])
May 11 03:28:51.665: INFO: Trying to dial the pod
May 11 03:28:56.674: INFO: Controller my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9: Got expected result from replica 1 [my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9-wxx2n]: "my-hostname-basic-e2ac8e4f-739c-11e9-80a1-becd2a02efc9-wxx2n", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:28:56.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1235" for this suite.
May 11 03:29:02.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:29:02.751: INFO: namespace replicaset-1235 deletion completed in 6.075029932s

• [SLOW TEST:16.119 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:29:02.752: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5484
May 11 03:29:04.791: INFO: Started pod liveness-http in namespace container-probe-5484
STEP: checking the pod's current state and verifying that restartCount is present
May 11 03:29:04.793: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:33:05.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5484" for this suite.
May 11 03:33:11.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:33:11.350: INFO: namespace container-probe-5484 deletion completed in 6.07817166s

• [SLOW TEST:248.599 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:33:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:33:11.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-486" for this suite.
May 11 03:33:33.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:33:33.476: INFO: namespace pods-486 deletion completed in 22.082909422s

• [SLOW TEST:22.124 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:33:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 11 03:33:37.530: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 03:33:37.531: INFO: Pod pod-with-prestop-http-hook still exists
May 11 03:33:39.532: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 03:33:39.536: INFO: Pod pod-with-prestop-http-hook still exists
May 11 03:33:41.532: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 11 03:33:41.535: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:33:41.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7330" for this suite.
May 11 03:34:03.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:34:03.629: INFO: namespace container-lifecycle-hook-7330 deletion completed in 22.080720066s

• [SLOW TEST:30.153 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:34:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5163
May 11 03:34:05.670: INFO: Started pod liveness-exec in namespace container-probe-5163
STEP: checking the pod's current state and verifying that restartCount is present
May 11 03:34:05.672: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:38:06.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5163" for this suite.
May 11 03:38:12.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:38:12.270: INFO: namespace container-probe-5163 deletion completed in 6.076605293s

• [SLOW TEST:248.640 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:38:12.270: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6293/configmap-test-33d21575-739e-11e9-80a1-becd2a02efc9
STEP: Creating a pod to test consume configMaps
May 11 03:38:12.300: INFO: Waiting up to 5m0s for pod "pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9" in namespace "configmap-6293" to be "success or failure"
May 11 03:38:12.303: INFO: Pod "pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358109ms
May 11 03:38:14.309: INFO: Pod "pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00867698s
STEP: Saw pod success
May 11 03:38:14.309: INFO: Pod "pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9" satisfied condition "success or failure"
May 11 03:38:14.312: INFO: Trying to get logs from node craig-k8s-certification-2-sprout-hero-1 pod pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9 container env-test: <nil>
STEP: delete the pod
May 11 03:38:14.328: INFO: Waiting for pod pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9 to disappear
May 11 03:38:14.331: INFO: Pod pod-configmaps-33d26f9d-739e-11e9-80a1-becd2a02efc9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:38:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6293" for this suite.
May 11 03:38:20.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:38:20.423: INFO: namespace configmap-6293 deletion completed in 6.088831703s

• [SLOW TEST:8.153 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 11 03:38:20.423: INFO: >>> kubeConfig: /tmp/kubeconfig-966612213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 11 03:38:20.467: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"38f6dece-739e-11e9-b14d-000c2942e394", Controller:(*bool)(0xc000b1ccca), BlockOwnerDeletion:(*bool)(0xc000b1cccb)}}
May 11 03:38:20.471: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"38f5dc13-739e-11e9-b14d-000c2942e394", Controller:(*bool)(0xc003427d2a), BlockOwnerDeletion:(*bool)(0xc003427d2b)}}
May 11 03:38:20.475: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"38f65092-739e-11e9-b14d-000c2942e394", Controller:(*bool)(0xc003427ee6), BlockOwnerDeletion:(*bool)(0xc003427ee7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 11 03:38:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7922" for this suite.
May 11 03:38:31.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 11 03:38:31.557: INFO: namespace gc-7922 deletion completed in 6.070225081s

• [SLOW TEST:11.133 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSMay 11 03:38:31.557: INFO: Running AfterSuite actions on all nodes
May 11 03:38:31.557: INFO: Running AfterSuite actions on node 1
May 11 03:38:31.557: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5258.117 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h27m39.349325641s
Test Suite Passed
